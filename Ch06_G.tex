\documentclass[leqno]{book}
\usepackage[small,nohug,heads=vee]{diagrams}
\diagramstyle[labelstyle=\scriptstyle]
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[pdftex]{graphicx}
\usepackage{mathrsfs}
\usepackage{mathabx}
\usepackage{enumitem}
\usepackage{multicol}
%\usepackage[utf8]{inputenc}

\makeatletter
\newcommand*\bcd{\mathpalette\bcd@{.5}}
\newcommand*\bcd@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\begin{document} % General notation ambition: \boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta for the Frenet trihedron; \kappa for curvature and \tau for torsion.  FFF coefficients are E,F,G for surfaces; F_{ij} for arbitrary dimensions.  SFF coefficients are L,M,N for surfaces (the Do Carmo uses lowercase e,f,g but we frown upon that for various reasons), and L_{ij} for arbitrary dimensions.  The Gauss map is denoted \mathbf N (the boldface distinguishes it from the SFF coefficient).  And, as usual, \Gamma_{ij}^k denotes the Christoffel symbols.

\chapter{Introduction to Differential Geometry} %\emph{The material and exposition for this chapter follows Do Carmo, M.P., \emph{Differential Geometry of Curves \& Surfaces}, Second Edition, Chs. 1-4.}\\

\noindent % Opportunity: bring up some of the material from Fall 2019's graduate course!  It'll help generalize spherical, Euclidean, hyperbolic spaces in all dimensions.
In the previous chapters of this book, we have studied various types of ``homogeneous'' geometry: Euclidean, projective, hyperbolic and spherical.  We have defined them purely algebraically, which made lines and circles, etc.~straightforward and easy to define; but this has its unsatisfying limitations.  It is difficult to generalizing the space.  The notion of the length of a curve segment is undefined, unless the curve is from a line.  Third, the area of a region is undefined (in the three kinds of geometry, we gave our own definitions of triangle area, but they are impractical since they do not generalize to arbitrary regions).

In this final chapter, we will study geometric spaces using analysis (calculus), which will enable us to make these generalizations.  We will start by considering regular curves and surfaces in $\mathbb R^3$, and learn how to deal with them by parametrizing them and using a few pieces of equipment (no reference to the ambient 3-space).  Then we will go into further detail about regular surfaces, and establish the ``straight lines'' on them (which are technically called \textbf{geodesics}).  The Gaussian curvature of a surface will also be covered in Proposition 6.24, and in Corollary 6.29 we will relate this to the sum of the angle measures of a triangle (compares Propositions 2.11, 4.8 and 5.4). % I wouldn't say it's analysis.  Analysis means you're really going rigorous about continuity and (e.g.) proving that certain things are equal by showing they're within a distance of an arbitrary close number.  This chapter uses calculus significantly without going into that rigor.

Finally, Section 6.10 will cover how to customize a surface (presumably an open set of $\mathbb R^2$) with any metric which need not be the usual Euclidean one.  This will provide new ways of viewing the Poincar\'e disk, half-plane and Beltrami-Klein models of the hyperbolic plane, as well as the stereographic projection and Lambert azimuthal equal-area projection models of the spherical plane.

Due to the vast generality of the concepts, it is fairly difficult for beginners to study just the two-dimensional case, and plainly more convoluted if we generalize to higher dimensions at once.  A few concepts barely even generalize without extra assumptions being imposed.  Thus, each section of this chapter (except the first) sticks to the two-dimensional case, but has guided exercises which transfer the material to higher dimensions.

\emph{Throughout this chapter, ``differentiable'' and ``smooth'' shall mean infinitely differentiable, also known as $\mathcal C^\infty$.}% "We will specify if it means anything else."

\subsection*{6.1. Regular Curves.  Frenet Trihedron}
\addcontentsline{toc}{section}{6.1. Regular Curves.  Frenet Trihedron}
To get a good general feel of how we will do things, it is useful to start with regular curves.  Not only are they a warmup, but we will use them when we deal with surfaces.

Think of a particle moving in space along a smooth path which we have chosen.  It may twist around, or it may stay in one plane.  It may go in a straight line, but because it is differentiable, it cannot have corners.  It may have either limited or unlimited length.

To make this notion rigorous, we let $I$ be an open interval of $\mathbb R$, which could be any of the following things:
$$(a,b)\text{ with }a<b;~~~~(a,\infty);~~~~(-\infty,b);~~~~(-\infty,\infty)=\mathbb R.$$
Then a \textbf{curve} in $\mathbb R^n$ is defined as a differentiable map $\alpha:I\to\mathbb R^n$.  The parameter in $I$ is commonly referred to as $t$ for ``time.''  \\

\noindent\textbf{Examples.}

(1) Basic examples are the curves that have generally been dealt with in Chapters 2-5; i.e., lines, circles and conics.  For example, $\alpha(t)=(2t,t+1)$ parametrizes the line $x-2y=-2$, as shown on the left; and $\alpha(t)=(3\cos t,2\sin t)$ parametrizes the ellipse $\frac{x^2}9+\frac{y^2}4=1$, as shown on the right.
\begin{center}
\includegraphics[scale=.3]{ParamLine.png}~~~~
\includegraphics[scale=.3]{ParamEllipse.png}
\end{center}
This can be seen by plugging in the formulas for $x$ and $y$ in terms of $t$: if $(x,y)=(2t,t+1)$ then $x-2y=2t-2(t+1)=2t-(2t+2)=-2$, and if $(x,y)=(3\cos t,2\sin t)$ then $\frac{x^2}9+\frac{y^2}4=\frac{(3\cos t)^2}9+\frac{(2\sin t)^2}4=\frac{9\cos^2t}9+\frac{4\sin^2t}4=\cos^2t+\sin^2t=1$.

We know $\alpha(t)=(\cos t,\sin t)$ parametrizes the unit circle, and $\alpha(t)=(\cosh t,\sinh t)$ parametrizes a branch of a hyperbola.  [The entire hyperbola can be parametrized via $\alpha(t)=(\sec t,\tan t)$ with $t\notin\left\{\frac{\pi}2+n\pi:n\in\mathbb Z\right\}$, but we shall not deal with curves whose domains are disconnected.]\\

(2) Sometimes we will take only a certain interval for the domain instead of all of $\mathbb R$.  For instance, the line segment from $(-2,0)$ to $(4,3)$ can be parametrized as $\alpha(t)=(2t,t+1)$, for $t\in[-1,2]$.  Also, if you parametrize $\alpha(t)=(\cos t,\sin t)$ only for $t\in[0,\pi]$, you get the semicircle arc in the upper half of the plane.

You may have noticed that we asked for the domain $I$ to be an \emph{open} interval, but we're using closed intervals in this example.  The fact of the matter is that the closed intervals make it easier to connect to earlier in the book (because, for instance, line segments in Section 2.1 were considered to include their endpoints).  However, in differential geometry, we want to be able to differentiate things at any point, and this can only be done if the function is defined on an open neighborhood of the point.\\

(3) Here is an example of a curve which may not seem familiar to you.  The cycloid is given by $\alpha(t)=(t-\sin t,1-\cos t)$:
\begin{center}
\includegraphics[scale=.3]{Cycloid1.png}
\end{center}
It is the locus of a point on a circle of radius $1$ rolling on the $x$-axis without slipping:
\begin{center}
\includegraphics[scale=.3]{Cycloid1_with_circle.png}
\end{center}
You can get different curves by taking the locus of a point \emph{inside} or \emph{outside} the circle as well.  Examples of these are $\left(t-\frac 12\sin t,1-\frac 12\cos t\right)$ and $\left(t-\frac 32\sin t,1-\frac 32\cos t\right)$, which look like:
\begin{center}
\includegraphics[scale=.3]{Cycloid2.png}~~~~\includegraphics[scale=.3]{Cycloid3.png}
\end{center}
Notice how the one on the right has loops, and the one on the left is simple.  It is an interesting exercise to try to figure out why this is so exactly.\\

(4) Curves exist in higher dimensions as well.  For example, if $a,b>0$, there is the curve $\alpha:\mathbb R\to\mathbb R^3$ given by $\alpha(t)=(a\cos t,a\sin t,bt)$.  This is called the \textbf{helix}.  There is also the \textbf{moment curve} $\alpha(t)=(t,t^2,t^3)$.\\

\noindent It is tempting to think that a curve in $\mathbb R^n$ must be a smooth path without any corners along the way.  However, without further assumptions, this may not be the case.  Consider, for instance, the curve $\alpha:\mathbb R\to\mathbb R^2$ given by $\alpha(t)=(t^3,t^2)$; this is what it looks like:
\begin{center}
\includegraphics[scale=.3]{SingularPoint.png}
\end{center}
This curve is not smooth at the origin; it has a visible corner.  The cycloid from Example 3 also has this issue, (unlike the other two curves in the example).  This is a curious phenomenon to the reader who notices that all of the coordinates are still differentiable, so what is causing it?

Let us take a look at the derivative $\alpha'(t)$ at each of the ``corner'' times.  For the curve $\alpha(t)=(t^3,t^2)$, we have $\alpha'(t)=(3t^2,2t)$ so that $\alpha'(0)=(0,0)$.  Similarly, for $\alpha(t)=(t-\sin t,1-\cos t)$, the corners occur when $t=2\pi n$ for some $n\in\mathbb Z$, and $\alpha'(t)=(1-\cos t,\sin t)$ which is zero if and only if $t=2\pi n$.

This suggests that sharp corners can only occur at points where $\alpha'(t)=\vec 0$.  This is indeed true: for if $\alpha'(t)\ne\vec 0$, then it is a tangent vector to the curve at $\alpha(t)$, and no parts of the curve can go in any other directions.  If a part of a curve went in another direction, its starting velocity would be a vector in that direction, and this is a contradiction if the velocity is nonzero because then it is contained in only one line.  Thus, it seems reasonable to demand that the derivative $\alpha'(t)$ never be zero.\footnote{If $\alpha'(t)\ne\vec 0$ then the curve is nonsingular (i.e., traveling in one pair of opposite directions) at $t$, but not conversely: the curve $\alpha:\mathbb R\to\mathbb R^2$ given by $\alpha(t)=(t^3,t^3)$ has a zero derivative at $t=0$, yet its image is the line $y=x$, which is nonsingular.}  We would like to stick with curves satisfying this particular property, so we can avoid the anomaly of a corner or cusp.  Thus we give these curves a definition.\\

\noindent\textbf{Definition.} \emph{If $I$ is an open interval in $\mathbb R$, a \textbf{(parametrized) regular curve} is defined as a differentiable map $\alpha:I\to\mathbb R^n$, such that $\alpha'(t)\ne\vec 0$ for all $t\in I$.  $t$ is called the \textbf{parameter}, and the points $\alpha(t)$ are called the \textbf{values} of the curve.}\\

\noindent Thus a regular curve is a smooth path, even though it may have self-intersections.  Its \textbf{velocity} is the vector $\alpha'(t)$ (which we always assume nonzero), and its \textbf{speed} is the magnitude $\|\alpha'(t)\|$ of the velocity.  The \textbf{acceleration} of $\alpha$ is its second derivative, $\alpha''(t)$, which may be zero, unlike the velocity [e.g., take $\alpha(t)=(2t,t+1)$ from Example 2].

These terms may be familiar from an elementary introduction to physics.  However, we are more interested in the curve itself than the time parameter used to form it.  Thus, our ambition in the rest of this section is to study the intrinsics of the curve, which have nothing to do with $t$.

The first fundamental step is to find a canonical way to parametrize curves which does not rely on any particular time parameter.  To do this, we fix $t_0\in I$, and define
$$\varphi(t)=\int_{t_0}^t\|\alpha'(u)\|\,du.$$
Then $\varphi$ is a diffeomorphism from $I$ to some interval.  Indeed, $\varphi(t_0)=0$ and $\varphi'(t)=\|\alpha'(t)\|$ (by the first fundamental theorem of calculus), which is strictly positive (since $\alpha$ is regular, and hence $\alpha'(u)\ne\vec 0$).  It follows that, $\varphi$ is an increasing function from $I$ to some interval $J\subset\mathbb R$, and we may let $\psi:J\to I$ be its inverse, which is also a differentiable increasing function (why?).  We set $\alpha_1=\alpha\circ\psi:J\to\mathbb R^n$, which is an alternate way to parametrize the same curve.

Now, what can we say about $\alpha_1'(s)$ for $s\in J$?  Well, first, $\varphi(\psi(s))=s$ for all $s$; differentiating both sides and using the Chain Rule gives $\varphi'(\psi(s))\psi'(s)=1$, and so $\psi'(s)=\frac 1{\varphi'(\psi(s))}=\frac 1{\|\alpha'(\psi(s))\|}$.  Furthermore, again by the Chain Rule,
$$\alpha_1'(s)=\alpha'(\psi(s))\psi'(s)=\frac{\alpha'(\psi(s))}{\|\alpha'(\psi(s))\|},$$
which is a \emph{unit vector} \---- in fact, it is exactly the normalization of the vector $\alpha'(\psi(s))$, which is the velocity of $\alpha$ on the same point of the curve.  Hence, $\|\alpha_1'(s)\|=1$ for all $s\in J$.

We have just shown that every regular curve can be parametrized so that the speed is equal to $1$ everywhere.  As we will see later, this is actually a convenient assumption on the parametrization, as it both simplifies the study of curve intrinsics, and makes computations less complicated.  These kinds of parametrizations have a special name.\\

\noindent\textbf{Definition.} \emph{A regular curve $\alpha:I\to\mathbb R^n$ is said to be \textbf{parametrized by arc length} (or to have \textbf{unit speed}) provided that $\|\alpha'(s)\|=1$ for all $s\in I$.  In this case, the parameter in $I$ is usually denoted as $s$ instead of $t$.}\\

\noindent The name ``arc length'' is explained in Exercise 3, which shows that $\varphi(t)$ is actually the length of the curve from $\alpha(t_0)$ to $\alpha(t)$.

Using the preceding argument, it is easy to get examples of arc length parametrizations of curves.  An example is the unit circle $\alpha(s)=(\cos s,\sin s)$, because $\|\alpha'(s)\|=\|(-\sin s,\cos s)\|=\sin^2s+\cos^2s=1$.  For general $r>0$, the circle of radius $r$ can be parametrized via $\alpha(t)=(r\cos t,r\sin t)$, but this is not an arc length parametrization unless $r=1$, because $\|\alpha'(t)\|=r$.  However, the circle of radius $r$ does have another parametrization by arc length:
$$\alpha(s)=(r\cos(s/r),r\sin(s/r)).$$
This is an arc length parametrization because $\frac d{ds}\left(r\sin\frac sr\right)=r\frac 1r\cos\frac sr=\cos\frac sr$, and $\frac d{ds}\left(r\cos\frac sr\right)$ can be found similarly; hence $\alpha'(s)=(-\sin(s/r),\cos(s/r))$, a unit vector.  Note that $\alpha$ is periodic with period $2\pi r$, the circumference of the circle.

We first claim that arc length parametrizations have an essential uniqueness property.  For the purpose of this, we define a \textbf{reparametrization} of $\alpha$ as a composition $\alpha\circ\psi$ where $\psi:J\to I$ is a differentiable bijection with strictly positive derivative.\\

\noindent\textbf{Proposition 6.1.} \emph{Every regular curve $\alpha:I\to\mathbb R^n$ has a reparametrization by arc length.  If, moreover, $\alpha_1:J_1\to\mathbb R^n$ and $\alpha_2:J_2\to\mathbb R^n$ are two such reparametrizations, there exists $c\in\mathbb R$ such that $J_2=\{r+c:r\in J_1\}$ and $\alpha_1(s)=\alpha_2(s+c)$ for all $r\in J_1$.}
\begin{proof}
We have already seen an example of such a reparametrization: fix $t_0\in I$ and let $\varphi(t)=\int_{t_0}^t\|\alpha'(u)\|\,du$; then $\alpha\circ\varphi^{-1}$ is a reparametrization by arc length.  Now suppose $\alpha_1:J_1\to\mathbb R^n$ and $\alpha_2:J_2\to\mathbb R^n$ are two reparametrizations of $\alpha$, say $\alpha_i=\alpha\circ\psi_i$ for $j=1,2$.  Then since the $\psi_i:J_i\to I$ are bijections with strictly positive derivative, so is
$$\zeta=\psi_2^{-1}\circ\psi_1:J_1\to J_2.$$
By construction, $\psi_2\circ\zeta=\psi_1$, and hence $\alpha_2\circ\zeta=\alpha_1$.  Moreover, differentiating $\alpha_2(\zeta(s))=\alpha_1(s)$ for $s\in J_1$ entails
$$\alpha_1'(s)=\alpha_2'(\zeta(s))\zeta'(s);$$
since $\|\alpha_1'(s)\|=\|\alpha_2'(\zeta(s))\|=1$ [$\alpha_1,\alpha_2$ are arc length parametrizations], taking the magnitude of both sides entails $|\zeta'(s)|=1$.  Therefore, $\zeta'(s)=1$ for all $s\in J_1$ (because it is strictly positive).  Elementary integration then shows that there exists $c\in\mathbb R$ such that $\zeta$ is the map $s\mapsto s+c$.  Since $\zeta$ is a bijection $J_1\to J_2$, it follows that $J_2=\{r+c:r\in J_1\}$, and $\alpha_1(s)=\alpha_2(s+c)$ by construction.
\end{proof}

\noindent Despite what has just been proven, there are parametrizations of the curve which are not technically reparametrizations; they traverse the curve in the opposite direction.  Specifically, if $\alpha:I\to\mathbb R^n$, one can define $I_-=\{a\in\mathbb R:-a\in I\}$ and then define $\alpha_-:I_-\to\mathbb R^n$ via $\alpha_-(t)=\alpha(-t)$.  By elementary calculus, $\alpha_-'(t)=-\alpha'(-t)$, and so $\|\alpha_-'(t)\|=\|\alpha'(t)\|$; hence $\alpha_-$ is an arc length parametrization if and only if $\alpha$ is.

If $\sigma:I_-\to I$ is the map $t\mapsto -t$, then $\sigma$ is differentiable with a strictly \emph{negative} derivative, and $\alpha_-=\alpha\circ\sigma$.  Thus, $\alpha_-$ is parametrizes the same curve, but as if it were going the other way.  The direction in which a curve travels (technically called its \emph{orientation}) will be needed for most of the results in the rest of this section to be rigorous, hence we will only admit ``positive'' reparametrizations which keep the curve going in the same direction.\\

\noindent\textbf{CURVATURE AND TORSION OF A CURVE}\\

\noindent For the rest of this section, we shall focus on regular curves in $\mathbb R^3$, where they can be visualized easily.  We let $\alpha:J\to\mathbb R^3$ be a regular curve parametrized by arc length, and construct an orthonormal basis of $\mathbb R^3$ which varies differentiably with the parameter $s\in J$.  We will then use it to define the curvature, which indicates \emph{curviness} of the curve, or the strength at which it curves (see Exercises 9-12 and 16-17 below).  We will also use it to define the torsion, which indicates \emph{twisting} of the curve, or how far it is from being a plane curve (see Exercises 8 and 17 below).

We start by taking $\boldsymbol\tau(s)=\alpha'(s)$.  This is a unit vector, since $\alpha$ is an arc length parametrization.  Hence $\boldsymbol\tau(s)$ is a perfectly good first vector for our orthonormal basis.  For the second vector, we note that by Exercise 4(b), $\boldsymbol\tau(s)\cdot\boldsymbol\tau'(s)=0$.  Thus we may define $\boldsymbol\nu(s)=\frac{\boldsymbol\tau'(s)}{\|\boldsymbol\tau'(s)\|}$ (unless $\boldsymbol\tau'(s)=\vec 0$, which we will assume is not the case).  Given this data, $\boldsymbol\tau,\boldsymbol\nu$ are orthonormal.  Finally, we take the third vector, $\boldsymbol\beta(s)$ to be the cross product, $\boldsymbol\tau(s)\times\boldsymbol\nu(s)$.  Then $\{\boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta\}$ is a positive orthonormal basis (which varies with $s$), by Exercise 5.  This is called the \textbf{Frenet trihedron} (or \textbf{Frenet frame}) of the space curve; they are to be visualized at $\alpha(s)$ as $s$ varies.

$\boldsymbol\tau(s)$ is called the \textbf{tangential unit vector} to the curve at $\alpha(s)$ (for clear reasons), $\boldsymbol\nu(s)$ is called the \textbf{normal unit vector} and $\boldsymbol\beta(s)$ is called the \textbf{binormal vector}.  We shall study how they vary with respect to arc length, as that will effectively tell us some of the structures of the curve.

The first important observation comes from the orthonormality of the three vectors:
\begin{equation}\tag{1}\boldsymbol\tau\cdot\boldsymbol\tau=1,~~~~\boldsymbol\nu\cdot\boldsymbol\nu=1,~~~~\boldsymbol\beta\cdot\boldsymbol\beta=1,\end{equation}
$$\boldsymbol\tau\cdot\boldsymbol\nu=0,~~~~\boldsymbol\tau\cdot\boldsymbol\beta=0,~~~~\boldsymbol\nu\cdot\boldsymbol\beta=0.$$
Differentiating each equation in (1) with respect to $s$, and using Exercise 4, gives
\begin{equation}\tag{2}\boldsymbol\tau\cdot\boldsymbol\tau'=0,~~~~\boldsymbol\nu\cdot\boldsymbol\nu'=0,~~~~\boldsymbol\beta\cdot\boldsymbol\beta'=0,\end{equation}
$$\boldsymbol\tau'\cdot\boldsymbol\nu+\boldsymbol\tau\cdot\boldsymbol\nu'=0,~~~~\boldsymbol\tau'\cdot\boldsymbol\beta+\boldsymbol\tau\cdot\boldsymbol\beta'=0,~~~~\boldsymbol\nu'\cdot\boldsymbol\beta+\boldsymbol\nu\cdot\boldsymbol\beta'=0.$$
We define $\kappa:J\to\mathbb R$ by $\kappa(s)=\|\boldsymbol\tau'(s)\|=\|\alpha''(s)\|$; then $\boldsymbol\tau'(s)=\kappa(s)\boldsymbol\nu(s)$ by how we defined $\boldsymbol\nu(s)$ earlier.  $\kappa(s)$ is a positive differentiable function (wherever $\boldsymbol\tau'(s)\ne\vec 0$), called the \textbf{curvature} of the curve.  From this we conclude that $\boldsymbol\tau'\cdot\boldsymbol\beta=\kappa(\boldsymbol\nu\cdot\boldsymbol\beta)=0$, and hence $\boldsymbol\tau\cdot\boldsymbol\beta'=0$ by (2).

As $\boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta$ is an orthonormal basis, we have, for any $\mathbf x\in\mathbb R^3$,
\begin{equation}\tag{3}\mathbf x=(\mathbf x\cdot\boldsymbol\tau)\boldsymbol\tau+(\mathbf x\cdot\boldsymbol\nu)\boldsymbol\nu+(\mathbf x\cdot\boldsymbol\beta)\boldsymbol\beta,\end{equation} % Re. the difficulty of distinguishing \tau from \boldsymbol\tau, the reader should be able to infer whether each \tau is a vector or a scalar (e.g., we can't have vector\cdot scalar).  Also, I didn't find such a symbol in https://detexify.kirelabs.org/classify.html
because you can write $\mathbf x$ as a linear combination of the basis vectors, and then taking dot products of both sides instantly reveal what the coefficients are.  In particular, taking $\mathbf x=\boldsymbol\beta'$, we have $\boldsymbol\beta'=(\boldsymbol\beta'\cdot\boldsymbol\tau)\boldsymbol\tau+(\boldsymbol\beta'\cdot\boldsymbol\nu)\boldsymbol\nu+(\boldsymbol\beta'\cdot\boldsymbol\beta)\boldsymbol\beta=(\boldsymbol\beta'\cdot\boldsymbol\nu)\boldsymbol\nu$, since $\boldsymbol\beta'\cdot\boldsymbol\tau=\boldsymbol\beta'\cdot\boldsymbol\beta=0$.  We define $\tau(s)=-\boldsymbol\beta'(s)\cdot\boldsymbol\nu(s)$, from which we have $\boldsymbol\beta'(s)=-\tau(s)\boldsymbol\nu(s)$.  $\tau(s)$ is called the \textbf{torsion} of the curve.

The equations (2) imply $\boldsymbol\nu'\cdot\boldsymbol\beta=-\boldsymbol\nu\cdot\boldsymbol\beta'=\tau(s)$, and $\boldsymbol\nu'\cdot\boldsymbol\tau=-\boldsymbol\nu\cdot\boldsymbol\tau'=-\boldsymbol\nu\cdot(\kappa\boldsymbol\nu)=-\kappa$.  Taking $\mathbf x=\boldsymbol\nu'$ in (3) then yields $\boldsymbol\nu'(s)=-\kappa(s)\boldsymbol\tau(s)+\tau(s)\boldsymbol\beta(s)$.

Thus we have expressed the derivative of each basis vector with respect to the basis itself:
$$\boldsymbol\tau'(s)=\kappa(s)\boldsymbol\nu(s)$$
$$\boldsymbol\nu'(s)=-\kappa(s)\boldsymbol\tau(s)+\tau(s)\boldsymbol\beta(s)$$
$$\boldsymbol\beta'(s)=-\tau(s)\boldsymbol\nu(s)$$
These are called the \textbf{Frenet equations}, and they can be rephrased in this matrix form:
$$\begin{bmatrix}\leftarrow\boldsymbol\tau'\rightarrow\\\leftarrow\boldsymbol\nu'\rightarrow\\\leftarrow\boldsymbol\beta'\rightarrow\end{bmatrix}=\begin{bmatrix}0&\kappa&0\\-\kappa&0&\tau\\0&-\tau&0\end{bmatrix}\begin{bmatrix}\leftarrow\boldsymbol\tau\rightarrow\\\leftarrow\boldsymbol\nu\rightarrow\\\leftarrow\boldsymbol\beta\rightarrow\end{bmatrix}$$
Here $\boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta$ and their derivatives are viewed as row vectors, which is what the arrows $\leftarrow$ and $\rightarrow$ mean.

Using these relations, we can show that given any differentiable functions $\kappa$ and $\tau$ on the arc length parameter, there is a curve with curvature $\kappa$ and torsion $\tau$, which is unique up to rigid motions.  However, this fact depends on the fundamental theorem of differential equations, which is beyond the scope of this book.  Since this fundamental theorem will spark significant results several times in the chapter, we will state it here without proof.\\

\noindent\textbf{Theorem 6.2.} \textsc{(Fundamental Theorem of Differential Equations)} \emph{If $f_i(t,x_1,\dots,x_n),1\leqslant i\leqslant n$ are differentiable functions which are defined for $t$ in an open interval $J$, and $t_0$ is a fixed element of $J$, then the system of differential equations}
$$\frac{dx_1}{dt}=f_1(t,x_1,x_2,\dots,x_n)$$
$$\frac{dx_2}{dt}=f_2(t,x_1,x_2,\dots,x_n)$$
$$\vdots$$
$$\frac{dx_n}{dt}=f_n(t,x_1,x_2,\dots,x_n)$$
\begin{center}
\emph{with given initial conditions $x_i(t_0)=a_i$ ($a_i\in\mathbb R$)}
\end{center}
\emph{has a unique solution in some open neighborhood of $t_0$ in $J$.  If, moreover, the system is linear; i.e., we have}
$$f_i(t,x_1,x_2,\dots,x_n)=a_{i1}(t)x_1+\dots+a_{in}(t)x_n+a_i(t)$$
\emph{for some differentiable functions $a_{ij},a_i$, then the solution exists throughout $J$.}\\

\noindent For a proof, see A.L.~Cauchy and G.~Peano [Peal], \'E.~Picard [Pi] or E.~Lindel\"of [Lind1, Lind2]. %https://link.springer.com/chapter/10.1007/978-1-4612-1506-6_1

It is readily seen that elements of $\operatorname{Isom}^+(\mathbb R^3)$ keep $\kappa$ and $\tau$ unchanged, as one can verify (using, e.g., the linearity of the derivative) that $[A,\vec v]\in\operatorname{Isom}^+(\mathbb R^3)$ merely applies the underlying $A\in SO(3)$ to the Frenet trihedron.  However, if you apply an orientation-\emph{reversing} isometry of $\mathbb R^3$ to the curve, $\kappa$ stays the same and $\tau$ becomes negative.\\

\noindent\textbf{Proposition 6.3.} \emph{Let $J\subset\mathbb R$ be an open interval, and $\kappa,\tau:J\to\mathbb R$ differentiable functions with $\kappa(s)>0$ for all $s\in J$.  Then there exists a curve $\alpha:J\to\mathbb R^3$ parametrized by arc length, with curvature $\kappa$ and torsion $\tau$.  Any two such curves are related by an element of $\operatorname{Isom}^+(\mathbb R^3)$.}
\begin{proof}
Let us first write the Frenet trihedron (the one for the curve $\alpha$ that we wish to define) in the form of scalar functions:
$$\boldsymbol\tau(s)=(\tau_1(s),\tau_2(s),\tau_3(s))$$
$$\boldsymbol\nu(s)=(\nu_1(s),\nu_2(s),\nu_3(s))$$
$$\boldsymbol\beta(s)=(\beta_1(s),\beta_2(s),\beta_3(s))$$
Consider the Frenet equations
$$\begin{bmatrix}\leftarrow\boldsymbol\tau'\rightarrow\\\leftarrow\boldsymbol\nu'\rightarrow\\\leftarrow\boldsymbol\beta'\rightarrow\end{bmatrix}=\begin{bmatrix}0&\kappa&0\\-\kappa&0&\tau\\0&-\tau&0\end{bmatrix}\begin{bmatrix}\leftarrow\boldsymbol\tau\rightarrow\\\leftarrow\boldsymbol\nu\rightarrow\\\leftarrow\boldsymbol\beta\rightarrow\end{bmatrix}.$$
They can be rephrased as a system of differential equations in the nine scalar functions given above,
\begin{equation}\tag{A}\tau_1'(s)=\kappa(s)\nu_1(s),~~~~\tau_2'(s)=\kappa(s)\nu_2(s),~~~~\tau_3'(s)=\kappa(s)\nu_3(s),\end{equation}
$$\nu_1'(s)=-\kappa(s)\tau_1(s)+\tau(s)\beta_1(s),~~~~\nu_2'(s)=-\kappa(s)\tau_2(s)+\tau(s)\beta_2(s),$$
$$\nu_3'(s)=-\kappa(s)\tau_3(s)+\tau(s)\beta_3(s),$$
$$\beta_1'(s)=-\tau(s)\nu_1(s),~~~~\beta_2'(s)=-\tau(s)\nu_2(s),~~~~\beta_3'(s)=-\tau(s)\nu_3(s).$$
Let us fix $s_0\in J$ and impose initial conditions via $\boldsymbol\tau(s_0)=\vec e_1,\boldsymbol\nu(s_0)=\vec e_2,\boldsymbol\beta(s_0)=\vec e_3$.  In other words, $\tau_1(s_0)=\nu_2(s_0)=\beta_3(s_0)=1$ and the other six functions send $s_0\mapsto 0$.  By Theorem 6.2, this system has a unique solution, which is defined throughout $J$ since the system is linear.  Such a solution then becomes a trio $\boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta$ of vector-valued functions $J\to\mathbb R^3$, satisfying the Frenet equations and the initial conditions.

However, we do not yet know if $\boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta$ are orthonormal vectors; so far we only know they are orthonormal at $s=s_0$, where they are the standard basis vectors.  But we can easily use this fact.  Define functions $\delta_i:J\to\mathbb R$ via
$$\delta_1:s\mapsto\boldsymbol\tau(s)\cdot\boldsymbol\tau(s),~~~~\delta_2:s\mapsto\boldsymbol\nu(s)\cdot\boldsymbol\nu(s),~~~~\delta_3:s\mapsto\boldsymbol\beta(s)\cdot\boldsymbol\beta(s),$$
$$\delta_4:s\mapsto\boldsymbol\tau(s)\cdot\boldsymbol\nu(s),~~~~\delta_5:s\mapsto\boldsymbol\nu(s)\cdot\boldsymbol\beta(s),~~~~\delta_6:s\mapsto\boldsymbol\tau(s)\cdot\boldsymbol\beta(s).$$
Then using the Frenet equations, one can readily see that the $\delta_i$ are a solution to the following system of differential equations with initial conditions:
$$\frac{d\zeta_1}{ds}=2\kappa(s)\zeta_4(s),~~~~\frac{d\zeta_2}{ds}=2(-\kappa(s)\zeta_4(s)+\tau(s)\zeta_5(s)),~~~~\frac{d\zeta_3}{ds}=-2\tau(s)\zeta_5(s),$$
$$\frac{d\zeta_4}{ds}=\kappa(s)[\zeta_2(s)-\zeta_1(s)]+\tau(s)\zeta_6(s),~~~~\frac{d\zeta_5}{ds}=-\kappa(s)\zeta_6(s)+\tau(s)[\zeta_3(s)-\zeta_2(s)],$$
$$\frac{d\zeta_6}{ds}=\kappa(s)\zeta_5(s)-\tau(s)\zeta_4(s),$$
$$\zeta_1(s_0)=\zeta_2(s_0)=\zeta_3(s_0)=1,~~~~\zeta_4(s_0)=\zeta_5(s_0)=\zeta_6(s_0)=0.$$
But the constant functions $\zeta_1(s)=\zeta_2(s)=\zeta_3(s)=1,\zeta_4(s)=\zeta_5(s)=\zeta_6(s)=0$ clearly satisfy this system as well.  Therefore the uniqueness part of Theorem 6.2 entails that the $\delta_i$ are said constant functions, and so $\boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta$ are orthonormal throughout $J$.  They also form a \emph{positive} orthonormal basis, because $\boldsymbol\tau\cdot(\boldsymbol\nu\times\boldsymbol\beta)$ is a smooth function on $J$, taking only the values $\pm 1$, and sending $s_0\mapsto 1$; hence it must be $1$ everywhere.

Now define a curve $\alpha:J\to\mathbb R^3$ via
$$\alpha(s)=\int_{s_0}^s\boldsymbol\tau(u)\,du.$$
Then $\alpha'(s)=\boldsymbol\tau(s)$ by the first fundamental theorem of calculus, and hence $\alpha(s)$ is an arc length parametrization (because $\|\boldsymbol\tau(s)\|=1$) and $\boldsymbol\tau(s)$ is its tangential unit vector.  By the Frenet equation $\boldsymbol\tau'(s)=\kappa(s)\boldsymbol\nu(s)$ and the hypothesis $\kappa(s)>0$, we get that $\|\boldsymbol\tau'(s)\|=\kappa(s)$, and hence $\kappa(s)$ is the curvature of the curve.  Moreover, $\boldsymbol\nu(s)=\frac{\boldsymbol\tau'(s)}{\kappa(s)}=\frac{\boldsymbol\tau'(s)}{\|\boldsymbol\tau'(s)\|}$, hence is the normal unit vector.  Since $\{\boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta\}$ is a positive orthonormal basis, we have $\boldsymbol\beta(s)=\boldsymbol\tau(s)\times\boldsymbol\nu(s)$, hence is the binormal unit vector.  Finally, since $\boldsymbol\beta'(s)=-\tau(s)\boldsymbol\nu(s)$, we have $\tau(s)=-\boldsymbol\beta'(s)\cdot\boldsymbol\nu(s)$ by orthonormality, and so $\tau(s)$ is the torsion of the curve.

This proves the existence of the curve $\alpha$.  Now suppose $\alpha_1:J\to\mathbb R$ is another curve parametrized by arc length, with the same curvature $\kappa$ and torsion $\tau$.  Let $A$ be the matrix $\begin{bmatrix}\leftarrow\boldsymbol\tau(s_0)\rightarrow\\\leftarrow\boldsymbol\nu(s_0)\rightarrow\\\leftarrow\boldsymbol\beta(s_0)\rightarrow\end{bmatrix}$, and let $\vec v=-A(\alpha_1(s_0))$.  Then $A\in SO(3)$, and the isometry $[A,\vec v]\in\operatorname{Isom}^+(\mathbb R^3)$ passes the curve $\alpha_1$ to a curve $\alpha_2:J\to\mathbb R$, also with curvature $\kappa$ and torsion $\tau$, and with the same initial conditions $\boldsymbol\tau(s_0)=\vec e_1,\boldsymbol\nu(s_0)=\vec e_2,\boldsymbol\beta(s_0)=\vec e_3$.  Note also $\alpha(s_0)=\alpha_2(s_0)=0$.  We claim that $\alpha_2=\alpha$, and that will imply that $\alpha,\alpha_1$ are related by an element of $\operatorname{Isom}^+(\mathbb R^3)$.

The trick is to notice that the Frenet trihedra of $\alpha$ and $\alpha_2$ both satisfy the \emph{same} differential equations (A), with the same initial conditions at $s_0$ [because they have the same functions for curvature and torsion].  By the uniqueness of solutions to differential equations, the Frenet trihedra of $\alpha$ and $\alpha_2$ coincide.  Hence $\alpha$ and $\alpha_2$ have the same tangential unit vector, so that (since they have the same derivative with respect to $s$), we have $\alpha_2(s)=\alpha(s)+c$ for some fixed $c\in\mathbb R$.  Finally, taking $s=s_0$ we get $0=0+c$, hence $c=0$.  Therefore $\alpha_2=\alpha$, completing the proof.
\end{proof}

\subsection*{Exercises 6.1. (Regular Curves.  Frenet Trihedron)} % This section intends to get the reader used to dealing with a ton of calculus...
% along with a bunch of other obvious factors.  Basically cover regular curves, and the Frenet trihedron.
% Also, bring in the fundamental theorem of differential equations, and cite some source which proves it (proving it in this book would be a complete throw-off).
% Use this to show that genuine functions \kappa,\tau in the arc length s determine the curve up to an element of \operatorname{Isom}^+(\mathbb R^3).
\begin{enumerate}
\item Which of the following functions $\alpha:I\to\mathbb R^n$ are regular curves?

(a) $I=(0,\infty),\alpha(t)=(\sqrt t,t^2)$

(b) $I=\mathbb R,\alpha(t)=(e^t,t^2)$

(c) $I=\mathbb R,\alpha(t)=(t-e^t,t^2,t^3)$

(d) $I=(-1,1),\alpha(t)=(\sin^{-1}t,\cos^{-1}t)$

(e) $I=\mathbb R,\alpha(t)=(2t^3,3t^5,5t^7)$

\item (a) Parametrize the regular curve $\alpha(t)=(7\cos t,7\sin t,3t)$ by arc length.  Sketch this helix.

(b) Parametrize $\alpha(t)=(t,t^2)$ by arc length.  [To integrate $\int_0^t\sqrt{1+4u^2}\,du$, make the substitution $u=\frac 12\sinh v$.]

(c) Parametrize $\alpha(t)=(t,\cosh t)$ by arc length.  [First show that $\|\alpha'(t)\|=\cosh t$.]

\item Let $\alpha:I\to\mathbb R^n$ be a regular curve, and $a,b\in I$.

(a) Show that the length of the curve from $\alpha(a)$ to $\alpha(b)$ is equal to $\int_a^b\|\alpha'(t)\|\,dt$.  [It would help to think of the integral using Riemann approximations.]

(b) If $\alpha$ is parametrized by arc length, the length of the curve from $\alpha(a)$ to $\alpha(b)$ is equal to $b-a$.

\item Let $I$ be an interval, and $\vec v,\vec w:I\to\mathbb R^n$ differentiable vector-valued functions.

(a) Show that $\frac{d}{dt}(\vec v(t)\cdot\vec w(t))=\vec v'(t)\cdot\vec w(t)+\vec v(t)\cdot\vec w'(t)$.

(b) Use part (a) to show that $\|\vec v(t)\|$ is constant if and only if $\vec v(t)\cdot\vec v'(t)=0$ for all $t\in I$.

(c) If $n=3$, show that $\frac{d}{dt}(\vec v(t)\times\vec w(t))=\vec v'(t)\times\vec w(t)+\vec v(t)\times\vec w'(t)$.

\item If $\vec v,\vec w$ are orthonormal vectors in $\mathbb R^3$, show that $\{\vec v,\vec w,\vec v\times\vec w\}$ is a positive orthonormal basis.

\item Let $\alpha:I\to\mathbb R^3$ be a regular curve, not necessarily parametrized by arc length.  Let $\vec v(t)$ be the velocity $\alpha'(t)$.

If $\varphi:I\to J$ is a bijection such that $J\overset{\varphi^{-1}}\to I\overset{\alpha}\to\mathbb R^3$ is a reparametrization by arc length, we shall say $\boldsymbol\tau(t),t\in I$ when we mean the tangential unit vector at $s=\varphi(t)\in J$ [i.e., at the point $\alpha(t)$ on the curve's image]; similarly for $\boldsymbol\nu,\boldsymbol\beta,\kappa,\tau$.

(a) Show that the acceleration $\alpha''(t)$ is equal to $a_T\boldsymbol\tau+a_N\boldsymbol\nu$, where $a_T=\frac d{dt}\|\vec v(t)\|$ and $a_N=\kappa(t)\|\vec v(t)\|^2$.  [Note that $\alpha'(t)=\|\alpha'(t)\|\boldsymbol\tau(t)$ and use the Product Rule for differentiation.  Be careful, however: $\boldsymbol\tau'(t)$ is \emph{not} equal to $\kappa(t)\boldsymbol\nu(t)$; that is the derivative of $\boldsymbol\tau$ with respect to the \emph{arc length parameter $s$}.  But given this, and the fact that $ds/dt=\|\alpha'(t)\|$, one can readily find $\boldsymbol\tau'(t)$ using the Chain Rule.]

$a_T$ is called the \textbf{tangential acceleration} of the parametrization, and $a_N$ is called the \textbf{normal acceleration}.

(b) Conclude that $\|\alpha''(t)\|=\sqrt{a_T^2+a_N^2}$.

(c) What are $a_T$ and $a_N$ if $\alpha$ is an arc length parametrization?

\item Let $\alpha:I\to\mathbb R^3$ be a regular curve, not necessarily parametrized by arc length.  Show that

(a) $\boldsymbol\tau(t)=\frac{\alpha'(t)}{\|\alpha'(t)\|}$.

(b) $\boldsymbol\nu(t)=\frac{1}{\|\boldsymbol\tau'(t)\|}\boldsymbol\tau'(t)$.

(c) $\kappa(t)=\frac 1{\|\alpha'(t)\|}\|\boldsymbol\tau'(t)\|=\frac{\|\alpha'(t)\times\alpha''(t)\|}{\|\alpha'(t)\|^3}$.  [Use Exercise 6.]

(d) Show by example that $\alpha''(t)$ need not be perpendicular to $\boldsymbol\tau(t)$.

(e) $\tau(t)=-\frac 1{\|\alpha'(t)\|}\boldsymbol\beta'(t)\cdot\boldsymbol\nu(t)=-\frac{\alpha'(t)\cdot(\alpha''(t)\times\alpha'''(t))}{\|\alpha'(t)\times\alpha''(t)\|^2}$.

The above formulas give easy ways to compute curvature, torsion and the trihedron, for any parametrization of the curve, without referring to the arc length parametrization (which can be difficult to find for most curves).  However, if we had given the formulas just like this from the start, it would have been far from obvious that the curvature and torsion are independent of the particular parametrization.

\item If $\alpha:J\to\mathbb R^3$ is a regular curve, show that the following are equivalent.

~~~~(i) $\alpha$ is a plane curve; i.e., contained in a plane.

~~~~(ii) The binormal vector $\boldsymbol\beta(s)$ is constant.

~~~~(iii) The torsion $\tau(s)$ is identically zero.

[(i) $\implies$ (ii): Show that $\boldsymbol\beta(s)$ must always be orthogonal to the plane. (ii) $\implies$ (i): Suppose $\boldsymbol\beta(s)=\boldsymbol\beta_0$, and fix $s_0\in J$.  Show that $s\mapsto[\alpha(s)-\alpha(s_0)]\cdot\boldsymbol\beta_0$ is identically zero, by showing its derivative is identically zero.  Conclude that the curve is contained in the plane through $\alpha(s_0)$ perpendicular to $\boldsymbol\beta_0$. (ii) $\iff$ (iii): Use the Frenet equations.]

\item\emph{(Signed curvature of a plane curve.)} \---- Let $\alpha:J\to\mathbb R^2$ be a regular curve parametrized by arc length.  Let $\boldsymbol\tau(s)=\alpha'(s)$ for $s\in J$; then $\|\boldsymbol\tau(s)\|=1$, and hence $\boldsymbol\tau(s)\cdot\boldsymbol\tau'(s)=0$ by Exercise 4(b).

(a) Let $\boldsymbol\nu(s)=\begin{bmatrix}0&-1\\1&0\end{bmatrix}\boldsymbol\tau(s)$.  In other words, if $\boldsymbol\tau(s)=(a(s),b(s))$ then $\boldsymbol\nu(s)=(-b(s),a(s))$.  Show that $\{\boldsymbol\tau(s),\boldsymbol\nu(s)\}$ is a positive orthonormal basis of $\mathbb R^2$, for each $s\in J$.

(b) Show that there is a unique differentiable function $\kappa:J\to\mathbb R$, such that $\boldsymbol\tau'(s)=\kappa(s)\boldsymbol\nu(s)$ for all $s\in J$.  Note that $\kappa$ can be negative, unlike the case for curves in $\mathbb R^3$.  $\kappa(s)$ is called the \textbf{signed curvature} of the plane curve.

(c) Show that there exists a differentiable function $\theta:J\to\mathbb R$, such that $\boldsymbol\tau(s)=(\cos\theta(s),\sin\theta(s))$ for all $s\in J$.  [$\theta$ tells the angle from the positive $x$-axis at which the curve is pointing.]  Then show that $\theta'(s)=\kappa(s)$.  Hence the signed curvature measures how much turning happens relative to the arc length.

(d) If $\alpha:J\to\mathbb R^3$ is a space curve, but its image happens to be contained in the $xy$-plane, how do the vectors $\boldsymbol\tau$, $\boldsymbol\nu$ and the scalar function $\kappa$ in this problem relate to those for space curves covered in this section?

\item Suppose $\alpha:I\to\mathbb R^2$ is a regular curve, not necessarily parametrized by arc length.  Show that $\boldsymbol\tau(t)=\frac{\alpha'(t)}{\|\alpha'(t)\|}$, and $\kappa(t)=\frac 1{\|\alpha'(t)\|}\|\boldsymbol\tau'(t)\|$.

\item\emph{(Osculating circles.)} \---- Let $\alpha:J\to\mathbb R^2$ be a regular curve, parametrized by arc length.  Suppose for some $s\in J$ that $\kappa(s)\ne 0$.  Then we define $\rho(s)=1/\kappa(s)$.  The absolute value $|\rho(s)|$ is called the \textbf{radius of curvature}.

(a) Show that the radius of curvature of a circle is the radius of the circle.

(b) The \textbf{center of curvature} is defined to be the point $\alpha(s)+\rho(s)\boldsymbol\nu(s)$.  The \textbf{osculating circle} at $\alpha(s)$ is the circle whose center is the center of curvature and whose radius is $|\rho(s)|$.  Show that this is the unique circle that can be parametrized via $\psi:\mathbb R\to\mathbb R^2$ in such a way that:
$$\psi(0)=\alpha(s),~~~~\psi'(0)=\alpha'(s),~~~~\psi''(0)=\alpha''(s).$$
[For this reason we say that $\alpha$ and the circle have \textbf{contact of order $\geqslant 2$}.]
\begin{center}
\includegraphics[scale=.3]{OscCircle.png}
\end{center}

(c) Is there an $\alpha$ besides a circle where the osculating circle is pretty easy to compute?

\item\emph{(Evolute and approaching normal lines.)} \---- Let $\alpha:I\to\mathbb R^2$ be a regular curve, not necessarily parametrized by arc length.  The locus of the center of curvature, i.e., the curve $\beta(t)=\alpha(t)+\rho(t)\boldsymbol\nu(t)$, is called the \textbf{evolute} of $\alpha$.  Though it is clear this is differentiable, it is not necessarily regular; e.g., if $\alpha$ is a circle then $\beta$ is constant. % How did evolute get its name?  Maybe it says somewhere here? https://en.wikipedia.org/wiki/Evolute

(a) Show that the tangent line to the evolute at $t\in I$ is the normal line to $\alpha$ at this value of $t$.  [$\beta(t)$ is clearly on the normal line to $\alpha$, hence it suffices to show the lines are parallel; i.e., $\alpha'(t)\perp\beta'(t)$.]

(b) If $\alpha(t)=(x(t),y(t))$, show that the signed curvature of $\alpha$ is
$$\kappa(t)=\frac{y''(t)x'(t)-x''(t)y'(t)}{[x'(t)^2+y'(t)^2]^{3/2}}.$$
[Let $v(t)=\|\alpha'(t)\|$ and write $x'(t)=v(t)\cos\theta(t),y'(t)=v(t)\sin\theta(t)$.  Then work out the above expression.  Note that the derivative of $\theta$ with respect to the arc length parameter is $\kappa$ by Exercise 9(c), and hence, $\theta'(t)=\kappa(t)v(t)$ when we differentiate with respect to $t$.]

(c) Let $\ell_1$ and $\ell_2$ be normal lines to $\alpha$, at two close-together points, $t_1$ and $t_2$.  As $t_2\to t_1$ and $\ell_2$ remains the normal line to $\alpha$ at $t_2$, show that $\ell_1\cap\ell_2$ approaches the center of curvature at $t_1$.  [Explain why one may assume $\alpha$ is an arc-length parametrization.  That should simplify the process.]

\item\emph{(Polar coordinates.)} \---- Suppose $A\subset\mathbb R$ is an open interval and $r(\theta)$ is a differentiable function for $\theta\in A$, and that $r(\theta)>0$ throughout $A$.

(a) Show that $\alpha(t)=(r(t)\cos t,r(t)\sin t)$ is a regular curve, and that
$$\alpha'(t)=(r'(t)\cos t-r(t)\sin t,r'(t)\sin t+r(t)\cos t).$$

(b) Use part (a) to show that $\alpha'(t)=\sqrt{r'(t)^2+r(t)^2}$.

(c) Use Exercise 12(b) to find a formula for the signed curvature $\kappa(t)$.

\item Let $\kappa:J\to\mathbb R$ be any differentiable function.  Fix $s_0\in J$.

For some $\mathbf x_0\in\mathbb R^2$ and $\psi\in\mathbb R$, define a curve $\alpha:J\to\mathbb R^2$ as follows:
$$\alpha(s)=\mathbf x_0+\int_{s_0}^s(\cos\theta(u),\sin\theta(u))\,du,\text{ with }\theta(s)=\psi+\int_{s_0}^s\kappa(u)\,du.$$

(a) Show that $\alpha$ is an arc length parametrization, and its signed curvature is $\kappa$.  [Use Exercise 9(c).]  Moreover, $\alpha(s_0)=\mathbf x_0$ and $\alpha'(s_0)=(\cos\psi,\sin\psi)$.

(b) Show that every curve parametrized by arc length, with signed curvature $\kappa$, is of the above form for some $\mathbf x_0$ and $\psi$.

\item The graph $y=f(x)$ of an explicit function can be expressed as a plane curve via $\alpha(t)=(t,f(t))$.  Show that $\alpha$ is regular and find its signed curvature.

\item Let $\alpha:J\to\mathbb R^3$ be a regular curve, parametrized by arc length.  For $s\in J$, the \textbf{radius of curvature} is defined as $\rho(s)=1/\kappa(s)>0$, and the \textbf{center of curvature} is defined to be $\alpha(s)+\rho(s)\boldsymbol\nu(s)$.  The \textbf{osculating plane} refers to the plane through $\alpha(s)$, parallel to the linear span of $\boldsymbol\tau(s),\boldsymbol\nu(s)$.  Finally, the \textbf{osculating circle} is the intersection of the osculating plane with the sphere of radius $\rho(s)$ centered at $\alpha(s)+\rho(s)\boldsymbol\nu(s)$.

(a) Show that the osculating circle is the unique circle with contact of order $\geqslant 2$ at $\alpha(s)$; i.e., it can be parametrized via $\psi:\mathbb R\to\mathbb R^3$ in such a way that $\psi(0)=\alpha(s),\psi'(0)=\alpha'(s),\psi''(0)=\alpha''(s)$.

(b) More generally, a plane through $\alpha(s)$ is the osculating plane if and only if it contains some curve meeting $\alpha(s)$ with contact of order $\geqslant 2$.

\item\emph{(Space curves with constant curvature and torsion.)} \---- Suppose $\alpha:J\to\mathbb R^3$ is an arc-length-parametrized curve, with constant curvature $\kappa>0$ and constant torsion $\tau$.  Show that there is an isometry in $\operatorname{Isom}^+(\mathbb R^3)$ sending $\alpha$ to this curve:
$$s\mapsto\left(\frac{\kappa}{\kappa^2+\tau^2}\cos(s\sqrt{\kappa^2+\tau^2}),\frac{\kappa}{\kappa^2+\tau^2}\sin(s\sqrt{\kappa^2+\tau^2}),\frac{\tau}{\sqrt{\kappa^2+\tau^2}}s\right).$$
[Use Proposition 6.3.]  Conclude that $\alpha$ is a circle if $\tau=0$ and a helix if $\tau\ne 0$.  The helix models constant curvature as it curves around a central axis (here the $z$-axis), and models constant torsion as it rises steadily in the $z$-direction while it curves around.

\item\emph{(Tractrix.)} \---- The \textbf{tractrix} is defined to be the curve $\alpha:(0,\pi)\to\mathbb R^2$ given by
$$\alpha(t)=(\sin t,\cos t+\ln(\tan(t/2))).$$
(a) Show that $\alpha'(t)=(\cos t,-\sin t+\csc t)$. [Use familiar differentiation rules and trigonometric laws to show $\frac{d}{dt}\ln(\tan(t/2))=\frac 1{\tan(t/2)}\cdot\frac 12\sec^2(t/2)=\csc t$.]  Moreover, $\alpha$ is regular on $(0,\pi/2)$ and $(\pi/2,\pi)$, but stationary at $t=\pi/2$.

(b) The tangent line at $t=t_0$ may be parametrized via $t\mapsto\alpha(t_0)+t\alpha'(t_0)$.  Show that this line meets the $y$-axis at $t=-\tan t_0$, at coordinates $(0,\ln(\tan(t/2)))$.

(c) Use part (b) to show that the distance from the point of tangency to where the tangent line meets the $y$-axis is equal to $1$ for all tangent lines.

\begin{center}
\includegraphics[scale=.3]{TractrixPicture.png}
\end{center}
\end{enumerate}

\subsection*{6.2. Regular Surfaces and Parametrization}
\addcontentsline{toc}{section}{6.2. Regular Surfaces and Parametrization}
In Section 6.1 we covered regular curves.  In this section, we shall introduce regular surfaces.  Surfaces are significantly different in the following sense: between any two regular curves, there are local transformations that preserve lengths of all arcs (since regular curves can be parametrized by arc length), but this is not true for surfaces.  Even more, it does not do good to define a surface using just one ``interval'' in $\mathbb R^2$ for the parameters; a surface can be \emph{locally} viewed as a subset of $\mathbb R^2$, but not always globally [take the closed sphere, for instance].

A regular surface in $\mathbb R^3$ is a smooth two-dimensional region.  The smoothness enables various notions to be defined, such as the tangent plane, normal vectors, and the Dupin indicatrix.

Just like curves, it is best to define surfaces via parametrizations, but as previously mentioned we should not require ourselves to get the entire surface in one shot.  Here is the technical definition.  A subset $U$ of $\mathbb R^n$ is said to be \textbf{open} if for any $\vec x\in U$, there exists $\varepsilon>0$ such that
$$B_{\varepsilon}(\vec x)=\{\vec v\in\mathbb R^n:\|\vec v-\vec x\|<\varepsilon\}\subset U.$$
\noindent\textbf{Definition.} \emph{A nonempty subset $S$ of $\mathbb R^3$ is said to be a \textbf{regular surface} provided that for every point $p\in S$, there exist open sets $p\in W\subset\mathbb R^3$ and $0\in U\subset\mathbb R^2$, along with a differentiable map $\mathbf x:U\to W\cap S$ such that:}

(i) \emph{At every point in $U$, the differential of $\mathbf x$ has rank $2$; in particular, it is injective as a linear transformation $\mathbb R^2\to\mathbb R^3$;}

(ii) \emph{$\mathbf x$ has a continuous inverse from $W\cap S$ to $U$.}

(iii) \emph{$\mathbf x(0)=p$.}

\emph{In this case, the function $\mathbf x$ is called a \textbf{local coordinate chart}.  A collection of local coordinate charts whose images cover $S$ (i.e., the union of the images is $S$) is called an \textbf{atlas}.}\\

\noindent If $\mathbf x(u,v)=(x(u,v),y(u,v),z(u,v))$, then the differential of $\mathbf x$, of course, refers to the linear map given by the matrix $\begin{bmatrix}\frac{\partial x}{\partial u}&\frac{\partial x}{\partial v}\\\frac{\partial y}{\partial u}&\frac{\partial y}{\partial v}\\\frac{\partial z}{\partial u}&\frac{\partial z}{\partial v}\end{bmatrix}$.

The importance of the map having rank $2$ is essentially identical to the importance of $\alpha'(t)\ne\vec 0$ for curves $\alpha:I\to\mathbb R^n$.  For one, if $\mathbf x$ did not need to have rank $2$, it could be given for example by $\mathbf x(u,v)=(u^3,v,u^2)$; this ``surface'' has a crease in the middle:
\begin{center}
\includegraphics[scale=.3]{SingularSurface.png}
\end{center}
The crease occurs where $u=0$.  And indeed, the differential of $\mathbf x$ is $\begin{bmatrix}3u^2&0\\0&1\\2u&0\end{bmatrix}$; this clearly has rank $2$ for $u\ne 0$, but if $u=0$ then $d\mathbf x_{(u,v)}=\begin{bmatrix}0&0\\0&1\\0&0\end{bmatrix}$ which has rank $1$.

For a more obvious example of what could go wrong when $d\mathbf x_p$ is not injective, consider $\mathbf x(u,v)=(\cos u,\sin u,u)$.  The image is a helix, a curve, not a surface. % (You basically suggested I add my comment to the book. LOL)  In this case we have a helix, this is a curve
Note that the condition that $d\mathbf x_p$ be injective is sometimes called the \textbf{regularity condition}.

There is also an important reason for condition (ii) [$\mathbf x$ having a continuous inverse].  Without it, one could have self-intersections, leading to there being more than one tangent plane to a point and causing confusion.  For example, suppose $\mathbf x(u,v)=(u^3-u,v,u^2-1)$, as shown below.
\begin{center}
\includegraphics[scale=.2]{SurfaceLoop.png}
\end{center}
The differential of $\mathbf x$ is $\begin{bmatrix}3u^2-1&0\\0&1\\2u&0\end{bmatrix}$, which can readily be seen to have rank $2$ for all $u,v\in\mathbb R$.  Thus condition (i) is met.  However, when $u=\pm 1$, $\mathbf x(u,v)$ is on the line where the two parts of the surface pass through [one part where $u$ is near $1$, and the other where $u$ is near $-1$].  No matter what open neighborhood $W$ of $\mathbf x(\pm 1,v)$ is involved, it will meet the surface in an open set on both parts.  It is hence impossible for $\mathbf x$ to possess a continuous inverse on $W\cap S$.  Indeed, $\mathbf x(\pm 1,v)$ is a limit of points in $W$ on each sheet part, hence continuity would imply $\mathbf x^{-1}$ sends that point to both $(1,v)$ and $(-1,v)$, so $\mathbf x^{-1}$ would not be a well-defined function.

This anomaly also entails the non-uniqueness of tangent planes; as we will see later, the tangent plane at a point $\mathbf x(u,v)\in U$ is parallel to the image of $d\mathbf x_{(u,v)}$ (as a subspace).  But the point $\mathbf x(\pm 1,v)$ in the example above has two ``tangent planes,'' $\operatorname{span}\left(\begin{bmatrix}1\\0\\1\end{bmatrix},\begin{bmatrix}0\\1\\0\end{bmatrix}\right)$ and $\operatorname{span}\left(\begin{bmatrix}1\\0\\-1\end{bmatrix},\begin{bmatrix}0\\1\\0\end{bmatrix}\right)$.  These different results are obtained by thinking of $u$ as $1$ or $-1$.

Now that we have covered the issues that could occur in the absence of the conditions, let us give some examples of surfaces which \emph{do} satisfy the conditions.\\

\noindent\textbf{Examples.}

(1) If $\vec a,\vec b\in\mathbb R^3$ are linearly independent vectors and $\vec x\in\mathbb R^3$ is any vector, the image of $\mathbf x:\mathbb R^2\to\mathbb R^3$ given by $\mathbf x(u,v)=u\vec a+v\vec b+\vec x$ is a regular surface.  It is a plane, and it has in fact been parametrized in one shot.  The reader can readily verify that $\vec a,\vec b$ are the columns of the differential of $\mathbf x$, hence the differential has rank $2$.  It is easy to prove using linear algebra that $\mathbf x$ has a continuous inverse.\\

(2) More generally, let $U\subset\mathbb R^2$ be open and $f:U\to\mathbb R$ a differentiable function.  Then the image of $\mathbf x:U\to\mathbb R^3$ given by $\mathbf x(u,v)=(u,v,f(u,v))$ is a regular surface, called the \textbf{graph of $f$}.  Indeed, $d\mathbf x_{(u,v)}=\begin{bmatrix}1&0\\0&1\\\frac{\partial f}{\partial u}&\frac{\partial f}{\partial v}\end{bmatrix}$, which has rank $2$ everywhere because the upper $2\times 2$ submatrix is nonsingular.  As for condition (ii), the restriction of $(x,y,z)\mapsto(x,y)$ to $\mathbf x$'s image is a continuous inverse of $\mathbf x$.\\

(3) Here is an example of a surface which cannot be parametrized in just a single coordinate chart.  Let $S^2=\{(x,y,z)\in\mathbb R^3:x^2+y^2+z^2=1\}$ be the unit sphere.  Then there are many \emph{partial} parametrizations, such as stereographic projection,
$$\mathbf x:\mathbb R^2\to S^2,\mathbf x(u,v)=\left(\frac{2u}{u^2+v^2+1},\frac{2v}{u^2+v^2+1},\frac{u^2+v^2-1}{u^2+v^2+1}\right),$$
which parametrizes the whole sphere except the North Pole.
There is also the spherical-coordinate parametrization
$$\mathbf x(u,v)=\left(\sin u\cos v,\sin u\sin v,\cos u\right)\text{ for }u\in(0,\pi),v\in(0,2\pi),$$
which uses the fact that $U=(0,\pi)\times(0,2\pi)$ is open in $\mathbb R^2$.  We shall mostly stick with the latter parametrization when dealing with the sphere.

The function $\mathbf x:U\to\mathbb R^3$ given by $\mathbf x(u,v)=\left(\sin u\cos v,\sin u\sin v,\cos u\right)$ is readily seen to have a differential of rank $2$.  We have
$$d\mathbf x_{(u,v)}=\begin{bmatrix}\cos u\cos v&-\sin u\sin v\\\cos u\sin v&\sin u\cos v\\-\sin u&0\end{bmatrix}$$
and $u\in(0,\pi)$ implies $\sin u>0$, so we can confirm that the rank is $2$ as follows.  The upper $2\times 2$ submatrix has determinant $\cos u\sin u$, hence if $\cos u\ne 0$ then this submatrix is nonsingular and the differential has rank $2$.  So suppose $\cos u=0$?  Then $u=\pi/2$ and $\sin u=1$, so that the differential is $\begin{bmatrix}0&-\sin v\\0&\cos v\\-1&0\end{bmatrix}$ \---- this obviously has rank $2$ because $\cos v,\sin v$ are not both zero.

As for a continuous inverse, we leave it to the reader to show that the image of $\mathbf x$ is $V=\{(x,y,z)\in S^2:y\ne 0\text{ or }x<0\}$, and on this set we may define an inverse via
$$\mathbf x^{-1}(x,y,z)=\left(\tan^{-1}_2(\sqrt{x^2+y^2},z),\tan^{-1}_2(y,x)\right)$$
where $\tan^{-1}_2(b,a)$ is the argument of the complex number $a+bi$ in the range $[0,2\pi)$; this can be regarded as a version of $\tan^{-1}(b/a)$ where the signs of $a,b$ both matter.  [See Exercise 1 for the basic properties of $\tan^{-1}_2(b,a)$.]

The parametrization $\mathbf x$ does have a continuous inverse, but its image is missing several points on the sphere; specifically the points $(x,y,z)\in S^2$ where $x\geqslant 0,y=0$.  However, it is clear that a few other parametrizations (also with missing points in the image) can be used along with $\mathbf x$, to form an atlas.\\%so that every point on the sphere is covered by at least one parametrization.

(4) Start with the $x$-axis in $\mathbb R^3$.  Then rotate it steadily around the $z$-axis, while translating it steadily along the $z$-axis at the same time.  The surface thus traced is called a \textbf{helicoid}.  It can be thought of as starting with a helix $(\cos t,\sin t,t)$ then scaling it in the $x$ and $y$ directions by all possible scalar multiples.  Specifically, it is parametrized via
$$\mathbf x(u,v)=(u\cos v,u\sin v,v).$$
We leave it to the reader to check conditions (i) and (ii) in the definition of a regular surface.  Hence the helicoid may actually be parametrized in one shot (i.e., one coordinate chart covering the whole surface).\\ % Helicoid

(5) \emph{(Surfaces of revolution.)} \---- Graphs of explicit functions (Example 2) are an important general class of surfaces.  Another important general class are \emph{surfaces of revolution}, which are obtained by taking a curve in the $xz$-plane, and rotating it all around the $z$-axis.  Specifically, let $\alpha(t)=(f(t),g(t))$ be any \emph{embedded} regular curve $I\to\mathbb R$ [Exercise 5(g) below], such that $f(t)>0$ for all $t\in I$.  Then define $\mathbf x:I\times(0,2\pi)\to\mathbb R^3$ as follows:
$$\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u)).$$
You may notice that since $v$ cannot touch either $0$ or $2\pi$, $\mathbf x$ is missing the section in the part of the $xz$-plane where $x$ is positive.  This is inevitable, because our definition requires $\mathbf x$ to have a continuous inverse (which it would not if its domain contained both $0$ and $2\pi$).  Fortunately, once this parametrization is studied, it will be clear that one more parametrization can be used to get an atlas, i.e., cover the surface.  Indeed, if $\theta\in(0,2\pi)$ is a constant, then substituting $v+\theta$ for $v$ in the above formula will do.

To show that $\mathbf x$ has rank $2$, we first compute that its differential is
$$d\mathbf x_{(u,v)}=\begin{bmatrix}f'(u)\cos v&-f(u)\sin v\\f'(u)\sin v&f(u)\cos v\\g'(u)&0\end{bmatrix}.$$
The upper $2\times 2$ submatrix has determinant $f'(u)f(u)$, and $f(u)$ is nonzero because it is positive.  Hence if $f'(u)\ne 0$, we have a $2\times 2$ submatrix with a nonzero determinant, so that the matrix has rank $2$.  If $f'(u)=0$, then, because $\alpha$ is regular, $g'(u)\ne 0$ and then it is clear that neither column is a scalar multiple of the other (because, for example, $\sin v$ or $\cos v$ is nonzero), so the matrix has rank $2$.

As for condition (ii), we use the assumption that $\alpha$ is an embedding, hence has a continuous inverse $\alpha^{-1}:\alpha(I)\to I$.  We then use the generalized arc tangent $\tan^{-1}_2$ (with range $[0,2\pi)$) to formulate $\mathbf x^{-1}$:\\
$$\mathbf x^{-1}(x,y,z)=(\alpha^{-1}(\sqrt{x^2+y^2},z),\tan^{-1}_2(y,x)).$$
Some examples of this are the sphere (when $\alpha$ is the circular arc $t\mapsto(\sin t,\cos t)$ for $t\in(0,\pi)$), the cylinder (when $\alpha$ is the line $t\mapsto(1,t)$ for $t\in\mathbb R$), the cone (when $\alpha$ is the line $t\mapsto(t,t)$ for $t>0$ \---- this time the line is not parallel to the axis of rotation), and the hyperboloid (when $\alpha$ is the hyperbola branch $t\mapsto(\cosh t,\sinh t)$ for $t\in\mathbb R$).

Another example is the \emph{catenoid}, which is the surface of revolution given by the hyperbolic cosine curve, $\alpha(t)=(\cosh t,t)$.  We will see later that, in fact, the catenoid is locally isometric to the helicoid.

Note that since regular curves can be parametrized by arc length, one may assume $(f')^2+(g')^2=1$ throughout $I$.  This will provide many advantages to studying surfaces of revolution, as will be seen in later sections.\\

\noindent In the rest of the section, we will be covering the concept of the tangent plane.  Many other concepts, along with generalizations to higher dimensions, will be covered in the exercises.\\

\noindent\textbf{Definition.} \emph{Let $S$ be a regular surface and $p\in S$.  The \textbf{tangent plane} at $p$, denoted $T_pS$, is defined as follows: Let $\mathbf x:U\to W\cap S$ be a local coordinate chart with $\mathbf x(0)=p$.  Then the tangent plane is the image of the linear map $d\mathbf x_0$, depicted as a plane with its ``origin'' at $p$.  Elements of the tangent plane are called \textbf{tangent vectors}.}\\

\noindent Note also that the tangent plane at $p$ is the span of the vectors $\frac{\partial\mathbf x}{\partial u}\big|_{(u,v)=(0,0)}$ and $\frac{\partial\mathbf x}{\partial v}\big|_{(u,v)=(0,0)}$, as these vectors are the columns of $d\mathbf x_0$.

To verify the definition we must check two things: (a) the span is indeed a plane, and (b) it is independent of the particular coordinate chart.  (a) follows from condition (i) of a regular surface, stating that $d\mathbf x_0$ is injective as a linear map; hence its image is isomorphic to its domain and has dimension $2$.

As for (b), let $\mathbf x_1:U_1\to W_1\cap S$ be another local coordinate chart with $\mathbf x_1(0)=p$; we may assume $W=W_1$ by restricting both of them to $W\cap W_1$ and reassigning $U=\mathbf x^{-1}(W\cap W_1),U_1=\mathbf x_1^{-1}(W\cap W_1)$.  Let $\psi=\mathbf x_1^{-1}\circ\mathbf x:U\to U_1$ ($\mathbf x_1^{-1}$ exists by condition (ii) of a regular surface); then $\psi$ is a diffeomorphism by Exercise 2(b), and $\mathbf x_1\circ\psi=\mathbf x$.  Note also $\psi(0)=0$.  Differentiating at $0$ and using the Chain Rule,
$$d(\mathbf x_1)_0\circ d\psi_0=d\mathbf x_0.$$
Since $d\psi_0$ is a linear isomorphism, it follows that $d\mathbf x_0$ and $d(\mathbf x_1)_0$ have the same image, hence the different choice of chart entails the same tangent plane.\\

\noindent In the rest of this section we shall give examples of tangent planes.

We recall that if $\vec a,\vec b$ are linearly independent vectors and $\vec x$ is any vector, then $\mathbf x(u,v)=u\vec a+v\vec b+\vec x$ parametrizes a plane.  Its tangent plane at any point is just the plane itself, as it is spanned by $d\mathbf x$'s columns, $\vec a$ and $\vec b$.

If $U\subset\mathbb R^2$ is open, $f:U\to\mathbb R$ is a differentiable function and $\mathbf x:U\to\mathbb R^3$ is the graph of $f$ [$(u,v)\mapsto(u,v,f(u,v))$], then the tangent plane at a point $p$ is spanned by the vectors $\left(1,0,\frac{\partial f}{\partial u}(p)\right)$ and $\left(0,1,\frac{\partial f}{\partial v}(p)\right)$.  This probably looks familiar from multivariable calculus; the partial derivatives give the slope of the tangent lines in each direction, and the tangent plane is spanned by those lines.

Finally, consider the sphere $S^2$.  Its standard spherical-coordinate parametrization is $\mathbf x(u,v)=(\sin u\cos v,\sin u\sin v,\cos u)$ for $0<u<\pi,0<v<2\pi$.  [Of course, this is missing several points, but the tangent planes at those points will be equally easy to find using another parametrization.]  $d\mathbf x_{(u,v)}=\begin{bmatrix}\cos u\cos v&-\sin u\sin v\\\cos u\sin v&\sin u\cos v\\-\sin u&0\end{bmatrix}$, so the columns of this matrix span the tangent plane at $\mathbf x(u,v)$.  We have already shown this matrix to have rank $2$ (so that its image is a plane), and it can be shown (either directly from here or using Exercise 4(b) of Section 6.1) that each column of the matrix is perpendicular to $\mathbf x(u,v)$.  Hence the tangent plane at a point $p\in S^2$ is the orthogonal complement of that point regarded as a unit vector; i.e., the vector space $\{\vec v\in\mathbb R^3:\vec p\cdot\vec v=0\}$.  The set of points $\vec v$ such that $\vec p\cdot\vec v=1$ is an affine plane which is literally tangent to the sphere at $\vec p$.

\subsection*{Exercises 6.2. (Regular Surfaces and Parametrization)} % Introduce the concept of a regular surface and parametrizations used to study them.
% Do not customize the metric; that is to be saved for Section 6.10.  Remember to cover tangent planes.
\begin{enumerate}
\item For $x,y\in\mathbb R$, not both zero, define $\tan^{-1}_2(y,x)$ to be the argument of the complex number $x+yi$ in the range $[0,2\pi)$.  Show that

(a) $\cos\tan^{-1}_2(y,x)=\frac x{\sqrt{x^2+y^2}}$.

(b) $\sin\tan^{-1}_2(y,x)=\frac y{\sqrt{x^2+y^2}}$.

(c) If $y>0$, then $0<\tan^{-1}_2(y,x)<\pi$.

(d) If $x\ne 0$ and $y>0$, then $\tan^{-1}_2(y,x)=\tan^{-1}(y/x)$.  Show by example that this may be false if $y\leqslant 0$.  [Also note that $\tan^{-1}_2(1,0)=\pi/2$, but $\tan^{-1}(1/0)$ is undefined.]

\item (a) Let $S$ be a regular surface, and $\mathbf x:U\to W\cap S$ a local coordinate chart.  Show that $\mathbf x^{-1}$ is differentiable.  [Remember that $\mathbf x$ has injective differentials at all points.]

(b) Suppose $\mathbf x_1:U_1\to W\cap S$ is another local coordinate chart using the same open set of $S$.  Then $\mathbf x_1^{-1}\circ\mathbf x:U\to U_1$ is a diffeomorphism.  [It is smooth by part (a); now diagnose its inverse by swapping the roles of $\mathbf x,\mathbf x_1$.]

\item Let $S_1$ and $S_2$ be regular surfaces.  A function $f:S_1\to S_2$ is said to be \textbf{smooth} / \textbf{differentiable} provided that whenever $U\subset\mathbb R^2$ is an open set and $g:U\to S_1$ is a differentiable function, then $f\circ g:U\to S_2$ is differentiable.

(a) Show that $f$ is smooth if and only if it composes with every local coordinate chart of $S_1$ to a differentiable map.

(b) Moreover, if $\{U_\alpha,\mathbf x_\alpha\}_{\alpha\in A}$ is an atlas, $f$ is smooth if and only if every $f\circ\mathbf x_\alpha$ is differentiable.

(c) Composition of smooth functions and the identity map on a regular surface are smooth.

(d) If $f:S_1\to S_2$ is smooth and bijective, show by example that $f^{-1}:S_2\to S_1$ need not be smooth.

For $p\in S_1$, the \textbf{differential} of $f$ at $p$, denoted $df_p$, is a linear map $T_pS_1\to T_{f(p)}S_2$ defined as follows.  Let $\mathbf x:U\to S_1$ be any local coordinate chart sending $0\mapsto p$.  Then $d\mathbf x_0$ has rank $2$, hence is an \emph{isomorphism} $\mathbb R^2\to T_pS_1$.  $df_p$ is thereby defined to be $d(f\circ\mathbf x)_0\circ d\mathbf x_0^{-1}$ (noting that $f\circ\mathbf x$ is differentiable but its differential may not have rank $2$).

(e) Show that $df_p$ is independent of the particular local coordinate chart.

(f) Prove the \textbf{Chain Rule}: if $f:S_1\to S_2$ and $g:S_2\to S_3$ are smooth and $p\in S_1$, then $d(f\circ g)_p=dg_{f(p)}\circ df_p$.

(g) Use part (f) to show that if $I\subset\mathbb R$ is an interval and $\alpha:I\to S_1$ is a (differentiable curve), such that $\alpha(t_0)=p$ and $\alpha'(t_0)=\vec v$, then $df_p(\vec v)=(f\circ\alpha)'(t_0)$.  Hence, the differential of $f$ sends tangent vectors of a curve to tangent vectors of the target curve.

(h) If $f:S_1\to S_2$ is smooth, show that $f$ has a smooth inverse if and only if it is bijective and its differential at every point is a linear isomorphism.  [Such a map $f$ is said to be a \textbf{diffeomorphism}.]

\item We can take curves \emph{on} the regular surfaces.  Let us suppose $S$ is a regular surface, and $\alpha:I\to S$ is a differentiable function.

(a) Assume $\mathbf x:U\to S$ is a local coordinate chart, and $\alpha$ is given by $\alpha(t)=\mathbf x(u(t),v(t))$.  Then $\alpha'(t)=\vec 0$ if and only if $u'(t)=v'(t)=0$.  [See condition (i) in the definition of a local coordinate chart.]  Thus $\alpha$ is a regular curve in $\mathbb R^3$ if and only if $t\mapsto(u(t),v(t))$ is a regular curve in $\mathbb R^2$.

(b) Let $\alpha:I\to S$ be a regular curve.  Show by example that its normal vector (from its Frenet trihedron) may not be normal to the surface $S$.

(c) Let $\mathbf N=\frac{\mathbf x_u\times\mathbf x_v}{\|\mathbf x_u\times\mathbf x_v\|}$ applied to $(u,v)\in U$.  Show that $\mathbf N$ is a unit vector normal to the surface at $\mathbf x(u,v)$ (i.e., perpendicular to the tangent plane $T_{\mathbf x(u,v)}S$).  [$\mathbf x_u$, of course, means $\frac{\partial\mathbf x}{\partial u}$.  Note that $\mathbf N$ is defined because $d\mathbf x$ has rank $2$.]

(d) If $\alpha:J\to S$ is a regular curve parametrized by arc length, let $\kappa(s)$ be its curvature and $\boldsymbol\nu(s)$ its normal vector.  Then $\alpha''(s)=\kappa(s)\boldsymbol\nu(s)$.  Show that there is a unique expression $\alpha''(s)=\kappa_g\mathbf n+\kappa_n\mathbf N$ where $\mathbf n$ is a unit vector, tangent to $S$ and normal to $\alpha$, pointing in the direction of $\alpha''(s)$.  [Take the orthogonal projection of $\alpha''(s)$ to the tangent plane.]  $\kappa_g$ is called the \textbf{geodesic curvature} of $\alpha$ and $\kappa_n$ its \textbf{normal curvature}.

\item The aim of this exercise is to generalize the results of this chapter to arbitrary dimensions.  A typical course on differential geometry would actually do the general case in the first place, but since $\mathbb R^n$ can only be visualized in real life when $n\leqslant 3$, this chapter will generally start in the low-dimensional visualizable areas.

Fix nonnegative integers $m\leqslant n$.  A subset $S\subset\mathbb R^n$ is defined to be a \textbf{regular manifold of dimension $m$}, provided that for each $p\in S$, there exist open sets $U\subset\mathbb R^m,p\in W\subset\mathbb R^n$ along with a differentiable map $\mathbf x:U\to W\cap S$, such that (i) at each $q\in U$ the differential $d\mathbf x_q$ has rank $m$; i.e., is injective as a linear map; and (ii) $\mathbf x$ has a continuous inverse $W\cap S\to U$.  [$\mathbf x$ is called a \textbf{local coordinate chart}, just as in the case for surfaces.]

If $m=n-1$, a regular manifold of dimension $m$ is called a \textbf{hypersurface}.  For example, $m$-planes and $m$-spheres in $\mathbb R^n$ are regular manifolds of dimension $m$, and hyperplanes and hyperspheres are hypersurfaces.

(a) Show that the connected regular manifolds of dimension $1$ are the embedded regular curves.

(b) We will assume the \textbf{Inverse Function Theorem}: if $U\subset\mathbb R^n$, $f:U\to\mathbb R^n$ is a differentiable function, $p\in U$ and $df_p$ is a linear isomorphism, then there exist open sets $p\in V\subset U$ and $W\subset\mathbb R^n$ such that $f|_V$ is a diffeomorphism from $V$ to $W$.  [The proof of this fact involves analysis which is beyond the scope of this book.]  Show that the regular manifolds of dimension $n$ in $\mathbb R^n$ are precisely the open sets.

(c) If $S$ is a regular $m$-dimensional manifold, come up with a definition for the tangent space $T_pS$ for any point $p\in S$, by imitating the definition of a tangent plane in the case $n=3,m=2$ covered in the text.  [It is an $m$-dimensional vector subspace of $\mathbb R^n$.]  Show that the tangent space of an $m$-plane is the vector space parallel to this $m$-plane.  Also, the tangent space of the hypersphere $S^{n-1}=\{\vec v\in\mathbb R^n:\|\vec v\|=1\}$ at a point $p$ is the set of vectors perpendicular to $p$.  [Of course, all of these tangent spaces are depicted with the origin at $p$.] % Hope this helps.

(d) If $M$ and $M'$ are manifolds with respective dimensions $m,m'$, a function $f:M\to M'$ is said to be \textbf{smooth} provided that whenever $U\subset\mathbb R^m$ is an open set and $g:U\to M$ is differentiable, then $f\circ g:U\to M'$ is differentiable.  [Note that $M$ and $M'$ could have different dimensions and they could be embedded into different dimensional Euclidean spaces.]

Generalize Exercise 3(a)-(c) to smooth maps of manifolds of arbitrary dimension.

(e) Imitate Exercise 3 to define the \textbf{differential} of a smooth function $f:M\to M'$ at $p\in M$; this is a linear map of the tangent spaces $T_pM\to T_{f(p)}M'$.  Then generalize Exercise 3(e)-(h) to arbitrary dimensions.

(f) A smooth map $f:M\to M'$ is said to be an \textbf{immersion} if $df_p$ is injective for all $p\in M$.  Show by example that an immersion of manifolds need not be injective.

(g) Show by example that if $f:M\to M'$ is an injective immersion, $f$ need not be a homeomorphism from $M$ to $f(M)$.  [Let $M$ be an open interval and $M'=\mathbb R^2$.  Then bend the interval in a figure 6 shape, so that one of its endpoints (note that they are excluded) would coincide with one of its interior points.]  If $f$ is a homeomorphism from $M$ to $f(M)$, $f$ is called an \textbf{embedding}.

It is worth noting a few facts that involve topics beyond the scope of the book: % "Rather than putting these at the end of the exercises, how about end of the section?"  Because, the section doesn't cover arbitrary-dimensional manifolds.  It only covers 2-dimensional ones.  This exercise introduces higher-dimensional analogues.  Aren't these facts worth stating for arbitrary dimensions?  Best not barge a mention of that into the section just to place these facts.

~~~~(i) If $M$ is compact, every injective immersion $M\to M'$ is an embedding.

~~~~(ii) Define $p\in M$ to be a \textbf{regular point} if $df_p$ is surjective (note that this is only possible if $m\geqslant m'$), and a \textbf{singular point} otherwise.  Define $x\in M'$ to be a \textbf{singular value} if there exists a singular point $p\in M$ such that $f(p)=x$, and a \textbf{regular value} otherwise (i.e., every point $p$ such that $f(p)=x$ is a regular point).  Then if $x$ is a regular value, $f^{-1}(\{x\})$ is a regular manifold of dimension $m-m'$.

~~~~(iii) A \textbf{Lie group}\footnote{Named after 19th-century Norwegian mathematician, Marius Sophus Lie.} is a regular manifold equipped with a group structure, for which the maps $(g,h)\mapsto gh$ and $g\mapsto g^{-1}$ are smooth.  See Exercise 5 of Section 6.8 to learn more about them.

~~~~(iv) Usually, regular manifolds [a.k.a., smooth manifolds] are not defined as subsets of Euclidean space.  They are usually defined as topological spaces where every point has an open neighborhood, homeomorphic to $\mathbb R^m$, and the transition functions between neighborhoods are diffeomorphisms.  (E.g., the projective space $P^m(\mathbb R)$ is readily seen to satisfy this.)  Hassler Whitney in 1944 proved that they can always be embedded into Euclidean space, specifically with twice the dimension of the manifold.  This is known as the \textbf{Whitney embedding theorem}.
\end{enumerate}

\subsection*{6.3. The First Fundamental Form.  Length and Area}
\addcontentsline{toc}{section}{6.3. The First Fundamental Form.  Length and Area}
In the previous section, we introduced regular surfaces.  The advantage to parametrizing a surface, is that we will be able to do define mathematical concepts (such as lengths of curves) on the surface, just using the surface parameters, without referring to the ambient space $\mathbb R^3$.  This will be especially important when customized surfaces are defined in Section 6.10.

The first natural step is to know how to compute dot products of tangent vectors.  Suppose $S$ is a regular surface, $p\in S$ and $\mathbf x:U\to S$ is a local coordinate chart sending $0\mapsto p$.  Then since $d\mathbf x_0$ is a linear isomorphism from $\mathbb R^2$ to $T_pS$ (by regularity), a tangent vector to $S$ at $p$ can be uniquely expressed as $d\mathbf x_0((a,b))$ with $(a,b)\in\mathbb R^2$.  It is clear that $d\mathbf x_0(\vec e_1)=\frac{\partial x}{\partial u}\big|_{(u,v)=0}=\mathbf x_u(0)$ and $d\mathbf x_0(\vec e_2)=\frac{\partial x}{\partial v}\big|_{(u,v)=0}=\mathbf x_v(0)$ \---- after all, applying any matrix to the standard basis vectors gives the columns of the matrix.  Hence, we have
$$d\mathbf x_0((a,b))=a\mathbf x_u(0)+b\mathbf x_v(0)$$
as a consequence of the linearity.

Similarly, for any other $\zeta\in U$, if we let $q=\mathbf x(\zeta)$, each element of $T_qM$ is uniquely of the form $a\mathbf x_u(\zeta)+b\mathbf x_v(\zeta)$ with $a,b\in\mathbb R$.  The reader probably observed this before, because the linear independence of $\mathbf x_u$ and $\mathbf x_v$ implies that they form a basis of the tangent plane.

The natural question to ask is this.  Given the coordinate chart, what is the \emph{best} way to formulate the dot product of any two tangent vectors $a_1\mathbf x_u+a_2\mathbf x_v$ and $b_1\mathbf x_u+b_2\mathbf x_v$ at the same point?  We would like a basic formula in terms of $a_1,a_2,b_1,b_2$ so that we don't need to keep looking back at the differential of the parametrization.  Fortunately, such a formula is easy to get due to the bilinearity of the dot product:
$$(a_1\mathbf x_u+a_2\mathbf x_v)\cdot(b_1\mathbf x_u+b_2\mathbf x_v)=(a_1\mathbf x_u)\cdot(b_1\mathbf x_u+b_2\mathbf x_v)+(a_2\mathbf x_v)\cdot(b_1\mathbf x_u+b_2\mathbf x_v)$$
$$=(a_1\mathbf x_u)\cdot(b_1\mathbf x_u)+(a_1\mathbf x_u)\cdot(b_2\mathbf x_v)+(a_2\mathbf x_v)\cdot(b_1\mathbf x_u)+(a_2\mathbf x_v)\cdot(b_2\mathbf x_v)$$
$$=a_1b_1\mathbf x_u\cdot\mathbf x_u+a_1b_2\mathbf x_u\cdot\mathbf x_v+a_2b_1\mathbf x_v\cdot\mathbf x_u+a_2b_2\mathbf x_v\cdot\mathbf x_v$$
$$=a_1b_1[\mathbf x_u\cdot\mathbf x_u]+(a_1b_2+a_2b_1)[\mathbf x_u\cdot\mathbf x_v]+a_2b_2[\mathbf x_v\cdot\mathbf x_v],$$
using the symmetry of the dot product in the last step.  Note that $\mathbf x_u\cdot\mathbf x_u$, $\mathbf x_u\cdot\mathbf x_v$ and $\mathbf x_v\cdot\mathbf x_v$ are scalar functions on the base $U$, which are determined by the local coordinate chart; they do not depend on the particular tangent vectors we are thinking about.  They hereby inherit a title.\\

\noindent\textbf{Definition.} \emph{Let $S$ be a regular surface, $\mathbf x:U\to S$ a local coordinate chart.  Then the differentiable functions $\mathbf x_u\cdot\mathbf x_u$, $\mathbf x_u\cdot\mathbf x_v$ and $\mathbf x_v\cdot\mathbf x_v$ from $U$ to $\mathbb R$ are denoted $E$, $F$ and $G$ respectively, and are called the \textbf{coefficients of the first fundamental form}.}\\

\noindent These coefficients give us the immediate formula for the dot product of tangent vectors:
\begin{equation}\tag{*}(a_1\mathbf x_u+a_2\mathbf x_v)\cdot(b_1\mathbf x_u+b_2\mathbf x_v)=Ea_1b_1+F(a_1b_2+a_2b_1)+Ga_2b_2\end{equation}
Several observations are in order.

(1) If $E,F,G$ are the coefficients of the first fundamental form, then $E>0$ and $EG-F^2>0$.  The first inequality is clear, because $E=\mathbf x_u\cdot\mathbf x_u=\|\mathbf x_u\|^2>0$ (strict because, by regularity, $\mathbf x_u$ cannot be zero).  As for the second inequality, we recall [Exercise 4(h) of Section 2.5] that for $\vec v,\vec w\in\mathbb R^3$, $\|\vec v\|^2\|\vec w\|^2=(\vec v\cdot\vec w)^2+\|\vec v\times\vec w\|^2$.  In particular, it follows that
$$EG-F^2=\|\mathbf x_u\|^2\|\mathbf x_v\|^2-(\mathbf x_u\cdot\mathbf x_v)^2=\|\mathbf x_u\times\mathbf x_v\|^2>0;$$
again the inequality is strict because $\mathbf x_u,\mathbf x_v$ are linearly independent.

These can alternatively be viewed by observing that for $p\in U$,
$$(d\mathbf x_p)^T(d\mathbf x_p)=\begin{bmatrix}E&F\\F&G\end{bmatrix}(p)$$
since the product clearly consists of dot products of $d\mathbf x_p$'s columns.  Moreover, the symmetric matrix $\begin{bmatrix}E&F\\F&G\end{bmatrix}$ must be positive definite by Exercise 8 of Section 3.5, and by Sylvester's Criterion this is tantamount to saying $E>0,EG-F^2>0$.

(2) $E,F,G$ depend on the parametrization.  For instance, if the $xy$-plane is parametrized via $\mathbf x(u,v)=(u,v,0)$ then $E=G=1$ and $F=0$ (constant functions).  However, if we parametrize the same plane by $\mathbf x(u,v)=(2u+v,3v,0)$, we have $E=4$, $F=2$ and $G=10$.  [Because $\mathbf x_u=(2,0,0)$ and $\mathbf x_v=(1,3,0)$.]

If we use polar coordinates, $\mathbf x(u,v)=(u\cos v,u\sin v,0)$ for $u>0$, then the situation is different yet: $\mathbf x_u=(\cos v,\sin v,0)$ and $\mathbf x_v=(-u\sin v,u\cos v,0)$, from which we derive $E=1$, $F=0$ and $G=u^2$.  So in this case, $G$ is not even constant.

The important point is that the coefficients can be used to derive things which don't depend on the parametrization; this will be seen in sections 6.6-6.9.

(3) There are many concepts that can be defined directly from the a local coordinate chart and the first fundamental form, and this chapter will cover some of them.  These concepts will have meaning in Section 6.10, when only the chart and the form are given.  Mathematicians call this \emph{intrinsic geometry}.\\

\noindent Let us work out some important examples.

Suppose $U\subset\mathbb R^2$ is open and $f:U\to\mathbb R$ is differentiable.  Then we recall that $\mathbf x(u,v)=(u,v,f(u,v))$ is a parametrization for a regular surface.  Moreover, $\mathbf x_u=(1,0,f_u)$ and $\mathbf x_v=(0,1,f_v)$, where $f_u,f_v$ are the partial derivatives of $f$.  We have the first fundamental form coefficients by direct computation:
$$E=1+f_u^2,~~~~F=f_uf_v,~~~~G=1+f_v^2.$$
Then $EG-F^2=1+f_u^2+f_v^2=1+\|\vec\nabla f\|^2$ (where $\vec\nabla f$ is the gradient of $f$).  Hence $E$ and $EG-F^2$ are both positive, as previously stated.

(4) Similarly, suppose $\alpha(t)=(f(t),g(t))$ is an embedded regular curve with $f(t)>0$.  Then we have the surface of revolution parametrized by
$$\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u)),$$
and we compute
$$\mathbf x_u=(f'(u)\cos v,f'(u)\sin v,g'(u))$$
$$\mathbf x_v=(-f(u)\sin v,f(u)\cos v,0)$$
$$\therefore E=f'(u)^2+g'(u)^2,~~~~F=0,~~~~G=f(u)^2$$
If we assume $\alpha$ is an arc length parametrization (i.e., $(f')^2+(g')^2=1$), then $E$ will be $1$, and $G$ will still be $f(u)^2$.\\

\noindent It usually helps to take the dot product of a tangent vector with itself.  After all, this is essentially the only kind of thing a quadratic form involves, and by integration it also provides a formula for the lengths of curves.  It even determines the rest of the dot products (by the identity $\vec v\cdot\vec w=\frac 12(\|\vec v+\vec w\|^2-\|\vec v\|^2-\|\vec w\|^2)$, so we may focus on the quadratic form.

By (*), the dot product of a vector $a_1\mathbf x_u+a_2\mathbf x_v$ (tangent at a certain point) with itself is equal to
$$\|a_1\mathbf x_u+a_2\mathbf x_v\|^2=Ea_1^2+2Fa_1a_2+Ga_2^2.$$
This quadratic form is called the \textbf{first fundamental form}, and the expression will henceforth be denoted as $I_p(a_1\mathbf x_u+a_2\mathbf x_v)$ [or $I_p((a_1,a_2))$ if it is clear the point is coming from $U$].  The I is a Roman numeral 1.

Suppose $\alpha:I\to S$ is a curve, say $\alpha(t)=\mathbf x(u(t),v(t))$.  Then its length is given by $\int_I\|\alpha'(t)\|\,dt$.  Now, by the Chain Rule, $\alpha'(t)=u'(t)\mathbf x_u+v'(t)\mathbf x_v$, hence the above formula entails
$$\|\alpha'(t)\|^2=E(u')^2+2Fu'v'+G(v')^2.$$
Thus the length of the curve can be readily computed via
$$L(\alpha)=\int_I\sqrt{E(u')^2+2Fu'v'+G(v')^2}\,dt.$$
If $s$ is the arc length parameter, then we have $\frac{ds}{dt}=\|\alpha'(t)\|$, and so
$$\frac{ds}{dt}=\sqrt{E(u')^2+2Fu'v'+G(v')^2},$$
which may be rewritten as
$$\left(\frac{ds}{dt}\right)^2=E\left(\frac{du}{dt}\right)^2+2F\frac{du}{dt}\frac{dv}{dt}+G\left(\frac{dv}{dt}\right)^2.$$
Mathematicians then use a notational trick, of multiplying the equation by $(dt)^2$ throughout to eliminate denominators.  Thus, they would write the first fundamental form in this fashion:
$$ds^2=E\,du^2+2F\,du\,dv+G\,dv^2.$$
Though it is not precise semantics, it is a useful way to remember the first fundamental form of a parametrization.

As the first fundamental form simplifies the finding of dot products, it simplifies the finding of angles.  We recall that if $\vec v,\vec w$ are nonzero vectors, then the angle in between them is $\cos^{-1}\frac{\vec v\cdot\vec w}{\|\vec v\|\|\vec w\|}$.  Thus, using (*),
\begin{center}
The angle between $a_1\mathbf x_u+a_2\mathbf x_v$ and $b_1\mathbf x_u+b_2\mathbf x_v$ is equal to
\end{center}
$$\cos^{-1}\frac{Ea_1b_1+F(a_1b_2+a_2b_1)+Ga_2b_2}{\sqrt{Ea_1^2+2Fa_1a_2+Ga_2^2}\sqrt{Eb_1^2+2Fb_1b_2+Gb_2^2}}.$$
To illustrate this principle, we shall do two things with the sphere, which we recall is (mostly) parametrized via
$$\mathbf x(u,v)=(\sin u\cos v,\sin u\sin v,\cos u),0<u<\pi,0<v<2\pi.$$
Taking $f(u)=\sin u,g(u)=\cos u$ in example (4) above (the surface of revolution) entails
\begin{equation}\tag{*}E=1,~~~~F=0,~~~~G=\sin^2u,\end{equation}
or what is the same thing,
$$ds^2=du^2+\sin^2u\,dv^2.$$
We fix $0<u_0<\pi$ and find the length of the latitude $\alpha(t)=\mathbf x(u_0,t),0<t<2\pi$.  This is an easy integration now that we have that metric; note that $u(t)=u_0$ and $v(t)=t$, so that $u'=0$ and $v'=1$.
$$L(\alpha)=\int_0^{2\pi}\sqrt{(u')^2+\sin^2u\,(v')^2}\,dt=\int_0^{2\pi}\sqrt{0+\sin^2u\cdot 1}\,dt=\int_0^{2\pi}\sqrt{\sin^2u}\,dt$$
$$=\int_0^{2\pi}\sin u\,dt=\int_0^{2\pi}\sin u_0\,dt=2\pi\sin u_0.$$
This is maximized at $2\pi$ when $u_0=\pi/2$; in this case the latitude is the equator.

Likewise, one can let fix $v_0$ and find the length of the meridian $\alpha_1(t)=\mathbf x(t,v_0),0<t<\pi$.  This results in
$$L(\alpha_1)=\int_0^\pi\sqrt{(u')^2+\sin^2u\,(v')^2}\,dt=\int_0^\pi\sqrt{1+\sin^2u\cdot 0}\,dt=\int_0^\pi 1\,dt=\pi,$$
and surely one can tell that all meridians have the same length.

Now we will demonstrate an example of what the angle formula can find for us.  Let $0<\beta<\pi/2$ be a fixed angle.  A \textbf{loxodrome} (or \textbf{rhumb line}) of a sphere is a curve meeting all of the meridians at the constant angle $\beta$, as shown below.
\begin{center}
\includegraphics[scale=.5]{loxodromie_1.png} %https://www.mathcurve.com/courbes3d.gb/loxodromie/sphereloxodromie.shtml
\end{center}
Note that a loxodrome never meets the north pole, and in fact spins around it infinitely many times while converging there.  After all, since the loxodrome makes a constant angle with the meridian, it can never be tangent to the meridian unless they coincide; and any curve at the north pole is tangent to a meridian no matter which way it goes.

We wish to find a curve $\alpha:I\to S^2$ which meets every meridian at an angle of $\beta$.  To do this, we first extend $\mathbf x$ as follows:
$$\mathbf x(u,v)=(\sin u\cos v,\sin u\sin v,\cos u)\text{ for }0<u<\pi,v\in\mathbb R$$
In other words, we let the revolution parameter $v$ go throughout the real number line.  Now this function is not injective, yet it still restricts to local coordinate charts at all points in its domain, so it makes sense to talk about $E,F,G$ applied to a point $(u,v),0<u<\pi$.

We then write $\alpha(t)=\mathbf x(u(t),v(t))$ for some differentiable functions $u,v$ such that $0<u<\pi$.  Then at each $t=t_0$, we want the angle between the curve and the meridian going through the point to be $\beta$.  To find the equation, we let $u_0=u(t_0)$, $v_0=v(t_0)$; the meridian $v=v_0$ can be parametrized via
$$\mu(t)=\mathbf x(t,v_0)\text{ for }0<t<\pi,$$
From this it is clear that at $(u_0,v_0)$, $\mu'(u_0)=\mathbf x_u$, and $\alpha'(t_0)=u'\mathbf x_u+v'\mathbf x_v$.  The angle between these tangent vectors, which is supposed to be $\beta$, is
$$\cos^{-1}\frac{\mu'\cdot\alpha'}{\|\mu'\|\|\alpha'\|}=\cos^{-1}\frac{\mathbf x_u\cdot(u'\mathbf x_u+v'\mathbf x_v)}{\|\mathbf x_u\|\|u'\mathbf x_u+v'\mathbf x_v\|}=\cos^{-1}\frac{Eu'+Fv'}{\sqrt E\sqrt{E(u')^2+2Fu'v'+G(v')^2}}$$
$$=\cos^{-1}\frac{u'}{\sqrt{(u')^2+\sin^2u(v')^2}},$$
using the formulas (*) for the first fundamental form coefficients.

Of course, it is very hard to establish any specifics about $\alpha$, because the parametrization is arbitrary; any re-parametrization of $\alpha$ will also meet the meridians at the same angles that $\alpha$ does.  For our sake we shall assume $\alpha$ is parametrized with respect to the latitude variable $v$ (i.e., $v(t)=t$); your intuition should tell you that this is feasible.

Under said assumption, $v'=1$, and hence $\beta=\cos^{-1}\frac{u'}{\sqrt{(u')^2+\sin^2u}}$.  We then arrange our work to find $u$ as follows:

(1) Clearly $\cos\beta=\frac{u'}{\sqrt{(u')^2+\sin^2u}}$.

(2) Squaring both sides, $\cos^2\beta=\frac{(u')^2}{(u')^2+\sin^2u}$.

(3) Furthermore, $1+\tan^2\beta=\sec^2\beta=\frac 1{\cos^2\beta}=\frac{(u')^2+\sin^2u}{(u')^2}=1+\frac{\sin^2u}{(u')^2}$.  Moreover, $\tan^2\beta=\frac{\sin^2u}{(u')^2}$.

(4) By assuming $\alpha$ goes in the upward direction, we may assume $u'>0$.  Then since $0<\beta<\pi$ and $0<u<2\pi$, we may take square roots of (3) to get $\tan\beta=\frac{\sin u}{u'}$; our hypotheses imply both sides of this are positive.

(5) By basic algebraic manipulation, $u'=\frac{\sin u}{\tan\beta}=\sin u\cot\beta$.

(6) This differential equation may be solved using separation of variables.  After all, it can be rewritten as $u'\csc u=\cot\beta$, and to integrate both sides, we take the left-hand side to be an antiderivative of $\csc u$ with respect to $u$.  Well, $\ln(\tan(u/2))$ is such an antiderivative, as one can check using familiar differential laws.  Hence integrating both sides (with respect to $t$) lands us with $\ln(\tan(u/2))=t\cot\beta+A$ for some $A$.  Consequently,
$$u=2\tan^{-1}(e^{t\cot\beta+A}),$$
parametrizing the loxodrome as $\alpha(t)=\mathbf x(2\tan^{-1}(e^{t\cot\beta+A}),t)$.  Or, it could just be easier to think of it as $v\cot\beta+A=\ln(\tan(u/2))$.

In Section 6.4, we will cover \textbf{Mercator's projection}, which is a projection of the sphere that passes loxodromes to straight lines.  It is useful for geographers, since the compass directions are constant throughout the map and angle measures at them coincide.\\

\noindent\textbf{AREAS OF REGIONS ON REGULAR SURFACES}\\

\noindent Just like arc length, computing the area of a region on a surface is not hard when you have the first fundamental form.  To illustrate this point, let $\mathbf x:U\to S$ be a local coordinate chart with $U$ bounded, and $E,F,G$ its first fundamental form coefficients.

In calculus, it is learned that the area under a graph $y=f(x)$ can be approximated by thin rectangles of the form $[x_0,x_0+\varepsilon]\times[0,f(x_0)]$.  Similar things apply to multivariable calculus.  And we can do something likewise with $\mathbf x$: if $U$ is divided into miniature squares of side length $\varepsilon$, then $\mathbf x$ sends them to miniature ``quadrilaterals,'' which approach the parallelograms spanned by $\mathbf x_u,\mathbf x_v$ as the squares become infinitesimal.  We can multiply each parallelogram's area by $\varepsilon^2$ (the area of the squares in $U$), and add them together to approximate the surface area.

Taking the limit as the number of squares in the division approaches infinity, we conclude that integrating (with respect to $u$ and $v$) the area of the parallelogram spanned by $\mathbf x_u,\mathbf x_v$ gives the surface area of $\mathbf x(U)$.  By Exercise 4(i) of Section 2.6, this area is $\|\mathbf x_u\times\mathbf x_v\|$.  Furthermore, $\|\mathbf x_u\times\mathbf x_v\|=\sqrt{EG-F^2}$, as shown in observation (1) following the definition of the first fundamental form.

Thus we have proven\\

\noindent\textbf{Proposition 6.4.} \textsc{(Area from first fundamental form)} \emph{Let $S$ be a regular surface, and $\mathbf x:U\to S$ a local coordinate chart with $U$ bounded.  Then the (surface) area of $\mathbf x(U)$ is equal to}
$$\int_U\sqrt{EG-F^2}\,du\,dv.$$

\noindent For example, taking $\mathbf x(u,v)=(u,v,f(u,v))$ (graph of $z=f(x,y)$), we recall that [Example 3 above] $EG-F^2=1+f_u^2+f_v^2=1+\|\vec\nabla f\|^2$.  Hence the surface area is obtained by integrating $\sqrt{1+\|\vec\nabla f\|^2}$ over the input-variable region. % I don't usually indent the paragraph right after a Proposition?

As a final illustration, we shall find the surface area of the sphere.  We recall the standard parametrization
$$\mathbf x(u,v)=(\sin u\cos v,\sin u\sin v,\cos u),0<u<\pi,0<v<2\pi$$
and the first fundamental form, $E=1,F=0,G=\sin^2u$.
This parametrization has no overlaps.  It does leave some points missing, but those points form a one-dimensional line segment which clearly has a surface area of zero.  Thus we may merely apply Proposition 6.4 to $U=(0,\pi)\times(0,2\pi)$, in order to get the surface area of the sphere:
$$\sqrt{EG-F^2}=\sqrt{1\sin^2u-0^2}=\sqrt{\sin^2u}=\sin u$$
$$\therefore\int_U\sqrt{EG-F^2}\,du\,dv=\int_0^{2\pi}\int_0^\pi\sin u\,du\,dv=\int_0^{2\pi}\left(\int_0^\pi\sin u\,du\right)\,dv.$$
$$\int_0^\pi\sin u\,du=-\cos u\big|_0^\pi=-\cos\pi-(-\cos 0)=-(-1)-(-1)=2$$
$$\therefore\int_U\sqrt{EG-F^2}\,du\,dv=\int_0^{2\pi}2\,dv=2v\big|_0^{2\pi}=4\pi.$$
It is clear that one can adapt the same argument to find the surface area of a sphere with arbitrary radius.  If $R>0$ is fixed, then
$$\mathbf x(u,v)=(R\sin u\cos v,R\sin u\sin v,R\cos u)$$
($0<u<\pi,0<v<2\pi$) parametrizes a sphere of radius $R$, and its first fundamental form coefficients are $E=R^2,F=0,G=R^2\sin^2u$.  From this one derives the surface area to be $4\pi R^2$.

\subsection*{Exercises 6.3. (The First Fundamental Form.  Length and Area)} % Explain that we want to be able to do metric things
% using coordinate charts and not the ambient 3-space.  Then bring up the FFF, and use it to establish length, area, angles...
% Also, establish the formula for loxodromes on the sphere.
% POTENTIAL EXERCISES: Similar exercises for the cylinder and cone;  surface; Pseudosphere; Use the notion of area to show LAEAP is area-preserving.
\begin{enumerate}
\item Find the first fundamental form for each of the following surface parametrizations:

(a) $\mathbf x(u,v)=(3u,u+2v,2)$

(b) $\mathbf x(u,v)=(u\cos v,u\sin v,v)$

(c) $\mathbf x(u,v)=(u,v,u^2+\sin v)$

(d) $\mathbf x(u,v)=(\cosh u\cos v,\cosh u\sin v,\sinh u)$

\item Let $\mathbf x:U\to S$ be a local coordinate chart, $p\in U$ and let $E,F,G$ be the coefficients of the first fundamental form.  Show that the following are equivalent:

~~~~(i) $\mathbf x$ is conformal at $p$, i.e., for all $\vec v,\vec w\in\mathbb R^2$ tangent to $p$, the angle between $d\mathbf x_p(\vec v)$ and $d\mathbf x_p(\vec w)$ equals the angle between $\vec v$ and $\vec w$;

~~~~(ii) $E(p)=G(p)$ and $F(p)=0$.

[Recall that $d\mathbf x_p((a_1,a_2))=a_1\mathbf x_u+a_2\mathbf x_v$.]

Universally quantifying these statements over all $p\in U$, we get that $\mathbf x$ is conformal $\iff$ $E=G$ and $F=0$ on $U$.  In this case $\mathbf x$ is said to give \textbf{isothermal} (or \textbf{conformal}) \textbf{coordinates}.

\item If $\mathbf x:U\to S$ is a local coordinate chart, show that the following are equivalent:\\

~~~~(i) $\mathbf x$ preserves curve lengths: whenever $\alpha:I\to U$ is a curve in $U$, $\alpha$ and $\mathbf x\circ\alpha$ (in $S$) have the same length;

~~~~(ii) $\mathbf x$ preserves dot products: for $p\in U$ and $\vec v,\vec w$ tangent to $p$, $\mathbf dx_p(\vec v)\cdot\mathbf dx_p(\vec w)=\vec v\cdot\vec w$;

~~~~(iii) $E=G=1$ and $F=0$.

In this case, $\mathbf x$ is said to give \textbf{isometric coordinates}.

\item If $\mathbf x:U\to S$, then the \textbf{coordinate curves} are the curves of the form $u\mapsto\mathbf x(u,v_0)$ and $v\mapsto\mathbf x(u_0,v)$ for $u_0,v_0\in U$ fixed.  Note that these are two families of curves; each family consists of curves where any two are disjoint, but each curve in one family typically intersects each curve in the other.

The parametrization is said to be a \textbf{Chebyshev net} if any quadrilateral made up of coordinate curves has opposite sides of the same length.  Show that this holds if and only if $\frac{\partial E}{\partial v}=0=\frac{\partial G}{\partial u}$.

\item\emph{(A line segment is the shortest path between points in $\mathbb R^n$.)} \---- Let $\vec p,\vec q\in\mathbb R^n$ be distinct points.  Then our ambition is to show that line segment
$$\alpha(t)=(1-t)\vec p+t\vec q,~~~~0\leqslant t\leqslant 1$$
has the shortest length out of all paths from $\vec p$ to $\vec q$.

(a) Show that the length of the segment $\alpha$ is $\|\vec q-\vec p\|$.  Do not apply any isometries; do this directly using the integral.

Now we take an arbitrary path $\alpha:I\to\mathbb R^n$ from $\vec p$ to $\vec q$, and show that its length is $\geqslant\|\vec q-\vec p\|$.  We may assume $\alpha:[0,1]\to\mathbb R^n$, with $\alpha(0)=\vec p,\alpha(1)=\vec q$.

(b) Show that if $\vec v$ is a constant unit vector ($\|\vec v\|=1$), then
$$\int_0^1\alpha'(t)\cdot\vec v\,dt\leqslant\int_0^1\|\alpha'(t)\|.$$
[You may assume that integration preserves ordering, i.e., $f\leqslant g$ implies $\int f\leqslant\int g$.]

(c) Show that $\int_0^1\alpha'(t)\cdot\vec v\,dt=(\vec q-\vec p)\cdot\vec v$.  [Explain why the dotting with $\vec v$ may be moved outside the integral.  Then use the fundamental theorem of calculus.]  Hence, by part (b), $(\vec q-\vec p)\cdot\vec v$ is less than or equal to $\int_0^1\|\alpha'(t)\|$, the length of $\alpha$.

(d) Now conclude by taking $\vec v=\frac{\vec q-\vec p}{\|\vec q-\vec p\|}$.

\item Find the first fundamental form for each of the following surfaces: % Parametrize cylinder, cone, Enneper's surface, and pseudosphere, and ask for FFF coefficients

(a) Cylinder: $\mathbf x(u,v)=(\cos v,\sin v,u)$.

(b) Cone: $\mathbf x(u,v)=(u\cos v,u\sin v,u)$ with $u>0$.

(c) \emph{Enneper's surface}: $\mathbf x(u,v)=\left(u-\frac 13u^3+uv^2,v-\frac 13v^3+vu^2,u^2-v^2\right)$.  [Once you find the coefficients, see what is implied by Exercise 2.]

(d) \emph{Pseudosphere}: $\mathbf x(u,v)=(\sin u\cos v,\sin u\sin v,\cos u+\ln(\tan(u/2)))$ with $\pi/2<u<\pi$.  Note that this is the surface of revolution given by the tractrix (Exercise 18 of Section 6.1). % Loxodrome example?  But the loxodrome is a curve, not a surface.

\item (a) Let $S$ be the cylinder $x^2+y^2=1$, parametrized via $\mathbf x(u,v)=(\cos v,\sin v,u)$.  Consider the ``meridian lines'' $u\mapsto\mathbf x(u,v_0)$.  Show that any curve on $S$ which makes a constant angle with all the meridian lines is either a helix or a ``latitude'' circle.

(b) Now let $C$ be the cone $z^2=x^2+y^2$ ($z>0$), parametrized via $\mathbf x(u,v)=(u\cos v,u\sin v,u)$ with $u>0$.  Consider the lines $u\mapsto\mathbf x(u,v_0)$.  If a curve on $C$ makes a constant angle of $\beta$ with all those lines, show that the curve has an equation of the form $u=Ae^{v\cot\beta/\sqrt 2}$ for some $A$.

\item The aim of this exercise is to show that the Lambert azimuthal equal-area projection of the sphere [Exercise 6 of Section 5.1] preserves area, hence is true to its name.

(a) If $\mathbf x:U\to S$ is a local coordinate chart of a surface, then $\mathbf x$ preserves areas of two-dimensional regions if and only if $EG-F^2=1$, where $E,F,G$ are the first fundamental form coefficients. [Recall how area is computed directly from the coordinates.]

(b) Recall [Exercise 6(b) of Section 5.1] that Lambert azimuthal equal-area projection is given by this parametrization of $S^2$:
$$\mathbf x(u,v)=\left(\sqrt{1-\frac{u^2+v^2}4}u,\sqrt{1-\frac{u^2+v^2}4}v,\frac{u^2+v^2}2-1\right),~~~~u^2+v^2<4.$$
Compute the first fundamental form coefficients for this parametrization, and use part (a) to conclude that area is preserved.

\item\emph{(Surface area of a surface of revolution.)} \---- Let $\alpha(s)=(f(s),g(s))$ be an embedded regular curve parametrized by arc length [$(f')^2+(g')^2=1$] and such that $f>0$.  Let $S$ be the surface of revolution generated by $\alpha$,
$$\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$$
Recall that the first fundamental form coefficients are $E=1,F=0,G=f(u)^2$.  Use this to show that the surface area of $\alpha$ \---- all around the $z$-axis for $s\in J$ \---- is equal to
$$\int_J 2\pi f(s)\,ds.$$

\item (a) Let $A$ be an $n\times m$ matrix.  If $A$ has linearly independent columns, show that $A$ admits a unique factorization $A=QR$ where $Q$ is an $n\times m$ matrix with orthonormal columns, and $R$ is an $m\times m$ upper triangular matrix with positive diagonal entries.  [Use the Gram-Schmidt process to turn $A$'s columns into an orthonormal basis.]

(b) If $A=\begin{bmatrix}\uparrow&\dots&\uparrow\\\vec v_1&\dots&\vec v_m\\\downarrow&\dots&\downarrow\end{bmatrix}$ is an $n\times m$ matrix, let $P$ be the parallelepiped with $A$'s columns as the side lengths; this is
$$P=\{c_1\vec v_1+c_2\vec v_2+\dots+c_m\vec v_m:c_1,c_2,\dots,c_m\in[0,1]\}.$$
Then the $m$-dimensional volume of this parallelepiped is $\sqrt{\det(A^TA)}$.  [If $A$ has linearly dependent columns, it should be clear that $A^TA$ is singular and the parallelepiped is contained in an $(m-1)$-plane, hence both values are zero.  Thus, assume $A$ has linearly independent columns.  Then write the factorization $A=QR$ of part (a); show that $A^TA=R^TR$ and $\sqrt{\det(A^TA)}=\det R$.  Since $R$ is upper triangular, $\det R$ is the product of its diagonal entries.  Now use the familiar properties of volume (rescaling in one dimension results in the volume being multiplied by that amount, and linear shearing does not change the volume).]

\item Fix real numbers $a>b>0$.  The \textbf{torus} is obtained by taking a circle of radius $b$ contained in the $xz$-plane, with center $(a,0,0)$, then revolving it around the $z$-axis.  In other words, it is the surface of revolution with generating curve $\alpha(t)=(a+b\cos t,b\sin t)$, so that it has the parametrization
$$\mathbf x(u,v)=((a+b\cos u)\cos v,(a+b\cos u)\sin v,b\sin u)\text{ for }0<u<2\pi,0<v<2\pi.$$
Find the first fundamental form of the torus, and use it to show that its surface area is $4\pi^2ab$.

Does it make sense that the answer is $(2\pi a)(2\pi b)$?

\item This exercise generalizes the results of this chapter to higher dimensions.  Recall [Exercise 5 of the previous section] that a subset $S\subset\mathbb R^n$ is a regular manifold of dimension $m$, if for each $p\in S$ there exist open sets $U\subset\mathbb R^m,p\in W\subset\mathbb R^n$ along with a differentiable map $\mathbf x:U\to W\to S$, such that the differential has rank $m$ everywhere and $\mathbf x$ has a continuous inverse.

If $\mathbf x$ is such a local coordinate chart, the \textbf{first fundamental form coefficients} refer to the scalar functions $F_{ij}=\mathbf x_{u_i}\cdot\mathbf x_{u_j}$ for $1\leqslant i,j\leqslant m$.  Note that the symmetry of the dot product entails $F_{ij}=F_{ji}$, hence there are really $\frac{m(m+1)}2$ such coefficients (for $1\leqslant i\leqslant j\leqslant m$).  It is worth observing that in the case $n=3,m=2$ covered in the section, we used $E=F_{11},F=F_{12},G=F_{22}$. % Do Carmo uses g_{ij} for F_{ij} but I personally think that's a bit dumb

(a) The symmetric matrix $F=\begin{bmatrix}F_{ij}\end{bmatrix}_{1\leqslant i,j\leqslant m}$ is positive definite at every point of $U$.  [Use Exercise 8 of Section 3.5.]

(b) If $\mathbf x$ is a bounded coordinate chart, the $m$-dimensional volume of $\mathbf x(U)$ is
$$\int_U\sqrt{\det F}\,du_1\dots du_n.$$
[Approximate the volume using parallelepipeds.  Then use Exercise 10(b), noting that $F=(d\mathbf x)^T(d\mathbf x)$.]

(c) Suppose $S$ is a regular manifold of dimension $m$.  If $m'\leqslant m$, $V\subset\mathbb R^{m'}$ and $\alpha:V\to S$ is a differentiable function.  Assume $\alpha$ factors as $\mathbf x\circ\beta$ with $\beta:V\to U$ differentiable.  Show that $\alpha$'s differential is injective everywhere if and only if $\beta$'s is.

(d) Moreover, the $m'$-dimensional volume of $\alpha(V)$ is equal to
$$\int_V\sqrt{\det((d\beta)^TF(d\beta))}\,dv_1\dots dv_{m'}.$$
[Apply part (b) to $\alpha$ directly, as giving an $m'$-dimensional manifold.]  In particular, taking $m'=1$, the length of a curve $\alpha:I\to S$ is equal to $\int_I\sqrt{\beta'(t)\cdot F\beta'(t)}\,dt$, where $\beta:I\to U$ is vector-valued in the single variable $t$.

(e) Find the three-dimensional volume of the 3-sphere $\{(x,y,z,w)\in\mathbb R^4:x^2+y^2+z^2+w^2=1\}$.  [Parametrize it via
$$\mathbf x(u_1,u_2,u_3)=(\sin u_1\sin u_2\cos u_3,\sin u_1\sin u_2\sin u_3,\sin u_1\cos u_2,\cos u_1)$$
with $0<u_1<\pi,0<u_2<\pi,0<u_3<2\pi$.] % It's 2\pi^2, by the way.  [The (unit) n-sphere's rim volume is a rational number times \pi^{\lceil n/2\rceil}.  Since the n-sphere's interior volume is exactly 1/(n+1) of that, it's also a rational number times \pi^{\lceil n/2\rceil}]

(f) Assume $m=n-1$ and $\mathbf x(u_1,\dots,u_{n-1})=(u_1,\dots,u_{n-1},f(u_1,\dots,u_{n-1}))$ is a graph of an explicit function.  Then $F_{ij}=\delta_{ij}+f_{u_i}f_{u_j}$,\footnote{We recall that the Kronecker delta $\delta_{ij}$ is equal to $1$ if $i=j$ and $0$ otherwise.} and $\det F=1+f_{u_1}^2+\dots+f_{u_{n-1}}^2=1+\|\vec\nabla f\|^2$.  [To find $\det F$, first note that $F=I_{n-1}+(\vec\nabla f)(\vec\nabla f)^T$ where $\vec\nabla f$ is considered as a column vector.  Clearly $(\vec\nabla f)(\vec\nabla f)^T$ has rank $1$ (unless $\vec\nabla f=0$, in which case it is zero), so that $n-2$ of its eigenvalues are zero.  The remaining eigenvalue is the sum of the eigenvalues, which is the trace, which is $\|\vec\nabla f\|^2$ (why?).  Consequently, $(\vec\nabla f)(\vec\nabla f)^T$ has characteristic polynomial $x^{n-1}-\|\vec\nabla f\|^2x^{n-2}$.  From here it should be easy to find the determinant of $F$.]
\end{enumerate}

\subsection*{6.4. Isometries and Conformal Mappings}
\addcontentsline{toc}{section}{6.4. Isometries and Conformal Mappings}
We recall (from earlier chapters) that the best way to study intrinsics of geometry in a plane is to understand the isometries of the plane.  This is not exactly so for differential geometry, but it would still help to know how isometries are defined.

By an isometry is meant a map which preserves distances.  However, we cannot directly transfer this definition because we do not yet have a notion of distance on a surface, and even if we did, it would deviate from the purpose of this chapter.  The lengths of curves and areas of regions were computed using calculus and infinitesimal data.  Thus we want a definition of isometries in terms of infinitesimal data, rather than just defining it to be a map preserving a distance metric $S\times S\to\mathbb R_{\geqslant 0}$. % Yep, Chapter 6's purpose is to study geometry with differential metrics, after the previous chapters used purely algebra. ^^

The ``infinitesimal piece of data'' giving the length of a curve (provided the curve is parametrized) is the magnitude of its derivative vector, as we recall that the length of $\alpha:I\to S$ is equal to $\int_I\|\alpha'(t)\|\,dt$.  Thus our desire is for isometries to preserve the magnitudes of tangent vectors.  This is a surprisingly easy property to deal with.\\

\noindent\textbf{Proposition 6.5 and Definition.} \emph{Let $S_1$ and $S_2$ be regular surfaces, and $\varphi:S_1\to S_2$ a bijective smooth map.  Then the following are equivalent:}

(i) \emph{$\varphi$ preserves dot products; for all $p\in S_1$ and $\vec v,\vec w\in T_pS_1$, we have $d\varphi_p(\vec v)\cdot d\varphi_p(\vec w)=\vec v\cdot\vec w$.}

(ii) \emph{$\varphi$ preserves lengths of tangent vectors; for all $p\in S_1$ and $\vec v\in T_pS_1$, we have $\|d\varphi_p(\vec v)\|=\|\vec v\|$.}

(iii) \emph{$\varphi$ preserves lengths of curves; if $\alpha:I\to S_1$ is a regular curve, then the length of $\varphi\circ\alpha:I\to S_2$ is equal to the length of $\alpha$.}

\emph{Under these conditions, $\varphi$ is said to be a \textbf{(global) isometry} from $S_1$ to $S_2$.}\\
\begin{proof}
(i) $\implies$ (ii) by taking $\vec v=\vec w$ in (i), since $\|\vec v\|=\sqrt{\vec v\cdot\vec v}$.

(ii) $\implies$ (i): Recall the formula $\vec v\cdot\vec w=\frac 12(\|\vec v+\vec w\|^2-\|\vec v\|^2-\|\vec w\|^2)$.  The argument of Lemma 2.44 (iii) $\implies$ (ii) then applies.

(ii) $\implies$ (iii): By the Chain Rule, $(\varphi\circ\alpha)'(t)=d\varphi_{\alpha(t)}(\alpha'(t))$.  Moreover,
$$\int_I\|(\varphi\circ\alpha)'(t)\|\,dt=\int_I\|d\varphi_{\alpha(t)}(\alpha'(t))\|\,dt=\int_I\|\alpha'(t)\|\,dt$$
(using (ii)), so that $\varphi\circ\alpha$ and $\alpha$ have the same length.

(iii) $\implies$ (ii): Let $\alpha:(-\varepsilon,\varepsilon)\to S_1$ be any smooth curve with $\alpha(0)=p,\alpha'(0)=\vec v$.  Then for any $0\leqslant\delta<\varepsilon$, (iii) [for the curve $\alpha|_{[0,\delta]}$] implies
$$\int_0^\delta\|\alpha'(t)\|\,dt=\int_0^\delta\|(\varphi\circ\alpha)'(t)\|\,dt=\int_0^\delta\|d\varphi_{\alpha(t)}(\alpha'(t))\|\,dt.$$
Differentiating with respect to $\delta$ gives $\|\alpha'(\delta)\|=\|d\varphi_{\alpha(\delta)}(\alpha'(\delta))\|$ by the first fundamental theorem of calculus, and taking $\delta=0$ we get $\|\vec v\|=\|d\varphi_p(\vec v)\|$ as desired.
\end{proof}

\noindent Thus we have a notion of isometries of regular surfaces; it uses smooth maps which preserve lengths of tangent vectors. % , rather than arbitrary maps which preserve a distance metric (in the sense $S\times S\to\mathbb R_{\geqslant 0}$). (moved before)

Several examples are in order.  For instance, if $S_1$ is the strip $z=0,-\pi<x<\pi$ (which can be parametrized via $\mathbf x(u,v)=(u,v,0),u\in(-\pi,\pi),v\in\mathbb R$), and $S_2$ is the portion of the cylinder $x^2+y^2=1$ which omits the meridian $x=-1$, then the map $\varphi:S_1\to S_2$ given by
$$\varphi(u,v,0)=(\cos u,\sin u,v)$$
is an isometry.  After all, it is readily seen to be a smooth bijection, and for $p=(u,v,0)\in S_1$ and $\vec v\in\mathbb R^2=T_pS_1$, say $\vec v=(a,b)$, we have
$$d\varphi_p(\vec v)=\begin{bmatrix}-\sin u&0\\\cos u&0\\0&1\end{bmatrix}(\vec v)=(-a\sin u,a\cos u,b),$$
so that $d\varphi_p(\vec v)$ and $\vec v$ both have magnitude $\sqrt{a^2+b^2}$.

On the other hand, suppose $S$ is an arbitrary surface of revolution; i.e.,
$$\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$$
where $\alpha(t)=(f(t),g(t))$ is an embedded regular curve with $f(t)>0$.  Then all rotations of $S$ around the $z$-axis are global isometries from $S$ to itself; we leave it to the reader to work out the details.\\

\noindent We recall (Section 2.6) that every isometry of $\mathbb R^n$ (in the metric preserving sense) is the composition of an orthogonal matrix and a translation.  The same is true for the differential definition of an isometry of $\mathbb R^n$.  We will prove the case $n=2$ by consideration of the $xy$-plane, henceforth to be denoted $P$. % Recall Section 2.6.  Prove that the *new* kind of isometries of the plane satisfies Proposition 2.46.  Exercise 1 will do this for the cylinder and sphere; the last exercise of the section will generalize to higher dimensions.
% You say "in the distance metric preserving sense" should be "in the metric preserving sense", though Sagun Chanillo also used the word "metric" for the differential kind

Let $\varphi:P\to P$ be an isometry.  Then $\varphi$ can be regarded as a smooth bijection from $\mathbb R^2$ to itself.  Every $d\varphi_p$ for $p\in\mathbb R^2$ is a $2\times 2$ matrix which preserves dot products (by definition of an isometry); hence Lemma 2.44 implies $d\varphi_p\in O(2)$.  Thus $\varphi$ has an orthogonal differential at every point.  However, we do not yet know if its differential is the \emph{same} at every point.

To diagnose this, we first note that
$$d\varphi_p=\begin{bmatrix}\uparrow&\uparrow\\\varphi_x&\varphi_y\\\downarrow&\downarrow\end{bmatrix}(p),$$
and since this is orthogonal, we have $\varphi_x\cdot\varphi_x=1,\varphi_x\cdot\varphi_y=0,\varphi_y\cdot\varphi_y=1$ throughout the domain plane $\mathbb R^2$.  Taking partial derivatives of these equations with respect to $x$ and $y$,
$$\varphi_x\cdot\varphi_{xx}=0,~~~~\varphi_x\cdot\varphi_{xy}=0$$
$$\varphi_{xx}\cdot\varphi_y+\varphi_x\cdot\varphi_{xy}=0,~~~~\varphi_{xy}\cdot\varphi_y+\varphi_x\cdot\varphi_{yy}=0$$
$$\varphi_y\cdot\varphi_{xy}=0,~~~~\varphi_y\cdot\varphi_{yy}=0.$$
[We have made use of Exercise 4(a) of Section 6.1, which implies, e.g., that $\frac{\partial}{\partial x}(\varphi_x\cdot\varphi_y)=\varphi_{xx}\cdot\varphi_y+\varphi_x\cdot\varphi_{xy}$.]  Note also that $\{\varphi_x,\varphi_y\}$ is an orthonormal \emph{basis}; hence $\varphi_x\cdot\vec v=\varphi_y\cdot\vec v=0$ implies $\vec v=\vec 0$ for any vector $\vec v$.  Using this:
\begin{itemize}
\item We have $\varphi_{xy}=0$, because $\varphi_x\cdot\varphi_{xy}$ and $\varphi_y\cdot\varphi_{xy}$ are both zero.

\item We may thus eliminate the terms involving $\varphi_{xy}$ from the middle two equations, to get $\varphi_{xx}\cdot\varphi_y=0$ and $\varphi_x\cdot\varphi_{yy}=0$.

\item Consequently, $\varphi_{xx}=\varphi_{yy}=0$ because each one is orthogonal to both $\varphi_x$ and $\varphi_y$.
\end{itemize}
Thus we've proven all of the second-order derivatives $\varphi_{xx},\varphi_{xy},\varphi_{yy}$ to be zero.  This means that $\varphi_x$ and $\varphi_y$ are \emph{constant}.  Hence, $d\varphi_p$ is constant and independent of the point $p$.

We now know that $d\varphi_p$ is a constant orthogonal matrix, say $A=\begin{bmatrix}a&b\\c&d\end{bmatrix}\in O(2)$.  If we write $\varphi(x,y)=(f(x,y),g(x,y))$ with $f,g:\mathbb R^2\to\mathbb R$ smooth, this means that $A=d\varphi_p=\begin{bmatrix}f_x&f_y\\g_x&g_y\end{bmatrix}(p)$; since $f_x$ and $f_y$ are the respective constants $a,b$, we have $f(x,y)=ax+by+j$ for some constant $j\in\mathbb R$.  Similarly, $g(x,y)=cx+dy+k$ for some $k\in\mathbb R$.

From this we conclude at once that $\varphi(\vec x)=A\vec x+\vec v$ with $\vec v=(j,k)$; in other words, $\varphi$ is the composition of an orthogonal matrix and a translation.  The same argument works in higher dimensions, but a little more casework will be called for; See Exercise 8(b).\\

\noindent Sometimes it helps to know that surfaces are isometric in small regions but not necessarily as their whole selves.  If $p\in S_1$ and $q\in S_2$, the surfaces are said to be \textbf{locally isometric} (at $p$ and $q$), if there exist open neighborhoods $p\in U_1\subset S_1,q\in U_2\subset S_2$ and an isometry $\varphi:U_1\to U_2$ such that $\varphi(p)=q$.

For example, the plane $z=0$ is locally isometric to the cylinder $x^2+y^2=1$; if we let $U=\{(u,v,0):-\pi<u<\pi\}$ and $V=\{(x,y,z):x^2+y^2=1,x\ne -1\}$ then the map
$$(u,v,0)\mapsto(\cos u,\sin u,v)$$
is a local isometry; yet it is clear that they are not \emph{globally} isometric.  The plane is also locally isometric to the cone [Exercise 2].

$S_1$ and $S_2$ are said to be \textbf{locally isometric} if for all $p\in S_1$ and $q\in S_2$, the surfaces are locally isometric at $p$ and $q$. % "\forall p,\forall q,locally isometric at p and q" and "\forall p,\exists q,locally isometric at p and q" are both worthy terms.  I think the former's the right definition.  Also, I think you have the definition of f being a local isometry, whereas I was trying to define two surfaces being locally isometric (without a prescribed function).
The fundamental thing about local isometries is that they can be easily read off of the first fundamental form coefficients:\\

\noindent\textbf{Proposition 6.6.} \emph{Let $S_1$ and $S_2$ be regular surfaces, and $p\in S_1,q\in S_2$ points.  Then $S_1$ is locally isometric to $S_2$ at $p$ and $q$, if and only if there exist an open set $0\in U\subset\mathbb R^2$ and local coordinate charts $\mathbf x:U\to S_1,\mathbf y:U\to S_2$ with $\mathbf x(0)=p,\mathbf y(0)=q$, which have the same first fundamental form as functions in $U$.}
\begin{proof}
Suppose $S_1$ is locally isometric to $S_2$ at $p$ and $q$, and that $\varphi:W\to V$ is an isometry of open neighborhoods sending $p\mapsto q$.  Let $\mathbf x:U\to S_1$ be a local coordinate chart sending $0\mapsto p$; by shrinking $U$ to $U\cap\mathbf x^{-1}(W)$, we may assume $\mathbf x(U)\subset W$.  Moreover, let $\mathbf y=\varphi\circ\mathbf x$.  Then $\mathbf y$ has a continuous inverse, since it is a composition of two functions that do.  We have
$$\mathbf y_u=(\varphi\circ\mathbf x)_u=d\varphi(\mathbf x_u),$$
where $d\varphi$ is taken at whatever point it needs to be.  Likewise $\mathbf y_v=d\varphi(\mathbf x_v)$.  From this we get $\mathbf y_u\cdot\mathbf y_u=d\varphi(\mathbf x_u)\cdot d\varphi(\mathbf x_u)=\mathbf x_u\cdot\mathbf x_u$ ($\varphi$ is an isometry), hence $\mathbf x$ and $\mathbf y$ have the same first fundamental coefficient $E$.  Similar arguments show that the other first fundamental form coefficients match, hence we have the desired charts.  [In addition, we have only now confirmed that $\mathbf y$'s differential has rank $2$, because that is equivalent to saying $EG-F^2>0$ and $\mathbf x,\mathbf y$ have that expression the same.]

Conversely, suppose the local coordinate charts $\mathbf x:U\to S_1,\mathbf y:U\to S_2$ exist with the same coefficients $E,F,G$.  Then if $W=\mathbf x(U)$, $\mathbf x$ has a continuous inverse $W\to U$ (condition (ii) in the definition of a regular surface).  Let $\varphi=\mathbf y\circ\mathbf x^{-1}:W\to S_2$; we claim that $\varphi$ is an isometry sending $p\mapsto q$.  It is clear that $\varphi$ sends $p$ to $q$ and is a bijection between open sets.  To show that $\varphi$ is an isometry, we just need to take $b=\mathbf x(a)\in W$ and show that $\|d\varphi_b(\vec v)\|=\|\vec v\|$ for tangent vectors $\vec v=c\mathbf x_u+d\mathbf x_v$ (Proposition 6.5).  Indeed,
$$\|\vec v\|=\|c\mathbf x_u+d\mathbf x_v\|=\sqrt{Ec^2+2Fcd+Gd^2};$$
and since $d\varphi_b(\vec v)=c\mathbf y_u+d\mathbf y_v$ by $\varphi\circ\mathbf x=\mathbf y$ and the Chain Rule, we have
$$\|d\varphi_b(\vec v)\|=\|c\mathbf y_u+d\mathbf y_v\|=\sqrt{Ec^2+2Fcd+Gd^2}$$
as well.
\end{proof}
\noindent Using this, we shall prove an isometry which is slightly less obvious: the helicoid and the catenoid from Section 6.2.  We recall that the helicoid is parametrized by taking the line $y=z=0$, and rotating around the $z$-axis while translating it along the $z$-axis:
$$\mathbf h(u,v)=(u\cos v,u\sin v,v)$$
But for our purposes, we will think of $u$ as a different parameter from the arc length along the generating line.  We will specifically reparametrize the generating line so that the arc length is the hyperbolic sine of $u$:
\begin{equation}\tag{1}\mathbf h(u,v)=(\sinh u\cos v,\sinh u\sin v,v)\end{equation}
The catenoid, on the other hand, is the surface of revolution given by the curve $t\mapsto(\cosh t,t)$:
\begin{equation}\tag{2}\mathbf c(u,v)=(\cosh u\cos v,\cosh u\sin v,u)\end{equation}
Direct computation shows that $E=G=\cosh^2u$ and $F=0$ for each of the equations (1), (2).  For instance, $\mathbf h_u=(\cosh u\cos v,\cosh u\sin v,0)$ and $\mathbf h_v=(-\sinh u\sin v,\sinh u\cos v,1)$, and one can directly take the dot products (recalling $\sinh^2a+1=\cosh^2a$).  Hence, the helicoid and catenoid are locally isometric by Proposition 6.6, and going backwards through one local coordinate chart and forwards through the other gives the isometry.\\

\noindent\textbf{CONFORMAL MAPPINGS}\\

\noindent We recall (Section 4.1) that a mapping is said to be \textbf{conformal} if it preserves angles.  In differential geometry, there is a rather brief way to characterize this property, just as there was for isometries.  Take, for example, stereographic projection:
$$\mathbf x:\mathbb R^2\to S^2,\mathbf x(u,v)=\left(\frac{2u}{u^2+v^2+1},\frac{2v}{u^2+v^2+1},\frac{u^2+v^2-1}{u^2+v^2+1}\right)$$
Exercise 4(b) of Section 4.1 shows that this map is conformal, by showing that for each $p\in\mathbb R^2$ there is a constant $\lambda>0$ which depends only on $p$, such that $d\mathbf x_p(\vec v_1)\cdot d\mathbf x_p(\vec v_2)=\lambda^2\vec v_1\cdot\vec v_2$ for all $\vec v_1,\vec v_2$ tangent vectors at $p$.

The concept carries over to general regular surfaces:\\

\noindent\textbf{Proposition 6.7 and Definition.} \emph{Let $S_1$ and $S_2$ be regular surfaces, and $\varphi:S_1\to S_2$ a smooth map.  Then the following are equivalent:}

(i) \emph{$\varphi$ preserves angles between curves.}

(ii) \emph{For each $p\in S_1$, $d\varphi_p$ preserves angles between tangent vectors.}

(iii) \emph{For each $p\in S_1$, there exists a scalar $\lambda>0$ (which depends only on $p$) such that $d\varphi_p(\vec v)\cdot d\varphi_p(\vec w)=\lambda^2\vec v\cdot\vec w$ for all $\vec v,\vec w\in T_pS_1$.}

\emph{In these conditions, $\varphi$ is said to be a \textbf{conformal mapping}.  If, moreover, $\varphi$ is bijective, it is called a \textbf{conformal equivalence}.}
\begin{proof}
(i) $\iff$ (ii) because by the Chain Rule, $\varphi$ and $d\varphi_p$ commute with taking tangent vectors to curves.

(ii) $\implies$ (iii). Let $\{\vec u_1,\vec u_2\}$ be an \emph{orthonormal} basis of $T_pS_1$.  Then $d\varphi_p(\vec u_1)\perp d\varphi_p(\vec u_2)$, because $d\varphi_p$ preserves the right angle by assumption.  Moreover, $\vec u_1,\vec u_1+\vec u_2$ have an angle of exactly $\pi/4$ (why?), hence so do $d\varphi_p(\vec u_1)$ and $d\varphi_p(\vec u_1+\vec u_2)=d\varphi_p(\vec u_1)+d\varphi_p(\vec u_2)$.  A basic geometric argument then shows that $\|d\varphi_p(\vec u_1)\|=\|d\varphi_p(\vec u_2)\|$ (and this is nonzero because the angle is defined); let $\lambda$ be this common value.  Then one can easily prove (iii) is satisfied by writing $\vec v,\vec w$ as linear combinations of $\vec u_1,\vec u_2$.

(iii) $\implies$ (ii). The angle between $\vec v$ and $\vec w$ is $\cos^{-1}\frac{\vec v\cdot\vec w}{\|\vec v\|\|\vec w\|}$.  For each $p\in S_1$, we may let $\lambda$ be the scalar from the hypothesis (iii).  Note that $\|d\varphi(\vec v)\|=\lambda\|\vec v\|$ follows from taking $\vec v=\vec w$.  Hence we have
$$\cos^{-1}\frac{d\varphi_p(\vec v)\cdot d\varphi_p(\vec w)}{\|d\varphi_p(\vec v)\|\|d\varphi_p(\vec w)\|}=\cos^{-1}\frac{\lambda^2\vec v\cdot\vec w}{\lambda\|\vec v\|~\lambda\|\vec w\|}=\cos^{-1}\frac{\vec v\cdot\vec w}{\|\vec v\|\|\vec w\|},$$
from which (ii) follows.
\end{proof}

\noindent If $p\in S_1$ and $q\in S_2$, then $S_1$ is said to be \textbf{locally conformal} to $S_2$ (at $p$ and $q$), if there exist open neighborhoods $p\in U_1\subset S_1,q\in U_2\subset S_2$ and a conformal equivalence $\varphi:U_1\to U_2$ such that $\varphi(p)=q$.  $S_1$ and $S_2$ are said to be \textbf{locally conformal} at $p,q$ whenever $p\in S_1$ and $q\in S_2$. % Same response
We recall that local isometries can be measured via charts with the same first fundamental form; local conformalities have a similar situation:\\

\noindent\textbf{Proposition 6.8.} \emph{Let $S_1$ and $S_2$ be regular surfaces, and $p\in S_1,q\in S_2$ points.  Then $S_1$ is locally conformal to $S_2$ at $p$ and $q$, if and only if there exist an open set $0\in U\subset\mathbb R^2$ and local coordinate charts $\mathbf x:U\to S_1,\mathbf y:U\to S_2$ with $\mathbf x(0)=p,\mathbf y(0)=q$, such that if $E,F,G$ are $\mathbf x$'s first fundamental form coefficients and $\overline E,\overline F,\overline G$ are $\mathbf y$'s, then there is a smooth positive function $\mu:U\to\mathbb R_{>0}$ such that $\overline E=\mu E,\overline F=\mu F,\overline G=\mu G$.}\\

\noindent In effect, two local coordinate charts relate conformally if and only if the first fundamental form coefficients of one are a single-scalar-function multiple of those of the other.  The special case where $S_1=\mathbb R^2$ and $\mathbf x:U\to\mathbb R^2$ is the inclusion map shows that $\mathbf y$ is a conformal parametrization if and only if $\overline E=\overline G(=\mu)$ and $\overline F=0$.
\begin{proof}
Suppose $S_1$ is locally conformal to $S_2$ at $p$ and $q$, and that $\varphi:W\to V$ is an conformal equivalence of open neighborhoods sending $p\mapsto q$.  Then let $\mathbf x:U\to W\subset S_1$ be a local coordinate chart sending $0\mapsto p$, and let $\mathbf y=\varphi\circ\mathbf x$.  Let $E,F,G$ be the first fundamental form coefficients for $\mathbf x$, and let $\overline E,\overline F,\overline G$ be those for $\mathbf y$.  Since $\varphi$ is conformal, there is (by Proposition 6.5) a smooth positive function $\lambda:W\to\mathbb R_{>0}$ such that $d\varphi_p(\vec v)\cdot d\varphi_p(\vec w)=\lambda(p)^2\vec v\cdot\vec w$ whenever $p\in W$ and $\vec v,\vec w$ are tangent at $p$.  Moreover,
$$\overline E=\mathbf y_u\cdot\mathbf y_u=d\varphi_p(\mathbf x_u)\cdot d\varphi_p(\mathbf x_u)=\lambda(p)^2\mathbf x_u\cdot\mathbf x_u=\lambda(p)^2E,$$
and likewise $\overline F=\lambda^2F$ and $\overline G=\lambda^2G$, so we may take $\mu=\lambda^2$.  [Again we only now know that $\mathbf y$ has injective differentials \---- why?]

Conversely, suppose the local coordinate charts $\mathbf x:U\to S_1,\mathbf y:U\to S_2$ have first fundamental form coefficients such that $\overline E=\mu E,\overline F=\mu F,\overline G=\mu G$.  As in Proposition 6.6, we use the fact that $\mathbf x$ has a continuous inverse to construct $\varphi=\mathbf y\circ\mathbf x^{-1}:W\to S_2$.  We claim that $\varphi$ is a conformal mapping sending $p\mapsto q$.  For $b=\mathbf x(a)\in W$, we claim that $d\varphi_b(\vec v)\cdot d\varphi_b(\vec w)=\mu\vec v\cdot\vec w$ whenever $\vec v,\vec w$ are tangent at $b$; then we can take $\lambda=\sqrt{\mu}$ and satisfy criterion (iii) in Proposition 6.5.

Well, this is a straightforward calculation.  Write $\vec v=c_1\mathbf x_u+d_1\mathbf x_v$ and $\vec w=c_2\mathbf x_u+d_2\mathbf x_v$.  Then
$$\vec v\cdot\vec w=c_1c_2E+(c_1d_2+d_1c_2)F+d_1d_2G$$
Also, $d\varphi_b(\vec v)=c_1\mathbf y_u+d_1\mathbf y_v$ and $d\varphi_b(\vec w)=c_2\mathbf y_u+d_2\mathbf y_v$ since $\varphi\circ\mathbf x=\mathbf y$; moreover,
$$d\varphi_b(\vec v)\circ d\varphi_b(\vec w)=(c_1\mathbf y_u+d_1\mathbf y_v)\cdot(c_2\mathbf y_u+d_2\mathbf y_v)$$
$$=c_1c_2\overline E+(c_1d_2+d_1c_2)\overline F+d_1d_2\overline G=\mu[c_1c_2E+(c_1d_2+d_1c_2)F+d_1d_2G].$$
Since the last expression is equal to $\mu[\vec v\cdot\vec w]$, this completes the proof.
\end{proof}

\noindent We conclude this section by giving examples of conformal mappings and projections.

First, suppose $S_1$ is any regular surface; fix $\lambda>0$ and let $S_2=\{\lambda\vec x:\vec x\in S_1\}$.  Then $S_2$ is also a regular surface, as one can readily see.  The map $\vec x\mapsto\lambda\vec x$ is a conformal equivalence from $S_1$ to $S_2$ (Exercise 4(b)), though it is not an isometry unless $\lambda=1$.

Stereographic projection is conformal, when regarded as a map from $S^2-\{(0,0,1)\}$ to the $xy$-plane.  For open sets $U\subset\mathbb C$, holomorphic functions $f:U\to\mathbb C$ with nonzero derivative are conformal.  M\"obius transformations are conformal when viewed on dense open subsets of the $xy$-plane.  In particular, the central inversion $(x,y)\mapsto\left(\frac x{x^2+y^2},\frac y{x^2+y^2}\right)$ is a conformal map on the $xy$-plane with the origin deleted.

An example of a conformal map which we have not previously covered is \textbf{Mercator's projection}, which takes all latitudes and longitudes of the sphere to Euclidean lines in the plane which are horizontal and vertical respectively.  Geographical maps of the world can be shown using this projection (since the Earth is spherical):
\begin{center}
\includegraphics[scale=.2]{MercatorsProjection.jpg}
\end{center}
To establish this, we will parametrize the sphere and the $xy$-plane in ways that satisfy Proposition 6.8's conditions:
$$\mathbf x(u,v)=(\sin u\cos v,\sin u\sin v,\cos u),0<u<\pi,0<v<2\pi$$
$$\mathbf y(u,v)=(v,\ln(\tan(u/2)),0),\text{ for all }u,v\in\mathbb R$$
We recall that $\mathbf x$'s first fundamental form coefficients are $E=1,F=0,G=\sin^2u$.  Direct computation shows that the first fundamental coefficients of $\mathbf y$ are $\overline E=\csc^2u,\overline F=0,\overline G=1$ (we recall that $\frac{d}{du}\ln(\tan(u/2))=\csc u$).  Proposition 6.8's criterion is thus satisfied with $\mu=\csc^2u$.  Hence the proposition implies that $\mathbf x$ and $\mathbf y$ relate by a local conformal mapping $\varphi$.  We may define $\varphi$ on the dense subset of the sphere in the image of $\mathbf x$, and it turns out to be
$$\varphi(x,y,z)=\left(\tan^{-1}_2(y,x),\ln\frac{\sqrt{x^2+y^2}}{1+z},0\right)\text{ for }(x,y,z)\in S^2,y\ne 0\text{ or }x<0$$
(this formula uses $\tan(u/2)=\frac{\sin u}{1+\cos u}$).  A formula for the inverse (from the plane to the sphere) would be
$$\varphi^{-1}(x,y,0)=(\operatorname{sech}y\cos x,\operatorname{sech}y\sin x,\tanh y).$$
[We leave it to the reader to show that $(u,v)\mapsto(\operatorname{sech}v\cos u,\operatorname{sech}v\sin u,\tanh v)$ is in fact a conformal map $\mathbb R^2\to\mathbb R^3$, which gives another way to view Mercator's projection.]

We recall loxodromes on the sphere from the previous section ($v\cot\beta+A=\ln(\tan(u/2))$).  Observe that \emph{Mercator's projection sends loxodromes to straight lines} \---- the projection is conformal, so as the loxodrome meets all the meridians at a constant angle, its projection meets all vertical lines at a constant angle.  This fact and the formula for the projection together provide another way to find the equations of loxodromes.

It is actually a deep theorem that any two regular surfaces are locally conformal.  In view of Proposition 6.6, this is equivalent to saying that any regular surface has isothermal coordinates (Exercise 2 of Section 6.3).  We will not delve into details here. % No, it's not the Riemann mapping theorem ---- the Riemann mapping theorem states that every open set of \mathbb R^2, which is simply connected, and not all of \mathbb R^2, is globally conformal to the open disk.  This statement is local conformality of regular surfaces in \mathbb R^3.  See page 227 of the green do Carmo book.

\subsection*{Exercises 6.4. (Isometries and Conformal Mappings)} % Recall isometries from Chapters 2, 4, 5 of Euclidean, hyperbolic and spherical geometry.
% Then explain the desire to generalize the idea.  A diff. map is an isometry iff it preserves lengths of curves, i.e., its differential everywhere
% is an orthogonal map of inner product spaces.  Also discuss conformal maps, using stereographic projection as motivation.
% POTENTIAL: Show that the plane, cone, cylinder are locally isometric; as are the helicoid and catenoid
\begin{enumerate}
\item (a) Show that every isometry of the cylinder $x^2+y^2=1$ in $\mathbb R^3$ is the composition of an orthogonal transformation in the $xy$-plane, with either a translation or reflection in the $z$-direction.  [Imitate the argument given for the plane.  It would help to use local coordinates.]  Note that there exist isometries of the cylinder with exactly two fixed points. %Do Carmo, Exercise 12 of p. 229

(b) Show that every isometry of the sphere $x^2+y^2+z^2=1$ is an orthogonal transformation (in $O(3)$).  [Use Proposition 1.9 to assume the isometry fixes a certain point and tangent vector at your convenience.  Then use a local coordinate argument.]

\item Show that the plane is locally isometric to the cone.  [Think of the plane in polar coordinates.  The lines on the cone from the tip correspond to the rays in the plane coming from the origin.]

\item If $S$ is a surface of revolution, every rotation of $S$ around the $z$-axis is a global isometry.

\item Let $S$ be a regular surface.

(a) Applying an element of $\operatorname{Isom}(\mathbb R^3)$ to $S$ results in a surface which is globally isometric to $S$.

(b) Applying a dilation $\vec v\mapsto\lambda\vec v,\lambda>0$ results in a surface which is conformally equivalent to $S$.  [Show that dilations are conformal.]

\item Suppose a triangle on $S^2$ is made up of three segments of loxodromes (see Section 6.3), and does not touch either of the poles.  Then the interior angles of the triangle add to $\pi=180^\circ$.  [Consider Mercator's projection.]

\item (a) Let $\varphi:S_1\to S_2$ be a smooth map of regular surfaces.  Then $\varphi$ preserves area if and only if for every point $p\in S_1$, there exist local coordinate charts $\mathbf x:U\to S_1$ (sending $0\mapsto p$) and $\mathbf y=\varphi\circ\mathbf x:U\to S_2$, such that if $E,F,G$ are $\mathbf x$'s first fundamental form coefficients and $\overline E,\overline F,\overline G$ are $\mathbf y$'s, then $EG-F^2=\overline E\,\overline G-\overline F^2$.  [Compare to Exercise 8 of Section 6.3.]

(b) Prove that every area-preserving conformal equivalence is an isometry.  [Use part (a) and Proposition 6.8.]

\item\emph{(Surfaces of revolution relate to the plane.)} \---- Let $S$ be a surface of revolution, parametrized via
$$\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$$
with $f>0$ and $(f',g')\ne 0$.

(a) Parametrize the $xy$-plane via
$$\mathbf y(u,v)=\left(v,\int_{u_c}^u\frac{\sqrt{f'(t)^2+g'(t)^2}}{f(t)}\,dt\right);$$
with $u_c$ fixed, and show that $\mathbf x$ and $\mathbf y$ have first fundamental coefficients satisfying Proposition 6.8's criterion.

(b) Conclude that there is a local conformal mapping from $S$ to the plane, sending parallels of latitude ($v\mapsto\mathbf x(u_0,v)$) to horizontal lines, and meridians to vertical lines.

(c) Now parametrize the $xy$-plane via
$$\mathbf z(u,v)=\left(v,\int_{u_c}^uf(t)\sqrt{f'(t)^2+g'(t)^2}\,dt\right)$$
and use Exercise 6(a) to get a (local) area-preserving map from $S$ to the plane, also sending parallels of latitude to horizontal lines and meridians to vertical lines.

\item All of the material of this section can readily be generalized to arbitrary dimensions.

Let $m_1,m_2\leqslant n$ and let $S_1,S_2\subset\mathbb R^n$ be regular manifolds of respective dimensions $m_1,m_2$.  If $\varphi:S_1\to S_2$ is a smooth map, then $\varphi$ is said to be \textbf{isometric} if $\varphi$ preserves lengths of curves, and an \textbf{isometry} if it is an isometric homeomorphism.

(a) Recall the first fundamental form coefficients $F_{ij}$ from Exercise 11 of the previous section.  Prove analogues of Propositions 6.5 and 6.6 for arbitrary dimensions.  [Note that an isometric mapping $\varphi$ is necessarily an immersion, and hence $m_1\leqslant m_2$.  If $m_1\ne m_2$, then Proposition 6.6's analogue is a little different.]

(b) Prove that any isometry from $\mathbb R^n$ to itself (in the sense of this chapter) is the composition of an orthogonal matrix and a translation.  Is an isometric mapping from $\mathbb R^m$ to $\mathbb R^n$ necessarily the composition of a matrix with orthonormal columns and a translation?

$\varphi$ is to be \textbf{conformal} if $\varphi$ is an immersion which preserves angles between curves.  [This implies $m_1\leqslant m_2$.]  By reasons we already know, this is equivalent to $\varphi$ preserving angles between tangent vectors.

(c) Prove analogues of Propositions 6.7 and 6.8 for arbitrary dimensions.  [A conformal map is necessarily an immersion.]

(d) If $\varphi$ is conformal and preserves volumes, then $\varphi$ is an isometry.
\end{enumerate}

\subsection*{6.5. The Gauss Map.  Gaussian Curvature}
\addcontentsline{toc}{section}{6.5. The Gauss Map.  Gaussian Curvature}
In Section 6.3, we have used parametrizations of surfaces to measure lengths and areas on them.  We would still like to be able to tell the ``level of curviness'' of a surface, which is not determined by the isometry class.  For this we introduce the concept of the Gauss map.

If $S$ is a regular surface, then at each $p\in S$, there are two unit normal vectors \---- specifically the unit vectors perpendicular to the tangent plane.  If $\mathbf x:U\to S$ is a local coordinate chart, we can actually form a continuous family of these unit normal vectors, by defining
$$\mathbf N:U\to\mathbb R^3\text{ via }\mathbf N(p)=\frac{\mathbf x_u\times\mathbf x_v}{\|\mathbf x_u\times\mathbf x_v\|}(p).$$
\begin{center}
\includegraphics[scale=.5]{GaussMap.png}
\end{center}
Then $\mathbf N(p)$ is a unit vector normal to $S$ at $\mathbf x(p)$ [note $\|\mathbf x_u\times\mathbf x_v\|\ne 0$ since $d\mathbf x_p$ has rank $2$].  It is worth asking whether one can define a continuous map on the \emph{entire surface} which outputs unit normal vectors like this.  On certain surfaces, this is clearly possible: if $U=S^2$, define $\mathbf N(p)=\vec p$, where $p$ is regarded as a unit vector (noting that the tangent plane to $S^2$ at $p$ is precisely the orthogonal complement of $p$).  If $U$ is the $xy$-plane, define $\mathbf N(p)=\vec e_3=(0,0,1)$ for all $p$.  If $U$ is the cylinder $x^2+y^2=1$, define $\mathbf N((x,y,z))=(x,y,0)$.  All of these are examples of \emph{outward} normals, since the normal vector points outward.  However, there are regular surfaces on which you cannot define $\mathbf N$ globally (see Exercise 1).

Note first that $\mathbf N:S\to\mathbb R^3$ and every $\mathbf N(p)$ is a unit vector, and therefore, $\mathbf N$ can be regarded as a function $S\to S^2$.  Any continuous function $S\to S^2$ (or $U\to S^2$ with $U$ an open set of $S$) sending each $p$ to a normal unit vector to the surface at $p$ is called a \textbf{Gauss map}.  If a regular surface has a global Gauss map, it is said to be \textbf{orientable}.  Observe in this case that it has exactly two distinct Gauss maps: why?

From this point on, we will stick to orientable surfaces, equipped with a particular Gauss map.  If $\mathbf x:U\to S$ is a local coordinate chart, $\mathbf N$ may be regarded as a function in either $U$ or $\mathbf x(U)\subset S$, and we shall use the two interchangeably when $\mathbf x$ is clear from the discussion.  For $p\in U$, $\mathbf N(p)$ is either equal to $\frac{\mathbf x_u\times\mathbf x_v}{\|\mathbf x_u\times\mathbf x_v\|}(p)$ or its negative; and by continuity, either the expressions are equal throughout $U$ (in which case we say $\mathbf x$ is \textbf{orientation-preserving}) or negatives of one another throughout $U$ (where we say $\mathbf x$ is \textbf{orientation-reversing}).
We shall assume every local coordinate chart used to cover the surface is orientation-preserving.\\

\noindent To study curviness, we differentiate the Gauss map.  First, for $p\in S$, since $\mathbf N(p)$ is normal to the surface at $p$, it is orthogonal to the tangent plane of $S$ at $p$.  Yet since $\mathbf N(p)\in S^2$, it is also orthogonal to the tangent plane to the sphere at $\mathbf N(p)$ itself (since the tangent plane is orthogonal to the radius) \---- this means that the two tangent planes coincide in practice.  Hence, $d\mathbf N_p$ is a linear \emph{operator} on a two-dimensional vector space, and we also have an extra property involving dot products:\\

\noindent\textbf{Proposition 6.9.} \emph{Let $S$ be an oriented regular surface (i.e., equipped with a particular Gauss map).  For each $p\in S$,}

(i) \emph{$d\mathbf N_p$ is a linear operator on the two-dimensional space $T_pS$;}

(ii) \emph{$d\mathbf N_p$ is self-adjoint; i.e., $d\mathbf N_p(\vec v)\cdot\vec w=\vec v\cdot d\mathbf N_p(\vec w)$ for all $\vec v,\vec w\in T_pS$.}\\

In the next section, we shall find the matrix for $d\mathbf N_p$ with respect to the basis $\mathbf x_u,\mathbf x_v$.

Note, however, that (ii) does \emph{not} imply said matrix will be symmetric.  In the ordinary inner product space $\mathbb R^n$, it is known that the adjoint of any matrix is its transpose, hence a matrix is self-adjoint if and only if it symmetric.  This means that by (ii), $d\mathbf N_p$ would be symmetric when written with respect to any \emph{orthonormal} basis (in this case the dot product would coincide with the coordinate one); but $\mathbf x_u,\mathbf x_v$ is seldom an orthonormal basis for these parametrizations.
\begin{proof}
(i) has been shown in the paragraph preceding the proposition statement.

(ii) Let $\mathbf x:U\to S$ be a local coordinate chart, and write $\vec v=a\mathbf x_u+b\mathbf x_v$ and $\vec w=c\mathbf x_u+d\mathbf x_v$.  Note that $d\mathbf N_p(\mathbf x_u)=\mathbf N_u$ by the Chain Rule, where the right-hand side views $\mathbf N$ as a function on $U$.  Likewise, $d\mathbf N_p(\mathbf x_v)=\mathbf N_v$.  Hence,
$$d\mathbf N_p(\vec v)\cdot\vec w=(a\mathbf N_u+b\mathbf N_v)\cdot(c\mathbf x_u+d\mathbf x_v)$$
$$=ac(\mathbf N_u\cdot\mathbf x_u)+ad(\mathbf N_u\cdot\mathbf x_v)+bc(\mathbf N_v\cdot\mathbf x_u)+bd(\mathbf N_v\cdot\mathbf x_v)$$
and
$$\vec v\cdot d\mathbf N_p(\vec w)=(a\mathbf x_u+b\mathbf x_v)\cdot(c\mathbf N_u+d\mathbf N_v)$$
$$=ac(\mathbf N_u\cdot\mathbf x_u)+ad(\mathbf N_v\cdot\mathbf x_u)+bc(\mathbf N_u\cdot\mathbf x_v)+bd(\mathbf N_v\cdot\mathbf x_v);$$
to complete the proof we need only show $\mathbf N_u\cdot\mathbf x_v=\mathbf N_v\cdot\mathbf x_u$.

To do this, we note that $\mathbf N\cdot\mathbf x_u=0$ throughout the open set $U$, because $\mathbf N$ is normal to the surface and $\mathbf x_u$ is tangent to it, hence they are perpendicular vectors.  Since this equation holds throughout $U$, its partial derivative with respect to $v$ holds:
$$(\mathbf N\cdot\mathbf x_u)_v=\mathbf N_v\cdot\mathbf x_u+\mathbf N\cdot\mathbf x_{uv}=0$$
(using Exercise 4(a) of Section 6.1).  Similarly, taking the partial derivative of $\mathbf N\cdot\mathbf x_v=0$ with respect to $u$ entails $\mathbf N_u\cdot\mathbf x_v+\mathbf N\cdot\mathbf x_{uv}=0$.  Consequently, $\mathbf N_u\cdot\mathbf x_v=\mathbf N_v\cdot\mathbf x_u$ because both are equal to $-\mathbf N\cdot\mathbf x_{uv}$ (note that we used the symmetry of mixed partial derivatives here).
\end{proof}

\noindent As a linear operator, $d\mathbf N_p$ has a determinant.  Changing the orientation of the Gauss map negates the matrix, but leaves the determinant unchanged (because the matrix has even dimensions).  The determinant of $d\mathbf N_p$ is called the \textbf{Gaussian curvature} of the surface at $p$, and is denoted $K$ (or $K(p)$).

Since $d\mathbf N_p$ is self-adjoint, examining it with respect to an orthonormal basis and using the Spectral Theorem (Exercise 1 of Section 3.5), shows that $d\mathbf N_p$ is diagonalizable over $\mathbb R$, and its eigenvectors are perpendicular (unless $d\mathbf N_p$ is a scalar multiple of the identity, in which case every nonzero vector is an eigenvector).  The eigenvectors, which are tangent vectors to the surface, are called \textbf{principal directions}, and their eigenvalues are called the \textbf{principal curvatures} in these directions.  Note that the principal curvatures negate when the orientation is switched (even though the Gaussian curvature does not change).\\

\noindent\textbf{Proposition 6.10 and Definition.} \emph{Let $S$ be an oriented regular surface and $p\in S$.  Then exactly one of the following holds:}

(i) \emph{$K>0$, and $d\mathbf N_p$'s eigenvalues have the same sign.  In this case the point $p$ is said to be an \textbf{elliptic} point.}

(ii) \emph{$K<0$, and $d\mathbf N_p$'s eigenvalues have opposite signs.  In this case the point $p$ is said to be a \textbf{hyperbolic} (or \textbf{saddle}) point.}

(iii) \emph{$K=0$ and $d\mathbf N_p\ne 0$.  In this case the point $p$ is said to be a \textbf{parabolic} point.}

(iv) \emph{$d\mathbf N_p=0$.  In this case the point $p$ is said to be a \textbf{planar} point.}

\emph{Moreover, if $d\mathbf N_p$ is a scalar multiple of the identity, $p$ is said to be an \textbf{umbilical} (or \textbf{spherical}) point.}
\begin{proof}
The statements follow readily from elementary linear algebra.  $K$, being the determinant of $d\mathbf N_p$, is the product of its eigenvalues.
\end{proof}

\noindent\textbf{Examples.}

To illustrate the principle we shall take some sample surfaces, determine their Gaussian curvature, and tell what kinds of points they have.

(1) The $xy$-plane $\mathbf x(u,v)=(u,v,0)$ has constant Gauss map $\mathbf N(p)=(0,0,1)$.  Hence $d\mathbf N_p=0$ for all $p$, so that $K=0$ and every point is planar [the terminology should make sense here!]\\

(2) If $S$ is the sphere $S^2$, then since the outward normal vector to each $p\in S^2$ is parallel to $p$ itself, $\mathbf N:S\to S^2$ is the identity map.  Hence every $d\mathbf N_p$ is the identity, so that $K=1$, and every point on $S^2$ is umbilical, and in particular elliptic.\\

(3) The cylinder $x^2+y^2=1$, when parametrized by $\mathbf x(u,v)=(\cos u,\sin u,v)$, is readily seen to have $K=0$ and consist purely of parabolic points.  For example, at $p=(1,0,0)$, $\mathbf N(p)=(1,0,0)$, so that $d\mathbf N_p$ is a linear operator on the $yz$-plane; with respect to the basis $\vec e_2,\vec e_3$ its matrix is $\begin{bmatrix}1&0\\0&0\end{bmatrix}$, satisfying case (iii) of Proposition 6.10.

Similarly, the cone $z^2=x^2+y^2$ consists of parabolic points.\\

(4) For our benefit, we will find explicit formulas for $\mathbf N$ for our two particularly important kinds of surfaces.

If $U\subset\mathbb R^2$, $f:U\to\mathbb R$ and $S$ is the graph of $z=f(x,y)$, it can be parametrized via $\mathbf x(u,v)=(u,v,f(u,v))$.  We recall that $\mathbf x_u=(1,0,f_u)$ and $\mathbf x_v=(0,1,f_v)$.  With that, we get $\mathbf N=\frac{\mathbf x_u\times\mathbf x_v}{\|\mathbf x_u\times\mathbf x_v\|}=\frac 1{\sqrt{1+f_u^2+f_v^2}}(-f_u,-f_v,1)=\frac 1{\sqrt{1+\|\vec\nabla f\|^2}}(-f_u,-f_v,1)$.

Now, let $\alpha(t)=(f(t),g(t))$ be a regular curve with $f>0$; and consider the surface of revolution $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$.  Then $\mathbf x_u=(f'(u)\cos v,f'(u)\sin v,g'(u))$ and $\mathbf x_v=(-f(u)\sin v,f(u)\cos v,0)$, from which we compute $\mathbf x_u\times\mathbf x_v=(-g'(u)f(u)\cos v,-g'(u)f(u)\sin v,f'(u)f(u))$, and therefore $\mathbf N=\frac 1{\sqrt{f'(u)^2+g'(u)^2}}(-g'(u)\cos v,-g'(u)\sin v,f'(u))$.  We recall the feasibility and convenience of assuming $(f')^2+(g')^2=1$; if that holds, then $\mathbf N=(-g'(u)\cos v,-g'(u)\sin v,f'(u))$.\\

(5) Here is an example of a planar point on a surface which is not locally a plane.  Consider the graph of the quartic surface $z=x^4+y^4$.  By the formulas in the previous example, $\mathbf N((x,y,z))=\frac 1{1+(4x^3)^2+(4y^3)^2}(-4x^3,-4y^3,1)$.  We leave it to the reader to show that the derivative of this is entirely zero at $x=y=0$; hence $(0,0,0)$ is a planar point, even though the graph is clearly not locally a plane.\\

(6) The paraboloid $z=x^2+y^2$ has an (elliptic) umbilical point at the vertex $(0,0,0)$; however, its other points are elliptic but not umbilical.  Contrast with the sphere in example (2).\\

(7) Recall the torus from Exercise 11 of Section 6.3:
$$\mathbf x(u,v)=((a+b\cos u)\cos v,(a+b\cos u)\sin v,b\sin u)$$
In the next section we will see that the outer surface of the torus consists of elliptic points, the inner surface consists of hyperbolic points, and there are two circles worth of parabolic points.\\

\noindent In the remainder of this section we shall show a particularly curious fact about umbilical points:\\

\noindent\textbf{Proposition 6.11.} \emph{Let $S$ be a connected regular surface, on which all points are umbilical.  Then $S$ is contained in either a plane or a sphere.}\\

\noindent It is obvious why one must assume $S$ is connected; otherwise, e.g., it could be the disjoint union of two planes and three spheres and the statement would not hold.
\begin{proof}
Let us first prove this for the image of a local coordinate chart $\mathbf x:U\to S$ ($U$ connected).  Since $\mathbf x(p)$ is umbilical for all $p\in U$, we have $d\mathbf N_p=\lambda(p)\cdot I$ for some scalar $\lambda(p)$ depending on $p$.  It is readily verified that $\lambda$ is smooth with respect to $p$.  Moreover, since $d\mathbf N_p(\mathbf x_u)=\mathbf N_u$ and $d\mathbf N_p(\mathbf x_v)=\mathbf N_v$, we get these equations:
\begin{equation}\tag{1}\mathbf N_u=\lambda\mathbf x_u\end{equation}
\begin{equation}\tag{2}\mathbf N_v=\lambda\mathbf x_v\end{equation}
Differentiating (1) with respect to $v$, (2) with respect to $u$, and subtracting, one ends up with
$$\lambda_v\mathbf x_u-\lambda_u\mathbf x_v=0.$$
[This uses the Product Rule, which entails that, e.g., $\frac{\partial}{\partial v}(\lambda\mathbf x_u)=\lambda_v\mathbf x_u+\lambda\mathbf x_{uv}$.]  Since $\mathbf x_u,\mathbf x_v$ are linearly independent, we conclude $\lambda_u=\lambda_v=0$ throughout $U$.  Hence $\lambda$ is actually constant.

Either $\lambda=0$ or $\lambda\ne 0$.  If $\lambda=0$, then $\mathbf N_u=\mathbf N_v=0$, so that $\mathbf N$ is constant, say $\mathbf N_0$.  For fixed $p_0\in U$, the function $p\mapsto\mathbf N_0\cdot(\mathbf x(p)-\mathbf x(p_0))$ is identically zero, because it sends $p_0\mapsto 0$, and it is constant (its partial derivatives are $\mathbf N\cdot\mathbf x_u$ and $\mathbf N\cdot\mathbf x_v$, which are both zero).  This means that $\mathbf x(U)$ is contained in the plane through $\mathbf x(p_0)$ perpendicular to $\mathbf N_0$.

Now suppose $\lambda\ne 0$.  Then set $\mathbf y=\mathbf x-\frac 1{\lambda}\mathbf N$; equations (1) and (2) above entail $\mathbf y_u=\mathbf y_v=0$, and hence $\mathbf y$ is constant.  Moreover, for all $p\in U$,
$$\left\|\mathbf x(p)-\mathbf y\right\|=\left\|\frac 1{\lambda}\mathbf N\right\|=\frac 1{|\lambda|}$$
(because $\mathbf N$ is a unit vector), so that $\mathbf x(U)$ is contained in the sphere centered at $\mathbf y$ with radius $\frac 1{|\lambda|}$.

This proves the proposition locally.  For the global case, the connectedness of $S$ will be essential.  We will use the topological fact that $S$ must be path-connected; i.e., for all $x,y\in S$, there is a continuous function $[0,1]\to S$ sending $0\mapsto x,1\mapsto y$.  Let $\mathbf x_\alpha:U_\alpha\to S$ be a family of local coordinate charts that cover the surface. % Regarding your footnote, connected => path-connected for locally path-connected spaces (1).  "Locally path-connected" means every point has a neighborhood base consisting entirely of path-connected sets.  Moreover, topological manifolds (like S) are locally path-connected (2), hence connected => path-connected for manifolds.  Nothing to do with second countability or separability.
% PROOF OF (1): For x,y\in S, define x\sim y if there is a path from x to y (i.e., a continuous map [0,1] -> S sending 0 to x and 1 to y).  This is an equivalence relation, and is all of S\times S \iff S is path connected.  Yet if S is locally path-connected, every equivalence class is open: indeed, for x\in S, x has a path-connected neighborhood (a neighborhood base consisting of them for that matter), and such a neighborhood is clearly contained in the equivalence class of x.  Thus every point of the equivalence class is an interior point.  Since the equivalence classes are open and partition S, they are clopen.  Thus if S is connected, they must be the entire space, so that S is path connected.
% PROOF OF (2): If S is a topological manifold of dimension n, then every x\in S has a neighborhood U and a homeomorphism \mathbb R^n\cong U sending 0 to x.  Moreover, the images of the B_r(0)\subset\mathbb R^n for all positive rational r, form the desired neighborhood base of x.

Fix $p\in S$ and pick a coordinate chart $\mathbf x_\rho$ which covers $p$.  This chart is contained in some sphere or plane; our claim is that the entire surface is contained in said sphere or plane as well.  First suppose the chart is contained in a plane.  Then for any $q\in S$, let $\alpha:[0,1]\to S$ be a path from $p$ to $q$.  Since $[0,1]$ is compact, the open covering consisting of the sets $\alpha^{-1}(\mathbf x_\alpha(U_\alpha))\subset[0,1]$ has a Lebesgue number $\delta$ (this means $\delta>0$ and every interval of the form $(x,x+\delta)$ is contained within some set of the covering); let $N=1+\lfloor 1/\delta\rfloor$.  By definition of $\delta$, for every interval $I_m=[m/N,(m+1)/N]$ for $m=0,1,\dots,N-1$ there exists $\alpha_m$ with $\alpha(I_m)\subset\mathbf x_{\alpha_m}(U_{\alpha_m})$. % Lebesgue number concept is used rarely in the book.  I'll add the definition here.

Since $\mathbf x_{\alpha_0}(U_{\alpha_0})$ is an open set of the surface containing $p$, we must have that $\mathbf x_{\alpha_0}(U_{\alpha_0})$ is contained in the plane that $\mathbf x_\rho$ is contained in.  Moreover, $\mathbf x_{\alpha_0}(U_{\alpha_0})\cap\mathbf x_{\alpha_1}(U_{\alpha_1})$ is an open neighborhood of $\alpha(1/N)$, also contained in this plane.  Such an open set is not contained in any other plane or in any sphere; hence $\mathbf x_\rho$'s plane must contain $\mathbf x_{\alpha_1}(U_{\alpha_1})$, too.  Like reasoning shows that since $\mathbf x_{\alpha_1}(U_{\alpha_1})\cap\mathbf x_{\alpha_2}(U_{\alpha_2})$ is an open neighborhood of $\alpha(2/N)$ in the surface, the same plane contains $\mathbf x_{\alpha_2}(U_{\alpha_2})$.  The argument continues across the path, and eventually shows that $\mathbf x_{\alpha_N}(U_{\alpha_N})$ is contained in the plane.  Yet $q=\alpha(1)\in\mathbf x_{\alpha_N}(U_{\alpha_N})$, and hence $q$ is in the plane.  Since $q$ is arbitrary, we conclude that the plane containing $\mathbf x_\rho$ contains the entire surface.

If $\mathbf x_\rho$ is contained in a sphere, pretty much the same argument is valid; for that use the fact that a nonempty open set of the sphere is not contained in any other sphere or in any plane.
\end{proof}

\subsection*{Exercises 6.5. (The Gauss Map.  Gaussian Curvature)}
% POTENTIAL EXERCISE: mean curvature, principal directions, lines of curvature, normal curvature, Joachimsthal's
\begin{enumerate}
\item The \textbf{M\"obius band} is obtained by taking a rectangular strip and gluing its ends together, but after applying a half-twist, so that opposite vertices of the original rectangle meet.
\begin{center}
\includegraphics[scale=.4]{Moebius_Band.png}
\end{center}
(a) Show that it can be parametrized as follows:
$$\mathbf x(u,v)=((2+u\sin(v/2))\cos v,(2+u\sin(v/2))\sin v,u\cos(v/2)),~~~~-1<u<1$$
The fact that $\mathbf x(u,v)=\mathbf x(-u,v+2\pi)$ illustrates the half-twist.

(b) Show that the M\"obius band is a regular surface, but there is no globally continuous Gauss map on it.

\item Let $S$ be a regular surface.  If $S$ is tangent to a plane along a curve, show that every point on the curve is a parabolic or planar point of $S$.

\item Show that every compact regular surface $S$ has an elliptic point.  [By compactness, $S$ is bounded, hence there is a sphere which engulfs $S$ and is tangent to $S$ at a particular point.  Show that this point is elliptic by considering fundamental results from calculus.]

\item If $S$ is a regular surface and $p\in S$, the \textbf{mean curvature} of $S$ at $p$ \-- denoted $H$ \-- is defined to be $\frac 12\operatorname{tr}(d\mathbf N_p)$.  Note that unlike Gaussian curvature, $H$ negates when the orientation is switched.  Find the mean curvature of the sphere, plane, cylinder and cone.

Note that the plane, cylinder and cone are locally isometric; they all have Gaussian curvature $0$, but they have different mean curvatures.  We shall later see that locally isometric surfaces \emph{always} have the same Gaussian curvature; this is what makes this kind of curvature particularly special.

\item Recall that the \textbf{principal directions} to a point $p\in S$ are the tangent vectors which are eigenvectors of $d\mathbf N_p$, and that the \textbf{principal curvatures} are their eigenvalues.

(a) A point is umbilical if and only if all its directions are principal directions.

(b) If a point is umbilical, then all of its principal curvatures are equal.  Explain why this is false for arbitrary points on regular surfaces.

(c) Directions $\vec v,\vec w\in T_pS$ are said to be \textbf{conjugate} if $d\mathbf N_p(\vec v)\cdot\vec w=0$.  Since $d\mathbf N_p$ is self-adjoint, this is a symmetric relation on directions.  For the sphere, plane and cylinder, give explicit descriptions of conjugate directions.  Use this to show that conjugate directions need \emph{not} be preserved by local isometries.

(d) A direction $\vec v\in T_pS$ is said to be \textbf{asymptotic} if it is conjugate to itself; i.e., $d\mathbf N_p(\vec v)\cdot\vec v=0$.  Show that such directions exist at $p$ if and only if $p$ is not elliptic.

\item A regular curve $\alpha:I\to S$ is called a \textbf{line of curvature} if all its derivatives are principal directions on the surface; i.e., $\alpha'(t)$ is a principal direction at $\alpha(t)$ for each $t\in I$.

(a) Show that $\alpha$ is a line of curvature if and only if there is a smooth function $\lambda:I\to\mathbb R$ such that $\mathbf N'(t)=\lambda(t)\alpha'(t)$.

(b) Assume $S$ has no umbilical points.  If the coordinate curves ($u\mapsto\mathbf x(u,v_0)$ and $v\mapsto\mathbf x(u_0,v)$) of a coordinate chart are lines of curvature, show that $F=0$ ($F$ being the first fundamental form coefficient).  Is the converse necessarily true? % If L,M,N are the SFF coefficients, then the coordinate curves are lines of curvature \iff F=M=0.

\item\emph{(Normal curvature.)} \---- Let $\alpha:I\to S$ be a regular curve on a regular surface $S$.  An important observation is that, though $\mathbf N$ is normal to the curve where it appears on the curve, it is not generally parallel to the normal vector given in the Frenet trihedron.  The latter normal vector thinks directly in the curve's sense of direction, and doesn't know about the surface structure.  Yet, the normal vectors do relate.

Let $\alpha(t)$ be a particular point of the curve.  Then let $\mathbf N$ be the Gauss map, $\boldsymbol\nu$ the unit normal vector to $\alpha$ (in the Frenet trihedron), and $\kappa$ the curvature of $\alpha$.  Then if $\theta$ is the angle between $\mathbf N$ and $\boldsymbol\nu$, the number $\kappa\cos\theta$ is denoted $\kappa_n$ and called the \textbf{normal curvature}.

(a) If $\alpha$ is parametrized by arc length, show that the absolute value of the normal curvature is equal to the magnitude of the orthogonal projection of $\alpha''(s)$ onto $\mathbf N$, and that the normal curvature is negative if and only if the projection points opposite to $\mathbf N$.

(b) Show that the normal curvature of $\alpha$ at $t$ is equal to $-d\mathbf N_p(\boldsymbol\tau(t))\cdot\boldsymbol\tau(t)$, where $\boldsymbol\tau(t)=\frac{\alpha'(t)}{\|\alpha'(t)\|}$ (the tangential unit vector).  [Explain why one may assume $\alpha$ is parametrized by arc length; then the goal is to show $\kappa_n=-d\mathbf N_p(\alpha'(s))\cdot\alpha'(s)$.  To do this, differentiate the equation $\mathbf N(s)\cdot\alpha'(s)=0$, and show that $\kappa_n=\kappa(\boldsymbol\nu\cdot\mathbf N)=\mathbf N\cdot\alpha''$.]  Hence, the normal curvature only depends on the point and the tangent line at the point, not the particular curve (Meusnier).

(c) Show that at $p\in S$, the mean curvature $H=-\frac 1{\pi}\int_0^\pi\kappa_n(\theta)\,d\theta$, where $\kappa_n(\theta)$ is the normal curvature at $p$ in a direction of angle $\theta$ from a fixed direction.  [Since $d\mathbf N_p$ is self-adjoint, it has an orthonormal eigenbasis.  If $\{\vec u_1,\vec u_2\}$ is such an eigenbasis, and the fixed direction is $\vec u_1$, use part (b) to show that $\kappa_n(\theta)=-\kappa_1\cos^2\theta-\kappa_2\sin^2\theta$ where $\kappa_1,\kappa_2$ are the principal curvatures (the eigenvalues corresponding to $\vec u_1$ and $\vec u_2$ respectively).  Then familiar trigonometric laws can be used to evaluate the integral.]

\item\emph{(Joachimsthal's Theorem.)} \---- Let $S$ and $\overline S$ be regular surfaces in $\mathbb R^3$ which intersect in a regular curve $\alpha:I\to\mathbb R^3$.  (Thus $\alpha$ is both a curve on $S$ and a curve on $\overline S$.)  Assume also that $\alpha$ is a line of curvature of $S$.  Show that $\alpha$ is a line of curvature of $\overline S$ if and only if the dihedral angle between the surfaces is constant along the curve.

[Let $\mathbf N,\overline{\mathbf N}$ be the respective Gauss maps of the surfaces.  Then the dihedral angle is constant along the curve if and only if $\mathbf N\cdot\overline{\mathbf N}$ is constant along the curve (why?).  Show that $\alpha$ is a line of curvature of $S$ if and only if $\frac{d\mathbf N}{dt}\cdot\overline{\mathbf N}$ (where $\mathbf N$ is regarded as a function in $t$) is zero; a similar statement holds with the surfaces switched.  Then use Exercise 4(a) of Section 6.1.]

\item Show that the parallels of latitude and meridians of any surface of revolution are lines of curvature.  [Use the previous exercise.]

\item In preparation for the next exercise, we will cover cross products of arbitrary-dimensional vectors.  In $\mathbb R^n$, the cross product takes exactly $n-1$ operands.

(a) If $\vec v_1,\dots\vec v_{n-1}\in\mathbb R^n$, then there is a unique vector $\vec a$ such that $\vec a\cdot\vec v=\det\begin{bmatrix}\uparrow&\dots&\uparrow&\uparrow\\\vec v_1&\dots&\vec v_{n-1}&\vec v\\\downarrow&\dots&\downarrow&\downarrow\end{bmatrix}$ for all $\vec v$.  [It suffices to show that the map from $\vec v$ to said determinant is a linear map $\mathbb R^n\to\mathbb R$.]  This vector $\vec a$ is called the \textbf{cross product} of $\vec v_1,\dots,\vec v_{n-1}$, and is occasionally denoted $\vec v_1\times\dots\times\vec v_{n-1}$.

(b) This cross product is multilinear, and alternating (i.e., if $\vec v_j=\vec v_k$ for some $j\ne k$ then the cross product is zero).  Moreover, $\vec e_1\times\dots\times\vec e_{n-1}=\vec e_n$.

(c) $\vec v_1\times\dots\times\vec v_{n-1}=\vec 0$ if and only if $\vec v_1,\dots,\vec v_{n-1}$ are linearly dependent.

(d) $\vec v_1\times\dots\times\vec v_{n-1}$ is perpendicular to the vectors $\vec v_1,\dots,\vec v_{n-1}$, and its magnitude is the $(n-1)$-dimensional volume of the parallelepiped spanned by the vectors.

(e) In $\mathbb R^2$, the operation is unary, and one denotes the cross product of $\vec v\in\mathbb R^2$ as $^\times\vec v$.  Show that $^\times(a,b)=(-b,a)$.  [Note the exact way that the unit normal vector for a plane curve was defined.]

(f) In $\mathbb R^3$, the operation is binary, and coincides with the cross product covered in Exercise 4 of Section 2.5.

\item As in every section, we will generalize the material to arbitrary dimensions.

Let $S\subset\mathbb R^n$ be a \textbf{hypersurface}, i.e., a regular manifold of dimension $n-1$.  Then a Gauss map on an open set $U\subset S$ is a continuous map $U\to S^{n-1}$ which takes each point to a unit normal vector to the hypersurface.  [I.e., a normal vector in the orthogonal complement of the tangent space to the point.]  Here, $S^{n-1}$ is the unit hypersphere in $\mathbb R^n$.

(a) Show that Gauss maps exist on local coordinate charts.  [Use the previous exercise to imitate the argument in the text for $n=3$.]

If $S$ has a global Gauss map, it is said to be \textbf{orientable}; otherwise, it is said to be \textbf{nonorientable}.  There exist nonorientable hypersurfaces whenever $n\geqslant 3$, such as $M\times\mathbb R^{n-3}$ with $M$ the M\"obius band (Exercise 1).

For the rest of the exercise, we assume $S$ is an orientable hypersurface equipped with a Gauss map $\mathbf N$.

(b) For each $p\in\mathbf N$, $d\mathbf N_p$ is a self-adjoint linear operator on $T_pS$.  [Pretty much everything is identical to the case $n=3$.  To show the operator is self-adjoint, show that if $\mathbf x$ is a local coordinate chart, $\mathbf N_{u_j}\cdot\mathbf x_{u_k}=\mathbf N_{u_k}\cdot\mathbf x_{u_j}=-\mathbf N\cdot\mathbf x_{u_ju_k}$.]

It follows that $d\mathbf N_p$ has an orthonormal eigenbasis.  Its eigenvectors are called \textbf{principal directions} and the eigenvectors are called the \textbf{principal curvatures}.  The product of the principal curvatures, which is $\det(d\mathbf N_p)$, is called the \textbf{main curvature} of the hypersurface.

(c) When does switching the orientation keep the main curvature the same?  When doesn't it?

(d) For a regular curve (the case where $n=2$), show that the main curvature at a point is the signed curvature.

(e) A point $p$ is said to be \textbf{umbilical} if $d\mathbf N_p$ is a scalar multiple of the identity operator.  Show that if $n>2$, a connected hypersurface on which every point is umbilical is contained in a hyperplane or a hypersphere.  [Adapt the proof of Proposition 6.11.]  Explain why this is false for $n=2$.

General types of curvature are not intrinsic to the isometry class of the manifold, hence will not be covered in Section 6.10 when metrics are customized.  However, there are types of curvature for general manifolds; the Riemann curvature tensor and Ricci curvature.  They are beyond the scope of this book. % Weird to say, now that the last exercise of Section 6.10 introduces the Levi-Civita connection.  The curvature tensor and Ricci curvature can easily be defined from that, am I missing an opportunity?
\end{enumerate}

\subsection*{6.6. The Second Fundamental Form}
\addcontentsline{toc}{section}{6.6. The Second Fundamental Form}
In the previous section we defined the Gauss map and went over its basic properties.  In this section, our aim is to express its differential in coordinates, for the benefit of the study.

Given a local coordinate chart $\mathbf x:U\to S$, the first fundamental form determines (in effect) how much distance is covered on $S$ by paths in $U$.  However, it does not indicate how much the surface curves in certain directions.  The curving nature of the surface is captured by the Gauss map, as seen in the last section.  Thus we would like for there to be another form, which has to do with the Gauss map.

To define this, we first recall how the first fundamental form works: if $\vec v=a\mathbf x_u+b\mathbf x_v\in T_pS$, then
$$I_p(\vec v)=\vec v\cdot\vec v=a^2\mathbf x_u\cdot\mathbf x_u+2ab\mathbf x_u\cdot\mathbf x_v+b^2\mathbf x_v\cdot\mathbf x_v=Ea^2+2Fab+Gb^2.$$
This is the quadratic form induced by the symmetric bilinear form $(\vec v,\vec w)\mapsto\vec v\cdot\vec w$.  If $\mathbf N$ is the Gauss map, then $(\vec v,\vec w)\mapsto -d\mathbf N_p(\vec v)\cdot\vec w$ is also a bilinear form, which is symmetric by Proposition 6.9(ii).  [The reason we adjoined a negative sign will be clear in a moment.  Some mathematicians omit it.]  This symmetric bilinear form corresponds with a quadratic form itself.\\

\noindent\textbf{Definition.} \emph{If $S$ is a regular surface and $\mathbf N$ its Gauss map, then the \textbf{second fundamental form} on $T_pS$ is defined to be the quadratic form $\vec v\mapsto I\!I_p(\vec v)=-d\mathbf N_p(\vec v)\cdot\vec v$.}\\ % I couldn't find such a symbol on DeTeXify.  I changed it to I\!I

\noindent Just as the first fundamental form has its coefficients ($E,F,G$) which make it easy to apply to any tangent vector given in the basis $\{\mathbf x_u,\mathbf x_v\}$, we have the same thing for the second fundamental form.  If $\vec v=a\mathbf x_u+b\mathbf x_v$, then regarding $\mathbf N$ as a function in $U$ and using the Chain Rule, $d\mathbf N_p(\vec v)=a\mathbf N_u+b\mathbf N_v$.  Moreover,
$$-d\mathbf N_p(\vec v)\cdot\vec v=-(a\mathbf N_u+b\mathbf N_v)\cdot(a\mathbf x_u+b\mathbf x_v)$$
$$=a^2(-\mathbf N_u\cdot\mathbf x_u)+ab(-\mathbf N_u\cdot\mathbf x_v-\mathbf N_v\cdot\mathbf x_u)+b^2(-\mathbf N_v\cdot\mathbf x_v).$$
We recall, from the proof of Proposition 6.9(ii), that $\mathbf N_u\cdot\mathbf x_v=\mathbf N_v\cdot\mathbf x_u$ since both are equal to $-\mathbf N\cdot\mathbf x_{uv}$.  We may do similar things to the other dot products; differentiating $\mathbf N\cdot\mathbf x_u=0$ and $\mathbf N\cdot\mathbf x_v=0$ with respect to $u$ and $v$ respectively leads to $\mathbf N_u\cdot\mathbf x_u=-\mathbf N\cdot\mathbf x_{uu}$ and $\mathbf N_v\cdot\mathbf x_v=-\mathbf N\cdot\mathbf x_{vv}$.  Hence we may write
$$-d\mathbf N_p(\vec v)\cdot\vec v=a^2(\mathbf N\cdot\mathbf x_{uu})+2ab(\mathbf N\cdot\mathbf x_{uv})+b^2(\mathbf N\cdot\mathbf x_{vv}),$$
and we have our coefficients, which do not involve differentiating $\mathbf N$ at all.  [Due to being consciously normalized, $\mathbf N$ usually has square roots in its expression, making it a tad bit unsatisfying to differentiate it, so it is convenient that we have eliminated the computation of $\mathbf N$'s derivatives.]\\ % Your revisions give "If we had not defined \mathbf N to be a unit vector, it would have square roots in its expression, making it a tad bit unsatisfying to differentiate it, so it is convenient that we have defined it to be a unit vector."
% That statement is not only not what I meant, but it's wrong.  The cross product \mathbf x_u\times\mathbf x_v doesn't involve any square roots unless \mathbf x_u,\mathbf x_v already have them.  It's the *normalization* which brings in square roots.  So if we didn't define \mathbf N to be a unit vector, it wouldn't have square roots more often than the chart itself.  (You're probably thinking about the fact that if \|\mathbf N\| is not assumed to be 1, it is an expression which often has square roots.)
% Also, the convenient thing is not that \mathbf N is normalized.  It's that we eliminated the need to differentiate \mathbf N.  The naturally-materializing square roots from the normalization make it harder to differentiate \mathbf N once than \mathbf x three times.  (I'll change it to make it clear.)

\noindent\textbf{Proposition 6.12 and Definition.} \emph{The second fundamental form $I\!I_p(a\mathbf x_u+b\mathbf x_v)=La^2+2Mab+Nb^2$, where $L=\mathbf N\cdot\mathbf x_{uu}$, $M=\mathbf N\cdot\mathbf x_{uv}$ and $N=\mathbf N\cdot\mathbf x_{vv}$.  The scalars $L,M,N$ are called the \textbf{coefficients of the second fundamental form}.}\\

\noindent Let us compute these coefficients in some particular cases.

For the plane or a surface contained in the plane, $\mathbf N$ is constant, hence the second fundamental form is clearly zero (because $-d\mathbf N_p(\vec v)\cdot\vec v=-\vec 0\cdot\vec v=0$).  In other words, for the plane, $L=M=N=0$.  [In fact, see Exercise 1.]

Now let us consider a surface of revolution $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$ ($f>0$).  Then we compute
$$\mathbf x_u=(f'(u)\cos v,f'(u)\sin v,g'(u))$$
$$\mathbf x_v=(-f(u)\sin v,f(u)\cos v,0)$$
$$\therefore\mathbf x_u\times\mathbf x_v=(-f(u)g'(u)\cos v,-f(u)g'(u)\sin v,f(u)f'(u))$$
Furthermore, $\|\mathbf x_u\times\mathbf x_v\|=\sqrt{f(u)^2g'(u)^2(\cos^2v+\sin^2v)+f(u)^2f'(u)^2}$\\
\noindent$=\sqrt{f(u)^2(f'(u)^2+g'(u)^2)}=f(u)\sqrt{f'(u)^2+g'(u)^2}$, so that normalizing the above entails
$$\mathbf N=\frac 1{\sqrt{f'(u)^2+g'(u)^2}}(-g'(u)\cos v,-g'(u)\sin v,f'(u)).$$
We may also compute the second-order derivatives of $\mathbf x$, which will lead us to the second fundamental form coefficients:
$$\mathbf x_{uu}=(f''(u)\cos v,f''(u)\sin v,g''(u))$$
$$\mathbf x_{uv}=(-f'(u)\sin v,f'(u)\cos v,0)$$
$$\mathbf x_{vv}=(-f(u)\cos v,-f(u)\sin v,0)$$
And therefore,
$$L=\mathbf N\cdot\mathbf x_{uu}=\frac{f'(u)g''(u)-f''(u)g'(u)}{\sqrt{f'(u)^2+g'(u)^2}}$$
$$M=\mathbf N\cdot\mathbf x_{uv}=0$$
$$N=\mathbf N\cdot\mathbf x_{vv}=\frac{f(u)g'(u)}{\sqrt{f'(u)^2+g'(u)^2}}$$
Earlier we mentioned that we may assume $(f')^2+(g')^2=1$.  In fact, such an assumption is convenient, as it gets rid of the radicals in the expressions: $L=f'(u)g''(u)-f''(u)g'(u)$, $M=0$ and $N=f(u)g'(u)$.

In particular, taking $f(u)=\sin u,g(u)=\cos u$ shows that for the standard parametrization $(\sin u\cos v,\sin u\sin v,\cos u)$ of the sphere, the second fundamental form coefficients are $L=-1$, $M=0$ and $N=-\sin^2u$.  Taking $f(u)=1$, $g(u)=u$ shows that for the cylinder, $L=0$, $M=0$ and $N=1$.

The final example concerns an explicit-function graph, $z=f(x,y)$, which can be parametrized via $\mathbf x(u,v)=(u,v,f(u,v))$.  We recall that $\mathbf x_u=(1,0,f_u)$, $\mathbf x_v=(0,1,f_v)$ and $\mathbf N=\frac 1{\sqrt{1+\|\vec\nabla f\|^2}}(-f_u,-f_v,1)$.  Since $\mathbf x_{uu}=(0,0,f_{uu})$, $\mathbf x_{uv}=(0,0,f_{uv})$ and $\mathbf x_{vv}=(0,0,f_{vv})$, we compute the second fundamental form coefficients to be
$$L=\frac{f_{uu}}{\sqrt{1+\|\vec\nabla f\|^2}},~~~~M=\frac{f_{uv}}{\sqrt{1+\|\vec\nabla f\|^2}},~~~~N=\frac{f_{vv}}{\sqrt{1+\|\vec\nabla f\|^2}}.$$
Now that we understand the coefficients for both the first and second fundamental forms, we can readily derive formulas for the matrix of $d\mathbf N_p$ with respect to the basis $\{\mathbf x_u,\mathbf x_v\}$.

Let us suppose that this matrix is $\begin{bmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix}$, where $a_{ij}$ are differentiable functions in $u,v$.  Then by construction,
$$\mathbf N_u=d\mathbf N_p(\mathbf x_u)=a_{11}\mathbf x_u+a_{21}\mathbf x_v;$$
$$\mathbf N_v=d\mathbf N_p(\mathbf x_v)=a_{12}\mathbf x_u+a_{22}\mathbf x_v.$$
Taking the dot products of each equation with $\mathbf x_u$ and $\mathbf x_v$, we get
$$\left\{\begin{array}{c l}-L=\mathbf N_u\cdot\mathbf x_u=a_{11}E+a_{21}F\\-M=\mathbf N_u\cdot\mathbf x_v=a_{11}F+a_{21}G\end{array}\right.$$
$$\left\{\begin{array}{c l}-M=\mathbf N_v\cdot\mathbf x_u=a_{12}E+a_{22}F\\-N=\mathbf N_v\cdot\mathbf x_v=a_{12}F+a_{22}G\end{array}\right.$$
This can be rewritten in matrix form:
\begin{equation}\tag{*}-\begin{bmatrix}L&M\\M&N\end{bmatrix}=\begin{bmatrix}E&F\\F&G\end{bmatrix}\begin{bmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix}.\end{equation}
Since $\det\begin{bmatrix}E&F\\F&G\end{bmatrix}=EG-F^2>0$, the matrix is nonsingular, and therefore this system in the variables $a_{ij}$ has a unique solution.  In fact, $\begin{bmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix}=-\frac 1{EG-F^2}\begin{bmatrix}G&-F\\-F&E\end{bmatrix}\begin{bmatrix}L&M\\M&N\end{bmatrix}$, from which we derive
$$a_{11}=\frac{FM-GL}{EG-F^2},~~~~a_{12}=\frac{FN-GM}{EG-F^2},~~~~a_{21}=\frac{FL-EM}{EG-F^2},~~~~a_{22}=\frac{FM-EN}{EG-F^2}$$
These equations that formulate the matrix entries are called the \textbf{Weingarten equations}.  Incidentally, the Gaussian curvature $K$ is the determinant of the matrix $\begin{bmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix}$ (being the linear operator's determinant), hence can be computed in terms of these formulas; however, there is a quicker way to get it.  The determinant respects matrix multiplication, so we apply the determinant to both sides of (*) to get
$$LN-M^2=(EG-F^2)K,\text{ and hence }K=\frac{LN-M^2}{EG-F^2}.$$
This provides an easy formula for the Gaussian curvature.  Now for some examples.\\

\noindent The stuation is obvious for the plane $z=0$.  There, $\mathbf N$ is constant, hence $L=M=N=0$ and the Gauss map differential is also zero.

For an explicit-function graph $z=f(x,y)$, parametrized by $\mathbf x(u,v)=(u,v,f(u,v))$, we recall that:
$$E=1+f_u^2,~~~~F=f_uf_v,~~~~~G=1+f_v^2,~~~~EG-F^2=1+f_u^2+f_v^2=1+\|\vec\nabla f\|^2$$
$$L=\frac{f_{uu}}{\sqrt{1+\|\vec\nabla f\|^2}},~~~~M=\frac{f_{uv}}{\sqrt{1+\|\vec\nabla f\|^2}},~~~~N=\frac{f_{vv}}{\sqrt{1+\|\vec\nabla f\|^2}}$$
With that, we immediately compute
$$K=\frac{LN-M^2}{EG-F^2}=\frac{f_{uu}f_{vv}-f_{uv}^2}{(1+\|\vec\nabla f\|)^2}.$$
From this we conclude that, for the graph of an implicit function $z=f(x,y)$, elliptic points occur where $f_{uu}f_{vv}-f_{uv}^2>0$, parabolic and planar points occur where $f_{uu}f_{vv}-f_{uv}^2=0$ and hyperbolic (saddle) points occur where $f_{uu}f_{vv}-f_{uv}^2<0$.  The number $f_{uu}f_{vv}-f_{uv}^2$, which you may observe to be the determinant of $d(\vec\nabla f)=\begin{bmatrix}f_{uu}&f_{uv}\\f_{uv}&f_{vv}\end{bmatrix}$, is called the \textbf{Hessian} of $f$.\footnote{Named after German mathematician, Ludwig Otto Hesse.}

For explicit function graphs, a particularly interesting set of questions revolves around the critical points; i.e., those where $f_u=f_v=0$.  In this case, $K=f_{uu}f_{vv}-f_{uv}^2$ (the denominator is $1$).  If the Hessian is positive, then the graph has a local minimum or maximum at the point, which is a minimum if $f_{uu}>0$ at the point and a maximum if $f_{uu}<0$.  If the Hessian is negative, then the point is a saddle point. % I occasionally get 'implicit' and 'explicit' mixed up, because of the name of the "Implicit Function Theorem": If f:\mathbb R^n\to\mathbb R^m is a smooth map, S = f^{-1}(q), and x\in S where df_x has the first m columns forming a basis of \mathbb R^m, then in some neighborhood of p, S is given by the first m variables being explicit smooth functions in the other n-m.  I keep thinking the theorem is named after what it claims exists, like the Inverse Function Theorem.

We shall now segue to the case of a surface of revolution,\\
\noindent $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$.  Assume further that $(f')^2+(g')^2=1$.  We recall that
$$E=1,~~~~F=0,~~~~G=f(u)^2$$
$$L=f'(u)g''(u)-f''(u)g'(u),~~~~M=0,~~~~N=f(u)g'(u)$$
from which we get:
$$d\mathbf N_p=\begin{bmatrix}-f'(u)g''(u)+f''(u)g'(u)&0\\0&-\frac{g'(u)}{f(u)}\end{bmatrix},$$
(from the Weingarten equations), and
$$K=\frac{LN-M^2}{EG-F^2}=\frac{g'(u)[f'(u)g''(u)-f''(u)g'(u)]}{f(u)}.$$
Note that none of these coefficients involve $v$, because the surface is symmetric about axial rotations, which are the only things that happen when $v$ changes.

If $f(u)=1$ and $g(u)=u$ (the cylinder), then $d\mathbf N_p=\begin{bmatrix}0&0\\0&-1\end{bmatrix}$, which is essentially like what has been found before.

In the case where $f(u)=\sin u$ and $g(u)=\cos u$ (the sphere), $d\mathbf N_p=\begin{bmatrix}1&0\\0&1\end{bmatrix}$ (which makes sense because $\mathbf N$ is the identity map on $S^2$!), and $K=1$.

The last example we will consider is the torus, which is given by $f(u)=a+b\cos(u/b)$ and $g(u)=b\sin(u/b)$, with $a>b>0$ fixed real numbers [we divide $u$ by $b$ in the inputs to ensure the condition $(f')^2+(g')^2=1$, which the above formulas assume].  Curiously enough, half of its points are elliptic and half are hyperbolic.  To see this, we compute the Gaussian curvature (using the above formula) and get
$$K=\frac{\cos(u/b)}{b(a+b\cos(u/b))}.$$
The denominator is always positive since $a>b>0$ and $|\cos(u/b)|\leqslant 1$.  Moreover, the elliptic points occur when $\cos(u/b)>0$, i.e., where $f(u)>a$.  These are the points on the ``outer'' part of the torus.  The hyperbolic points occur when $\cos(u/b)<0$ or $f(u)<a$, the ``inner'' part of the torus, which touches the hole.  Finally, the points where $\cos(u/b)=0$ (i.e., $f(u)=a$) are parabolic points, consisting of the two circles where the tangent planes $z=\pm b$ meet [we leave it to the reader to verify the points are parabolic and not planar].  When you put a sugar-coated donut on the table and pick it up again, the circle of sugar you see on the table came from the parabolic points.\\

\noindent We conclude this section by looking at conjugate directions.  We recall from Exercise 5 of the previous section that $\vec v,\vec w\in T_pS$ are said to be \textbf{conjugate directions} provided that $d\mathbf N_p(\vec v)\cdot\vec w=0$.

Since $d\mathbf N_p$ is self-adjoint, it has an orthonormal eigenbasis $\{\vec u_1,\vec u_2\}$.  We may further assume that this basis is positive in the sense that $\vec u_1\times\vec u_2=\mathbf N(p)$.  Moreover, let $\kappa_1$ and $\kappa_2$ be the respective eigenvalues of $\vec u_1,\vec u_2$.  Then if $\vec v,\vec w\in T_pS$ are unit vectors, one can write $\vec v=(\cos\theta)\vec u_1+(\sin\theta)\vec u_2$ and $\vec w=(\cos\varphi)\vec u_1+(\sin\varphi)\vec u_2$.  Moreover,
$$d\mathbf N_p(\vec v)=d\mathbf N_p((\cos\theta)\vec u_1+(\sin\theta)\vec u_2)=(\cos\theta)d\mathbf N_p(\vec u_1)+(\sin\theta)d\mathbf N_p(\vec u_2)$$
$$=(\cos\theta)(\kappa_1\vec u_1)+(\sin\theta)(\kappa_2\vec u_2)=(\kappa_1\cos\theta)\vec u_1+(\kappa_2\sin\theta)\vec u_2$$
$$\therefore d\mathbf N_p(\vec v)\cdot\vec w=[(\kappa_1\cos\theta)\vec u_1+(\kappa_2\sin\theta)\vec u_2]\cdot[(\cos\varphi)\vec u_1+(\sin\varphi)\vec u_2]$$
$$=\kappa_1\cos\theta\cos\varphi+\kappa_2\sin\theta\sin\varphi.$$
In particular, conjugate directions occur when $\kappa_1\cos\theta\cos\varphi+\kappa_2\sin\theta\sin\varphi=0$.  If the point is planar ($\kappa_1=\kappa_2=0$), clearly all directions are conjugate.  If the point is parabolic (say, $\kappa_1=0$ but $\kappa_2\ne 0$), then conjugate directions occur when $\sin\theta\sin\varphi=0$; i.e., either $\theta$ or $\varphi$ (or both) is an integer multiple of $\pi$.  Thus in the parabolic case, there is one pair of opposite directions conjugate to all the directions.

However, for elliptic or hyperbolic points, the situation is more subtle.  Every direction is then conjugate to exactly two (opposite) directions.  See Exercise 10 for a geometric construction of conjugate directions.

A direction $\vec v\in T_pS$ is said to be \textbf{asymptotic} if it is conjugate to itself.  If $\vec v=(\cos\theta)\vec u_1+(\sin\theta)\vec u_2$, then we get (from the above formula) that $\vec v$ is asymptotic if and only if $\kappa_1\cos^2\theta+\kappa_2\sin^2\theta=0$.  Observe that such directions do not exist at elliptic points, but they do exist at parabolic points (if $\kappa_1=0$ and $\kappa_2\ne 0$, then the only asymptotic directions are $\pm\vec u_1$).  At a planar point, every direction is asymptotic.  A hyperbolic point has exactly four asymptotic directions, since for such a point $\kappa_1$ and $\kappa_2$ have opposite signs and one can arrange for $\theta=\pm\tan^{-1}\sqrt{-\kappa_1/\kappa_2}$.

A curve $\alpha:I\to S$ is said to be an \textbf{asymptotic curve} if all its derivatives go in asymptotic directions; i.e., $\alpha'(t)$ is an asymptotic direction at $\alpha(t)$ for each $t\in I$.  Basic examples are any curve in the plane, and a generating line of the cylinder.  Exercise 11 covers a curious theorem about them.

\subsection*{Exercises 6.6. (The Second Fundamental Form)} % Introduce the SFF coefficients, use them to get the Gauss map's differential's matrix
% Also cover conjugate directions.
% POTENTIAL EXERCISE: the Dupin indicatrix, Beltrami-Enneper's (last page of https://faculty.math.illinois.edu/~kapovich/423-14/surfacesofrevolution.pdf )
\begin{enumerate}
\item Show that a connected regular surface is contained in a plane if and only if, for some parametrization, its second fundamental form coefficients $L,M,N$ are all zero.

\item Let $S$ be a regular surface with no umbilical points, and $\mathbf x$ a local coordinate chart.  Show that the coordinate curves are lines of curvature if and only if $F=M=0$.  [Compare with Exercise 6(b) of the previous section.]

\item Show that dilating a regular surface by a factor of $\lambda>0$ multiplies the Gaussian curvature by $\frac 1{\lambda^2}$.  In particular, the sphere of radius $r$ has Gaussian curvature $\frac 1{r^2}$.

\item Suppose $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$ (with $f>0$) is a surface of revolution, and that there is a smooth function $\theta(u)$ such that $f'(u)=\sin\theta(u)$ and $g'(u)=\cos\theta(u)$.  Hence, the generating curve is parametrized by arc length, and $\theta$ gives the direction in which it points.

Recall (from Section 6.3) that the first fundamental form is $ds^2=du^2+f(u)^2\,dv^2$; i.e., $E=1$, $F=0$ and $G=f(u)^2$.

(a) Find $\mathbf N$ in terms of $u$ and $v$.

(b) Show that $L=-\theta'(u)$, $M=0$ and $N=f(u)\cos\theta(u)$.  Conclude that the Gaussian curvature $K$ is equal to $\frac{-\theta'(u)\cos\theta(u)}{f(u)}$.  [See if you can get an informal understanding of why $K$ varies directly with the generating curve's curvature (which is $\theta'(u)$), directly with the slope toward or away from the revolution axis (which is $\cos\theta(u)$), and inversely with the parallel's radius (which is $f(u)$).]

\item If $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$ is a surface of revolution, but we are \emph{not} assuming that $(f')^2+(g')^2=1$, show that
$$K=\frac{g'(u)[f'(u)g''(u)-f''(u)g'(u)]}{f(u)[f'(u)^2+g'(u)^2]}.$$
[This essentially mirrors the argument when $(f')^2+(g')^2=1$.]

\item Let $\mathbf x$ be the pseudosphere (Exercise 6(d) of Section 6.3):
$$\mathbf x(u,v)=(\sin u\cos v,\sin u\sin v,\cos u+\ln(\tan(u/2)))$$
Show that the Gaussian curvature of $\mathbf x$ is $-1$ everywhere.  [This is the kind of surface on which hyperbolic geometry can be done, at least locally.  For further details, see Section 6.10.]

\item If $\vec v,\vec w\in T_pS$ are orthonormal, show that $K=I\!I_p(\vec v)I\!I_p(\vec w)-[d\mathbf N_p(\vec v)\cdot\vec w]^2$.  [This expression is the determinant of the matrix $\begin{bmatrix}-I\!I_p(\vec v)&d\mathbf N_p(\vec v)\cdot\vec w\\d\mathbf N_p(\vec v)\cdot\vec w&-I\!I_p(\vec w)\end{bmatrix}$, and this matrix is equal to
$$\begin{bmatrix}d\mathbf N_p(\vec v)\cdot\vec v&d\mathbf N_p(\vec v)\cdot\vec w\\d\mathbf N_p(\vec v)\cdot\vec w&d\mathbf N_p(\vec w)\cdot\vec w\end{bmatrix}=\begin{bmatrix}\leftarrow&\vec v&\rightarrow\\\leftarrow&\vec w&\rightarrow\end{bmatrix}d\mathbf N_p\begin{bmatrix}\uparrow&\uparrow\\\vec v&\vec w\\\downarrow&\downarrow\end{bmatrix}$$
Now take determinants throughout.]

\item Let $\alpha:I\to S$ be a regular curve, say $\alpha(t)=\mathbf x(u(t),v(t))$ where $\mathbf x$ is a local coordinate chart.  If $E,F,G$ and $L,M,N$ are the fundamental form coefficients for $\mathbf x$, show that $\alpha$ is a line of curvature if and only if $\det\begin{bmatrix}(v')^2&-u'v'&(u')^2\\E&F&G\\L&M&N\end{bmatrix}=0$.  [If $d\mathbf N_p(\alpha'(t))=\lambda(t)\alpha'(t)$, taking coordinates with respect to the basis $\mathbf x_u,\mathbf x_v$ shows that $a_{11}u'+a_{12}v'=\lambda u'$ and $a_{21}u'+a_{22}v'=\lambda v'$, where $a_{ij}$ are given by the Weingarten equations.  Now multiply these equations by $v'$ and $u'$ respectively and subtract them.]

\item\emph{(Surfaces of revolution with constant Gaussian curvature.)} \---- The aim of this exercise is to classify surfaces of revolution with constant Gaussian curvature.  We recall that we may parametrize a surface of revolution via $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$ with $f>0$ and $(f')^2+(g')^2=1$.  We also know that
$$K=\frac{g'(u)[f'(u)g''(u)-f''(u)g'(u)]}{f(u)}.$$
Now let us suppose $K$ is constant.

(a) Show that $f$ satisfies the differential equation $f''+Kf=0$.  [First show that $f'f''+g'g''=0$ by differentiating the equation $(f')^2+(g')^2=1$.  Then show that $K=\frac{(f'f''+g'g'')f'-f''}f$ by basic algebraic manipulation, at one point using the fact that $(g')^2=1-(f')^2$.  From this it follows that $K=-\frac{f''}f$, hence $f''+Kf=0$.]

(b) If $K=0$, then part (a) implies $f''=0$, hence $f'$ is constant, say $f'(u)=c$.  With that, $g'(u)=\sqrt{1-f'(u)^2}=\sqrt{1-c^2}$.  Use this to show that the surface is part of either a plane, cone or cylinder.

(c) If $K=1$, then we get $f(u)=c_1\cos u+c_2\sin u$ by solving part (a)'s equation.  We may assume $c_2=0$ by translating the input $u$ in the parametrization $\mathbf x$; hence $f(u)=c\cos(u)$.  Show that $g'(u)=\sqrt{1-c^2\sin^2u}$.  Determine that the surface may have three different appearances, depending on whether $c>1$, $c=1$ or $c<1$.  Sketch each appearance.

(d) If $K=-1$, then we have $f(u)=c_1e^u+c_2e^{-u}$ by solving part (a)'s equation.  Show that by translating the input $u$ and/or negating, $f(u)$ can be arranged to exactly one of the following forms:

~~~~(i) $f(u)=c\cosh u$;

~~~~(ii) $f(u)=c\sinh u$;

~~~~(iii) $f(u)=ce^u$.

In each case, determine and sketch the appearance of the surface.  Also show that in case (iii) the surface is the pseudosphere.

(e) Use Exercise 3 to complete the classification with $K$ any real constant whatsoever.

\item\emph{(The Dupin Indicatrix.)} \---- Let $p\in S$ be a point of a regular surface.  The \textbf{Dupin indicatrix} at $p$ is defined to be the set $\Delta(p)=\{\vec v\in T_pS:I\!I_p(\vec v)=\pm 1\}$.

(a) If $p$ is planar, $\Delta(p)=\varnothing$.  If $p$ is parabolic, $\Delta(p)$ is a union of two parallel lines.  If $p$ is elliptic, $\Delta(p)$ is an ellipse.  If $p$ is hyperbolic, $\Delta(p)$ is a union of two hyperbolas sharing the same asymptotes.

(b) A vector $\vec v\in T_pS$ is asymptotic if and only if either $p$ is planar, $p$ is parabolic and $\vec v$ is parallel to the lines, or $p$ is hyperbolic and $\vec v$ is contained in one of the asymptotes of the hyperbolas.

(c) If a vector $\vec v\in T_pS$ extends to meet the Dupin indicatrix $\Delta(p)$ at $q$, and $\ell$ is the tangent line to the Dupin indicatrix at $q$, then $\vec v$ is conjugate to any direction parallel to $\ell$.  This is a geometric derivation of conjugate directions from the Dupin indicatrix.  [Fix an orthonormal eigenbasis of $d\mathbf N_p$, find an algebraic formula for the Dupin indicatrix, and use multivariable calculus.]

\item\emph{(Beltrami-Enneper's Theorem.)} \---- Let $\alpha:I\to S$ be an asymptotic curve on a regular surface, and let $\tau$ be its torsion.

(a) Show that the normal curvature of $\alpha$ is zero; i.e., $\boldsymbol\nu\perp\mathbf N$ along $\alpha$ where $\boldsymbol\nu$ is the normal unit vector in the Frenet trihedron.  [Use Exercise 7(b) of the previous section, which states that the normal curvature in a direction is the second fundamental form applied to the unit vector in that direction.]

(b) Show that $\tau^2=-K$ throughout $\alpha$.  [Reparametrize to assume $\alpha$ is an arc-length parametrization $J\to S$.  In the Frenet trihedron of $\alpha$, $\boldsymbol\tau$ is tangent to $S$, and so is $\boldsymbol\nu$ by part (a).  Therefore $\boldsymbol\beta$ is normal to $S$; i.e., $\boldsymbol\beta=\pm\mathbf N$.  Moreover, $\tau(s)=-\boldsymbol\beta'(s)\cdot\boldsymbol\nu(s)=\mp\mathbf N'(s)\cdot\boldsymbol\nu(s)=\mp d\mathbf N_p(\boldsymbol\tau(s))\cdot\boldsymbol\nu(s)$.  Now apply Exercise 7 with $\vec v=\boldsymbol\tau(s),\vec w=\boldsymbol\nu(s)$.]

\item Let $S\subset\mathbb R^n$ be a hypersurface.  Recall that on a local coordinate chart $\mathbf x:U\to S$, one can define a Gauss map $\mathbf N$.  With enough mathematical experience, it should be easy to generalize this chapter's results.

For $1\leqslant i,j\leqslant n-1$, define $M_{ij}=-\mathbf N_{u_i}\cdot\mathbf x_{u_j}=\mathbf N\cdot\mathbf x_{u_iu_j}$.  Note that $M_{ij}=M_{ji}$.  The $M_{ij}$'s are called the \textbf{coefficients of the second fundamental form}.  We let $M$ be the (symmetric) matrix $\begin{bmatrix}M_{ij}\end{bmatrix}_{1\leqslant i,j\leqslant n-1}$.

(a) Let $A$ be the matrix of $d\mathbf N_p$ with respect to the basis $\{\mathbf x_{u_1},\dots,\mathbf x_{u_{n-1}}\}$.  Show that $-M=FA$.  [By definition, $\mathbf N_{u_i}=d\mathbf N_p(\mathbf x_{u_i})=\sum_{j=1}^{n-1}A_{ji}\mathbf x_{u_j}$.  Hence taking the dot product with $\mathbf x_{u_k}$ entails $-M_{ik}=\sum_{j=1}^{n-1}A_{ji}F_{jk}$.]  Conclude that $A=-F^{-1}M$, and hence each entry of $A$ can be expressed as the quotient of a polynomial in the $F_{ij}$ and $M_{ij}$, by $\det F$ (and that this polynomial doesn't depend on what the $F_{ij}$ and $M_{ij}$ are). % What I mean is that, there is a single element of \mathbb R[x_{ij},y_{ij}] such that, for any hypersurface whatsoever, substituting F_{ij}=x_{ij},M_{ij}=x_{ij} gives the matrix entries of (\det F)\cdot A.  The element of the polynomial ring doesn't depend on the particular hypersurface.  Is there a better way to say this?

(b) Show that the main curvature at the point is $(-1)^{n-1}\frac{\det M}{\det F}$.

(c) The main curvature of a hyperplane is $0$, and the main curvature of a hypersphere of radius $R$ (when suitably oriented) is $\frac 1{R^{n-1}}$.

(d) Suppose $\mathbf x(u_1,\dots,u_{n-1})=(u_1,\dots,u_{n-1},f(u_1,\dots,u_{n-1}))$ is a graph of an explicit function.  We recall (Exercise 12(f) of Section 6.3) that $F_{ij}=\delta_{ij}+f_{u_i}f_{u_j}$ (so that $F_{ij}=f_{u_i}f_{u_j}$ for $i\ne j$ and $F_{ii}=1+f_{u_i}^2$).  We also have $\det F=1+\|\vec\nabla f\|^2$.  Show that $\mathbf N=\frac 1{\sqrt{1+\|\vec\nabla f\|^2}}(-f_{u_1},-f_{u_2},\dots,-f_{u_{n-1}},1)$. [It would help to use Exercise 10 of the previous section, and think of Laplacian expansion when determining the entries of the cross product.]  Conclude that $M_{ij}=\frac{f_{u_iu_j}}{\sqrt{1+\|\vec\nabla f\|^2}}$, and that the main curvature of the graph is $\frac{\det\mathbf H}{(1+\|\vec\nabla f\|^2)^{(n+1)/2}}$, where $\mathbf H$ is the Hessian matrix $\begin{bmatrix}f_{u_1u_1}&\dots&f_{u_1u_{n-1}}\\\vdots&\ddots&\vdots\\f_{u_{n-1}u_1}&\dots&f_{u_{n-1}u_{n-1}}\end{bmatrix}$.

(e) Let $\alpha(t)=(f(t),g(t))$ be an embedded curve with $f>0$ and $(f')^2+(g')^2=1$, and let $\mathbf x$ be the \textbf{3-manifold of ball-and-socket revolution}:
$$\mathbf x(u,v,w)=(f(u)\sin v\cos w,f(u)\sin v\sin w,f(u)\cos v,g(u)).$$
[Notice that the coordinate $2$-surfaces $(v,w)\mapsto\mathbf x(u_0,v,w)$ are spheres.]  Find the partial derivatives of $\mathbf x$, the first fundamental form coefficients, the Gauss map, the second fundamental form coefficients, and the main curvature.
\end{enumerate}

\subsection*{6.7. The Gauss and Mainardi-Codazzi Equations}
\addcontentsline{toc}{section}{6.7. The Gauss and Mainardi-Codazzi Equations}
Now that we thoroughly understand the Gauss map and second fundamental form, we shall introduce yet \emph{another} collection of coefficients, which (unlike the second fundamental form), can be derived algebraically from the first fundamental form coefficients and some of their derivatives.  We will then use them to derive equations giving necessary and sufficient conditions on the coefficients $E,F,G,L,M,N$ (in addition to the inequalities $E>0,EG-F^2>0$) for regular surfaces to exist.

Let $\mathbf x:U\to S$ be a local coordinate chart, and assume $\mathbf N=\frac{\mathbf x_u\times\mathbf x_v}{\|\mathbf x_u\times\mathbf x_v\|}$.  Then at any point $p\in U$, $\mathbf x_u,\mathbf x_v,\mathbf N$ form a basis of $\mathbb R^3$ (because $\mathbf x_u,\mathbf x_v$ are linearly independent and $\mathbf N$ is normal to the plane they span).  Thus we can express any vector \---- in particular $\mathbf x_{uu},\mathbf x_{uv},\mathbf x_{vv},\mathbf N_u,\mathbf N_v$ \---- as a unique linear combination of them with scalar coefficients.  Of course, the scalars depend on the point $p$, but it is not hard to see that they are differentiable with respect to $p$.

In other words, we may fill in the question marks below with unique scalar functions:
\begin{align*}
\mathbf x_{uu}&=[?]~\mathbf x_u+[?]~\mathbf x_v+[?]~\mathbf N\\
\mathbf x_{uv}&=[?]~\mathbf x_u+[?]~\mathbf x_v+[?]~\mathbf N\\
\mathbf x_{vv}&=[?]~\mathbf x_u+[?]~\mathbf x_v+[?]~\mathbf N\\
\mathbf N_u&=[?]~\mathbf x_u+[?]~\mathbf x_v+[?]~\mathbf N\\
\mathbf N_v&=[?]~\mathbf x_u+[?]~\mathbf x_v+[?]~\mathbf N
\end{align*}
The reason we did not write fifteen separate variables is that most of the coefficients can already be found from the previous results of the chapter.  For example, the coefficients of $\mathbf N$ on each line are obtained by merely taking dot products of the linear combinations with $\mathbf N$.  (Though $\mathbf x_u,\mathbf x_v,\mathbf N$ is not generally an orthonormal basis, $\mathbf N$ is still a unit vector orthogonal to the other two.)

Thus, the coefficient of $\mathbf N$ on the top line is $\mathbf N\cdot\mathbf x_{uu}$, which is the second fundamental form coefficient $L$.  Similarly, the next two lines have $M$ and $N$ for the coefficient of $\mathbf N$, and the last two lines (with $\mathbf N_u$ and $\mathbf N_v$) have zero for the coefficient of $\mathbf N$: since $\mathbf N\cdot\mathbf N=1$, a constant, taking partial derivatives entails $\mathbf N\cdot\mathbf N_u=0$ and $\mathbf N\cdot\mathbf N_v=0$.  This fills in the third column of question marks.

We also know the remaining coefficients of $\mathbf N_u$'s and $\mathbf N_v$'s linear combinations; they are given by the Weingarten equations covered in the previous section.  If
$$a_{11}=\frac{FM-GL}{EG-F^2},~~~~a_{12}=\frac{FN-GM}{EG-F^2},~~~~a_{21}=\frac{FL-EM}{EG-F^2},~~~~a_{22}=\frac{FM-EN}{EG-F^2},$$
then $\mathbf N_u=a_{11}\mathbf x_u+a_{21}\mathbf x_v$ and $\mathbf N_v=a_{12}\mathbf x_u+a_{22}\mathbf x_v$, filling in the question marks on the bottom two rows.

However, the remaining six question marks \---- the coefficients of $\mathbf x_u,\mathbf x_v$ in the second partial derivatives of $\mathbf x$ \---- are actually new to us; we have not explicitly dealt with them before.  They are denoted $\Gamma_{ij}^k$ and are called the \textbf{Christoffel symbols} for the local parametrization.  We will shortly show how to derive them from the other coefficients; first, let us rewrite our handy linear combinations:
\begin{align*}
&~~~~\text{(A)}\\
\mathbf x_{uu}&=\Gamma_{11}^1\mathbf x_u+\Gamma_{11}^2\mathbf x_v+L\mathbf N\\
\mathbf x_{uv}&=\Gamma_{12}^1\mathbf x_u+\Gamma_{12}^2\mathbf x_v+M\mathbf N\\
\mathbf x_{vv}&=\Gamma_{22}^1\mathbf x_u+\Gamma_{22}^2\mathbf x_v+N\mathbf N\\
\mathbf N_u&=a_{11}\mathbf x_u+a_{21}\mathbf x_v\\
\mathbf N_v&=a_{12}\mathbf x_u+a_{22}\mathbf x_v
\end{align*}
[The superscript $2$'s have nothing to do with squaring; every Christoffel symbol has a superscript which indexes the term it touches.]  Observe that if we rewrite $u=u_1,v=u_2$, then in each case $\Gamma_{ij}^k$ is the coefficient of $\mathbf x_{u_k}$ in $\mathbf x_{u_iu_j}$.  Under said convention, we would also have $\mathbf x_{vu}=\Gamma_{21}^1\mathbf x_u+\Gamma_{21}^2\mathbf x_v+M\mathbf N$; however, the symmetry $\mathbf x_{vu}=\mathbf x_{uv}$ entails that $\Gamma_{12}^k=\Gamma_{21}^k$ for each $k$.  Thus Christoffel symbols are symmetric in their lower indices.

To derive the Christoffel symbols, we start by taking the dot products of each of the first three equations (A) with $\mathbf x_u$ and $\mathbf x_v$.  In each line below, the second expression is obtained by plugging in the linear combination and using the bilinearity of the dot product; and the third expression can be worked out using Exercise 4(a) of Section 6.1.  For example, $(\Gamma_{11}^1\mathbf x_u+\Gamma_{11}^2\mathbf x_v+L\mathbf N)\cdot\mathbf x_u=\Gamma_{11}^1(\mathbf x_u\cdot\mathbf x_u)+\Gamma_{11}^2(\mathbf x_v\cdot\mathbf x_u)+L(\mathbf N\cdot\mathbf x_u)=\Gamma_{11}^1E+\Gamma_{11}^2F$; and we have $E_u=\frac{\partial E}{\partial u}=\frac{\partial}{\partial u}(\mathbf x_u\cdot\mathbf x_u)=2\mathbf x_u\cdot\mathbf x_{uu}$.
$$\left\{\begin{array}{c l}\mathbf x_{uu}\cdot\mathbf x_u=\Gamma_{11}^1E+\Gamma_{11}^2F=\frac 12E_u~~~~~~~\\\mathbf x_{uu}\cdot\mathbf x_v=\Gamma_{11}^1F+\Gamma_{11}^2G=F_u-\frac 12E_v\end{array}\right.$$
$$\left\{\begin{array}{c l}\mathbf x_{uv}\cdot\mathbf x_u=\Gamma_{12}^1E+\Gamma_{12}^2F=\frac 12E_v~~~~~~~\\\mathbf x_{uv}\cdot\mathbf x_v=\Gamma_{12}^1F+\Gamma_{12}^2G=\frac 12G_u~~~~~~~\end{array}\right.$$
$$\left\{\begin{array}{c l}\mathbf x_{vv}\cdot\mathbf x_u=\Gamma_{22}^1E+\Gamma_{22}^2F=F_v-\frac 12G_u\\\mathbf x_{vv}\cdot\mathbf x_v=\Gamma_{22}^1F+\Gamma_{22}^2G=\frac 12G_v~~~~~~~\end{array}\right.$$
Each braced pair of equations is a system of linear equations in the variables $\Gamma_{ij}^k$, whose determinant is $EG-F^2>0$; hence the equations have unique solutions.  In fact, the Christoffel symbols are polynomials in $E,F,G$ and their first order derivatives divided by $EG-F^2$ (as linear algebra readily shows), and said polynomials apply universally to any local coordinate chart of any regular surface, without depending on the functions $E,F,G$ themselves.

Perhaps an example would be in order.  Let $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$ be a surface of revolution with $f>0$ and $(f')^2+(g')^2=1$.  We recall that:
$$E=1,~~~~F=0,~~~~G=f(u)^2$$
and we may solve the above linear equations.  For instance, $\Gamma_{11}^1E+\Gamma_{11}^2F=\frac 12E_u$ becomes $\Gamma_{11}^1=0$ when substituting $E,F,G$ from above.  Similarly, from $\Gamma_{12}^1F+\Gamma_{12}^2G=\frac 12G_u$, we get $\Gamma_{12}^2f(u)^2=f'(u)f(u)$, and hence $\Gamma_{12}^2=\frac{f'(u)}{f(u)}$.  The rest of the coefficients can be likewise computed:
$$\Gamma_{11}^1=0,~~~~\Gamma_{11}^2=0$$
$$\Gamma_{12}^1=0,~~~~\Gamma_{12}^2=\frac{f'(u)}{f(u)}$$
$$\Gamma_{22}^1=-f'(u)f(u),~~~~\Gamma_{22}^2=0$$
and the reader is encouraged to actually check the equations (A).\\

\noindent\textbf{THE EQUATIONS OF COMPABILITY}\\

\noindent We recall (Proposition 6.3) that regular curves exist with any prescribed curvature and torsion as differentiable functions in the arc length parameter (with the curvature positive).  As we are about to see, this is not quite true for the six coefficients $E,F,G,L,M,N$ for regular surfaces, as there are necessary conditions involving them (and their derivatives) which obviously don't hold for arbitrary functions in general.

To start, we use the symmetry of mixed partial derivatives to state:
$$(\mathbf x_{uu})_v=(\mathbf x_{uv})_u,~~~~~~~~(\mathbf x_{uv})_v=(\mathbf x_{vv})_u,~~~~~~~~(\mathbf N_u)_v=(\mathbf N_v)_u$$
We shall first substitute the equations (A) into $(\mathbf x_{uu})_v=(\mathbf x_{uv})_u$ to get:
$$\left(\Gamma_{11}^1\mathbf x_u+\Gamma_{11}^2\mathbf x_v+L\mathbf N\right)_v=\left(\Gamma_{12}^1\mathbf x_u+\Gamma_{12}^2\mathbf x_v+M\mathbf N\right)_u$$
Then we expand both sides, using the Product Rule several times:
$$(\Gamma_{11}^1)_v\mathbf x_u+\Gamma_{11}^1\mathbf x_{uv}+(\Gamma_{11}^2)_v\mathbf x_v+\Gamma_{11}^2\mathbf x_{vv}+L_v\mathbf N+L\mathbf N_v$$
$$=(\Gamma_{12}^1)_u\mathbf x_u+\Gamma_{12}^1\mathbf x_{uu}+(\Gamma_{12}^2)_u\mathbf x_v+\Gamma_{12}^2\mathbf x_{uv}+M_u\mathbf N+M\mathbf N_u$$
And now we have $\mathbf x_{uu},\mathbf x_{uv},\mathbf x_{vv},\mathbf N_u,\mathbf N_v$ once again, into which we can substitute the equations (A); e.g., we substitute $\Gamma_{11}^1\mathbf x_u+\Gamma_{11}^2\mathbf x_v+L\mathbf N$ into $\mathbf x_{uu}$. % The equations (A) already indicate which to substitute into which, but I guess I'll state an example.
However, doing so directly will give us long convoluted expressions, so we will focus on just \emph{one coordinate at a time}.  Let us write out the $\mathbf x_v$ coordinates when both sides of the above equation are written as linear combinations of $\mathbf x_u,\mathbf x_v,\mathbf N$.  In other words, we will pass the equation through the linear map $a\mathbf x_u+b\mathbf x_v+c\mathbf N\mapsto b$.  Then, for example, $\mathbf x_{uv}$ gets passed to $\Gamma_{12}^2$, and $\mathbf x_u$ and $\mathbf N$ get passed to zero. % Reworded.  I don't think the reader will think of it as \mathbf x_u becoming zero now.
$$\Gamma_{11}^1\Gamma_{12}^2+(\Gamma_{11}^2)_v+\Gamma_{11}^2\Gamma_{22}^2+La_{22}=\Gamma_{12}^1\Gamma_{11}^2+(\Gamma_{12}^2)_u+\Gamma_{12}^2\Gamma_{12}^2+Ma_{21}$$
We substitute the Weingarten equations for $a_{22}$ and $a_{21}$, then rearrange terms algebraically to get:
$$\Gamma_{11}^1\Gamma_{12}^2+(\Gamma_{11}^2)_v+\Gamma_{11}^2\Gamma_{22}^2-\Gamma_{12}^1\Gamma_{11}^2-(\Gamma_{12}^2)_u-\Gamma_{12}^2\Gamma_{12}^2=E\frac{LN-M^2}{EG-F^2}=EK.$$
This is known as the \textbf{Gauss equation}.  Since $E>0$, we can even divide both sides by $E$, to make the right-hand side equal to the Gaussian curvature $K$.

It is difficult to realize just what we have come to at this point.  Through an extreme amount of algebra and calculus, we have managed to derive $K$ directly from $E,F,G$ and their derivatives up to the second order.  [After all, the Christoffel symbols $\Gamma_{ij}^k$ are derivable from $E,F,G$ and their first-order derivatives, and the expression above uses the Christoffel symbols and \emph{their} first order derivatives.]  By Proposition 6.6, if surfaces are locally isometric, then (for suitable coordinate charts) they have identical first fundamental form coefficients, hence agree on anything directly derivable from them.  This proves\\

\noindent\textbf{Theorem 6.13.} \textsc{(Theorema Egregium)} \emph{The Gaussian curvature of regular surfaces is invariant under local isometries.}\\

\noindent Of course, this was not obvious before, as the Gaussian curvature seemed to depend on the Gauss map and the second fundamental form.  The above formula for $K$ in terms of $E,F,G$ and their derivatives up to the second order is called the \textbf{Gauss formula}.

For an example of how to use this, consider the surface of revolution $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$ with $f>0$ and $(f')^2+(g')^2=1$. % I guess overfull hboxes are fatal since you said "at the end, check all chapters for overfull hboxes" in red.  I could check overfull hboxes, but I don't know a better way to get rid of them besides making a new paragraph without an indent.
Then we have previously found $\Gamma_{11}^1=\Gamma_{11}^2=\Gamma_{12}^1=\Gamma_{22}^2=0$, $\Gamma_{12}^2=\frac{f'(u)}{f(u)}$ and $\Gamma_{22}^1=-f'(u)f(u)$.  Thus four terms vanish in the Gauss formula, leaving us with:
$$-(\Gamma_{12}^2)_u-\Gamma_{12}^2\Gamma_{12}^2=EK=K$$
Elementary calculus shows that $(\Gamma_{12}^2)_u=\frac{f''(u)f(u)-f'(u)^2}{f(u)^2}$, and hence
$$K=-\frac{f''(u)f(u)-f'(u)^2}{f(u)^2}-\left(\frac{f'(u)}{f(u)}\right)^2=-\frac{f''(u)}{f(u)}.$$
We have previously found the Gaussian curvature to be $\frac{g'(u)[f'(u)g''(u)-f''(u)g'(u)]}{f(u)}$, but from the assumption $(f')^2+(g')^2=1$ it follows that that expression is equal to $-\frac{f''(u)}{f(u)}$; see the hint for Exercise 9(a) of the previous section.  Thus when the generating curve is parametrized by arc-length, (well, this fact doesn't have to do with how it's parametrized anyway), the curvature is positive when the curve is concave towards the rotation axis (so that $f''<0$), negative when the curve is concave away from the axis, and zero at the curve's inflection points; this is easy to see just by mentally picturing the surface.

So far, we have obtained a formula by working out the $\mathbf x_v$ coordinate of the equation $(\mathbf x_{uu})_v=(\mathbf x_{uv})_u$.  Recall that there are three of these starting points:
$$(\mathbf x_{uu})_v=(\mathbf x_{uv})_u,~~~~~~~~(\mathbf x_{uv})_v=(\mathbf x_{vv})_u,~~~~~~~~(\mathbf N_u)_v=(\mathbf N_v)_u$$
and for each one, there are three ways to get equations: by taking the coordinates of $\mathbf x_u$, $\mathbf x_v$ or $\mathbf N$.  Thus there are nine equations in total we can get out of the symmetry of mixed partial derivatives.  However, most of them are actually redundant; for instance, taking the $\mathbf x_u$ coefficients in $(\mathbf x_{uu})_v=(\mathbf x_{uv})_u$ entails
$$(\Gamma_{11}^1)_v+\Gamma_{11}^2\Gamma_{22}^1-(\Gamma_{12}^1)_u-\Gamma_{12}^2\Gamma_{12}^1=Ma_{11}-La_{12}=-FK;$$
this can be proven to be equivalent to the Gauss formula (at least when $F=0$), but we will not delve into details here.  

We leave it to the reader skilled in manipulation of expressions to try all the other possibilities, and observe that there are only two more which are irredundant from each other and from the Gauss formula: these two are
$$L_v-M_u=\Gamma_{12}^1L+(\Gamma_{12}^2-\Gamma_{11}^1)M-\Gamma_{11}^2N$$
(obtained by taking $\mathbf N$'s coordinates in $(\mathbf x_{uu})_v=(\mathbf x_{uv})_u$), and
$$M_v-N_u=\Gamma_{22}^1L+(\Gamma_{22}^2-\Gamma_{12}^1)M-\Gamma_{12}^2N$$
(obtained by doing the same for $(\mathbf x_{uv})_v=(\mathbf x_{vv})_u$).  These two equations are called the \textbf{Mainardi-Codazzi equations}.

The Gauss and Mainardi-Codazzi equations are thus tantamount to $\mathbf x_u,\mathbf x_v,\mathbf N$ having symmetry of mixed partial derivatives when expanded.  Using this fact, one can show that they are exactly what's needed for regular surfaces to exist with prescribed coefficients $E,F,G,L,M,N$.  These equations are known as the \textbf{equations of compatibility}.

Before we can get going with the proof, we need another result on differential equations, similar to that of Theorem 6.2.  Its proof is due to the same authors as Theorem 6.2.\\

\noindent\textbf{Theorem 6.14.} \textsc{(Fundamental Theorem of Partial Differential Equations)} \emph{If $f_i(u,v,x_1,\dots,x_n),g_i(u,v,x_1,\dots,x_n),1\leqslant i\leqslant n$ are differentiable functions which are defined for $(u,v)\in U$, and $(u_0,v_0)$ is a fixed element of $U$, then consider the system of partial differential equations}
$$\frac{\partial x_1}{\partial u}=f_1(u,v,x_1,x_2,\dots,x_n),~~~~\frac{\partial x_1}{\partial v}=g_1(u,v,x_1,x_2,\dots,x_n)$$
$$\frac{\partial x_2}{\partial u}=f_2(u,v,x_1,x_2,\dots,x_n),~~~~\frac{\partial x_2}{\partial v}=g_2(u,v,x_1,x_2,\dots,x_n)$$
$$\vdots$$
$$\frac{\partial x_n}{\partial u}=f_n(u,v,x_1,x_2,\dots,x_n),~~~~\frac{\partial x_n}{\partial v}=g_n(u,v,x_1,x_2,\dots,x_n)$$
\begin{center}
\emph{with given initial conditions $x_i(u_0,v_0)=a_i$ ($a_i\in\mathbb R$).}
\end{center}
\emph{If, for every $1\leqslant i\leqslant n$, $\frac{\partial f_i}{\partial v}=\frac{\partial g_i}{\partial u}$ (i.e., the expressions coincide when the Chain Rule is used and then the above equations are substituted for the derivatives of the $x$'s), the system has a unique solution in some open neighborhood of $(u_0,v_0)$ in $U$.}\\

\noindent In other words, partial differential equations have a unique solution provided that the partial derivatives of the same functions satisfy mixed partial derivative symmetry.  With that, we have our desired theorem about surfaces with prescribed coefficients:\\

\noindent\textbf{Theorem 6.15.} \textsc{(Bonnet's Theorem)} \emph{Let $U\subset\mathbb R^2$ be an open set, and $E,F,G,L,M,N:U\to\mathbb R$ differentiable functions, such that $E>0$, $EG-F^2>0$, and the Gauss and Mainardi-Codazzi equations hold.  Then for any point $u_0\in U$ there is a neighborhood $u_0\in V\subset U$, a regular surface $S$ and a local coordinate chart $\mathbf x:V\to S$, for which $E,F,G$ are the first fundamental coefficients and $L,M,N$ are the second fundamental form coefficients.  Moreover, any two such charts on the same domain $V$ are related by an element of $\operatorname{Isom}^+(\mathbb R^3)$.}\\

\noindent Remember, the Christoffel symbols $\Gamma_{ij}^k$ are directly derivable from $E,F,G$ and their first order derivatives.  Hence it makes sense to say the equations of compatibility hold up above, even though these equations involve the Christoffel symbols.
\begin{proof}
We start by writing the functions $\mathbf x_u,\mathbf x_v,\mathbf N$ to be constructed in the proof, using scalar functions on $U$:
$$\mathbf x_u=\boldsymbol\zeta=(\zeta_1,\zeta_2,\zeta_3)$$
$$\mathbf x_v=\boldsymbol\eta=(\eta_1,\eta_2,\eta_3)$$
$$\mathbf N=\boldsymbol\theta=(\theta_1,\theta_2,\theta_3)$$
In accordance with the equations (A), we should have, for $i=1,2,3$,
$$(\zeta_i)_u=\Gamma_{11}^1\zeta_i+\Gamma_{11}^2\eta_i+L\theta_i$$
$$(\zeta_i)_v=\Gamma_{12}^1\zeta_i+\Gamma_{12}^2\eta_i+M\theta_i$$
$$(\eta_i)_u=\Gamma_{12}^1\zeta_i+\Gamma_{12}^2\eta_i+M\theta_i$$
$$(\eta_i)_v=\Gamma_{22}^1\zeta_i+\Gamma_{22}^2\eta_i+N\theta_i$$
$$(\theta_i)_u=a_{11}\zeta_i+a_{21}\eta_i$$
$$(\theta_i)_v=a_{12}\zeta_i+a_{22}\eta_i$$
(for instance, the first equation is obtained by taking coordinates of $(\mathbf x_u)_u=\mathbf x_{uu}=\Gamma_{11}^1\mathbf x_u+\Gamma_{11}^2\mathbf x_v+L\mathbf N$).  This is a system of partial differential equations in the form of Theorem 6.14, where the $x_i$ are the nine functions $\zeta_i,\eta_i,\theta_i,i=1,2,3$.  We have the condition of mixed partial derivative symmetry (e.g., $\frac{\partial}{\partial v}(\zeta_i)_u=\frac{\partial}{\partial u}(\zeta_i)_v$) because that condition is equivalent to the Gauss and Mainardi-Codazzi equations, which hold by assumption.

Now we fix a $u_0\in U$, and we also impose the following initial conditions, which are plausible because $E>0$ and $EG-F^2>0$:
$$\boldsymbol\zeta(u_0)=(\sqrt E,0,0)\big|_{u_0}$$
$$\boldsymbol\eta(u_0)=\left(\frac F{\sqrt E},\frac{\sqrt{EG-F^2}}{\sqrt E},0\right)\big|_{u_0}$$
$$\boldsymbol\theta(u_0)=(0,0,1).$$
We can henceforth use Theorem 6.14 to get a solution to the PDE system, in some open set $u_0\in V\subset U$.  Thus $\boldsymbol\zeta,\boldsymbol\eta,\boldsymbol\theta:V\to\mathbb R^3$ are differentiable, and their derivatives are given by substituting themselves in the equation (A).

Our next step of the proof is to show that $\boldsymbol\zeta\cdot\boldsymbol\zeta=E$, $\boldsymbol\zeta\cdot\boldsymbol\eta=F$, $\boldsymbol\eta\cdot\boldsymbol\eta=G$, $\boldsymbol\zeta\cdot\boldsymbol\theta=\boldsymbol\eta\cdot\boldsymbol\theta=0$ and $\boldsymbol\theta\cdot\boldsymbol\theta=1$.  Clearly all these hold at $u_0$, but we need for them to hold throughout $V$.  This follows from the uniqueness in Theorem 6.14, as this system of partial differential equations, with initial conditions
$$(\delta_1)_u=2\Gamma_{11}^1\delta_1+2\Gamma_{11}^2\delta_2+2L\delta_4$$
$$(\delta_1)_v=2\Gamma_{12}^1\delta_1+2\Gamma_{12}^2\delta_2+2M\delta_4$$
$$(\delta_2)_u=\Gamma_{12}^1\delta_1+(\Gamma_{11}^1+\Gamma_{12}^2)\delta_2+\Gamma_{11}^2\delta_3+M\delta_4+L\delta_5$$
$$(\delta_2)_v=\Gamma_{22}^1\delta_1+(\Gamma_{12}^1+\Gamma_{22}^2)\delta_2+\Gamma_{12}^2\delta_3+N\delta_4+M\delta_5$$
$$(\delta_3)_u=2\Gamma_{12}^1\delta_2+2\Gamma_{12}^2\delta_3+2M\delta_5$$
$$(\delta_3)_v=2\Gamma_{22}^1\delta_2+2\Gamma_{22}^2\delta_3+2N\delta_5$$
$$(\delta_4)_u=a_{11}\delta_1+a_{21}\delta_2+\Gamma_{11}^1\delta_4+\Gamma_{11}^2\delta_5+L\delta_6$$
$$(\delta_4)_v=a_{12}\delta_1+a_{22}\delta_2+\Gamma_{12}^1\delta_4+\Gamma_{12}^2\delta_5+M\delta_6$$
$$(\delta_5)_u=a_{11}\delta_2+a_{21}\delta_3+\Gamma_{12}^1\delta_4+\Gamma_{12}^2\delta_5+M\delta_6$$
$$(\delta_5)_v=a_{12}\delta_2+a_{22}\delta_3+\Gamma_{22}^1\delta_4+\Gamma_{22}^2\delta_5+N\delta_6$$
$$(\delta_6)_u=2a_{11}\delta_4+2a_{21}\delta_5$$
$$(\delta_6)_v=2a_{12}\delta_4+2a_{22}\delta_5$$
$$\delta_1(u_0)=E(u_0),~~\delta_2(u_0)=F(u_0),~~\delta_3(u_0)=G(u_0),~~\delta_4(u_0)=0,~~\delta_5(u_0)=0,~~\delta_6(u_0)=1,$$
has both of these as solutions:
\begin{center}
\begin{tabular}{c|cccccc}
&$\delta_1$&$\delta_2$&$\delta_3$&$\delta_4$&$\delta_5$&$\delta_6$\\\hline
Solution 1&$\boldsymbol\zeta\cdot\boldsymbol\zeta$&$\boldsymbol\zeta\cdot\boldsymbol\eta$&$\boldsymbol\eta\cdot\boldsymbol\eta$&$\boldsymbol\zeta\cdot\boldsymbol\theta$&$\boldsymbol\eta\cdot\boldsymbol\theta$&$\boldsymbol\theta\cdot\boldsymbol\theta$\\
Solution 2&$E$&$F$&$G$&$0$&$0$&$1$
\end{tabular}
\end{center}
so the solutions must coincide in $V$.  We observe, furthermore, that $\boldsymbol\zeta,\boldsymbol\eta$ are always linearly independent (since their cross product has norm $\sqrt{EG-F^2}$), and $\boldsymbol\theta$ is a unit normal vector to the plane they span.  In particular, $(\boldsymbol\zeta\times\boldsymbol\eta)\cdot\boldsymbol\theta$ is nonzero throughout $V$, hence is positive (because it is positive at $u_0$), so that $\boldsymbol\zeta,\boldsymbol\eta,\boldsymbol\theta$ forms a \emph{positive} basis of $\mathbb R^3$ and $\boldsymbol\theta=\frac{\boldsymbol\zeta\times\boldsymbol\eta}{\|\boldsymbol\zeta\times\boldsymbol\eta\|}$.

Our next step is to observe that $\boldsymbol\zeta_v=\boldsymbol\eta_u$, because both are equal to $\Gamma_{12}^1\boldsymbol\zeta+\Gamma_{12}^2\boldsymbol\eta+M\boldsymbol\theta$.  Hence by Theorem 6.14 or otherwise we may construct (at least locally) a function $\mathbf x$ with $\mathbf x_u=\boldsymbol\zeta,\mathbf x_v=\boldsymbol\eta,\mathbf x(u_0)=\vec 0$.  By construction we have $\mathbf x_u\cdot\mathbf x_u=E,\mathbf x_u\cdot\mathbf x_v=F,\mathbf x_v\cdot\mathbf x_v=G$.  Therefore $d\mathbf x$ is injective (since $\det((d\mathbf x)^T(d\mathbf x))=\det\begin{bmatrix}E&F\\F&G\end{bmatrix}=EG-F^2>0$), hence by shrinking $V$ one may assume $\mathbf x$ embeds $V$ into $\mathbb R^3$.

Then $\mathbf x$ is a local coordinate chart for a regular surface, and as we have effectively shown, $E,F,G$ are its first fundamental form coefficients.  Since $\mathbf x_u=\boldsymbol\zeta$, $\mathbf x_v=\boldsymbol\eta$, and $\mathbf N=\frac{\mathbf x_u\times\mathbf x_v}{\|\mathbf x_u\times\mathbf x_v\|}=\boldsymbol\theta$, the partial differential equations satisfied by $\boldsymbol\zeta,\boldsymbol\eta,\boldsymbol\theta$ (at the beginning of this proof) entail the equations (A) for $\mathbf x$.  Finally, reading off the first three such equations:
\begin{align*}
\mathbf x_{uu}&=\Gamma_{11}^1\mathbf x_u+\Gamma_{11}^2\mathbf x_v+L\mathbf N\\
\mathbf x_{uv}&=\Gamma_{12}^1\mathbf x_u+\Gamma_{12}^2\mathbf x_v+M\mathbf N\\
\mathbf x_{vv}&=\Gamma_{22}^1\mathbf x_u+\Gamma_{22}^2\mathbf x_v+N\mathbf N
\end{align*}
and dotting each one with $\mathbf N$, gives $\mathbf N\cdot\mathbf x_{uu}=L$, $\mathbf N\cdot\mathbf x_{uv}=M$ and $\mathbf N\cdot\mathbf x_{vv}=N$.  Hence $L,M,N$ are the second fundamental form coefficients.  This proves the existence of the local parametrization satisfying all the desired properties.

What's left is to show that any two such charts relate by an element of $\operatorname{Isom}^+(\mathbb R^3)$.  To do this, we let $\overline{\mathbf x}:V\to\mathbb R^3$ be another chart of another surface, with the same first fundamental form and second fundamental form coefficients.  The reader can readily verify that the vectors
$$\vec u_1=\frac{\overline{\mathbf x}_u}{\sqrt E}\big|_{u_0},~~~~\vec u_2=\frac{E\overline{\mathbf x}_v-F\overline{\mathbf x}_u}{\sqrt E\sqrt{EG-F^2}}\big|_{u_0},~~~~\vec u_3=\vec u_1\times\vec u_2$$
form a positive orthonormal basis of $\mathbb R^3$.  Hence we may let $A=\begin{bmatrix}\uparrow&\uparrow&\uparrow\\\vec u_1&\vec u_2&\vec u_3\\\downarrow&\downarrow&\downarrow\end{bmatrix}\in SO(3)$.  Then $A$ sends $(\sqrt E,0,0)\mapsto\overline{\mathbf x}_u$, $\left(\frac F{\sqrt E},\frac{\sqrt{EG-F^2}}{\sqrt E},0\right)\mapsto\overline{\mathbf x}_v$ and $(0,0,1)\mapsto\overline{\mathbf N}$ with all expressions being applied to $u_0$.

Now define $\mathbf x_1:V\to\mathbb R^3$ by $\mathbf x_1(u)=A^{-1}(\overline{\mathbf x}(u)-\overline{\mathbf x}(u_0))$.  This is the result of applying the isometry $[A^{-1},-A^{-1}\overline{\mathbf x}(u_0)]\in\operatorname{Isom}^+(\mathbb R^3)$ to $\overline{\mathbf x}$.  Hence it is clear that $\mathbf x_1$ has the same coefficients as $\overline{\mathbf x}$ (and $\mathbf x$), and we need only show that $\mathbf x_1=\mathbf x$.  Well, all this basically uses uniqueness in initial value problems; first $\mathbf x_u,\mathbf x_v,\mathbf N$ satisfy the same differential equations (A) that those things for $\mathbf x_1$ do, and they have the same values at $u_0$ (verify!).  Hence $\mathbf x_u,\mathbf x_v,\mathbf N$ coincide with $(\mathbf x_1)_u,(\mathbf x_1)_v,\mathbf N_1$.  Since $\mathbf x-\mathbf x_1$ has partial derivatives of zero, it is constant, and construction also shows it sends $u_0\mapsto\vec 0$.  Thus $\mathbf x-\mathbf x_1$ is identically zero and $\mathbf x_1=\mathbf x$ as desired.
\end{proof}

\noindent Note that in Section 6.10, we will handle things differently.  We will be dealing with a customized metric on an open set in $\mathbb R^2$, and there is no such thing as Gauss maps; hence there is no second fundamental form, and no equations of compatibility (but there will still be Gaussian curvature, given by the Gauss formula established in this section). % Okay, but I said "important to us", not "important".  I believe Bonnet's Theorem is important, it's just not relevant to customized surfaces

\subsection*{Exercises 6.7. (The Gauss and Mainardi-Codazzi Equations)}
% POTENTIAL EXERCISES: the first three on Page 237 of Do Carmo
\begin{enumerate}
\item Given a local coordinate parametrization $\mathbf x:U\to S$ with first fundamental form coefficients $E=1,F=\sin u,G=2$, find the Gaussian curvature in terms of $u$ and $v$.

\item Are any of the following surfaces locally isometric? % Theorema Egregium easily answers this with no.  See Do Carmo, p. 237, Ex. 9.

(a) Sphere $x^2+y^2+z^2=1$;

(b) Cylinder $x^2+y^2=1$;

(c) Hyperbolic paraboloid / saddle $z=x^2-y^2$.

\item If $\mathbf x$ is a parametrization in which $F\equiv 0$, show that:
$$\Gamma_{11}^1=\frac 12\frac{E_u}E,~~~~\Gamma_{11}^2=-\frac 12\frac{E_v}G$$
$$\Gamma_{12}^1=\frac 12\frac{E_v}E,~~~~\Gamma_{12}^2=\frac 12\frac{G_u}G$$
$$\Gamma_{22}^1=-\frac 12\frac{G_u}E,~~~~\Gamma_{22}^2=\frac 12\frac{G_v}G$$
and use this to show that
$$K=-\frac 1{2\sqrt{EG}}\left[\left(\frac{E_v}{\sqrt{EG}}\right)_v+\left(\frac{G_u}{\sqrt{EG}}\right)_u\right].$$

\item If $\mathbf x$ is a parametrization with isothermal coordinates $E\equiv G\equiv\lambda,F\equiv 0$, show that $K=-\frac 1{2\lambda}\Delta(\ln\lambda)$, where $\Delta(f)=f_{uu}+f_{vv}$ (the Laplacian operator).

\item Show that the converse of Theorema Egregium is false by showing the following parametrizations have the same Gaussian curvature but do not relate by an isometry:
$$\mathbf x(u,v)=(u\cos v,u\sin v,\ln u)$$
$$\overline{\mathbf x}(u,v)=(u\cos v,u\sin v,v)$$

\item Suppose $\mathbf x$'s coordinate curves form a Chebyshev net (Exercise 4 of Section 6.3).

(a) Show that $\mathbf x$ can be reparametrized so that $E=G=1$ and $F=\cos\theta(u,v)$.

(b) Now show that $K=-\frac{\theta_{uv}}{\sin\theta}$.

\item For which coefficients do there exist a regular surface?

(a) $E=G=1$, $F=0$, $L=N=-1$, $M=0$

(b) $E=1$, $F=0$, $G=\cos^2u$, $L=\cos^2u$, $M=0$, $N=1$

\item What happens to the coefficients of a surface when you apply an orientation-reversing isometry, i.e., an element of $\operatorname{Isom}(\mathbb R^3)-\operatorname{Isom}^+(\mathbb R^3)$? % E,F,G stay the same and L,M,N negate, because the Gauss map is the only thing which utilizes the orientation

\item Effectively everything in this section has generalizations to arbitrary dimensions.  Let $S$ be a hypersurface in $\mathbb R^n$, and let $\mathbf x:U\to S$ be a local coordinate chart with $U\subset\mathbb R^{n-1}$. % arbitrary dimensions, but don't cover vector fields yet (they're for 6.8); introduce the (A) equations for manifolds that need not be hypersurfaces.
Recall the Gauss map $\mathbf N=\frac{\mathbf x_{u_1}\times\dots\times\mathbf x_{u_{n-1}}}{\|\mathbf x_{u_1}\times\dots\times\mathbf x_{u_{n-1}}\|}$, and the coefficients $F_{ij}=\mathbf x_{u_i}\cdot\mathbf x_{u_j}$ and $M_{ij}=\mathbf N\cdot\mathbf x_{u_iu_j}$.

(a) Show that there are equations ($1\leqslant i,j\leqslant -1$)
$$\mathbf x_{u_iu_j}=\sum_{k=1}^{n-1}\Gamma_{ij}^k\mathbf x_{u_k}+M_{ij}\mathbf N$$
$$\mathbf N_{u_i}=\sum_{k=1}^{n-1}a_{ki}\mathbf x_{u_k}$$
where the $\Gamma_{ij}^k$ can be derived directly from the $F_{ij}$ and their first order derivatives, and the $a_{ij}$ are the entries of the matrix $A$ of Exercise 12(a) of the previous section.  [First show that $\mathbf x_{u_iu_j}\cdot\mathbf x_{u_k}=\frac 12\left[(F_{ik})_{u_j}+(F_{jk})_{u_i}-(F_{ij})_{u_k}\right]$.  The rest essentially mirrors the case $n=3$ covered in the section.]

(b) Explain how the facts $(\mathbf x_{u_iu_j})_{u_k}=(\mathbf x_{u_iu_k})_{u_j}$ and $(\mathbf N_{u_i})_{u_j}=(\mathbf N_{u_j})_{u_i}$ can be rewritten in the form of equations in $F_{ij},M_{ij}$ and their derivatives up to the second order.  These equations are called the \textbf{equations of compatibility}.

(c) Assume the generalization of Theorem 6.14 to more than two variables.  Generalize Bonnet's Theorem: suppose $U\subset\mathbb R^{n-1}$ is open, and $F,M:U\to\operatorname{Sym}_{n-1}(\mathbb R)$ are differentiable functions from $U$ to the symmetric matrices, such that $F$ is positive definite, and the equations of compatibility hold.  Then there is locally a hypersurface with $F$ and $M$ as the fundamental forms, and any two such hypersurfaces differ by a transform in $\operatorname{Isom}^+(\mathbb R^n)$.  [Exercise 8 of Section 3.5 may help.]

(d) The special case of (a)-(c) with $n=3$ are the exact same things covered in this section.

%Let $\boldsymbol\tau(t)=\frac{\alpha'(t)}{\|\alpha'(t)\|}$ and $\boldsymbol\nu(t)={^\times\boldsymbol\tau(t)}$ be the tangential and normal unit vectors.\footnote{We recall that $^\times(a,b)=(-b,a)$ for $(a,b)\in\mathbb R^2$, i.e., $^\times\vec v=\begin{bmatrix}0&-1\\1&0\end{bmatrix}\vec v$.}
(e) In the case $n=2$, $\mathbf x$ is a regular curve $\alpha:I\to\mathbb R^2$.  Then show that $F_{11}=\|\alpha'(t)\|^2$, $\Gamma_{11}^1=\frac 1{\|\alpha'(t)\|}\frac d{dt}\|\alpha'(t)\|$, $M_{11}=\kappa(t)\|\alpha'(t)\|^2$ and $a_{11}=-\kappa(t)$.  [Rewrite the equations in part (a) and use Exercise 6 of Section 6.1.]

(f) Now suppose $S$ is an $m$-dimensional manifold in $\mathbb R^n$, where $m$ need not equal $n-1$.  Explain why one may write
$$\mathbf x_{u_iu_j}=\sum_{k=1}^m\Gamma_{ij}^k\mathbf x_{u_k}+\mathbf P$$
with $\mathbf P\perp T_pS$ everywhere, and why the $\Gamma_{ij}^k$ and $\mathbf P$ vary differentiably with $S$.  Show (as in part (a)) that the $\Gamma_{ij}^k$ can be derived directly from the $F_{ij}$ and their first order derivatives.  Then work out the case $n=3,m=1$ for curves in space.
\end{enumerate}

\subsection*{6.8. Vector Fields and the Covariant Derivative}
\addcontentsline{toc}{section}{6.8. Vector Fields and the Covariant Derivative}
Vector fields are an important concept in the study of regular surfaces.  In a more sophisticated differential geometry course, they would be covered early, but here, we have decided to put them off until they were truly important.  In fact, this section will have more than one exercise generalizing to arbitrary dimensions, because that is the best way to learn the interesting general things about vector fields.

If $S$ is a regular surface, a \textbf{vector field} on $S$ is defined to be a differentiable map $X:S\to\mathbb R^3$ such that $X(p)\in T_pS$ for all $p\in S$.  In other words, it is a smooth assignment of each point of $S$ to a tangent vector.
\begin{center}
\includegraphics[scale=.5]{VectorField.png}
\end{center}
For example, if $S$ is the $xy$-plane, then a vector field is merely a differentiable map $S\to\mathbb R^2$, where $\mathbb R^2$ is regarded as the linear-algebra plane $z=0$.  However, for other surfaces, situations become a bit more complicated because the tangent plane is changing.  For example, a vector field on the sphere $S^2$ is a map $X:S^2\to\mathbb R^3$ such that every $X(p)$ is tangent to the sphere at $p$, or what is the same thing, every $p\perp X(p)$.

If $\alpha:I\to S$ is a regular curve, it is possible to have a vector field along $\alpha$ as well.  It is a map $\vec v:I\to\mathbb R^3$ such that for each $t\in I$, $\vec v(t)\in T_{\alpha(t)}S$.  Clearly every vector field on $S$ restricts to one on $\alpha$.  It turns out that vector fields on $\alpha$ can conversely be extended to $S$, but that will not be of importance to us.  Another important observation is that a vector field along $\alpha$, though it is tangent to $S$, need not be tangent to the curve $\alpha$.

This is a fairly basic concept (except for the introductory Lie theory in the exercises), but it will be extremely important in the next section.\\

\noindent\textbf{THE COVARIANT DERIVATIVE}\\

\noindent One thing we can use a vector field to do is differentiate a function.  If $X$ is a vector field on $S$ and $f:S\to\mathbb R$ is differentiable, we have a differentiable function $X(f)$ sending $p\mapsto df_p(X(p))$; in other words, at each point it differentiates $f$ in the direction of the vector field's value at that point.  [See Exercise 4(g).]
$X$ does not have to be defined throughout $S$; if it is just defined on a curve $\alpha$, then $X(f)$ is still defined on $\alpha$.  It is worth noting that if $X=\alpha'$ along $\alpha$, then $X(f)=(f\circ\alpha)'$ by the Chain Rule.

However, what if we try to differentiate a vector field $Y$ with respect to $X$?  This is where trouble ensues, as the codomain of the vector field is the tangent plane to the surface, which varies depending on the point.  So even if $Y$'s vectors are tangent to the surface $S$, differentiating them in the direction of $X$ may yield vectors that are \emph{not} tangent to $S$.  For example, suppose $\alpha(t)=(\cos t,\sin t,0)$ is the equator on the sphere and $Y=\alpha'(t)=(-\sin t,\cos t,0)$; then the derivative of $Y$ in the direction of $t$ is $(-\cos t,-\sin t,0)$, which is not tangent to the sphere.

It really shouldn't be a surprise that regular surfaces aren't closed under differentiating vector fields.  After all, it would be hard to tell when the derivative is zero, because that suggests the vectors must always point in the same direction, preventing them from being tangent at the different points.  There is not really such a thing as a vector field being ``absolutely stationary.''  Nevertheless, there does exist a concept of differentiating one vector field with respect to another.  This is accomplished by merely taking the orthogonal projection of the derivative to the tangent plane to the surface.

Let us take the time to formalize what was meant in the previous paragraph.  Suppose $\alpha:I\to S$ is a curve, and $\vec v:I\to\mathbb R^3$ is a vector field along $I$.  Then $\frac{d\vec v}{dt}\in\mathbb R^3$; however, $\frac{d\vec v}{dt}(t)$ may not be in $T_{\alpha(t)}S$ even though $\vec v(t)$ is.  We define the \textbf{covariant derivative} of $\vec v$ with respect to $t$, denoted $\frac{D\vec v}{dt}$, to be the orthogonal projection of $\frac{d\vec v}{dt}$ onto $T_{\alpha(t)}S$.  It captures how much the vector field changes, but only in accordance with the surface.\\

\noindent To illustrate this principle we recall the equator $\alpha(t)=(\cos t,\sin t,0)$ on the unit sphere.  Then $\vec v(t)=(-\sin t,\cos t,0)$ is a tangent vector field to the sphere along the equator.  To find $\frac{D\vec v}{dt}$, we first note that $\frac{d\vec v}{dt}=(-\cos t,-\sin t,0)$, and we want to orthogonally project that vector onto $T_pS^2$ (which is the orthogonal complement of $(\cos t,\sin t,0)$).  In this case $\frac{d\vec v}{dt}$ is actually perpendicular to the tangent space, and hence $\frac{D\vec v}{dt}=\vec 0$.  Similarly, if $\vec w(t)=(0,0,1)$ throughout the curve $\alpha$, then $\frac{D\vec w}{dt}=\vec 0$ because $\frac{d\vec w}{dt}=\vec 0$.

Now let $\eta(t)=(a\cos t,a\sin t,b)$ be a latitude with $a,b>0,a^2+b^2=1$ fixed, and let $\vec v(t)=\eta'(t)=(-a\sin t,a\cos t,0)$.  What is the covariant derivative $\frac{D\vec v}{dt}$ in this case?  Well, $\frac{d\vec v}{dt}=(-a\cos t,-a\sin t,0)$, and we wish to orthogonally project this to the tangent plane to $\eta(t)$.  Note that if $\vec a,\vec u\in\mathbb R^n$ and $\|\vec u\|=1$, then the orthogonal projection of $\vec a$ onto the hyperplane to which $\vec u$ is normal is $\vec a-(\vec a\cdot\vec u)\vec u$.  Taking $\vec u=\eta(t)$ and $\vec a=\frac{d\vec v}{dt}$, we compute that $\frac{D\vec v}{dt}=(-ab^2\cos t,-ab^2\sin t,a^2b)$.  This vector goes slightly up the sphere toward the north pole.\\

\noindent To conclude this section, we shall show that on a local coordinate chart, the covariant derivative is uniquely determined by the first fundamental form.  After all, suppose $\mathbf x:U\to S$ is a local coordinate chart, $\alpha:I\to U$ is a curve and $\vec v$ is a tangent vector field on $\alpha$.  With each tangent vector written in terms of the basis $\{\mathbf x_u,\mathbf x_v\}$, one may think of $\vec v$ as a function $I\to\mathbb R^2$, where $\vec v(t)=(a(t),b(t))=a(t)\mathbf x_u+b(t)\mathbf x_v$.

The first trick is to note that for any function $f$ in $u,v$, whether vector-valued or scalar valued, we have $\frac{df}{dt}=u'(t)f_u+v'(t)f_v$ by the Chain Rule.  In particular,
$$\frac d{dt}\mathbf x_u=u'(t)\mathbf x_{uu}+v'(t)\mathbf x_{uv}$$
and by the equations (A) from the previous section,
$$u'(t)\mathbf x_{uu}+v'(t)\mathbf x_{uv}=u'\left(\Gamma_{11}^1\mathbf x_u+\Gamma_{11}^2\mathbf x_v+L\mathbf N\right)+v'\left(\Gamma_{12}^1\mathbf x_u+\Gamma_{12}^2\mathbf x_v+M\mathbf N\right)$$
$$=(u'\Gamma_{11}^1+v'\Gamma_{12}^1)\mathbf x_u+(u'\Gamma_{11}^2+v'\Gamma_{12}^2)\mathbf x_v+(u'L+v'M)\mathbf N.$$
Similarly, $\frac d{dt}\mathbf x_v=(u'\Gamma_{12}^1+v'\Gamma_{22}^1)\mathbf x_u+(u'\Gamma_{12}^2+v'\Gamma_{22}^2)\mathbf x_v+(u'M+v'N)\mathbf N$.

Finally, we may compute $\frac{d\vec v}{dt}$ as $a'(t)\mathbf x_u+a(t)\frac{d\mathbf x_u}{dt}+b'(t)\mathbf x_v+b(t)\frac{d\mathbf x_v}{dt}$.  We leave it to the reader to combine like terms to get:
\begin{align*}
\frac{d\vec v}{dt}
&=\left(a'+au'\Gamma_{11}^1+(av'+bu')\Gamma_{12}^1+bv'\Gamma_{22}^1\right)\mathbf x_u\\
&+\left(b'+au'\Gamma_{11}^2+(av'+bu')\Gamma_{12}^2+bv'\Gamma_{22}^2\right)\mathbf x_v\\
&+\left(au'L+(av'+bu')M+bv'N\right)\mathbf N
\end{align*}
Finally, the covariant derivative is obtained by orthogonally projecting to the tangent plane.  Since $\mathbf x_u,\mathbf x_v$ are in the tangent plane and $\mathbf N$ is orthogonal to it, we simply slay the $\mathbf N$ term in order to do this:
\begin{align*}
\frac{D\vec v}{dt}
&=\left(a'+au'\Gamma_{11}^1+(av'+bu')\Gamma_{12}^1+bv'\Gamma_{22}^1\right)\mathbf x_u\\
&+\left(b'+au'\Gamma_{11}^2+(av'+bu')\Gamma_{12}^2+bv'\Gamma_{22}^2\right)\mathbf x_v
\end{align*}
This formula uses only $\alpha$ and $\vec v$ (as defined on the coordinate chart) and the first fundamental form, in the form of the Christoffel symbols.  Observe that the second fundamental form coefficients were in the coefficient of $\mathbf N$, hence the derivative \emph{would} have depended on them if we hadn't gotten rid of the term.  This is indeed the main idea of the second fundamental form: it measures how much differentiating one tangent vector field on $S$ with respect to another pushes vectors aside into the ambient space, and varies from the covariant derivative.

This may seem like a basic concept, but since the next section will be more intricate, it is important to grasp the concepts for a good understanding.

\subsection*{Exercises 6.8. (Vector Fields and the Covariant Derivative)}
% POTENTIAL EXERCISE: Lie groups (I said in 6.2 I would have one)
\begin{enumerate}
\item Let $\mathbf L$ be the set of all one-dimensional subspaces of the vector space $\mathbb R^3$.  A \textbf{direction field} on $S$ is defined to be a smooth map $D:S\to\mathbf L$ such that for every $p\in S$, $D(p)$ is tangent to $p$ at $S$.  For example, every \emph{nonvanishing} vector field $X$ entails a direction field, obtained by assigning each $D(p)$ to be the span of $X(p)$.

(a) Show by example that a direction field need not come from a nonvanishing vector field.  [In $\mathbb R^2-\{0\}$, draw three rays coming out of the (deleted) origin, which come at $120$-degree angles.  Then in each of the three resulting regions, draw a family of curves asymptotically approaching both bounding rays.  This should lead you to a direction field which cannot have a nonvanishing vector field without entailing a contradiction.]

(b) If two nonvanishing vector fields have the same direction field, show that they have the same flow curves, though parametrized differently in general.

\item If $\vec v$ and $\vec w$ are vector fields along a curve $\alpha:I\to S$, show that $\frac d{dt}(\vec v\cdot\vec w)=\frac{D\vec v}{dt}\cdot\vec w+\vec v\cdot\frac{D\vec w}{dt}$.  [First explain why $\frac{D\vec v}{dt}\cdot\vec w=\frac{d\vec v}{dt}\cdot\vec w$.]

\item (a) Let $X$ be a vector field on $S$ and $p\in S$.  Show that there exists, for some $\varepsilon$, a differentiable map $\alpha:(-\varepsilon,\varepsilon)\to S$ such that $\alpha(0)=p$ and $\alpha'(t)=X(\alpha(t))$ for all $t$, and that this map is unique when given the choice of $\varepsilon$.  [Take a local coordinate chart.  Then this works out to a system of differential equations.] $\alpha$ is called the \textbf{flow} of $X$ through $p$.

(b) If $p\in S$ and $X(p)\ne\vec 0$, show that there exists a local coordinate chart $U\to S$ at $p$, on which $X$ is the image of $\vec e_1\in U$.  [You may assume the flow of $X$ through $p$ varies smoothly with $p$.]

\item\emph{(Derivations and the Lie bracket.)} \---- Let $S\subset\mathbb R^n$ be an $m$-dimensional manifold. % Should be in the second half of exercises, since it is already generalizing dimensions

For fixed $p\in S$, let $\mathcal E(p)$ be the set of all pairs $(U,f)$ such that $p\in U\subset S$ is an open neighborhood and $f:U\to\mathbb R$ is a differentiable function.  [These are local differentiable functions at $p$.]  A \textbf{derivation at $p$} is defined to be a function $D:\mathcal E(p)\to\mathbb R$ such that for $a,b\in\mathbb R,(U,f),(V,g)\in\mathcal E(p)$, we have

~~~~(i) \emph{Linearity}: $D(af+bg)=aD(f)+bD(g)$

~~~~(ii) \emph{Leibniz's Law}: $D(fg)=f(p)D(g)+g(p)D(f)$

when all functions are restricted to $U\cap V$.

% (owing to the fact that $D(f)=D(f|_W)$, as can be seen by taking $a=1,b=0,g=0|_W$ in the linearity condition)
Observe that $\mathcal E(p)$ is not a vector space because of the varying domains of the functions.  However, one can define $(U,f)\sim(V,g)$ to mean that there exists an open set $p\in W\subset U\cap V$ such that $f|_W=g|_W$; this is an equivalence relation on $\mathcal E(p)$, and a derivation $D$ at $p$ necessarily maps equivalent elements to the same number, because taking $a=1,b=0,g=0|_W$ in the linearity condition gives $D(f)=D(f|_W)$. % No, equivalent elements of \mathcal E(p) literally map to the same number -- not like there's some kind of equivalence relation on \mathbb R !
Moreover, if $\mathcal G(p)=\mathcal E(p)/\sim$, the derivation $D$ induces a map $\mathcal G(p)\to\mathbb R$, which \emph{is} a linear map between vector spaces.  We will not delve into details as they are not important to us.  [Elements of $\mathcal G(p)$ are called \textbf{germs} of smooth functions at $p$.]

(a) If $\vec v\in T_pS$, show that $f\mapsto df_p(\vec v)$ is a derivation at $p$.

(b) Derivations at $p$ send constant functions to zero.  [Take $f,g$ to be the constant function $1$ in Leibniz's Law.]

By using a local coordinate chart, one may assume $n=m$, $S$ is an open set of $\mathbb R^m$, and $p=0$.  This shall be done in parts (c) and (d).

(c) If $f\in\mathcal E(p)$ and $df_p=0$, every derivation at $p$ sends $f\mapsto 0$.  [By part (b) and linearity, $D(f)=D(f-f(p))$, so one may assume $f(p)=0$.  With that, use L'H\^opital's Rule to write each function $f(u_1,\dots,u_{k-1},u_k,0,\dots,0)-f(u_1,\dots,u_{k-1},0,0,\dots,0)$ as a product of two functions which send $p$ to zero.  We thus get a summation $f=\sum_{i=1}^mg_ih_i$ where $g_i,h_i\in\mathcal E(p)$ and $g_i(p)=h_i(p)=0$.  Now use Leibniz's Law.]

(d) Show that the map from $\vec v$ to $f\mapsto df_p(\vec v)$ in part (a) is a linear isomorphism from the tangent vectors at $p$ to the derivations at $p$.  [Linearity is clear, as is injectivity, because for $\vec v\ne\vec 0$, the map $f:\vec x\mapsto\vec x\cdot\vec v$ satisfies $df_p(\vec v)\ne 0$.  As for surjectivity, use part (c) to show that if $D$ is a derivation at $p$, and $a_i=D(u_i)$ for each $1\leqslant i\leqslant m$, $D$ must coincide with the image of the vector $(a_1,\dots,a_m)$.]

Let $\mathcal C^\infty(S)$ be the linear space of all differentiable functions $S\to\mathbb R$.  A \textbf{global derivation} is defined to be a linear map $D:\mathcal C^\infty(S)\to\mathcal C^\infty(S)$ such that for all $f,g\in C^\infty(S)$, Leibniz's Law holds: $D(fg)=fD(g)+gD(f)$.

(e) Assume the following nontrivial fact: if $p\in U$ with $U$ open, there exists a differentiable function $f:S\to\mathbb R$ such that $f(p)=1$ and $f|_{S-U}=0$.  If $D$ is a global derivation, $f\in C^\infty(S)$, and $U$ is an open set such that $f|_U=0$, show that $D(f)|_U=0$.  [To show that $D(f)(p)=0$ for $p\in U$, use Liebniz's Law with $f$ and a function given by the first statement.]

(f) If $X$ is a vector field on $S$, define $X(f)(p)=df_p(X(p))$ for $p\in S$.  Show that $X(f)$ is a differentiable function, and that $f\mapsto X(f)$ is a global derivation.

(g) Show that a map $D$ is a global derivation if and only if $D$ is differentiable and, at every $p\in S$, $D$ restricts to a derivation at $p$.  [From part (e) it follows that $D$ can be well-restricted to open sets of $S$.]  Conclude, using part (d), that $X\mapsto[f\mapsto X(f)]$ is a linear isomorphism between vector fields and global derivations.

(h) If $V$ and $W$ are global derivations, show that $V\circ W-W\circ V$ is a global derivation.  This is denoted $[V,W]$ and called the \textbf{Lie bracket} of $V$ and $W$.

Give an example to show that $V\circ W$ may not be a global derivation.

Since global derivations correspond bijectively to vector fields by part (g), we may pass the operation over, to get what is known as the \textbf{Lie bracket of vector fields}: if $X$ and $Y$ are vector fields, then $[X,Y]$ is the vector field such that $[X,Y](f)=X(Y(f))-Y(X(f))$.

(i) Show that $[X,Y]$ is bilinear (over $\mathbb R$); $[X,X]=0$; $[X,Y]=-[Y,X]$; and the \textbf{Jacobi identity} holds: $[X,[Y,Z]]+[Y,[Z,X]]+[Z,[X,Y]]=0$.

(j) If $f\in\mathcal C^\infty(S)$, show that $[X,fY]=f[X,Y]+X(f)Y$ and $[fX,Y]=f[X,Y]-Y(f)X$.

(k) Now suppose we are in local coordinates ($S$ is an open subset of $\mathbb R^m)$, and $X=(g_1,\dots,g_m),Y=(h_1,\dots,h_m)$.  Find a formula for $[X,Y]$.  [By symmetry of mixed partial derivatives, $[u_i,u_j]=0$ for $1\leqslant i,j\leqslant m$.  Now use bilinearity and part (j) to expand $[X,Y]$ into already-known terms.]

\item\emph{(Lie groups.)} \---- A Lie group $G$ is a regular manifold which is equipped with the structure of a group, such that the group operations are smooth, by which we mean that the maps $\mu(g,h)=gh$ and $\iota(g)=g^{-1}$ are differentiable. % The group structure doesn't mean the smoothness of the maps!  I added a few extra words.
Examples include $\mathbb R^n$ under addition; the nonzero complex numbers under multiplication; $GL_n(\mathbb R)$ [as an open set of $\mathbb R^{n^2}$]; $SL_n(\mathbb R)$; $GL_n(\mathbb C)$; $SL_n(\mathbb C)$; $O(n)$; $SO(n)$; $U(n)$; and $SU(n)$.

For each $g\in G$, we let $L_g$ be the translation $h\mapsto gh$, which is smooth by definition.

(a) A vector field $X$ on $G$ is said to be \textbf{left invariant} provided that for all $g,h\in G$, $d(L_g)_h(X(h))=X(gh)$.  Show that this can be reduced to saying that $d(L_g)_1(X(1))=X(g)$, and that the map $X\mapsto X(1)$ is a linear isomorphism from left invariant vector fields to tangent vectors at $1$.

(b) Show that a vector field is left invariant if and only if the corresponding global derivation $X$ satisfies $X(f\circ L_g)=X(f)\circ L_g$ for all $f\in\mathcal C^\infty(G),g\in G$.  Conclude that left invariant vector fields are closed under the Lie bracket.

A \textbf{Lie algebra} (over $\mathbb R$ or $\mathbb C$) is defined to be a vector space $V$ equipped with a bracket operator $[-,-]:V\times V\to V$ such that: (i) the operator is bilinear; (ii) the operator is alternating: $[v,v]=0$ for all $v\in V$; (iii) the Jacobi identity holds: $[u,[v,w]]+[v,[w,u]]+[w,[u,v]]=0$.  They are usually denoted via lowercase fraktur symbols like $\mathfrak g$; we will eventually see why this is so.

A basic example is $\mathbb R^3$ under the cross product (the Jacobi identity for such is Exercise 4(g) of Section 2.5).  Also, $M_n(\mathbb R)$ \---- the set of \emph{all} $n\times n$ matrices with real entries \---- is a Lie algebra under the operation $[A,B]=AB-BA$.  By the previous exercise, the space of vector fields on a manifold is a Lie algebra.

(c) If $\mathfrak g$ is a Lie algebra, show that $[v,w]=-[w,v]$ for all $v,w\in\mathfrak g$.  [Expand $[v+w,v+w]$ via bilinearity.]

(d) If $G$ is a Lie group, then the left invariant vector fields form a Lie algebra (because by part (b), they are closed under the bracket).  Yet since this space is canonically isomorphic to $T_1G$ by part (a), we conclude that $T_1G$ is a Lie algebra whose vector space dimension equals the (manifold) dimension of $G$.  $T_1G$ is denoted $\operatorname{Lie}(G)$ or $\mathfrak g$ and is called the \textbf{associated Lie algebra of the Lie group $G$}.

If $G$ and $H$ are Lie groups, a \textbf{Lie group homomorphism} is defined to be a differentiable map $\varphi:G\to H$ which is also a group homomorphism.  Since $\varphi(1)=1$ by Proposition 1.10(i), $d\varphi_1$ (henceforth to be denoted $\varphi_*$) is a linear map of the associated Lie algebras, $\operatorname{Lie}(G)\to\operatorname{Lie}(H)$.

If $X$ is a vector field on $G$ and $Y$ is a vector field on $H$, $X$ and $Y$ are said to \textbf{relate via $\varphi$} provided that for all $p\in G$, $d\varphi_p(X(p))=Y(\varphi(p))$.  Since $\varphi$ need not be injective or surjective, it does not make sense to pass vector fields either way in general, but this is a useful notion.

(e) Show that $X$ and $Y$ relate via $\varphi$ if and only if $Y(f)\circ\varphi=X(f\circ\varphi)$ for $f\in\mathcal C^\infty(H)$.  Use this to show that if $X,X'$ are vector fields on $G$, $Y,Y'$ are vector fields on $H$, $X$ and $Y$ relate via $\varphi$, and $X'$ and $Y'$ relate via $\varphi$, then $[X,Y]$ and $[X',Y']$ relate via $\varphi$.

(f) If $\varphi$ is a Lie group homomorphism and $X$ and $Y$ are left invariant, then $X$ and $Y$ relate via $\varphi$ if and only if $\varphi_*(X)=Y$ [with $X,Y$ viewed as tangent vectors to the identities].  Conclude, using part (e), that $\varphi_*$ is a Lie algebra homomorphism; i.e., $\varphi_*([X,X'])=[\varphi_*(X),\varphi_*(X')]$.

(g) Show that $d\iota_1=-I_{\mathfrak g}$, where $\iota:G\to G$ is the inversion map.  [First find $d\mu_1$ by using curves at the identity, then differentiate the statement $\mu(g,\iota(g))=1$.]  Use this to show that if $G$ is an abelian Lie group, $\mathfrak g$'s Lie bracket is identically zero.  [$\iota$ is a Lie group homomorphism in this case.]

(h) For each $X\in\mathfrak g$, there is a unique Lie group homomorphism $\alpha:\mathbb R\to G$ such that $\alpha'(0)=X$.  [The differential equation $\alpha'(t)=d(L_{\alpha(t)})_1(X)$ has a solution for some interval in $\mathbb R$; now use the Lie group homomorphism property to extend it to all of $\mathbb R$.]  We set $\exp(X)=\alpha(1)$, thus defining a map $\exp:\mathfrak g\to G$ called the \textbf{exponential}.

(i) If $\varphi:G\to H$ is a Lie group homomorphism, this diagram commutes:
\begin{diagram}
\mathfrak g & \rTo{\varphi_*} & \mathfrak h \\
\dTo{\exp} & & \dTo{\exp} \\
G & \rTo{\varphi} & H
\end{diagram}
(j) If $G=GL_n(\mathbb R)$, show that $\mathfrak g=M_n(\mathbb R)$ and for $A\in\mathfrak g$, $\exp(A)=I+A+\frac{A^2}{2!}+\frac{A^3}{3!}+\dots$.  [The trick is to note that $\exp(tA)=\alpha(t)$ where $\alpha$ is the homomorphism from part (h) with $\alpha'(0)=A$.  Conclude that $\frac{d}{dt}\exp(tA)=A\exp(tA)$.]  This operation is known as the \textbf{matrix exponential}.  Moreover show that if $\exp(A^T)=\exp(A)^T$; if $AB=BA$ then $\exp(A+B)=\exp(A)\exp(B)$; $\exp(0)=I$ and $\exp(-A)=\exp(A)^{-1}$.

(k) Use part (i) to show that in the Lie algebra $\mathfrak g$ of $GL_n(\mathbb R)$, the bracket is given by $[A,B]=AB-BA$.

(l) If $G=SL_n(\mathbb R)$, then $\mathfrak g$ consists of matrices in $M_n(\mathbb R)$ with trace zero, and the bracket is again given by $[A,B]=AB-BA$.  If $G=O(n)$, then $\mathfrak g$ consists of skew-symmetric matrices ($A^T=-A$) and the bracket is given by $[A,B]=AB-BA$.  [In each case, for $X\in\mathfrak g$, find an equation for the curve $\alpha(t)=\exp(tX)$ to be contained in $G$, and differentiate it at zero; this will give the condition on $X$.  As for the bracket, use part (f).]  Figure out the Lie algebras for the other matrix groups as well.

\item\emph{(The covariant derivative in arbitrary dimensions.)} \---- This generalizes the final result of this section to arbitrary dimensions.  Let $S$ be a manifold in $\mathbb R^n$, let $\mathbf x:U\to S$ be a local coordinate chart, and let $\alpha:I\to U$ be a curve and $\vec v$ a vector field along $\alpha$.  The \textbf{covariant derivative} of $\vec v$ is obtained by differentiating $\vec v$ with respect to $t$, then orthogonally projecting to the tangent space of the manifold.

(a) If $S$ is a hypersurface, use Exercise 9 of the previous section to derive the Christoffel symbols from the First Fundamental Form.  Then show that the covariant derivative is uniquely determined by them, and work out the expression.

(b) Now show that this may be done without the assumption that $S$ is a hypersurface, and (using Exercise 9(f) of the previous section to get $\Gamma_{ij}^k$), the covariant derivative is given by the same expression as in part (a).  [It will help to take the dot product of the covariant derivative with the $\mathbf x_{u_i}$'s, noting that the dot product is the same if the derivative is used instead of the covariant derivative.]
\end{enumerate}

\subsection*{6.9. Parallel Transport and Geodesics}
\addcontentsline{toc}{section}{6.9. Parallel Transport and Geodesics}
Now that we have covered vector fields, we are ready to do several final things before going to custom surfaces: defining what it means for a curve to be a ``straight line,'' a theorem regarding the sum of the angles of a triangle, and the exponential map (used to get circles).

Let $S$ be a regular surface, and $\alpha:I\to S$ be a regular curve, and let $\vec v$ be a vector field on $S$ along $\alpha$ (it need not be tangent to $\alpha$).  We recall that, from the previous section, one cannot talk about the derivative of $\vec v$ as a vector field on $S$, but one can talk about the \emph{covariant} derivative of $\vec v$; i.e., the orthogonal projection of $\frac{d\vec v}{dt}$ to the tangent planes to the surface.  The first natural thing to ask is when the covariant derivative is zero, i.e., $\frac{d\vec v}{dt}$ is normal to the surface.  Vector fields with that property do their best to stay stationary while also staying tangent to the surface, so their only motions relieve ``normal stick-out.''  Such vector fields are thus given a special name.\\

\noindent\textbf{Definition.} \emph{If $\vec v$ is a vector field along a curve $\alpha$, $\vec v$ is said to be \textbf{parallel} provided that $\frac{D\vec v}{dt}=0$ throughout $\alpha$.}\\

\noindent Several examples are in order.  First, if $S=\mathbb R^2$ (viewed as the $xy$-plane), then covariant derivatives coincide with the ordinary derivatives, and hence a parallel vector field along $\alpha$ is just a vector field along $\alpha$ which is a constant vector in $\mathbb R^2$.  It is hard to construct other genuine examples of parallel vector fields, thus we will start by proving a few results.

First, we formulate parallel vector fields in local coordinates.  Let $\mathbf x:U\to S$ be a local coordinate chart, and assume the curve can be given by $\alpha(t)=\mathbf x(u(t),v(t))$.  Then a vector field $\vec v$ can be given by $\vec v(t)=(a(t),b(t))=a(t)\mathbf x_u+b(t)\mathbf x_v$.  As shown in the previous section,
\begin{align*}
\frac{D\vec v}{dt}
&=\left(a'+au'\Gamma_{11}^1+(av'+bu')\Gamma_{12}^1+bv'\Gamma_{22}^1\right)\mathbf x_u\\
&+\left(b'+au'\Gamma_{11}^2+(av'+bu')\Gamma_{12}^2+bv'\Gamma_{22}^2\right)\mathbf x_v,
\end{align*}
and hence $\vec v$ is parallel if and only if both coefficients above are zero.  Thus, a parallel vector field is characterized by the equations:
\begin{equation}\tag{PT}\begin{array}{c l}a'+au'\Gamma_{11}^1+(av'+bu')\Gamma_{12}^1+bv'\Gamma_{22}^1=0\\b'+au'\Gamma_{11}^2+(av'+bu')\Gamma_{12}^2+bv'\Gamma_{22}^2=0\end{array}\end{equation}
Now note that if the curve $\alpha$ is already prescribed, then the $\Gamma_{ij}^k$ are already known functions in $t$, as are $u$ and $v$.  Moreover, easy examination shows that we have a \emph{linear} system of differential equations in the unknowns $a,b$.  Thus by Theorem 6.2 we conclude\\

\noindent\textbf{Proposition 6.16 and Definition.} \emph{If $\alpha:I\to S$ is a regular curve, $t_0\in I$ and $\vec v_0\in T_{\alpha(t_0)}S$, there is a unique parallel $\vec v$ throughout $\alpha$ such that $\vec v(t_0)=\vec v_0$.  $\vec v$ is called \textbf{parallel transport} from $\vec v_0$.}\\

\noindent Also note that since the $\Gamma_{ij}^k$ are derivable from the coefficients $E,F,G$, any local coordinate chart of another regular surface with the same first fundamental form coefficients has the same differential equations (PT) for parallel transport.  In accordance with Proposition 6.6, this tells us\\ % What do you mean, "will call these"?  The (PT) label was already used.

\noindent\textbf{Proposition 6.17.} \emph{Parallel transport along vector fields is preserved under (local) isometries.}\\

\noindent In other words, parallel transport is part of intrinsic geometry.

From the homogeneous linear system, it is clear that parallel vector fields form a vector subspace of vector fields (Exercise 2).  One important thing about parallel vector fields is that they move ``rigidly'' along $\alpha$ in the following sense:\\

\noindent\textbf{Proposition 6.18.} \emph{If $\vec v$ and $\vec w$ are parallel vector fields along $\alpha$, then $\vec v\cdot\vec w$ is constant.  In particular, $\|\vec v\|$ and the angle between $\vec v$ and $\vec w$ are constant.}
\begin{proof}
By Exercise 2 of Section 6.8,
$$\frac{d}{dt}(\vec v\cdot\vec w)=\overset 0{\overbrace{\frac{D\vec v}{dt}}}\cdot\vec w+\vec v\cdot\overset 0{\overbrace{\frac{D\vec w}{dt}}}=0,$$
hence $\vec v\cdot\vec w$ is constant.  The second statement follows from $\|\vec v\|=\sqrt{\vec v\cdot\vec v}$ and the fact that the angle between $\vec v$ and $\vec w$ is $\cos^{-1}\frac{\vec v\cdot\vec w}{\|\vec v\|\|\vec w\|}$.
\end{proof}

\noindent Now we shall present a specific example of parallel transport other than the trivial one on the plane.

Suppose $\alpha$ is a latitude of the sphere $S^2$.  Then if $\alpha$ is the equator, the unit tangent vectors along $\alpha$ are a parallel vector field.  However, if $\alpha$ is a latitude other than the equator, the unit tangent vectors have derivative pointing to the center of the latitude circle; thus the derivative is not normal to the sphere, and the vector field is not parallel.

Nevertheless, if we take a unit tangent vector at one point, we can extend it to a parallel vector field $\vec v$ by Proposition 6.17.  To work out what $\vec v$ is, we first construct the cone tangent to the sphere along the latitude; it is obtained by taking the tangent line to the generating semicircle arc and revolving it around the $z$-axis:
\begin{center}
\includegraphics[scale=.3]{ConeOverSphere.png}
\end{center}
By Exercise 1, $\vec v$ is also a parallel vector field along the cone.  We recall that the cone is locally isometric to the plane (Exercise 2 of Section 6.4); specifically, lines on the cone from the tip correspond to rays in the plane from the origin.  Such an isometry takes the latitude to an origin-centered circle in the plane.  By Proposition 6.17, the isometry takes $\vec v$ to a parallel vector field in the plane; but such a vector field is a constant vector in $\mathbb R^2$, and hence it looks like:
\begin{center}
\includegraphics[scale=.3]{LatitudeParallelTransport.png}
\end{center}
Note that this vector immediately stops being tangent to the latitude as it turns down toward the equator.  It does so at a steady rate, and lands back at its starting point, but pointing in a different direction from its original (why?).\\ % The PT vector rotates down (away from the cone tip), I'll change the word "curves"

\noindent\textbf{GEODESICS}\\

\noindent Perhaps the natural thing to ask is when the velocity of a curve $\alpha$ is itself a parallel vector field along $\alpha$.  Intuitively, this occurs when forward travel along $\alpha$ does its best to keep a stationary direction; i.e., when $\alpha$ travels closest to a straight line.  Thus, we have a notion of ``straight lines'' which will be important to us:\\

\noindent\textbf{Definition.} \emph{A curve $\alpha:I\to S$ is called a \textbf{geodesic} provided that its velocity $\vec v=\frac{d\alpha}{dt}$ is a parallel vector field along $\alpha$.}\\

\noindent For instance, we recall that in $S=\mathbb R^2$, a parallel vector field along $\alpha$ is precisely a vector field which is a constant vector in $\mathbb R^2$.  Thus $\alpha$ is a geodesic if and only if $\alpha'$ is a constant vector, say $\vec v$; with that, $\alpha(t)=\vec u+t\vec v$ for some $\vec u$, and $\alpha$ is a straight line in the ordinary sense of Euclidean geometry.  Thus the geodesics in $\mathbb R^2$ are precisely the straight lines parametrized at a steady rate.

We have also shown before that the equator of a sphere, when parametrized by arc length, is a geodesic (because the unit tangent vectors are parallel).  Any great circle, parametrized by arc length, is a geodesic.  Yet, other latitudes are not geodesics.

It is worth noting that $\alpha$ being a geodesic does depend on the parametrization and not just the curve as a subset of $S$.  In $\mathbb R^2$, for example, $t\mapsto(t,t,0)$ is a geodesic, but $t\mapsto(\sin t+2t,\sin t+2t,0)$ is not, even though the image of the parametrizations is the same line.

Just like parallel vector fields, we may formulate geodesics in local coordinates.  But this time we have a property of just the curve, not a property of a tangent vector field along it.

Let $\mathbf x:U\to S$ be a local coordinate chart, and suppose $\alpha(t)=\mathbf x(u(t),v(t))$.  We recall that a vector field $\vec v$ along $\alpha$ can be given by $\vec v(t)=a(t)\mathbf x_u+b(t)\mathbf x_v$.  Note that $\frac{d\alpha}{dt}=u'(t)\mathbf x_u+v'(t)\mathbf x_v$ by the Chain Rule; hence in the special case $\vec v=\frac{d\alpha}{dt}$ we have $a=u'$ and $b=v'$.  $\alpha$ is a geodesic if and only if this $\vec v$ is a parallel vector field, which is expressed by taking $a=u',b=v'$ in the parallel equations (PT) to get:
\begin{equation}\tag{G}\begin{array}{c l}u''+(u')^2\Gamma_{11}^1+2u'v'\Gamma_{12}^1+(v')^2\Gamma_{22}^1=0\\v''+(u')^2\Gamma_{11}^2+2u'v'\Gamma_{12}^2+(v')^2\Gamma_{22}^2=0\end{array}\end{equation}
Observe that this is a \emph{second} order system of differential equations in the unknowns $u$ and $v$.  One can readily apply Theorem 6.2 by defining two new variables $\mathfrak u,\mathfrak v$ to be $u',v'$ respectively, obtaining a (first-order) system in the \emph{four} unknowns.\footnote{Specifically, $u'=\mathfrak u$, $\mathfrak u'+\mathfrak u^2\Gamma_{11}^1+2\mathfrak u\mathfrak v\Gamma_{12}^1+\mathfrak v^2\Gamma_{22}^1=0$, etc.}  However, it is not linear, because the derivatives of the unknowns are being multiplied together, and because the $\Gamma_{ij}^k$ are functions in the position on the chart (not of $t$), even they depend on $u$ and $v$.  Thus solutions may only exist locally. % "Is this the first time something in Ch. 6 has depended on a 2nd-order system ODE?"  Maybe the proof of Bonnet's Theorem involved them, but the existence and uniqueness of solutions to the equations still followed from (6.2) and (6.14).

We have the next two results exactly as we did for parallel vector fields:\\

\noindent\textbf{Proposition 6.19.} \emph{If $p\in S$ and $\vec v\in T_pS$, there exists $\varepsilon>0$ and a geodesic $\alpha:(-\varepsilon,\varepsilon)\to S$ such that $\alpha(0)=p$ and $\alpha'(0)=\vec v$.  Any two such geodesics coincide on the intersection of their domains.}\\

\noindent\textbf{Proposition 6.20.} \emph{Geodesics are preserved under (local) isometries.}\\

\noindent Geodesics also have constant speed; in fact, we leave it to the reader to verify that a reparametrization of a geodesic is itself a geodesic \emph{if and only if} it has constant speed.  Thus geodesics can be assumed to be parametrized by arc length, whenever convenient.\\

\noindent\textbf{Proposition 6.21.} \emph{If $\alpha$ is a geodesic, then the speed $\|\frac{d\alpha}{dt}\|$ is constant.}
\begin{proof}
Take $\vec v=\frac{d\alpha}{dt}$ in Proposition 6.18.
\end{proof}

\noindent A plane curve has a curvature, which is zero if and only if the curve is a line.  The same is true for curves on a surface:\\

\noindent\textbf{Proposition 6.22 and Definition.} \emph{Let $\alpha:J\to S$ be a regular curve, $\vec v$ a unit vector field along $\alpha$.  Then $\frac{D\vec v}{dt}=\lambda(t)(\mathbf N\times\vec v(t))$ for some $\lambda:J\to\mathbb R$.  Furthermore, $\lambda=\frac{d\vec v}{dt}\cdot(\mathbf N\times\vec v)$.  This $\lambda$ is called the \textbf{algebraic value of the covariant derivative} of $\vec v$, and is denoted $\left[\frac{D\vec v}{dt}\right]$.}

\emph{If $\alpha$ is parametrized by arc length, the algebraic value of the covariant derivative of $\frac{d\alpha}{ds}$ is called the \textbf{geodesic curvature} of $\alpha$ and is denoted $\kappa_g(s)$.}
\begin{proof}
Clearly $\vec v(t)$ and $\mathbf N$ are linearly independent.  Hence to show that $\frac{D\vec v}{dt}$ is a scalar multiple of $\mathbf N\times\vec v(t)$, it suffices to show that it is perpendicular to each of the vectors $\mathbf N,\vec v(t)$.  It is manifestly perpendicular to $\mathbf N$ because it is tangent to the surface.  As for $\vec v(t)$, note that $\vec v(t)$ is a \emph{unit} vector field.  Differentiating $\vec v\cdot\vec v=1$ and applying Exercise 2 of the previous section entails $2\frac{D\vec v}{dt}\cdot\vec v=0$.

This proves that $\frac{D\vec v}{dt}$ is orthogonal to the plane spanned by $\vec v,\mathbf N$, hence is of the form $\lambda(\mathbf N\times\vec v)$.  As for the second statement, note that $\frac{d\vec v}{dt}\cdot(\mathbf N\times\vec v)=\frac{D\vec v}{dt}\cdot(\mathbf N\times\vec v)$ because the right-hand operand is tangent to the surface, hence adding or subtracting a normal vector to the left-hand operand does not change the dot product.  Finally,
$$\frac{D\vec v}{dt}\cdot(\mathbf N\times\vec v)=\lambda(\mathbf N\times\vec v)\cdot(\mathbf N\times\vec v)=\lambda,$$
because $\mathbf N,\vec v$ are unit vectors which are orthogonal to one another, and hence their cross product is a unit vector.
\end{proof}

\noindent Several observations follow.  First, a unit vector field is parallel if and only if the algebraic value of its covariant derivative is zero.  In particular, $\alpha$ is a geodesic if and only if its geodesic curvature is zero.  Secondly, the geodesic curvature of $\alpha$ changes sign when you change the orientation of either $\alpha$ or the surface $S$.

To give an example of geodesic curvature, consider the latitude $z=\cos\beta$ of the sphere $S^2$, with $0<\beta<\pi$ fixed.  A suitable arc-length parametrization of such would be
$$\alpha(s)=\left(\sin\beta\cos\left(\frac s{\sin\beta}\right),\sin\beta\sin\left(\frac s{\sin\beta}\right),\cos\beta\right).$$
By Proposition 6.22, one can compute $\kappa_g(s)=\frac{d^2\alpha}{ds^2}\cdot\left(\mathbf N\times\frac{d\alpha}{ds}\right)$.  On the sphere, $\mathbf N$ is equal to $\alpha(s)$ itself, and:
$$\alpha'(s)=\left(-\sin\left(\frac s{\sin\beta}\right),\cos\left(\frac s{\sin\beta}\right),0\right)$$
$$\alpha''(s)=\left(-\frac 1{\sin\beta}\cos\left(\frac s{\sin\beta}\right),-\frac 1{\sin\beta}\sin\left(\frac s{\sin\beta}\right),0\right)$$
so that $\mathbf N\times\alpha'(s)=\left(-\cos\beta\cos\left(\frac s{\sin\beta}\right),-\cos\beta\sin\left(\frac s{\sin\beta}\right),\sin\beta \right )$, and $\kappa_g(s)=\alpha''(s)\cdot(\mathbf N\times\alpha'(s))=\frac{\cos\beta}{\sin\beta}=\cot\beta$.  Hence, $\alpha$ is a geodesic $\iff$ this is zero $\iff\beta=\pi/2\iff\alpha$ is the equator.

We recall that geodesic curvature has already been mentioned in Exercise 4(d) of Section 6.2, where it was derived in a different way.  We can easily show that these notions of geodesic curvature are identical.  For, suppose $\alpha:J\to S$ is a curve paramterized by arc length, and let $\boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta$ be its Frenet trihedron.  Then we recall that $\alpha'(s)=\boldsymbol\tau(s)$, and $\alpha''(s)=\boldsymbol\tau'(s)=\kappa(s)\boldsymbol\nu(s)$.  We may then let $\mathbf n=\mathbf N\times\alpha'(s)$, so that $\mathbf n$ is tangent to $S$, normal to the curve, and $\{\mathbf N,\alpha'(s),\mathbf n\}$ is a positive orthonormal basis of $\mathbb R^3$.  The idea is to write $\alpha''(s)$ with respect to this basis; since the basis is orthonormal we have
$$\alpha''(s)=\left[\alpha''(s)\cdot\mathbf N\right]\,\mathbf N+\left[\alpha''(s)\cdot\alpha'(s)\right]\,\alpha'(s)+\left[\alpha''(s)\cdot\mathbf n\right]\,\mathbf n.$$ % I didn't really like periods on math expressions
The coordinate of $\mathbf N$ is $\alpha''(s)\cdot\mathbf N=\kappa\boldsymbol\nu\cdot\mathbf N=\kappa_n$, the normal curvature (see either Exercise 4 of Section 6.2, or Exercise 7 of Section 6.5).  The coordinate of $\alpha'(s)$ is $\alpha''(s)\cdot\alpha'(s)$, which is zero, because $\alpha'(s)$ is a unit vector (Exercise 4(b) of Section 6.1).  Finally, the coordinate of $\mathbf n$ is $\alpha''(s)\cdot\mathbf n=\alpha''\cdot(\mathbf N\times\alpha')=\kappa_g$, the geodesic curvature (in the sense we have here).

This entails the exact formula from the exercise:
$$\alpha''(s)=\kappa_g\mathbf n+\kappa_n\mathbf N.$$
Intuitively, the way $\alpha$ curves in the ambient space $\mathbb R^3$ has a certain behavior from the surface's viewpoint, given by the geodesic curvature; and another behavior going normally away from the surface, given by the normal curvature.  Note that since $\mathbf n,\mathbf N$ are orthonormal vectors, taking the magnitude of both sides yields the \emph{Pythagorean identity for curvatures}:
$$\kappa(s)=\sqrt{\kappa_g^2+\kappa_n^2}.$$
This is a useful basic relation between the three types of curvatures of the curve on the surface.\\

\noindent We now turn our attention a surface of revolution:
$$\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u)),~~~~f>0,~~~~(f')^2+(g')^2=1$$
as we find a clever characterization of its geodesics.  We recall that:
$$E=1,~~~~F=0,~~~~G=f(u)^2$$
$$L=f'(u)g''(u)-f''(u)g'(u),~~~~M=0,~~~~N=f(u)g'(u)$$
$$\Gamma_{11}^1=0,~~~~\Gamma_{11}^2=0,~~~~\Gamma_{12}^1=0,~~~~\Gamma_{12}^2=\frac{f'(u)}{f(u)},~~~~\Gamma_{22}^1=-f'(u)f(u),~~~~\Gamma_{22}^2=0$$
If $\alpha:I\to S$ is a geodesic on the surface of revolution, we may write $\alpha(t)=\mathbf x(u(t),v(t))$ and substitute the above in the equations (G) to get:
$$u''-(v')^2f'(u)f(u)=0$$
$$v''+2u'v'\frac{f'(u)}{f(u)}=0$$
The first natural question is, which latitudes and meridians are necessarily geodesics?  First, \emph{all} meridians are geodesics: if $\alpha(t)=\mathbf x(t,v_0)$, so that $u=t$ and $v=v_0$, then the equations are easily seen to hold, since $u''=v'=v''=0$.  As for parallels of latitude $\alpha(t)=\mathbf x(u_0,t)$, we have $u''=u'=v''=0$ and $v'=1$; hence the equations boil down to $f'(u_0)f(u_0)=0$.  Since $f>0$, this is in turn equivalent to $f'(u_0)=0$, which occurs when the distance of the generating curve to the $z$-axis is at a critical point, and the surface is tangent to the $z$-axis centered cylinder along this latitude.  Hence such latitudes are the ony ones which are geodesics:
\begin{center}
\includegraphics[scale=.4]{ParallelGeodesics.png}
\end{center}
Now, what if $\alpha$ is neither a latitude nor a meridian?  Certainly many such geodesics exist, and this question is harder.  Given a geodesic which is not a parallel latitude, we can at least guarantee that it is closest to the $z$-axis when it is parallel to the latitude; there is this result from Alexis Claude de Clairaut:\\

\noindent\textbf{Theorem 6.23.} \textsc{(Clairaut's relation)} \emph{Let $\alpha$ be a geodesic on a surface of revolution $S$, which is not a parallel of latitude.  If, at any point on $\alpha$, $R$ is the radius of the latitude (the distance from the $z$-axis), and $\theta$ is the angle the geodesic makes with the latitude, then $R\cos\theta$ is constant along $\alpha$.  In particular, this constant is the closest distance $\alpha$ can have to the $z$-axis, and it attains this distance when and only when the geodesic is tangent to the latitude.}\\

\noindent Note that for meridians, the first statement is trivial, since $\theta=\pi/2$ and $\cos\theta=0$ throughout.  However, $\alpha$ would never be tangent to the latitude, unless the radius degenerates to zero.
\begin{proof}
We assume that $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$ with the convenient properties $f>0$ and $(f')^2+(g')^2=1$, and that $\alpha(t)=\mathbf x(u(t),v(t))$.  We compute $R$ and $\theta$ as follows:

1. $R(t)=f(u)$, because $f$ is the distance from the $z$-axis at any point on the surface.

2. As for $\theta(t)$, we note that the latitudes are just the coordinate curves where $u$ is held constant; hence their tangent vectors are the $\mathbf x_v$.  We compute the angle between that vector and $\alpha'(t)=u'(t)\mathbf x_u+v'(t)\mathbf x_v$ as follows:
$$\cos\theta(t)=\frac{\alpha'(t)\cdot\mathbf x_v}{\|\alpha'(t)\|\|\mathbf x_v\|}=\frac{(u'\mathbf x_u+v'\mathbf x_v)\cdot\mathbf x_v}{\|u'\mathbf x_u+v'\mathbf x_v\|\|\mathbf x_v\|}=\frac{u'F+v'G}{\sqrt{(u')^2E+2u'v'F+(v')^2G}\sqrt G}$$
$$=\frac{v'f(u)^2}{\sqrt{(u')^2+(v')^2f(u)^2}\sqrt{f(u)^2}}=\frac{v'f(u)}{\sqrt{(u')^2+(v')^2f(u)^2}}.$$

Multiplying the previous two results gives us $R\cos\theta=\frac{v'f(u)^2}{\sqrt{(u')^2+(v')^2f(u)^2}}$, which we wish to show is constant.  Our ambition is to show that its derivative with respect to $t$ is zero.  This will take a sophisticated amount of tabular differentiation, along with the equations that make $\alpha$ a geodesic, which we restate here for convenience:
$$u''-(v')^2f'(u)f(u)=0$$
$$v''+2u'v'\frac{f'(u)}{f(u)}=0$$
We arrange our work as follows:

1. Let $A=v'f(u)^2$ and $B=\sqrt{(u')^2+(v')^2f(u)^2}$.  Then $R\cos\theta=\frac AB$.  We claim that, in fact, $A'=B'=0$; from this it will follow that $A$ and $B$ are both constant, hence certainly $R\cos\theta$ is.

2. By the Product Rule, $\frac{d}{dt}(v'f(u)^2)=\frac{dv'}{dt}f(u)^2+v'\left(\frac{d}{dt}f(u)^2\right)=f(u)^2v''+v'\cdot 2f(u)\left(\frac{d}{dt}f(u)\right)$.

Here we need to be careful, however: $\frac{d}{dt}f(u)$ is \emph{not} equal to $f'(u)$: that would be the derivative of $f(u)$ with respect to $u$ instead of $t$.  To find the derivative with respect to $t$, we apply the Chain Rule to get $\frac{d}{dt}f(u)=\frac{d}{du}f(u)\cdot\frac{du}{dt}=f'(u)u'$.

Therefore, $A'=\frac{d}{dt}(v'f(u)^2)=f(u)^2v''+2f(u)f'(u)u'v'$.

3. The geodesic equations then entail $A'=0$, because
$$A'=f(u)^2v''+2f(u)f'(u)u'v'=f(u)^2\left(v''+2u'v'\frac{f'(u)}{f(u)}\right)=0.$$

4. To differentiate $B$, we first compute $\frac d{dt}(u')^2=2u'\left(\frac d{dt}u'\right)=2u'u''$.  Likewise, $\frac d{dt}(v')^2=2v'v''$, and $\frac d{dt}f(u)^2=2f(u)\left(\frac d{dt}f(u)\right)=2f(u)f'(u)u'$.  Therefore we have
$$\frac{d}{dt}\left((v')^2f(u)^2\right)=\left(\frac d{dt}(v')^2\right)f(u)^2+(v')^2\left(\frac d{dt}f(u)^2\right)$$
$$=2f(u)^2v'v''+2f(u)f'(u)u'(v')^2$$
$$\therefore\frac d{dt}(B^2)=\frac d{dt}\left((u')^2+(v')^2f(u)^2\right)=2u'u''+2f(u)^2v'v''+2f(u)f'(u)u'(v')^2.$$

5. Since clearly $\frac d{dt}(B^2)=2BB'$, we have $B'=\frac{\frac d{dt}(B^2)}{2B}$, which by the previous step is
$$B'=\frac{u'u''+f(u)^2v'v''+f(u)f'(u)u'(v')^2}{B}=\frac{u'u''+f(u)^2v'v''+f(u)f'(u)u'(v')^2}{\sqrt{(u')^2+(v')^2f(u)^2}}.$$

6. We claim that the numerator of the above expression is actually zero, making $B'=0$.  To see why, first observe that $u''=(v')^2f'(u)f(u)$ by the geodesic equations, so that
$$u'u''+f(u)^2v'v''+f(u)f'(u)u'(v')^2=u'(v')^2f'(u)f(u)+f(u)^2v'v''+f(u)f'(u)u'(v')^2$$
$$=f(u)^2v'v''+2f(u)f'(u)u'(v')^2.$$
Furthermore, we have $f(u)^2v'v''+2f(u)f'(u)u'(v')^2=f(u)^2v'\left(v''+2u'v'\frac{f'(u)}{f(u)}\right)=0$, proving our claim as desired.
\end{proof}

\noindent\textbf{THE GAUSS-BONNET THEOREM}\\

\noindent The remainder of this section aims to focus on how Gaussian curvature relates to the total turning angle of a simple loop.  This angle is $2\pi$ in the Euclidean plane, but on various surfaces, it is usually off by an amount caused by the consistent curving of the surface region inside the loop.  This curving is accounted for by the Gaussian curvature.  We wish to make this into a formal statement.

We start by letting $\alpha:[a,b]\to S$ be a closed, piecewise smooth curve.  This means that:
\begin{itemize}
\item $\alpha(a)=\alpha(b)$;

\item There is a finite sequence $a=a_0<a_1<\dots<a_m=b$ such that each $\alpha|_{[a_j,a_{j+1}]}$ is a smooth map $[a_j,a_{j+1}]\to S$.  However, there may be ``corners'' at the $a_j$'s.
\end{itemize}
This is the most convenient kind of loop one can deal with.  We will further want it to be \emph{simple}, i.e., it does not cross itself and it has a well-defined interior.  Examples of such loops are triangles and smooth curves.
\begin{center}
\includegraphics[scale=.4]{ArbTriangle.png}
\end{center}
$\alpha$ must also be positively oriented: this means that if you curl the fingers on your right hand in the direction that $\alpha$ goes in, and stick out your right thumb, your thumb points in the direction of $\mathbf N$ (the \emph{right-hand rule}).  The reader can readily see how the quantities we compute will be negative for negative orientation.

Finally, one may assume $\alpha$ is arc-length parametrized (Proposition 6.1 can be adapted).  Given such a curve, one can diagnose how it turns with just some of the notions we have previously gone over:\\

\noindent\textbf{Theorem 6.24.} \textsc{(Gauss-Bonnet theorem)} \emph{Suppose $\alpha:[a,b]\to S$ is a simple, closed, positively oriented, piecewise smooth curve parametrized by arc length, and $R$ is its interior.  Suppose further that $K$ is the Gaussian curvature, $\kappa_g$ is the geodesic curvature of $\alpha$, and the corners of $\alpha$ make exterior angles of $\theta_1,\theta_2,\dots,\theta_m$.  Then}
$$\int_R K\,d\sigma+\int_a^b\kappa_g(s)\,ds+\sum_{i=1}^m\theta_i=2\pi.$$
In effect, the theorem states that a full turn can always be achieved if you account for all the important attributes of the curve on the surface: (1) the geodesic curvature, which tells how much turning happens on the smooth parts (similar to Exercise 9(c) of Section 6.1); (2) the exterior angles of the corners, which can be thought of as ``discontinuous'' turns;\footnote{Those who have studied applied math may think of it as scalar multiples of Dirac delta functions added to the geodesic curvature.} and (3) the integral of the Gaussian curvature, which indicates how far the twisting of the surface prevents (1) and (2) from working alone.\\

\noindent Before proving the Gauss-Bonnet Theorem, we need a few lemmas, which we will not prove.  For proofs of Lemmas 6.25 and 6.26, see Manfredo Perdigao do~Carmo [12].  Lemma 6.27 can be found in many multivariable calculus texts.\\

\noindent\textbf{Lemma 6.25.} \emph{Every regular surface has an orthogonal parametrization, i.e., one where $F\equiv 0$ throughout.}\\ % Do Carmo page 183

\noindent\textbf{Lemma 6.26.} \textsc{(Theorem of Turning Tangents.)} \emph{If $\alpha:[a,b]\to\mathbb R^2-\{0\}$ is a simple closed positively-oriented curve around the origin, which is injective on $[a,b)$, its winding number is $1$.  In other words, if $\alpha(t)=(r(t)\cos\theta(t),r(t)\sin\theta(t))$ with $r,\theta$ smooth functions and $r>0$, then $\theta(b)-\theta(a)=2\pi$.}\\

\noindent\textbf{Lemma 6.27.} \textsc{(Green's Theorem)} \emph{If $\alpha(t)=(u(t),v(t)):[a,b]\to\mathbb R^2$ is a simple closed positively-oriented curve with interior $R$, and $f$ and $g$ are smooth real-valued functions on an open set containing both $R$ and the curve, then}
$$\int_a^b (fu'+gv')\,dt=\int\!\!\int_R\left(\frac{\partial g}{\partial u}-\frac{\partial f}{\partial v}\right)\,dxdy.$$

\noindent\textbf{Lemma 6.28.} \emph{If a compact region on a surface has an open covering, it can be split into finitely many pieces, each of which is contained in one of the sets of the covering.}
\begin{proof}
This follows from the general fact that any open covering of a compact metric space has a Lebesgue number.  For instance, given the closed cell $[0,1]^n\subset\mathbb R^n$, if $(U_\alpha)_{\alpha\in A}$ is an open covering, there exists $\delta>0$ such that every subset of $[0,1]^n$ with diameter $<\delta$ is contained in one of the $U_\alpha$'s.  Now let $N$ be an integer large enough so that $\frac 1N<\frac{\delta}{\sqrt n}$; then the cell can be gridded into $N^n$ smaller cells (of side length $\frac 1N$), and each will be contained in one of the $U_\alpha$.

The argument can readily be adapted to work for any compact regions.
\end{proof}

\noindent We are finally in a position where we can prove the Gauss-Bonnet Theorem.

\begin{proof}
\emph{(of Theorem 6.24)} \---- First, $S$'s local coordinate charts form an open covering of the curve $\alpha$ and the interior $R$.  In view of Lemma 6.28, the region can be split into finitely many pieces so that each one is contained in a single chart.  It is easy to show that if the Gauss-Bonnet theorem holds for each piece, then it holds for the original $\alpha$.

Hence, we may assume $\alpha$ is contained in a single local coordinate chart $\mathbf x:U\to S$, and we may think of $\alpha(t)=\mathbf x(u(t),v(t))$.  In view of Lemma 6.25, we may also assume $F\equiv 0$ throughout the chart.

By Exercise 6, we may let $\varphi:[a,b]\to\mathbb R$ be a piecewise smooth function such that $\cos\varphi(s)=\frac{\alpha'(s)\cdot\mathbf x_u}{\|\alpha'(s)\|\|\mathbf x_u\|}$ and
$$\kappa_g(s)=\frac 1{2\sqrt{EG}}\left(G_u\frac{dv}{ds}-E_v\frac{du}{ds}\right)+\frac{d\varphi}{ds}.$$
Note that by the Theorem of Turning Tangents, $\varphi(b)-\varphi(a)=2\pi$.  Integrating this with respect to the arc length parameter $s$ gives
\begin{equation}\tag{*}\int_a^b\kappa_g(s)\,ds=\int_a^b\frac 1{2\sqrt{EG}}\left(G_u\frac{dv}{ds}-E_v\frac{du}{ds}\right)\,ds+\int_a^b\frac{d\varphi}{ds}\,ds.\end{equation}
Green's Theorem (6.27) may be used to rewrite the left-hand summand as an integral over $R$:
$$\int_a^b\frac 1{2\sqrt{EG}}\left(G_u\frac{dv}{ds}-E_v\frac{du}{ds}\right)\,ds=\int_R\frac{\partial}{\partial u}\left(\frac 1{2\sqrt{EG}}G_u\right)+\frac{\partial}{\partial v}\left(\frac 1{2\sqrt{EG}}E_v\right)\,dudv$$
$$=\int_R\frac 12\left[\left(\frac{G_u}{\sqrt{EG}}\right)_u+\left(\frac{E_v}{\sqrt{EG}}\right)_v\right]\,dudv.$$
Since $d\sigma$ (the surface metric) is $\sqrt{EG-F^2}\,dudv=\sqrt{EG}\,dudv$, this becomes
$$=\int_R\frac 1{2\sqrt{EG}}\left[\left(\frac{G_u}{\sqrt{EG}}\right)_u+\left(\frac{E_v}{\sqrt{EG}}\right)_v\right]\,d\sigma=-\int_RK\,d\sigma;$$
the integrand is precisely $-K$ by Exercise 3 of Section 6.7.

As for $\int_a^b\frac{d\varphi}{ds}\,ds$, it is tempting to think it is $\varphi(b)-\varphi(a)=2\pi$ by the Second Fundamental Theorem of Calculus.  However, this is not the case, because $\varphi$ is not necessarily continuous.  The Fundamental Theorem of Calculus may only be applied to regions where the function is wholly continuous, so we suppose $a=a_0<a_1<\dots<a_m=b$ is a sequence with each $\alpha|_{[a_j,a_{j+1}]}$ smooth.  Then $\varphi|_{[a_j,a_{j+1}]}$ is smooth as well, and one may compute
$$\int_a^b\frac{d\varphi}{ds}\,ds=\sum_{j=0}^{m-1}\int_{a_j}^{a_{j+1}}\frac{d\varphi}{ds}\,ds=\sum_{j=0}^{m-1}\left(\lim_{t\to{a_{j+1}}^-}\varphi(t)-\lim_{t\to{a_j}^+}\varphi(t)\right).$$
The summand has been written like this because the $\varphi(a_j)$ are not necessarily defined, but you can take the limit as elements of the interval $[a_j,a_{j+1}]$ approach the endpoints.  Note that for $1\leqslant i<m$, $\theta_i=\lim_{t\to a_i^+}\varphi(t)-\lim_{t\to a_i^-}\varphi(t)$; being the exterior angle, it indicates how much the angle changed instantaneously at the corner.  Similarly, $\theta_m=\lim_{t\to a^+}\varphi(t)-\lim_{t\to b^-}\varphi(t)+2\pi$ (the $2\pi$ term arises from the fact that $\varphi(b)-\varphi(a)=2\pi$).  Thus rearranging the summands above gets us
$$\sum_{j=0}^{m-1}\left(\lim_{t\to a_{j+1}^-}\varphi(t)-\lim_{t\to a_j^+}\varphi(t)\right)=2\pi-\sum_{i=1}^m\theta_i.$$
We substitute our results into (*) to get
$$\int_a^b\kappa_g(s)\,ds=-\int_RK\,d\sigma+2\pi-\sum_{i=1}^m\theta_i,$$
which is manifestly equivalent to the theorem statement.
\end{proof}

\noindent\textbf{Corollary 6.29.} \emph{Let $T$ be a triangle made up of three segments of geodesics.  If $T$ has angles $\alpha,\beta,\gamma$, then $\alpha+\beta+\gamma-\pi=\int_RK\,d\sigma$ where $R$ is the interior of $T$.}
\begin{proof}
Apply Theorem 6.24, noting that $\kappa_g=0$ and the $\theta_i$ are equal to $\pi-\alpha,\pi-\beta,\pi-\gamma$.
\end{proof}

\noindent\textbf{Corollary 6.30.} \emph{Let $\alpha$ be a closed geodesic (a loop with no corners at all), and $R$ its interior.  Then $\int_R K\,d\sigma=2\pi$.  In particular, $K$ is strictly positive somewhere on $S$, and if $K$ is strictly positive throughout $S$ then any two closed geodesics intersect.}\\

\noindent This corollary applies to the sphere, where geodesics are great circles.
\begin{proof}
Apply Theorem 6.24, noting that $\kappa_g=0$ and there are no corners.  The last statement holds because, if two closed geodesics do not intersect, the Gaussian curvature must integrate to zero on the strip between them.
\end{proof}

\subsection*{Exercises 6.9. (Parallel Transport and Geodesics)} % Motivate and cover parallel transport and geodesics.
% Show that parallel transports have constant dot product, length and angle, and find one on sphere latitude.
% Cover the geodesic curvature concept as well; also Clairaut's relation for surfaces of revolution.
% AMBITION: Gauss-Bonnet Theorem here, since geodesics and geodesic curvature are finally covered ( http://www.math.uchicago.edu/~may/VIGRE/VIGRE2010/REUPapers/Rotskoff.pdf )
% POTENTIAL EXERCISES: Geodesic is line of curvature <=> plane curve; exponential map; torus geodesics exercise using Clairaut's relation; angle of holonomy
\begin{enumerate}
\item Let $S_1$ and $S_2$ be two regular surfaces, tangent to one another along a curve $\alpha$.  Show that a vector field along $\alpha$ is parallel on $S_1$ if and only if it is transport on $S_2$.

\item Show that parallel vector fields along $\alpha$ form a ($2$-dimensional) vector subspace of all vector fields along $\alpha$.

\item Show that all the geodesics on the sphere are the great circles.  [Use Proposition 6.19.]

\item Let $S$ be a regular surface, $\alpha:I\to S$ a geodesic.

(a) If $\alpha$ is a line of curvature, then it is a plane curve.  [Use Proposition 6.18 to assume $\alpha$ is parametrized by arc length.  Then assume (Exercise 6(a) of Section 6.5) that $\mathbf N'(s)=\lambda(s)\alpha'(s)$.  Since $\alpha$ is a geodesic, however, $\alpha'(s)$ is parallel, and so $\alpha''(s)$ (in $\mathbb R^3$) is normal to the surface.  Since $\alpha''(s)=\boldsymbol\tau'(s)=\kappa(s)\boldsymbol\nu(s)$, conclude that $\boldsymbol\nu=\pm\mathbf N$.  Hence $\boldsymbol\nu'(s)=\pm\lambda(s)\boldsymbol\tau(s)$, which means $\kappa(s)=\pm\lambda(s)$ and $\tau(s)=0$; now use Exercise 8 of Section 6.1.]

(b) Conversely, if $\alpha$ is a plane curve and its curvature in the plane is never zero, then $\alpha$ is a line of curvature.

(c) Let $S$ be a connected regular surface on which all geodesics are plane curves.  Show that $S$ is contained in either a plane or a sphere.  [By part (b), all geodesics are lines of curvature.  Use Proposition 6.19 to show that every direction from every point is a principal direction; i.e., every point is umbilical.  Then conclude using Proposition 6.11.]

\item Verify the Pythagorean identity for curvatures for a latitude on a sphere.

\item (a) If $\alpha:I\to S$ is a piecewise smooth, regular path on a surface, show that there exists a piecewise smooth map $\varphi:I\to\mathbb R$ such that for each $t\in I$, $\cos\varphi(t)=\frac{\alpha'(t)\cdot\mathbf x_u}{\|\alpha'(t)\|\|\mathbf x_u\|}$; in other words, a piecewise smooth function defined on $I$ which equals the angle between the path and $u$'s coordinate curves.

(b) Now suppose $\mathbf x:U\to S$ is a local coordinate chart for which $F\equiv 0$ throughout, and $\alpha:J\to U\to S$ is an arc length parametrization of a piecewise smooth regular path, say $\alpha(s)=(u(s),v(s))$.  Show that the geodesic curvature along $\alpha$ is given by
$$\kappa_g(s)=\frac 1{2\sqrt{EG}}\left(G_u\frac{dv}{ds}-E_v\frac{du}{ds}\right)+\frac{d\varphi}{ds}.$$
[The geodesic curvature can be computed as $\alpha''(s)\cdot(\mathbf N\times\alpha'(s))$.  Use the results from Sections 6.6 and 6.7 to write this as a linear combination of $\mathbf x_u\cdot(\mathbf N\times\mathbf x_v)=-\sqrt{EG}$ and $\mathbf x_v\cdot(\mathbf N\times\mathbf x_u)=\sqrt{EG}$.]

\item What is the generalization of Corollary 6.29 to arbitrary polygons?

\item\emph{(Exponential map for geodesics.)} \---- If $p\in S$, and $U$ is an open neighborhood of $\vec 0(=p)$ in $T_pS$, we say that $\exp_p:U\to S$ is an \textbf{exponential map} at $p$, provided that $\exp_p(\vec 0)=p$, and for each $\vec v\in U$, if $\alpha:I\to S$ is the geodesic with $\alpha(0)=p,\alpha'(0)=\vec v$, then $\exp_p(\vec v)=\alpha(1)$.

(a) Show that an exponential map exists for a sufficiently small neighborhood $U$.  Moreover, for each $\vec v\in T_pS$, the map $\beta:t\mapsto\exp_p(t\vec v)$ (for $t$ in some neighborhood $(-\varepsilon,\varepsilon)$ of the origin) is the geodesic with $\beta(0)=p,\beta'(0)=\vec v$.

(b) Assume that in Theorem 6.2, the solutions vary smoothly with respect to the equations and initial conditions.  Show that $\exp_p$ is a smooth map, and that $d(\exp_p)_{\vec 0}$ is the identity on $T_pS$.

From part (b) it follows that for sufficiently small $U$, $\exp_p$ is also a local coordinate chart.  Its coordinates, called \textbf{normal coordinates}, satisfy the property that any Euclidean line through the origin is a geodesic and its speed coincides with the Euclidean speed.

(c) For sufficiently small $r>0$, one can take the curve $t\mapsto\exp_p(r\cos t,r\sin t)$, which is a circle in the Euclidean coordinates.  This is called the \textbf{geodesic circle} of radius $r$ centered at $p$.  Show that its radii (i.e., the images of the Euclidean radii of $(r\cos t,r\sin t)$, which are geodesics) are orthogonal to the circle on $S$.  This was first stated by Gauss.  [Define $\varphi(r,\theta)=\exp_p(r\cos\theta,r\sin\theta)$.  By normality of the coordinates, $\frac{\partial^2\varphi}{\partial r^2}=0$.  Furthermore, $\|\frac{\partial\varphi}{\partial r}\|^2=1$ throughout; differentiating with respect to $\theta$ entails $\frac{\partial\varphi}{\partial r}\cdot\frac{\partial^2\varphi}{\partial r\partial\theta}=0$.  Use this to show that $\frac{\partial\varphi}{\partial r}\cdot\frac{\partial\varphi}{\partial\theta}$ is independent of $r$.  To see why that dot product is zero, take the limit as $r\to 0$ and note the conformality of the chart at the origin.]

If $S$ is a Lie group $G$ such that the translations $x\mapsto gx$ and $x\mapsto xg$ ($g\in G$) are isometries, it can be shown that Lie group homomorphisms $\mathbb R\to G$ are precisely geodesics sending $0\mapsto 1\in G$, and that this exponential map at $1$ coincides with the map of Exercise 5(h) of the previous section.  In this case, $\exp_1$ is defined on the \emph{whole} tangent plane, unlike the general case of regular surfaces (or, Riemannian manifolds).  However, this is harder to prove.

\item Fix $a>b>0$ in $\mathbb R$, and let $S$ be the torus:
$$\mathbf x(u,v)=((a+b\cos u)\cos v,(a+b\cos u)\sin v,b\sin u)$$
(a) Let $\alpha_1(t)=\mathbf x(\pi/2,t)$.  This is the circular arc along which the plane $z=b$ is tangent to the torus.  Find the geodesic curvature of $\alpha_1$ and the parallel transport along $\alpha_1$ of the vector $\alpha_1'(0)=(0,a,0)$ at $\alpha_1(0)=(a,0,b)$.

(b) Now let $\alpha$ be the geodesic tangent to $\alpha_1$ at the point $\alpha(0)=(a,0,b)$.  Show that $\alpha$ is only present on the outer half of the torus with $|u|\leqslant\pi/2$.  [Use Theorem 6.23.]

(c) Let $\alpha_2(t)=\mathbf x(0,t)$ be the outer ring; this is a geodesic.  If $\beta$ is the geodesic which passes through $\alpha_2$ at the point $(a+b,0,0)$, at an angle of $\theta$, show that $\beta$ passes through the inner ring $t\mapsto\mathbf x(\pi,t)$ if and only if $\cos\theta<\frac{a-b}{a+b}$.

\item Suppose $\alpha:[a,b]\to S$ is a closed, piecewise smooth curve, so that $\alpha(a)=\alpha(b)=p$.  If $\vec v\in T_pS$, the parallel transport along $\alpha$ which starts at $\vec v$ may end at a different vector from $\vec v$ at the same point $p$.

(a) Give an example of where this happens.  [On the sphere, consider either a latitude which is not the equator, or a triangle with three right angles (which has side length $\pi/2$ by Proposition 5.9).]

(b) Show that the \emph{angle} that the ending vector makes with the starting vector $\vec v$ is independent of the point on the geodesic and the starting vector at it.  [Proposition 6.18.]  This angle is called the \textbf{angle of holonomy} of the closed curve.

\item\emph{(Euler characteristic.)} \---- Using the Gauss-Bonnet theorem, it is easy to study the Euler characteristic of compact surfaces.

(a) Recall that the surface area of $S^2$ is $4\pi$.  Suppose $S^2$ is tiled with regions whose boundaries are curve segments.  The segments may not be geodesics, but graph-theoretically, they make the surface into $F$ faces, $E$ edges and $V$ vertices.  Show that $F-E+V=2$.  [Parametrize the rim of each face by arc length, positively oriented.  Explain why at each edge, the two faces meeting it have their parametrizations traverse the edge in opposite directions.  Apply the Gauss-Bonnet Theorem to each face, and sum over all the faces.  This gets you $2F\pi$ on the right-hand side, since there are $F$ faces.  The $\int_RK\,d\sigma$ terms sum to $\int_{S^2}K\,d\sigma=\int_{S^2}1\,d\sigma=4\pi$, the surface area of $S^2$.  Since reversing the direction of a path negates the geodesic curvature, the terms which integrate geodesic curvature all cancel each other out (why?).  Finally, each exterior angle can be written as $\pi$ minus the interior angle, and the interior angles of all the faces add to $2V\pi$.  The rest should be easy algebra.]

(b) Show that for \emph{any} regular surface $S$ globally diffeomorphic to $S^2$, the integral over $S$ of the Gaussian curvature $K$ is $4\pi$.  [If you tile $S$ into $F$ faces, $E$ edges and $V$ vertices, how do you know that $F-E+V=2$ like in part (a)?]  Note the special case where $S$ is a sphere of radius $r$: in such a case, $K=\frac 1{r^2}$ (Exercise 3 of Section 6.6) and the surface area is $4\pi r^2$.

(c) Let $S$ be a compact orientable surface, and tile the surface into $F$ faces, $E$ edges and $V$ vertices.  Show that $F-E+V=\frac 1{2\pi}\int_SK\,d\sigma$, and hence does not depend on the particular tiling.  This integer is called the \textbf{Euler characteristic} of $S$ and is denoted $\chi(S)$.

(d) The Euler characteristic of $S^2$ is $2$, and the Euler characteristic of the torus is $0$.

Any compact orientable surface other than the sphere is, in fact, obtained by taking a certain number of tori and pasting them in chain by cutting circular holes out of them and gluing them together around the holes.  If there are $n$ tori involved, the surface is said to have \textbf{genus $n$}, and its Euler characteristic is $2-2n$.

\item Generalize the concepts of parallel transport and geodesics to arbitrary dimensional manifolds, and prove analogues of Propositions 6.16-6.21.
\end{enumerate}

\subsection*{6.10. Customized Surfaces}
\addcontentsline{toc}{section}{6.10. Customized Surfaces}
Now that we have studied regular surfaces, fundamental form coefficients, curvatures, parallel vector fields, geodesics, and how to find length and area, we are ready to bring back the geometries from Chapters 2, 4 and 5, with the ability to study more about them!  They follow the general notion of a \textbf{customized surface}, which is an open set of $\mathbb R^2$ equipped with a formal collection of first fundamental form coefficients, as we will eventually see.

The mainof idea is to recall all the \emph{intrinsic geometry}: the things that are determined by the first fundamental form coefficients, or what is the same thing, preserved by local isometries.  For example, Gaussian curvature (Theorem 6.13) and geodesics (Proposition 6.20) are part of intrinsic geometry, but the second fundamental form and principal directions are not intrinsic geometry.

In order to be able to deal with intrinsic geometry, we only need a metric given by a first fundamental form.  We no longer imagine that the surface is embedded in $\mathbb R^3$ or in any $\mathbb R^n$.  Thus, we define\\

\noindent\textbf{Definition.} \emph{A \textbf{customized surface} is a pair $(U,g)$ where $U\subset\mathbb R^2$ is an open set, and $g=(E,F,G)$ where $E,F,G:U\to\mathbb R$ are differentiable functions such that $E>0$ and $EG-F^2>0$ throughout $U$.}

\emph{If $p\in U$ and $\vec v=(a,b),\vec w=(c,d)\in\mathbb R^2$ (viewed as vectors at $p$), we define $\left<\vec v,\vec w\right>=E(p)ac+F(p)(ad+bc)+G(p)bd$.  We set $\|\vec v\|_g=\sqrt{\left<\vec v,\vec v\right>}$ and declare the angle between $\vec v$ and $\vec w$ to be $\cos^{-1}\frac{\left<\vec v,\vec w\right>}{\|\vec v\|_g\|\vec w\|_g}$.}\\

\noindent Of course, the conditions $E>0$ and $EG-F^2>0$ must be explicitly imposed to ensure positive definiteness of the inner product.  Also, we are using the notation $\left<-,-\right>$ for inner products to distinguish them from the ordinary Euclidean dot product.  The idea is that the choice of $(E,F,G)$ gives the surface its own notion of length and angle.

Before digging into the examples, we note that (as in Section 6.3), it is customary to write the metric of a customized surface like this:
$$ds^2=E\,du^2+2F\,du\,dv+G\,dv^2.$$

\noindent\textbf{Examples.}

(1) If $U=\mathbb R^2$, and $E=G=1$ and $F=0$ throughout $U$, we have the Euclidean plane with the usual metric $\left<\vec v,\vec w\right>=\vec v\cdot\vec w$.\\

(2) If $U$ is an open set and $\lambda$ is a differentiable function from $U$ to the positive reals $\mathbb R_{>0}$, then $U$ becomes a customized surface with $E=G=\lambda$ and $F=0$.  These are called \textbf{isothermal} or \textbf{conformal coordinates}; they are precisely the kinds of coordinates for which angles have the same measures as the Euclidean angles.  Most of the customized surfaces we will deal with fall under this category.\\

(3) Let $U=\mathbb R^2$, equipped with:
$$E=\frac{v^2+1}{(u^2+v^2+1)^2},~~~~F=-\frac{uv}{(u^2+v^2+1)^2},~~~~G=\frac{u^2+1}{(u^2+v^2+1)^2}$$
Then $EG-F^2=\frac{1}{(u^2+v^2+1)^3}$; hence $E>0$ and $EG-F^2>0$, so we have a customized surface.  We will later see that this is isometric to half the sphere, under the gnomonic projection of Section 5.6.\\

\noindent We now find equations that characterize isometries and conformal mappings etween custized sface.

If $(U,g)$ and $(\overline U,\overline g)$ are customized surfaces with $g=(E,F,G)$ and $\overline g=(\overline E,\overline F,\overline G)$, then a differentiable map $\varphi:U\to\overline U$ is said to be an \textbf{isometric mapping} if for $p\in U$ and $\vec v,\vec w\in T_pU$, we have $\left<d\varphi_p(\vec v),d\varphi_p(\vec w)\right>_{\overline g}=\left<\vec v,\vec w\right>_g$; in other words, $\varphi$ preserves the inner products given by the surface structures.  Since both sides of the equation % "Does \varphi have to be injective?" For an isometric mapping, I don't think so.  However, isometry <=> isometric mapping & diffeomorphism.
$$\left<d\varphi_p(\vec v),d\varphi_p(\vec w)\right>_{\overline g}=\left<\vec v,\vec w\right>_g$$
are symmetric and bilinear, it suffices for the equation to hold for $\vec v,\vec w\in\{\vec e_1,\vec e_2\}$; then it will automatically hold in the general case:
$$\left<d\varphi_p(\vec e_1),d\varphi_p(\vec e_1)\right>_{\overline g}=\left<\vec e_1,\vec e_1\right>_g=E$$
$$\left<d\varphi_p(\vec e_1),d\varphi_p(\vec e_2)\right>_{\overline g}=\left<\vec e_1,\vec e_2\right>_g=F$$
$$\left<d\varphi_p(\vec e_2),d\varphi_p(\vec e_2)\right>_{\overline g}=\left<\vec e_2,\vec e_2\right>_g=G.$$
Let $\begin{bmatrix}a&b\\c&d\end{bmatrix}$ be the matrix of $d\varphi_p$ with respect to the basis $\vec e_1,\vec e_2$; then $d\varphi_p(\vec e_1)=a\vec e_1+c\vec e_2$ and $d\varphi_p(\vec e_2)=b\vec e_1+d\vec e_2$, from which we get
$$\left<d\varphi_p(\vec e_1),d\varphi_p(\vec e_1)\right>_{\overline g}=\overline Ea^2+2\overline Fac+\overline Gc^2$$
$$\left<d\varphi_p(\vec e_1),d\varphi_p(\vec e_2)\right>_{\overline g}=\overline Eab+\overline F(ad+bc)+\overline Gcd$$
$$\left<d\varphi_p(\vec e_2),d\varphi_p(\vec e_2)\right>_{\overline g}=\overline Eb^2+2\overline Fbd+\overline Gd^2.$$
For $\varphi$ to be an isometry, these expressions need to equal $E$, $F$ and $G$ respectively.  Note that this is tantamount to saying
$$\begin{bmatrix}a&c\\b&d\end{bmatrix}\begin{bmatrix}\overline E&\overline F\\\overline F&\overline G\end{bmatrix}\begin{bmatrix}a&b\\c&d\end{bmatrix}=\begin{bmatrix}E&F\\F&G\end{bmatrix},$$
because the left-hand side works out to be a symmetric matrix with the above three expressions in $\overline E,\overline F,\overline G,a,b,c,d$.  If $g$ and $\overline g$ are regarded as the symmetric matrices involving the given coefficients, this says $(d\varphi_p)^T\overline g(d\varphi_p)=g$.  The symmetric matrices involving the first fundamental form coefficients are important anyway, hence are called the \textbf{first fundamental form matrices}.  We have proved:\\

\noindent\textbf{Proposition 6.31.} \emph{Let $(U,g)$ and $(\overline U,\overline g)$ be customized surfaces.  A differentiable map $\varphi:U\to\overline U$ is isometric if and only if, for each $p\in U$, $(d\varphi_p)^T\overline g(d\varphi_p)=g$, where $g=\begin{bmatrix}E&F\\F&G\end{bmatrix}$ and $\overline g=\begin{bmatrix}\overline E&\overline F\\\overline F&\overline G\end{bmatrix}$, the first fundamental form matrices.}\\

\noindent As usual, an isometry is defined to be an isometric diffeomorphism.

Several remarks are in order.  First, suppose $(\overline U,\overline g)$ is a customized surface, and $U$ is an open set of $\mathbb R^2$ (not given a metric).  Assume further that $\varphi:U\to\overline U$ is a differentiable map such that each $d\varphi_p,p\in U$ is a linear isomorphism.  Then $U$ admits a unique metric $g$ such that $\varphi$ is an isometry.  After all, any such metric $g$ must be given by $(d\varphi_p)^T\overline g(d\varphi_p)$ by Proposition 6.31; to show that this is indeed a metric it suffices to show that $g$ is a positive definite symmetric matrix.  Symmetry is clear, and since $d\varphi_p$ is an isomorphism we have
$$\vec v\ne\vec 0\implies d\varphi_p(\vec v)\ne\vec 0\implies\vec v\cdot g\vec v=\vec v\cdot(d\varphi_p)^T\overline g(d\varphi_p)(\vec v)=d\varphi_p(\vec v)\cdot\overline g(d\varphi_p(\vec v))>0$$
(by positive definiteness of $\overline g$), hence $g$ is positive definite.

$g$ is called the \textbf{pullback} of the metric $\overline g$ under the map $\varphi$.  It can be thought of as answering the question ``If we think of $\overline U$ with the different coordinate system that $U$ has, what is our metric?''  If $\varphi$ is a conformal mapping (in the Euclidean sense), then Exercise 4 of Section 4.1 shows that $(d\varphi_p)^T(d\varphi_p)$ is a scalar multiple of the identity, hence the pullback of a conformal metric is a conformal metric.

Conformal mappings are similar to isometries; such maps are characterized by there being a fixed function $\lambda:U\to\mathbb R$ such that $\left<d\varphi_p(\vec v),d\varphi_p(\vec w)\right>_{\overline g}=\lambda(p)^2\left<\vec v,\vec w\right>_g$, which entails:\\

\noindent\textbf{Proposition 6.32.} \emph{Let $(U,g)$ and $(\overline U,\overline g)$ be customized surfaces.  A differentiable map $\varphi:U\to\overline U$ is a conformal mapping if and only if there is a differentiable function $\lambda:U\to\mathbb R$ such that, for each $p\in U$, $(d\varphi_p)^T\overline g(d\varphi_p)=\lambda^2g$, using the notation of Proposition 6.31.}\\

\noindent From this definition, it is clear that $U$ has conformal coordinates if and only if the identity map $(U,\mathfrak e)\to(U,g)$ (where $\mathfrak e$ is the usual Euclidean metric) is conformal.

We now introduce the intrinsic-geometry concepts we have covered in earlier sections:\\

\noindent\textbf{Definition.} \emph{Let $(U,g)$ be a customized surface.  Then:}

\emph{If $\alpha(t)=(u(t),v(t))$ is a curve $I\to U$, the \textbf{length} of $\alpha$ is defined to be $\int_I\|\alpha'(t)\|_g\,dt=\int_I\sqrt{E(u')^2+2Fu'v'+G(v')^2}\,dt$.}

\emph{The area of a region $R\subset U$, in the metric of $U$, is defined as $\int_R\sqrt{EG-F^2}\,du\,dv$.  More generally, $\sqrt{EG-F^2}\,du\,dv$ is the \textbf{surface metric} $d\sigma$ and for $f:R\to\mathbb R$, we set $\int_Rf\,d\sigma=\int_Rf\sqrt{EG-F^2}\,du\,dv$.}

\emph{The \textbf{Christoffel symbols} $\Gamma_{ij}^k,i,j,k\in\{1,2\}$ are the functions $U\to\mathbb R$ which solve the following linear systems:}
$$\left\{\begin{array}{c l}\Gamma_{11}^1E+\Gamma_{11}^2F=\frac 12E_u~~~~~~~\\\Gamma_{11}^1F+\Gamma_{11}^2G=F_u-\frac 12E_v\end{array}\right.$$
$$\left\{\begin{array}{c l}\Gamma_{12}^1E+\Gamma_{12}^2F=\frac 12E_v~~~~~~~\\\Gamma_{12}^1F+\Gamma_{12}^2G=\frac 12G_u~~~~~~~\end{array}\right.$$
$$\left\{\begin{array}{c l}\Gamma_{22}^1E+\Gamma_{22}^2F=F_v-\frac 12G_u\\\Gamma_{22}^1F+\Gamma_{22}^2G=\frac 12G_v~~~~~~~\end{array}\right.$$
\emph{The \textbf{Gaussian curvature} of $U$ at $p$ is defined to be}
$$K=\frac 1E\left(\Gamma_{11}^1\Gamma_{12}^2+(\Gamma_{11}^2)_v+\Gamma_{11}^2\Gamma_{22}^2-\Gamma_{12}^1\Gamma_{11}^2-(\Gamma_{12}^2)_u-\Gamma_{12}^2\Gamma_{12}^2\right).$$
\emph{If $\alpha(t)=(u(t),v(t))$ is a smooth curve $I\to U$, then a vector field $t\mapsto(a(t),b(t))$ along $\alpha$ is called \textbf{parallel} provided}
$$\begin{array}{c l}a'+au'\Gamma_{11}^1+(av'+bu')\Gamma_{12}^1+bv'\Gamma_{22}^1=0\\b'+au'\Gamma_{11}^2+(av'+bu')\Gamma_{12}^2+bv'\Gamma_{22}^2=0\end{array}$$
\emph{and $\alpha$ is called a \textbf{geodesic} if $\frac{d\alpha}{dt}$ is parallel along $\alpha$, or what is the same thing,}
$$\begin{array}{c l}u''+(u')^2\Gamma_{11}^1+2u'v'\Gamma_{12}^1+(v')^2\Gamma_{22}^1=0\\v''+(u')^2\Gamma_{11}^2+2u'v'\Gamma_{12}^2+(v')^2\Gamma_{22}^2=0\end{array}$$
\emph{holds.}\\

\noindent The formula for the area is inherited from Propsition 6.4, and it makes sense since $EG-F^2>0$ by law.  The equations for the Christoffel symbols are from Section 6.7; we may still use them to uniquely determine the Christoffel symbols even though $\mathbf x_u$ and $\mathbf x_v$ do not exist.  The formula for the Gaussian curvature is given by the Gauss equation, again from Section 6.7 (note that $L,M,N$ do not appear or are not needed in this context).  Parallel transport and geodesics are given by the differential equations (PT) and (G) of the previous section.

As before, it can be shown that these things are preserved by local isometries, the length of a curve is independent of how it is parametrized, and parallel vector fields and geodesics also satisfy Propositions 6.16-6.21.  Also, the Gauss-Bonnet Theorem (6.24) holds; this is a bit trickier to prove, but it imitates the argument of the previous section.

Furthermore, it is worth noting that Exercises 3 and 4 of Section 6.7 hold for customized surfaces.  In particular, in conformal coordinates $E=G=\lambda,F=0$, we have $K=-\frac 1{2\lambda}\Delta(\ln\lambda)$.

With this material on generic customized surfaces \-- which are really known as \emph{Riemannian 2-manifolds} \-- we may return to our study on the spherical, Euclidean and hyperbolic planes.\\

\noindent We first let $(U,g)$ be the customized surface with $U=\mathbb R^2$, and $E=G=\frac 4{(u^2+v^2+1)^2}$ and $F=0$.  In accordance with Exercise 2 below, stereographic projection is an isometry from $U$ to $S^2$, which covers $S^2-\{(0,0,1)\}$.  This customized surface is thus the stereographic projection model of the spherical plane: if $\alpha$ is a geodesic of $U$, $\alpha$ maps to a geodesic on $S^2$, which is a great circle by Exercise 3 of Section 6.9.  Hence Exercise 7 of Section 4.2 shows that $\alpha$ is either a Euclidean line through the origin or a Euclidean circle with radius $r$ and center $a$ such that $r^2-\|a\|^2=1$.

In fact, one can show that, when parametrized correctly, those lines and circles satisfy the differential equations for a geodesic (given in the definition).  We will not explicitly do this because it will be more important for us to do the analogous things for the hyperbolic plane.  See Exercise 5, however.

Taking $\lambda=\frac 4{(u^2+v^2+1)^2}$, we compute $-\frac 1{2\lambda}\Delta(\ln\lambda)=1$.  After all, $\ln\lambda=\ln 4-2\ln(u^2+v^2+1)$; differentiating with respect to $u$ entails $-\frac{4u}{u^2+v^2+1}$; differentiating again with respect to $u$ yields $-\frac{4(-u^2+v^2+1)}{(u^2+v^2+1)^2}$; and the rest can similarly be computed.  Thus the Gaussian curvature $K$ is equal to $1$ for this customized surface.  This makes sense, because it is isometric to the unit sphere.

We now set $U'=\{(u,v)\in\mathbb R^2:u^2+v^2<1\}$ and equip it with the same metric as $U$.  We let $(\overline U,\overline g)$ be the customized surface of Example 3 above: $\overline U=\mathbb R^2$ and
$$\overline E=\frac{v^2+1}{(u^2+v^2+1)^2},~~~~\overline F=-\frac{uv}{(u^2+v^2+1)^2},~~~~\overline G=\frac{u^2+1}{(u^2+v^2+1)^2}.$$
Define $\varphi:U'\to\overline U$ via $\varphi(u,v)=\left(\frac{2u}{1-u^2-v^2},\frac{2v}{1-u^2-v^2}\right)$.  We claim that $\varphi$ is an isometry of customized surfaces.  To see this, observe that if $p=(u,v)$ then
$$d\varphi_p=\begin{bmatrix}\frac{2(1+u^2-v^2)}{(1-u^2-v^2)^2}&\frac{4uv}{(1-u^2-v^2)^2}\\\frac{4uv}{(1-u^2-v^2)^2}&\frac{2(1-u^2+v^2)}{(1-u^2-v^2)^2}\end{bmatrix}.$$
And if $\overline u=\frac{2u}{1-u^2-v^2}$ and $\overline v=\frac{2v}{1-u^2-v^2}$ are the \emph{output points} of the input $p$, then the matrix $\overline g$ is given by
$$\begin{bmatrix}\frac{\overline v^2+1}{(\overline u^2+\overline v^2+1)^2}&-\frac{\overline u\,\overline v}{(\overline u^2+\overline v^2+1)^2}\\-\frac{\overline u\,\overline v}{(\overline u^2+\overline v^2+1)^2}&\frac{\overline u^2+1}{(\overline u^2+\overline v^2+1)^2}\end{bmatrix}=$$
$$\frac{(1-u^2-v^2)^2}{(1+u^2+v^2)^4}\begin{bmatrix}(1-u^2+v^2)^2+(2uv)^2&-4uv\\-4uv&(1+u^2-v^2)^2+(2uv)^2\end{bmatrix}.$$

To show that $\varphi$ is an isometry is merely a matter of  computation: by Proposition 6.31, it suffices to show that $(d\varphi_p)^T\overline g(d\varphi_p)=g=\frac 4{(u^2+v^2+1)^2}I_2$, or what is the same thing, $(d\varphi_p)\overline g(d\varphi_p)=\frac 4{(u^2+v^2+1)^2}I_2$ ($d\varphi_p$ is symmetric).  Since $AB=I_2\iff BA=I_2$ for square matrices $A,B$, one can alternatively show $(d\varphi_p)^2\overline g=\frac 4{(u^2+v^2+1)^2}I_2$.

Observe that $\varphi$ passes the stereographic projection of a point on the sphere to the gnomonic projection of the same point.  It is the composition $\mathbf y^{-1}\circ\mathbf x$ where $\mathbf x:U'\to S^2,\mathbf y:\mathbb R^2\to S^2$ are given by
$$\mathbf x(u,v)=\left(\frac{2u}{u^2+v^2+1},\frac{2v}{u^2+v^2+1},\frac{u^2+v^2-1}{u^2+v^2+1}\right)$$
$$\mathbf y(u,v)=\left(\frac u{\sqrt{u^2+v^2+1}},\frac v{\sqrt{u^2+v^2+1}},-\frac 1{\sqrt{u^2+v^2+1}}\right);$$
and $\mathbf x$ and $\mathbf y$ parametrize the southern hemisphere via stereographic projection and gnomonic projection respectively.  Since $\varphi$ is an isometry of customized surfaces, it follows that $\overline U$ has exactly the metric of the gnomonic projection.\\

\noindent\emph{Warning}: The equation $(d\varphi_p)^T\overline g(d\varphi_p)=g$ requires $\overline g$ to be the first fundamental form matrix at the target point $\varphi(p)$, not the domain point $p$.  Notice how $\overline g$ has been carefully computed above.  Inexperienced readers commonly make this mistake, especially when the domain and range of $\varphi$ are the same subset of $\mathbb R^2$.\\

\noindent We have just studied a customized surface which embodies spherical geometry.  Euclidean and hyperbolic geometries have them too.  For Euclidean geometry, you merely take $E=G=1$ and $F=0$ on $\mathbb R^2$.  Then the first fundamental form matrix is the identity matrix.  If $\varphi:\mathbb R^2\to\mathbb R^2$ is an isometry, Proposition 6.31 entails $(d\varphi_p)^T(d\varphi_p)=I_2$ for all $p$, i.e., $d\varphi_p\in O(2)$.  As shown in Section 6.4, it follows that $d\varphi_p$ is constant, and hence $\varphi$ is of the form $[A,\vec v]$ with $A\in O(2),\vec v\in\mathbb R^2$.

But what about the hyperbolic plane?  Suppose $U=\{(u,v)\in\mathbb R^2:u^2+v^2<1\}$ is the open disk, and we take
$$E=G=\frac 4{(1-u^2-v^2)^2},~~~~F=0$$
First, one can compute from the formula $K=-\frac 1{2\lambda}\Delta(\ln\lambda)$ that the Gaussian curvature of this customized surface is $-1$.  In fact, this surface is precisely the Poincar\'e disk model of the hyperbolic plane (without the ideal points).  Perhaps the easiest way to verify this is to relate it to the Poincar\'e half-plane model by an isometry.

For the half-plane model, we take $\overline U=\{(u,v)\in\mathbb R^2:v>0\}$, and $\overline E=\overline G=\frac 1{v^2},\overline F=0$.  Then to begin with, using Exercise 3 of Section 6.7 to get the Christoffel symbols, the differential equations for a geodesic $\alpha(t)=(u(t),v(t))$ are
$$\begin{array}{c l}u''-\frac{2u'v'}v=0\\v''+\frac{(u')^2}v-\frac{(v')^2}v=0\end{array}$$
For fixed $a,r\in\mathbb R,r>0$, one can verify that each of the following curves is a geodesic by plugging into the equations:
$$\alpha(t)=(a,e^t)$$
$$\alpha(t)=(a+r\tanh t,r\operatorname{sech}t)$$
The former is the vertical line $x=a$ and the latter is the semicircle arc of radius $r$ centered at $(a,0)$.  Hence, all lines of the half-plane model \---- vertical lines and semicircle arcs centered on the $x$-axis \---- are geodesics on the customized surface, when parametrized in a particular way.  Observe that every vector at every point in $\overline U$ is tangent to one of these curves; hence, since geodesics are determined by their starting speed and direction, it follows that conversely, every geodesic is a line of the half-plane model.
\begin{center}\includegraphics[scale=.3]{PoincareHalfPlane.png}\end{center}
We recall (Section 4.3, Exercise 8) that the isometry group of the half-plane model is generated by the maps $(u,v)\mapsto(u+r,v)$ for $r\in\mathbb R$, $(u,v)\mapsto(su,sv)$ for $s\in\mathbb R_{>0}$, and the maps $(u,v)\mapsto(-u,v)$ and $(u,v)\mapsto\left(\frac{u}{u^2+v^2},\frac{v}{u^2+v^2}\right)$.  We claim that these maps, and hence all the isometries of the half-plane model, are also isometries in the sense of this chapter.

Using the criterion of Proposition 6.31, this is clear for the maps $(u,v)\mapsto(u+r,v)$, $(u,v)\mapsto(su,sv)$ and $(u,v)\mapsto(-u,v)$.  [Not much work is even needed for $(u,v)\mapsto(u+r,v)$ if you use Exercise 4.]  For the map $\psi(u,v)=\left(\frac u{u^2+v^2},\frac v{u^2+v^2}\right)$, things are tougher to compute, but we will still reach an affirmative conclusion.  Note that its differential is
$$d\psi_{(u,v)}=\begin{bmatrix}\frac{-u^2+v^2}{(u^2+v^2)^2}&-\frac{2uv}{(u^2+v^2)^2}\\-\frac{2uv}{(u^2+v^2)^2}&\frac{u^2-v^2}{(u^2+v^2)^2}\end{bmatrix};$$
and the remaining calculation is straightforward (possibly using Exercise 1).

Thus, the customized surface $(\overline U,\overline g)$ shares all the same geodesics and isometries as the half-plane model we studied in Chapter 4.  According to Exercise 6 of Section 4.3, the conversion map from the disk $U$ to the half-plane $\overline U$ is given by $w\mapsto i\frac{1+w}{1-w}$ for $w\in\mathbb C,|w|<1$; in real coordinates this is
$$\varphi:U\to\overline U,~~~~\varphi(u,v)=\left(-\frac{2v}{u^2-2u+1+v^2},\frac{1-u^2-v^2}{u^2-2u+1+v^2}\right).$$
We leave it to the reader to compute the differential $d\varphi_p$ and use it to verify that $\varphi$ is indeed an isometry of customized surfaces.  Hence $(U,g)$, as a customized surface, shares the credentials of the Poincar\'e disk model, the same way $(\overline U,\overline g)$ shares those of the half-plane model.\\

\noindent We now have our previously studied geometries as customized surfaces:
\begin{itemize}
\item Sphere (stereographic projection): $U=\mathbb R^2$, $E=G=\frac 4{(u^2+v^2+1)^2},F=0$

\item Half-sphere / elliptic plane (gnomonic projection): $U=\mathbb R^2$, $E=\frac{v^2+1}{(u^2+v^2+1)^2},F=-\frac{uv}{(u^2+v^2+1)^2},G=\frac{u^2+1}{(u^2+v^2+1)^2}$

\item Euclidean plane: $U=\mathbb R^2$, $E=G=1,F=0$

\item Hyperbolic plane (Poincar\'e disk model): $U=\{(u,v)\in\mathbb R^2:u^2+v^2<1\}$, $E=G=\frac 4{(1-u^2-v^2)^2},F=0$

\item Hyperbolic plane (Poincar\'e half-plane model): $U=\{(u,v)\in\mathbb R^2:v>0\}$, $E=G=\frac 1{v^2},F=0$
\end{itemize}
Of course, the Beltrami-Klein model of the hyperbolic plane, and the Lambert azimuthal equal-area projection model of the sphere, also have (nonconformal) customized-surface credentials, but we will not compute them. % Honest question, is the Beltrami-Klein model's FFF worth computing?  My previous attempts were beyond my comfort zone

There is also a simultaneous generalization of both regular surfaces and customized surfaces that we will not delve into: one can take a regular surface, but give it each tangent plane a custom symmetric positive definite bilinear form, which varies smoothly with the point.\footnote{More generally, one can take a regular manifold, and equip each tangent space with a symmetric positive definite bilinear form, which varies smoothly from point to point.  Doing so yields a \textbf{Riemannian manifold}.  See Exercise 11 for generalizations to arbitrary manifolds.}  As before, the bilinear forms are a suitable metric, allowing computation of lengths, angles and areas.  In this case, \emph{both} the shape of the surface \emph{and} the metric on it are custom, even though for either regular surfaces or customized surfaces, only one of those is.  Examples of this are the hyperboloid and hemisphere models of the hyperbolic plane (Section 4.9); it is clear why they are not just regular surfaces.

In the remainder of this chapter, we will cover a few results that we were not able to cover before.  First, the spherical, Euclidean and hyperbolic geometries have respective Gaussian curvatures $1,0,-1$.  (By Theorema Egregium, it makes no difference which model we use in each case.)  To each case we apply Corollary 6.29 of the Gauss-Bonnet Theorem; we get the following results:\\

\noindent\textbf{Proposition 6.33.}

(i) \emph{In spherical geometry, the area of a triangle is equal to the sum of the angles minus $\pi$.}

(ii) \emph{In Euclidean geometry, the sum of the angles of a triangle is equal to $\pi$.}

(iii) \emph{In hyperbolic geometry, the area of a triangle is equal to $\pi$ minus the sum of the angles.}\\

\noindent This supports the definitions of area in Exercise 3 of Section 4.5 and Exercise 8 of Section 5.2.  It incidentally aids Exercise 1 of Section 5.3, as it yields an easier proof of part (c) than the one given in the hint.

Since we now know the areas of triangles (and hence polygons), it is only natural to study the circumference and area of a circle.  You may recall that a Euclidean circle of radius $r$ has circumference $2\pi r$ and area $\pi r^2$.  To see why this is really so, parametrize such a circle via
$$\alpha(t)=(r\cos t,r\sin t),~~~~0\leqslant t\leqslant 2\pi$$
in the Euclidean plane.  We have made the choice of range of $t$ so that the circle is traversed exactly once.  With that, the circumference of the circle \---- the arc length of $\alpha$ \---- is $\int_0^{2\pi}\|\alpha'(t)\|\,dt=\int_0^{2\pi}\|(-r\sin t,r\cos t)\|\,dt=\int_0^{2\pi}r\,dt=2\pi r$.
There are also many ways to find the area.  One way is to consider the map
$$\rho:[0,r]\times[0,2\pi]\to\mathbb R^2\text{ given by }\rho(a,\theta)=(a\cos\theta,a\sin\theta).$$
Then the range of $\rho$ is the disk $D$ of radius $r$, and $\rho$ does not have overlaps.  Hence, one can compute the area of $D$ by pulling back an integral to $[0,r]\times[0,2\pi]$ using the Jacobian determinant; in other words, $\operatorname{Area}(D)=\int_{[0,r]\times[0,2\pi]}\det(d\rho_p)\,da\,d\theta$.  Now, basic computation shows that for $p=(\theta,a)$,
$$d\rho_p=\begin{bmatrix}\cos\theta&-a\sin\theta\\\sin\theta&a\cos\theta\end{bmatrix},\text{ and hence }\det(d\rho_p)=a.$$
Therefore said integral is equal to
$$\int_{[0,r]\times[0,2\pi]}a\,da\,d\theta=\int_0^{2\pi}\int_0^r a\,da\,d\theta=\int_0^{2\pi}\left(\int_0^ra\,da\right)\,d\theta=\int_0^{2\pi}\frac{r^2}2\,d\theta=\pi r^2.$$

Our ambition is to do the same thing for circles in the spherical and hyperbolic planes.  It is not hard, given the material we have already covered.  We will do the hyperbolic case here; the spherical case will be left to the reader.

Let $C$ be circle of radius $r$ in the hyperbolic plane; we may assume we are in the Poincar\'e disk model and $C$ is centered at the origin.  Here we must be careful, because $r$ is the \emph{hyperbolic} radius of $C$, not the Euclidean radius.  We recall (Exercise 1(c) of Section 4.5) that if $z\in\mathbb C,|z|<1$ is regarded as a point of the Poincar\'e disk model, then its hyperbolic distance from the origin is $2\tanh^{-1}|z|$.  Hence $C$ is given by the equation $2\tanh^{-1}|z|=r$, or what is the same thing, $|z|=\tanh(r/2)$.  Hence % I added "then."  "And" would make the sentence have the same grammar as "I know that if I drink alcohol today, and I will feel unhappy tomorrow." -- clearly not right.
\begin{center}
\textbf{If $C$ is a circle of radius $r$, centered at the origin of the Poincar\'e disk, its Euclidean radius is $\tanh(r/2)$.}
\end{center}
This makes sense, because $r$ can be an arbitrary positive real number, but the Euclidean radius must be $<1$ so the circle is contained in the disk.  We henceforth parametrize $C$ via $\alpha(t)=(\tanh(r/2)\cos t,\tanh(r/2)\sin t)$; and we parametrize its interior $D$ by the coordinates
$$\rho(a,\theta)=(a\cos\theta,a\sin\theta),~~~~0\leqslant a\leqslant\tanh(r/2),0\leqslant\theta\leqslant 2\pi.$$
In a customized surface, the arc length of a curve $\alpha(t)=(u(t),v(t))$ is equal to $\int_I\sqrt{E(u')^2+2Fu'v'+G(v')^2}\,dt$.  For the Poincar\'e disk model of the hyperbolic plane, $E=G=\frac 4{(1-u^2-v^2)^2}$ and $F=0$; in this case the arc length is $\int_I\frac 2{1-u^2-v^2}\sqrt{(u')^2+(v')^2}\,dt=\int_I\frac 2{1-u^2-v^2}\|\alpha'(t)\|\,dt$.  Hence we compute the circumference of the above circle as
$$\int_0^{2\pi}\frac 2{1-u^2-v^2}\|\alpha'(t)\|\,dt,\text{ with }\alpha(t)=(\tanh(r/2)\cos t,\tanh(r/2)\sin t).$$
Direct computation shows $\alpha'(t)=(-\tanh(r/2)\sin t,\tanh(r/2)\cos t)$, so that $\|\alpha'(t)\|=\tanh(r/2)$; and $\frac 2{1-u^2-v^2}=\frac 2{1-\tanh^2(r/2)}=\frac 2{\operatorname{sech}^2(r/2)}=2\cosh^2(r/2)$.  Hence
$$\int_0^{2\pi}\frac 2{1-u^2-v^2}\|\alpha'(t)\|\,dt=\int_0^{2\pi}2\cosh^2(r/2)\tanh(r/2)\,dt.$$
Yet $2\cosh^2(r/2)\tanh(r/2)=2\cosh^2(r/2)\frac{\sinh(r/2)}{\cosh(r/2)}=2\cosh(r/2)\sinh(r/2)=\sinh r$, and hence $\int_0^{2\pi}2\cosh^2(r/2)\tanh(r/2)\,dt=\int_0^{2\pi}\sinh r\,dt$
$$=2\pi\sinh r.$$ % Okie, even though this result is prominent in (6.34)

It is really striking how, in hyperbolic geometry, the circumference of a circle of radius $r$ is proportional to $\sinh r$, hence grows (asymptotically) exponentially with $r$.

We may similarly find the area; since $\sqrt{EG-F^2}=\frac 4{(1-u^2-v^2)^2}$ in this case, this may be computed as $\int_D\frac 4{(1-u^2-v^2)^2}\,du\,dv$.  Taking the pullback to the domain of $\rho$, we get
$$\int_D\frac 4{(1-u^2-v^2)^2}\,du\,dv=\int_{[0,\tanh(r/2)]\times[0,2\pi]}\frac 4{(1-u^2-v^2)^2}\det(d\rho_p)\,da\,d\theta.$$
As before $\det(d\rho_p)=a$, and since $(u,v)=(a\cos\theta,a\sin\theta)$ we have $\frac 4{(1-u^2-v^2)^2}=\frac 4{(1-a^2)^2}$.  Hence this evaluates to
$$\int_{[0,\tanh(r/2)]\times[0,2\pi]}\frac{4a}{(1-a^2)^2}\,da\,d\theta=\int_0^{2\pi}\int_0^{\tanh(r/2)}\frac{4a}{(1-a^2)^2}\,da\,d\theta$$
$$=\int_0^{2\pi}\left(\int_0^{\tanh(r/2)}\frac{4a}{(1-a^2)^2}\,da\right)\,d\theta=\int_0^{2\pi}\left(\frac 2{1-a^2}\big|_0^{\tanh(r/2)}\right)\,d\theta$$
$$=\int_0^{2\pi}\left(\frac 2{1-\tanh^2(r/2)}-2\right)\,d\theta=\int_0^{2\pi}2\left(\frac 1{\operatorname{sech}^2(r/2)}-1\right)\,d\theta$$
$$=\int_0^{2\pi}2\left(\cosh^2(r/2)-1\right)\,d\theta=\int_0^{2\pi}2\sinh^2(r/2)\,d\theta$$
$$=4\pi\sinh^2(r/2).$$
For spherical geometry, just imitate the computations, with the stereographic projection model in place of the Poincar\'e disk model, $\frac 4{(u^2+v^2+1)^2}$ in place of $\frac 4{(1-u^2-v^2)^2}$, and trigonometric functions in place of hyperbolic functions.  Note in this case that we must have $0<r<\pi$.  The circumference works out to be $2\pi\sin r$, and the area is $4\pi\sin^2(r/2)$.\\

\noindent\textbf{Proposition 6.34.}

(i) \emph{In spherical geometry, a circle of radius $r$ has circumference $2\pi\sin r$ and area $4\pi\sin^2(r/2)$.}

(ii) \emph{In Euclidean geometry, a circle of radius $r$ has circumference $2\pi r$ and area $\pi r^2$.}

(iii) \emph{In hyperbolic geometry, a circle of radius $r$ has circumference $2\pi\sinh r$ and area $4\pi\sinh^2(r/2)$.}\\

\noindent The areas of various figures, such as lunes on the sphere, can likewise be computed.  There are vast geometrical areas to explore.  From the Ancient Greeks to the Middle Ages, from Euclid and Descartes to Euler and Gauss and Riemann, geometry has been around for centuries, and will continue to have interesting properties today and in the future.

\subsection*{Exercises 6.10. (Customized Surfaces)} % Mention that all the intrinsic geometry works if we declare any positive definite matrix we please to be the FFF.
% Then use it to define customized surfaces.  Define dot product, length, angle, area, isometries, conformal mappings, Gaussian curvature, parallel transport, geodesics
% through what can be purely derived from the FFF.  Then bring back the spherical, Euclidean and hyperbolic planes from previous chapters, but in the calculus setting!
% Take the opportunity to do several things that couldn't be done before; e.g., circumference and area of a circle.
% POTENTIAL EXERCISES: In Poincar\'e half-plane model of hyperbolic plane, area of triangle = \pi - sum of angles (there is an easier proof that avoids the general Gauss-Bonnet Theorem); find FFF for the other models of the hyperbolic plane; growth in a uniform tiling of number of faces a circle passes through w.r.t. to radius of circle; and, of course, the last exercise will generalize to higher dimensions.
\begin{enumerate}
\item Suppose $(U,g)$ and $(\overline U,\overline g)$ are customized surfaces, both with conformal coordinates, say $E=G=\lambda,F=0$ for $g$, and $\overline E=\overline G=\mu,\overline F=0$ for $\overline g$.  Then a differentiable map $\varphi:U\to\overline U$ is an isometry if and only if $(d\varphi_p)^T(d\varphi_p)=\frac{\lambda}{\mu}I_2$ for all $p\in U$.  [Proposition 6.31.]

Note that $\varphi$ is conformal by the criterion (vi) of Exercise 4 of Section 4.1.  It should be clear (from the get-go) that a differentiable map $U\to\overline U$ is conformal in the Euclidean sense if and only if it is a conformal mapping of the customized surfaces.

\item Suppose the sphere is parametrized by stereographic projection:
$$\mathbf x(u,v)=\left(\frac{2u}{u^2+v^2+1},\frac{2v}{u^2+v^2+1},\frac{u^2+v^2-1}{u^2+v^2+1}\right).$$
Show that if the sphere has the ordinary metric inherited from $\mathbb R^3$, then the first fundamental form coefficients of the parametrization are $E=G=\frac 4{(u^2+v^2+1)^2}$ and $F=0$.

\item Let $\alpha:I\to U$ be a smooth curve in a customized surface.  Come up with a rigorous definition of the algebraic value of the covariant derivative of a unit vector field, as well as of $\alpha$'s geodesic curvature (see the definition in Proposition 6.22).

\item Let $(U,g)$ be a customized surface with conformal coordinates $E=G=\lambda$, $F=0$, and let $T$ be an element of $\operatorname{Isom}(\mathbb R^2)$ which fixes $U$.  Show that $T$ is an isometry of $U$ if and only if $\lambda(p)=\lambda(T(p))$ for all $p\in U$.

\item Let $(\mathbb R^2,g)$ be the stereographic projection model of the sphere, $E=G=\frac 4{(u^2+v^2+1)^2}$ and $F=0$.  Fix $0\leqslant\theta<2\pi$.

(a) Show that $\alpha(t)=\left(\frac{\cos t}{1-\sin\theta\sin t},\frac{\cos\theta\sin t}{1-\sin\theta\sin t}\right)$ is a geodesic.  [If $\lambda=\frac 4{(u^2+v^2+1)^2}$, then $\frac{\lambda_u}{\lambda}=(\ln\lambda)_u=\frac{-4u}{u^2+v^2+1}$ and similarly $\frac{\lambda_v}{\lambda}=\frac{-4v}{u^2+v^2+1}$.  Use Exercise 3 of Section 6.7 to get the Christoffel symbols, then compute $\alpha$ and $\alpha'$ and verify the differential equations.]

(b) If $\theta\ne\pi/2,3\pi/2$, then $\alpha$ is the Euclidean circle centered at $(0,\tan\theta)$ with radius $\sec\theta$.  Conclude that it has radius $r$ and center $a$ such that $r^2-\|a\|^2=1$.

(c) Show that for $|t|<\pi$, $\beta(t)=(\tan(t/2),0)$ is a geodesic, which is a Euclidean line through the origin.

\item Verify that, if isometric maps are defined by the equation in Proposition 6.31, then the composition $(U_0,g_0)\overset{\varphi}{\to}(U_1,g_1)\overset{\psi}{\to}(U_2,g_2)$ of isometric maps is isometric, and the identity map $(U,g)\to(U,g)$ is an isometry.  Moreover, the inverse of a (bijective) isometry is also an isometry.  Do the same for conformal mappings.

\item Without using Proposition 6.33, prove that:

(a) If a triangle in the hyperbolic plane has exactly one ideal vertex, and angles $\alpha,\beta$ at the regular vertices, then the area of the triangle is $\pi-\alpha-\beta$.  [Assume we are in the Poincar\'e half-plane model, the ideal vertex is $\infty$, and the other two vertices are $(-\cos\alpha,\sin\alpha)$ and $(\cos\beta,\sin\beta)$ (on the origin-centered unit semicircle).  Since $\sqrt{EG-F^2}=\frac 1{v^2}$, the area of the triangle is $\int_R\frac 1{v^2}\,du\,dv$ where $R$ is the interior; now compute this as $\int_{-\cos\alpha}^{\cos\beta}\int_{\sqrt{1-u^2}}^\infty\frac 1{v^2}\,dv\,du$.]

(b) If a regular triangle in the hyperbolic plane has angles $\alpha,\beta,\gamma$, then its area is $\pi-\alpha-\beta-\gamma$.  [Extend one side to meet the rim at infinity, and connect this ideal point to the vertex opposite the side.  This yields two triangles with exactly one ideal vertex; apply part (a) to each one.]

\item Let $(U,g)$ be the customized surface with $U=\{(u,v)\in\mathbb R^2:|v|<\pi/2\}$, and the conformal coordinates $E=G=\sec^2v,F=0$.

(a) Show that the Gaussian curvature is $-1$ everywhere.

(b) Show that $(U,g)$ is the band model of the hyperbolic plane (Section 4.9, Exercise 4).  [Recall that $(u,v)\mapsto(-e^u\sin v,e^u\cos v)$ is a conversion map from the band model to the Poincar\'e half-plane model; verify that this is an isometry in this chapter's sense.]

\item If $(U,g)$ is the customized surface with $U=\mathbb R^2$, $E=G=\operatorname{sech}^2v,F=0$, show that $(U,g)$ is locally isometric to the sphere, under Mercator's projection (covered in Section 6.4).

\item Consider a face-transitive tiling in either the spherical, Euclidean or hyperbolic plane; i.e., the isometry group of the tiling acts transitively on the faces.  Then fix a point $p$ on the plane.  For each positive real $r$, one can consider the number of faces whose distance to $p$ is roughly $r$.  Though this is not exactly rigorous, one can still study its (asymptotic) growth with respect to $r$.  Explain why:

(a) In the Euclidean plane, the number of faces at a distance $r$ from $p$ grows roughly proportionally with respect to $r$.

(b) In the hyperbolic plane, the number of faces at a distance $r$ from $p$ grows roughly exponentially with respect to $r$.

[Use Proposition 6.34.]

\item This exercise, as before, generalizes the material to arbitrary dimensions.  If $m$ is a positive integer, we define a \textbf{customized $m$-manifold} (or a piece of a \textbf{Riemannian $m$-manifold}) to be a pair $(U,g)$ where $U\subset\mathbb R^m$ is an open set, and $g=\begin{bmatrix}g_{ij}\end{bmatrix}_{1\leqslant i,j\leqslant m}$ is a matrix consisting of differentiable functions $g_{ij}:U\to\mathbb R$ such that $g$ is symmetric and positive definite throughout $U$.

For $p\in U$ and $\vec v,\vec w$ vectors at $p$, define $\left<\vec v,\vec w\right>=\vec v\cdot g\vec w$.  Then this is a symmetric positive definite bilinear form.  As in the $2$-dimensional case, it is used to define the length of curves, and angles between vectors.

(a) If $V\subset\mathbb R^{m'}$ is open and $\alpha:V\to U$ is smooth, then the $m'$-dimensional volume of $\alpha(V)$ is defined as
$$\int_V\sqrt{\det((d\alpha)^Tg(d\alpha))}\,dv_1\dots dv_{m'}.$$
In Exercise 12(d) of Section 6.3, we have shown this for regular manifolds embedded in Euclidean space.  Here, it is really a bona fide \emph{definition} of the volume of the $m'$-dimensional region, because this volume would otherwise have no meaning.  One still needs to verify that this volume is well-defined, and independent of how the region is parametrized.

To show this, suppose $\overline V\subset\mathbb R^{m'}$ is an open set and $\sigma:\overline V\to V$ is a diffeomorphism.  Show that replacing $\alpha$ with $\overline\alpha=\alpha\circ\sigma:\overline V\to U$ leaves the above integral invariant, and explain why that suffices.  [Verify that $\det((d\overline\alpha)^Tg(d\overline\alpha))=\det(d\sigma)^2\det((d\alpha)^Tg(d\alpha))$ by using the Chain Rule.  Then recall how an integral can be pulled back using the Jacobian determinant.]

(b) Taking $m'=m$ in part (a), the volume of an $m$-dimensional region $V\subset U$ is $\int_V\sqrt{\det g}\,dv_1\dots dv_m$.  Taking $m'=1$ in part (a), the length of a curve $\alpha:I\to U$ is equal to $\int_I\sqrt{\alpha'(t)\cdot g\alpha'(t)}\,dt=\int_I\sqrt{\left<\alpha'(t),\alpha'(t)\right>}\,dt$.  Of course, we rewrite $\sqrt{\left<\alpha'(t),\alpha'(t)\right>}$ as $\|\alpha'(t)\|_g$, the magnitude of the vector in the customized metric.

(c) If $\varphi:U\to\overline U$ is a smooth map of customized manifolds $(U,g)$ and $(\overline U,\overline g)$ (with possibly different dimensions), then $\varphi$ is isometric (i.e., its differential preserves the inner products) if and only if at each $p\in U$, $(d\varphi_p)^T\overline g(d\varphi_p)=g$.  Also, $\varphi$ is conformal if and only if there is a smooth map $\lambda:U\to\mathbb R$ such that $(d\varphi_p)^T\overline g(d\varphi_p)=\lambda^2g$.

(d) Use Exercise 9(a) of Section 6.7 to define the Christoffel symbols $\Gamma_{ij}^k$ with $1\leqslant i,j,k\leqslant m$.  In accordance with Exercise 12 of the previous section, if $\alpha(t)=(u_1(t),\dots,u_m(t))$ is a curve in $U$, and $\vec v(t)=(a_1(t),\dots,a_m(t))$ is a vector field along $\alpha$, $\vec v$ is defined to be \textbf{parallel} along $\alpha$ if
$$a_k'+\sum_{i=1}^m\sum_{j=1}^ma_iu_j'\Gamma_{ij}^k=0\text{ for all }k,$$
and $\alpha$ is defined to be a \textbf{geodesic} if its velocity is parallel, i.e.,
$$u_k''+\sum_{i=1}^m\sum_{j=1}^mu_i'u_j'\Gamma_{ij}^k=0\text{ for all }k.$$
Verify that these are invariant under isometries.

(e) Show that: (i) $U=\mathbb R^m,g=I_m$ gives Euclidean $m$-space; (ii) $U=\mathbb R^m,g=\frac 4{(u_1^2+\dots+u_m^2+1)^2}I_m$ gives the stereographic projection model of the $m$-sphere; (iii) $U=B_1(0)=\{(u_1,\dots,u_m)\in\mathbb R^m:u_1^2+\dots+u_m^2<1\}$ and $g=\frac 4{(1-u_1^2-\dots-u_m^2)^2}I_m$ gives the Poincar\'e ball model of hyperbolic space; and (iv) $U=\{(u_1,\dots,u_m)\in\mathbb R^m:u_m>0\}$ and $g=\frac 1{u_m^2}I_m$ gives the Poincar\'e half-space model of hyperbolic space.

[In each case, think of isometries and geodesics.]

The remaining parts of this exercise use the concepts of vector fields and the Lie bracket, from Exercise 4 of Section 6.8.  Let $\mathcal V(U)$ be the space of all vector fields on $U$; they are smooth functions $U\to\mathbb R^m$.  An \textbf{affine connection} is a map $\nabla:\mathcal V(U)\times\mathcal V(U)\to\mathcal V(U)$ sending $(X,Y)\mapsto\nabla_XY$, such that for $X,X_1,X_2,Y,Y_1,Y_2\in\mathcal V(U),f\in\mathcal C^\infty(U)$, we have:
$$\nabla_{X_1+X_2}Y=\nabla_{X_1}Y+\nabla_{X_2}Y$$
$$\nabla_X(Y_1+Y_2)=\nabla_XY_1+\nabla_XY_2$$
$$\nabla_{fX}Y=f\nabla_XY,~~~~~~\nabla_X(fY)=f\nabla_XY+X(f)Y.$$
Observe that $\nabla_XY$ is bilinear over $\mathbb R$, because for $a\in\mathbb R$, $X(a)=0$ with $a$ regarded as a constant function, and hence $\nabla_X(aY)=a\nabla_XY$, and the rest is clear.

If $(U,g)$ is a customized surface, $\nabla_XY$ is said to be a \textbf{Levi-Civita connection} provided that for $X,Y,Z\in\mathcal V(U)$,
\begin{center}
$\nabla_XY-\nabla_YX=[X,Y]$ (the Lie bracket is the commutator of the connection);

$X(\left<Y,Z\right>)=\left<\nabla_XY,Z\right>+\left<Y,\nabla_XZ\right>$ (similar to the dot product rule for differentiation).
\end{center}
(f) Show that there is a unique Levi-Civita connection on any customized surface.  [Verify that a Levi-Civita connection necessarily satisfies the equation
$$\left<\nabla_XY,Z\right>=\frac 12\left(X\left<Y,Z\right>+Y\left<Z,X\right>-Z\left<X,Y\right>+\left<[X,Y],Z\right>+\left<[Z,X],Y\right>-\left<[Y,Z],X\right>\right),$$
and use this to show uniqueness.  As for existence, show that for every $X,Y\in\mathcal V(U)$, there is a vector field whose inner product with any $Z$ is the above expression, and the function which returns this vector field is indeed a Levi-Civita connection.]

(g) Let $E_i$ be the constant vector field $\vec e_i$; note that for $f\in\mathcal C^\infty(U)$, $E_i(f)=\frac{\partial f}{\partial u_i}$.  Also, every vector field is uniquely expressible as $\sum_{i=1}^m a_iE_i$ with $a_i\in\mathcal C^\infty(U)$; the $a_i$ are then the coordinates.

Show that $\nabla_{E_i}E_j=\sum_{\ell=1}^m\Gamma_{ij}^\ell E_\ell$.  [Since the $E_i$'s form a basis, it suffices to show that $\left<\nabla_{E_i}E_j,E_k\right>=\left<\sum_{\ell=1}^m\Gamma_{ij}^\ell E_\ell,E_k\right>=\sum_{\ell=1}^m\Gamma_{ij}^\ell g_{k\ell}$.  For that, use part (f) and Exercise 9(a) of Section 6.7.]

(h) If $X=\sum_{i=1}^m a_iE_i$ and $Y=\sum_{j=1}^m b_jE_j$, then
$$\nabla_XY=\sum_{k=1}^m\left(\sum_{i,j=1}^ma_ib_j\Gamma_{ij}^k+\sum_{i=1}^ma_i\frac{\partial b_k}{\partial u_i}\right)E_k.$$
Note that each index that is summed from $1$ to $m$ occurs in multiple factors of the summand.  This is natural in differential geometry, as summations come from dot products, the Chain Rule and matrix multiplications, and in any of these cases, the index simultaneously counts for two factors.  Thus some mathematicians use \textbf{Einstein's summation convention}, where the symbol ``$\sum$'' is omitted and indices which occur in multiple factors are intended to be summed over; e.g., $a_ib_{ij}$ means $\sum_{i=1}^ma_ib_{ij}$ ($j$ is not summed over because it only appears once).

(i) Show that $\nabla_XY$ takes the covariant derivative of $Y$ with respect to $X$.

(j) Let $N$ be a (regular) submanifold of $U$.  Show that if $X=0$ on $N$ then $\nabla_XY=0$ on $N$; and if $X$ is tangent to $N$ and $Y=0$ on $N$ then $\nabla_XY=0$ on $N$.  [This holds for any affine connection, not just the Levi-Civita one.]

(k) Assume vector fields $N$ can be extended to $U$ (this fact can be confirmed using bump functions, but we do not do that here).  Use part (i) to show that, if $X$ is only defined on $N$ but $Y$ is defined throughout $U$, then $\nabla_XY$ can at least be defined on $N$.  [Extend $X$ to a vector field on $U$; part (i) shows that on $N$, $\nabla_XY$ doesn't depend on the particular way it was extended.]  Also, if $X$ is tangent to $N$ and $Y$ is only defined on $N$, $\nabla_XY$ can be defined on $N$.

(l) Conclude that if $\alpha:I\to U$ is a curve and $X$ is a vector field along $\alpha$, $\nabla_{\alpha'}X$ is defined along $\alpha$.  Show that $X$ is parallel along $\alpha$ if and only if $\nabla_{\alpha'}X=0$, and $\alpha$ is a geodesic if and only if $\nabla_{\alpha'}\alpha'=0$.  [This is the actual definition of these terms in advanced differential geometry.]
\end{enumerate}

\end{document}