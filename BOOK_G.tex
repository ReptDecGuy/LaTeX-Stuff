\documentclass[leqno]{book}
\usepackage[small,nohug,heads=vee]{diagrams}
\diagramstyle[labelstyle=\scriptstyle]
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[pdftex]{graphicx}
\usepackage{mathrsfs}
\usepackage{mathabx}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage[utf8]{inputenc}

\makeatletter
\newcommand*\bcd{\mathpalette\bcd@{.5}}
\newcommand*\bcd@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\begin{document}

\noindent TBA

\cleardoublepage

\addcontentsline{toc}{section}{\textbf{Introduction}}
\section*{Introduction}

Geometry dates back thousands of years.  It was first recorded as early as the 2nd millennium BC in ancient Egypt, where it was used in finding area and volume, as well as in construction and astronomy.  This book serves as a course in many different kinds of geometry, namely, Euclidean, projective, hyperbolic and spherical.  It studies each type of geometry in depth and compares and contrasts them.  Here is an example typifying the three kinds of geometry: % https://en.wikipedia.org/wiki/Geometry
\begin{center}
\includegraphics[scale=.18]{SphDodecahedron.png}
\includegraphics[scale=.45]{Tilings_Hex.png}
\includegraphics[scale=.2]{HeptagonalTiling.png}\\
Spherical~~~~~~~~~~~~~~~~~~~~Euclidean~~~~~~~~~~~~~~~~~~~~Hyperbolic
\end{center}

The first chapter is a preliminary course on the abstract algebra concept of group theory.  Its purpose is to get the reader familiar with groups, as the study of geometry deals with many of them, such as isometry groups.  Isometry groups give ways that a space can be rigidly moved without stretches or bends.  It also has a few specific statements (such as Proposition 1.9) that build on abstract algebra for the benefit of proofs later in the book.  The last section covers group actions.  It has a guided exercise studying symmetric groups in detail, along with alternating groups, which we will need, for example, when we study the icosahedron.

The book then delves into Euclidean geometry, starting with a crash course on pure plane geometry from the axioms.  That is what geometry technically is, so what would a geometry book be without it?  The book then goes into the work of defining coordinates, then studies tilings of polygons, as well as straightedge and compass constructions (along with a brief exercise on fields).  Afterwards, it introduces Euclidean space in higher dimensions, and studies Euclidean isometry groups, identifying, for example, the isometry groups of all the Platonic solids.  The chapter concludes with a classification of finite point groups in 3-space.

Next, projective geometry is covered briefly, and used to cover certain subtle results which hold in the Euclidean plane, such as Desargues' Theorem.  This chapter precedes the one on hyperbolic geometry, so that it can be used when we study the Beltrami-Klein model.  Next, Sections 4.1-4.2 cover stereographic projection, central inversion and M\"obius transforms.  Sections 4.3-4.9 deal with hyperbolic geometry, where lines curve inwards to the Euclidean eye, and the sum of the angles of a triangle is less than two right angles.  Triangle patterns are also introduced in Chapter 4, for the aid of constructing various uniform tilings in the hyperbolic plane, and performing certain computations involving them.  The chapter concludes by bringing in various models of the hyperbolic plane and relating them to one another.

Chapter 5 covers the third kind of geometry, namely spherical geometry, the only one of our three geometries which takes place on a compact surface of finite area.  After spherical tilings and spherical polyhedra are covered, the final result of Chapter 2 is then used to classify the 13 Archimedean solids, along with their duals, the Catalan solids.  The chapter concludes with a section which links the sphere to the projective plane and studies single elliptic geometry. % Yes, it is the name, see Wikipedia!

The sixth and final chapter, goes into deeper mathematical waters than the earlier chapters.  It introduces differential geometry, where one uses multivariable calculus to study geometric constructions on various surfaces.  The concepts of the first and second fundamental forms, Gaussian curvature, equations of compatibility and geodesics are effectively covered most of the chapter.  The last section introduces open sets of the Euclidean plane with customized metrics, and brings back the spherical, Euclidean and hyperbolic geometries from previous chapters, proving new things about them, such as the circumference of a circle.

This book has been in the making since January 2019.  Credit is due to Professors Claire Burrin and Sagun Chanillo of Rutgers University, where I learned some of this material. % You think first person is suitable?

This book assumes the reader has a background in complex numbers, linear algebra, and, occasionally, basic topology.  It is meant to be suitable for high school / college level and beyond, except for a few elective exercises in Chapter 6.  Each section is followed by exercises, most of which are guided if they might be difficult for an average reader.  It is strongly recommended that the conscientious reader work out the exercises, as their statements are sometimes used in later parts of the book. % "When I WAS a child, and this child WAS-" > "Stevie Nix ignored the subjunctive."

\tableofcontents








































\chapter{Preliminary Group Theory}

The material to be covered in this book will depend heavily on symmetries and their invariants.  Certain complicated constructions may be more easily understood once we know how to transform them to easier constructions.  The symmetries of an object form the algebraic structure known as a group.  Thus, a brief course on group theory will be a beneficial place to start.

This chapter will go through the basics of group theory, as well as introduce a few basic propositions that will aid in some of the later chapters.

\subsection*{1.1. Definition and Examples of Groups}
\addcontentsline{toc}{section}{1.1. Definition and Examples of Groups}
To first get a glimpse of what a group is, we shall introduce the notion of a permutation.  Informally, a permutation of a set of elements is a mere rearrangement of the elements.  For example, the following are permutations of the letters A, C, N, E:
\begin{center}
E~N~C~A~~~~~~~~~~~~~~~~~C~E~A~N
\end{center}
Permutations show up frequently in combinatorics, where one questions how many ways the letters of a certain word can be rearranged.  However, it does not make sense to talk about a permutation if we don't know how the elements were arranged originally: without this information, we merely have an ordered list.  We thus require that any permutation specifically state the starting and ending position of each element.  For example, the permutation C~E~A~N of the letters A, C, N, E can be rewritten as
$$\begin{pmatrix}\text A&\text C&\text N&\text E\\\text C&\text E&\text A&\text N\end{pmatrix},$$
which shows that A maps to C, C maps to E, N maps to A and E maps to N.  With permutations defined this way, we can compose any two of them, by seeing where each element of the set goes through the permutation on the right, and then seeing where the result lands through the permutation on the left.  For example,
\begin{center}\includegraphics[scale=.35]{ACNE.png}\end{center}
%$$\begin{pmatrix}\text A&\text C&\text N&\text E\\\text C&\text E&\text A&\text N\end{pmatrix}\circ\begin{pmatrix}\text A&\text C&\text N&\text E\\\text E&\text N&\text C&\text A\end{pmatrix}=\begin{pmatrix}\text A&\text C&\text N&\text E\\\text N&\text A&\text E&\text C\end{pmatrix}$$
because A maps to E through the permutation on the right, and then E maps to N through the permutation on the left, so the composite permutation maps A to N; similarly for the other three letters.  Notice that the result is still a permutation.  This is true upon composition of any two permutations, as we now see.

If $X$ is any set, we formally define a \textbf{permutation} of $X$ to be a bijective function from $X$ to $X$.  Such a function rearranges the elements of $X$.  For instance, if $X$ is the four-element set $\{\text A,\text C,\text N,\text E\}$, then these functions above are examples of the permutations of the letters.  The set of all permutations of $X$ is denoted $S(X)$ and is called the \textbf{symmetric group with respect to $X$}.  We now make the following observations:

\---- If $f$ and $g$ are elements of $S(X)$, so is the composite function $f\circ g$.  After all, the composition of bijective functions is bijective.

\---- Whenever $f,g,h\in S(X)$, $(f\circ g)\circ h=f\circ(g\circ h)$ because function composition is associative: indeed, both of these functions are equal to the map $x\mapsto f(g(h(x)))$.

\---- The identity map $1_X:X\to X$ is a permutation.  Moreover, for any $f\in S(X)$, $1_X\circ f=f$ and $f\circ 1_X=f$, because for each $x\in X$, $1_X(f(x))=f(x)$ and $f(1_X(x))=f(x)$.

\---- If $f\in S(X)$, then since $f$ is bijective, it has an inverse $f^{-1}$, which is also bijective.  Thus, $f^{-1}\in S(X)$, and $f\circ f^{-1}=f^{-1}\circ f=1_X$.

Finally, we observe that if $f,g\in S(X)$, then $f\circ g$ may \emph{not} equal $g\circ f$.  For example, if $X=\{1,2,3\}$ then:
$$\begin{pmatrix}1&2&3\\3&1&2\end{pmatrix}\circ\begin{pmatrix}1&2&3\\2&1&3\end{pmatrix}=\begin{pmatrix}1&2&3\\1&3&2\end{pmatrix}$$
$$\begin{pmatrix}1&2&3\\2&1&3\end{pmatrix}\circ\begin{pmatrix}1&2&3\\3&1&2\end{pmatrix}=\begin{pmatrix}1&2&3\\3&2&1\end{pmatrix}$$
Thus composition is not generally commutative.  It is, however, associative, and every element of $S(X)$ has an inverse.  We make abstract these properties in the following definition.\\

\noindent\textbf{Definition.} \emph{A \textbf{group} is a nonempty set $G$ equipped with a binary operation $*$ satisfying the following conditions:}

(i) \emph{Whenever $a\in G$ and $b\in G$, $a*b\in G$.  We say $G$ is \textbf{closed under}~$*$.}

(ii) \emph{The operation is associative: $(a*b)*c=a*(b*c)$ for all $a,b,c\in G$.}

(iii) \emph{There is an element $1\in G$ (called the \textbf{identity element}) such that $1*a=a$ and $a*1=a$ for all $a\in G$.}

(iv) \emph{For each $a\in G$, there is an element $d\in G$ (called the \textbf{inverse} of $a$) such that $a*d=1$ and $d*a=1$.}\\

\noindent\textbf{Definition.} \emph{A group $G$ is said to be \textbf{abelian}\footnote{Named after the early 19th century mathematician, Niels Henrik Abel.} if for all $a,b\in G$, $a*b=b*a$.}\\

\noindent\textbf{Note.} Condition (i) \---- which states that $G$ is closed under the operation \---- is considered unnecessary by certain authors.  If the binary operation is written as $*:G\times G\to G$ in the definition, then condition (i) is automatic.  However, we will sometimes use operations $*$ without assuming from the start that they have this form.  Thus we explicitly list (i) as a condition that $G$ must satisfy.\\

\noindent\textbf{Examples.}

(1) The set $S(X)$ of permutations of $X$ is a group under function composition, as we have previously established.  (Hence the name ``symmetric \emph{group}.'')  This group is only abelian if $|X|\leqslant 2$, however, as one can readily show.

If $X$ is the set $\{1,2,\dots,n\}$ for some positive integer $n$, then $S(X)$ is denoted $S_n$.  One can intuitively see that, e.g., the groups $S(\{\text A,\text C,\text N,\text E\})$ and $S_4$ are alike.  Similarly, if $X$ is any finite set such that $|X|=n$ then $S(X)$ and $S_n$ are alike, so this characterizes finite symmetric groups.\\

(2) The integers $\mathbb Z$ form a group under the addition operation $+$.  After all, the sum of two integers is an integer, so condition (i) holds.  Since addition of integers is associative, we have condition (ii).  $0\in\mathbb Z$ serves as an additive identity; $0+a=a+0=a$ for all $a\in\mathbb Z$.  Thus condition (iii) holds.  Finally, condition (iv) holds, because for each $a\in\mathbb Z$, one can take $d=-a$, and then $a+d=d+a=0$.  Moreover, addition of integers is commutative, so $\mathbb Z$ is also an abelian group.

Note that the definition of a group has the identity element denoted as $1$.  However, depending on the operation equipped, the notation may vary.  When the operation is addition, we use $0$ for the identity element; we mostly use $1$ as the identity when the group is completely abstract.\\

(3) Under multiplication, however, $\mathbb Z$ is \emph{not} a group.  Conditions (i), (ii) and (iii) are satisfied, but $\mathbb Z$ does not possess a \emph{multiplicative} inverse for every element; for example, there is no $x\in\mathbb Z$ such that $2\cdot x=1$.\\

(4) Do the rational numbers $\mathbb Q$ form a group under multiplication?  Well, conditions (i), (ii) and (iii) are easy to check.  It may \emph{seem} like (iv) is true, because whenever $x\in\mathbb Q$, the multiplicative inverse $\frac 1x$ is also rational, and $x\cdot\frac 1x=1$.  However, $0$ is an element of $\mathbb Q$ with no multiplicative inverse, and is in fact the only one.  This violates condition (iv) in the definition of a group.

But now, suppose $G$ is the set $\mathbb Q-\{0\}$, obtained by removing $0$ from the set $\mathbb Q$ of rational numbers.  Then (i), (ii) and (iii) manifestly hold for $G$ ((i) because the product of nonzero rational numbers is nonzero).  Moreover, we also have (iv) now, because every element of $G$ is a \emph{nonzero} rational number, hence has a multiplicative inverse which is also a nonzero rational number.  Thus $G$ \---- alternatively denoted by $\mathbb Q_{\ne 0}$ or $\mathbb Q^\times$ \---- is a group under multiplication.  $G$ is abelian.\\

(5) Let $n$ be a positive integer, and let $\mathbb Z/n\mathbb Z$ be the set of congruence classes modulo $n$.  Thus for $a,b\in\mathbb Z$, $\overline a=\overline b$ in $\mathbb Z/n\mathbb Z$ if and only if $a\equiv b\pmod n$, that is, $n\mid(a-b)$.  Using the Division Algorithm, one observes that $\mathbb Z/n\mathbb Z=\{\overline 0,\overline 1,\dots,\overline{n-1}\}$.  Then $\mathbb Z/n\mathbb Z$ is readily seen to be an abelian group under addition.  For example, if $n=4$, the operation table looks like this:
\begin{center}
\begin{tabular}{c|cccc}
$+$ & $\overline 0$ & $\overline 1$ & $\overline 2$ & $\overline 3$\\\hline
$\overline 0$ & $\overline 0$ & $\overline 1$ & $\overline 2$ & $\overline 3$\\
$\overline 1$ & $\overline 1$ & $\overline 2$ & $\overline 3$ & $\overline 0$\\
$\overline 2$ & $\overline 2$ & $\overline 3$ & $\overline 0$ & $\overline 1$\\
$\overline 3$ & $\overline 3$ & $\overline 0$ & $\overline 1$ & $\overline 2$
\end{tabular}
\end{center}
Conditions (i), (ii) and (iii) hold (as addition is well-defined in modular arithmetic); condition (iv) also holds, but in a rather interesting way, because, for example, if $n=4$ as in the table above, then $\overline 1+\overline 3=\overline 0$.

The group $\mathbb Z/n\mathbb Z$ is called the \textbf{cyclic group of order $n$}.  The word ``cyclic'' is involved because there is one element of the group (in this case $\overline 1$) which can be iteratively combined with itself to cover the entire group.  The additive group $\mathbb Z$ in Example (2) is sometimes called the \textbf{infinite cyclic group}.\\

(6) Here is an example of a nonabelian group: let $GL_2(\mathbb R)$ be the set of nonsingular (i.e., invertible) $2\times 2$ matrices with real entries.  We show that this is a group under matrix multiplication.  (This is known as the \textbf{general linear group}.)  Firstly, (i) holds, because the formula $\det(AB)=\det A\det B$ implies that the product of nonsingular matrices is nonsingular.  It is well known in linear algebra that multiplication of matrices is associative; hence (ii).  The identity matrix $I_2=\begin{bmatrix}1&0\\0&1\end{bmatrix}$ satisfies $I_2A=AI_2=A$ for all matrices $A$, so that condition (iii) holds.  As for condition (iv), note that we are restricting ourselves to \emph{nonsingular} matrices.  Thus every matrix in $GL_2(\mathbb R)$ has an inverse, and this inverse is also in $GL_2(\mathbb R)$.  Hence, $GL_2(\mathbb R)$ is a group under multiplication.

However, matrix multiplication is \emph{not} commutative.  For instance, if $A=\begin{bmatrix}1&1\\0&1\end{bmatrix}$ and $B=\begin{bmatrix}2&0\\0&1\end{bmatrix}$, then $A,B\in GL_2(\mathbb R)$, but $AB=\begin{bmatrix}2&1\\0&1\end{bmatrix}\ne\begin{bmatrix}2&2\\0&1\end{bmatrix}=BA$.  Hence this group is nonabelian.\\

(7) Imagine a square on the table.  Now think of all possible ways to take the square, move it around, and put it back down to match the original shape.  Let us number the vertices so that we can distinguish these moves.
\begin{center}\includegraphics[scale=.4]{Square1.png}\end{center}

Of course, we do not care about where the square traveled midway; we only care about which vertices are at which positions at the end.  We could hand the square over to a circus clown, watch him do some juggling, then when it falls on the ground, take the square back and return it to the table.

So what positions can the square end up in?  First, the square could have the same side face-down as it did before.  Then it is visually clear that the square must have either been rotated by one, two or three quarters of a turn, or put back in its starting position:
\begin{center}\includegraphics[scale=.3]{SquaresOP.png}\end{center}

Alternatively, if the side that used to be face-down is now face-up, the square has been reflected over one of the four symmetry lines:
\begin{center}\includegraphics[scale=.3]{SquaresOR.png}\end{center}

It is readily seen that these are the only possible ways to move the square so that it'll match the original shape.  There is a natural composition on these transformations; $a\circ b$ is obtained by doing transformation $b$ to the square first, then transformation $a$.\footnote{We perform group operations from right to left, as we think about them as an abstraction of function composition.}  This will land the square to match its original shape, hence will always be one of the eight transformations above.

With a bit of work, you can figure out, for each of the $8^2=64$ ordered pairs of transformations, what their composition is.  This amounts to the following operation table (where the row label is the left operand):
\begin{center}
\begin{tabular}{c|cccccccc}
$\circ$ & $r_0$ & $r_1$ & $r_2$ & $r_3$ & $v$ & $t$ & $h$ & $d$\\\hline
$r_0$ & $r_0$ & $r_1$ & $r_2$ & $r_3$ & $v$ & $t$ & $h$ & $d$\\
$r_1$ & $r_1$ & $r_2$ & $r_3$ & $r_0$ & $d$ & $v$ & $t$ & $h$\\
$r_2$ & $r_2$ & $r_3$ & $r_0$ & $r_1$ & $h$ & $d$ & $v$ & $t$\\
$r_3$ & $r_3$ & $r_0$ & $r_1$ & $r_2$ & $t$ & $h$ & $d$ & $v$\\
$v$ & $v$ & $t$ & $h$ & $d$ & $r_0$ & $r_1$ & $r_2$ & $r_3$\\
$t$ & $t$ & $h$ & $d$ & $v$ & $r_3$ & $r_0$ & $r_1$ & $r_2$\\
$h$ & $h$ & $d$ & $v$ & $t$ & $r_2$ & $r_3$ & $r_0$ & $r_1$\\
$d$ & $d$ & $v$ & $t$ & $h$ & $r_1$ & $r_2$ & $r_3$ & $r_0$
\end{tabular}
\end{center}
We leave it to the reader to verify that the four conditions in the definition of a group are satisfied.  The eight elements $\{r_0,r_1,r_2,r_3,v,t,h,d\}$ thus form a group under the composition operation.  It is nonabelian, because, for example $r_1\circ v=d$ but $v\circ r_1=t$.  This group is denoted $D_4$ and is \textbf{the dihedral group of degree $4$}.\\

(8) More generally, imagine a regular $n$-gon (i.e., a regular polygon with $n$ sides) on the table.  Now think of all possible ways to take it, move it around, and put it back down to match the original shape.  If the face-down side of the polygon stays face-down, the polygon will be rotated at one of $n$ possible angles (see figure at left).  If the face-down side of the polygon becomes face-up, the polygon will be flipped over one of the lines of symmetry (see figure at right).
\begin{center}\includegraphics[scale=.3]{Heptagons.png}\end{center}

Now let $r$ be counterclockwise rotation by $360/n$ degrees (it cycles each vertex along one edge), and let $d$ be any one of the flips.  Then $r^n=1$, $d^2=1$ and $dr=r^{-1}d$, as one can verify.  (By $r^n$, of course, we mean $r\cdot r\cdot\dots r$, where there are $n$ operands.)  Moreover, $D_n=\{1,r,r^2,\dots,r^{n-1},d,rd,\dots,r^{n-1}d\}$ is a group under the composition operation, which we call \textbf{the dihedral group of degree $n$}.\\

\noindent We have given several examples of groups.  At this point we shall denote the operation of an abstract group via juxtaposition; in other words, we use $ab$ (or $a\cdot b$) rather than $a*b$.  Now we shall prove some basic properties of groups that follow from the definition of a group.  For example, is the identity element necessarily unique?  Is the inverse of any element necessarily unique?  As we now see, the answer is yes.\\

\noindent\textbf{Proposition 1.1.} \emph{Let $G$ be a group.  Then:} %1.1

(i) \emph{$G$ has a unique identity element.}

(ii) \emph{For each $a\in G$, $a$ has a unique inverse.  (This inverse is denoted $a^{-1}$.)}

(iii) \emph{For $a,b\in G$, $(ab)^{-1}=b^{-1}a^{-1}$.}

(iv) \emph{For each $a\in G$, $(a^{-1})^{-1}=a$.}

(v) \emph{For any $a,b\in G$, the equation $ax=b$ has a unique solution for $x$ in $G$, and the equation $ya=b$ has a unique solution for $y$ in $G$.}

\begin{proof}
(i) $G$ has an identity element $1$ by definition.  Now suppose $1'\in G$ is also an identity element.  To prove the uniqueness, we will show that $1'=1$.  Indeed, $1=1'\cdot 1=1'$; therefore $1'=1$.

(ii) Suppose $d,d'\in G$ are both inverses of $a$.  To prove the uniqueness, we will show $d=d'$.  We have
$$d=d1=d(ad')=(da)d'=1d'=d'$$
Hence $d=d'$.

(iii) By part (ii), $(ab)^{-1}$ is the \emph{unique} inverse of $ab$.  But $b^{-1}a^{-1}$ is also an inverse of $ab$, because
$$(ab)(b^{-1}a^{-1})=a(bb^{-1})a^{-1}=a1a^{-1}=aa^{-1}=1$$
and similarly $(b^{-1}a^{-1})(ab)=1$.  Therefore, $(ab)^{-1}=b^{-1}a^{-1}$.

(iv) By part (ii), $(a^{-1})^{-1}$ is the unique inverse of $a^{-1}$.  Yet $a$ is clearly an inverse of $a^{-1}$, because $aa^{-1}=a^{-1}a=1$.  Therefore $(a^{-1})^{-1}=a$.

(v) $x=a^{-1}b$ satisfies the first equation because $ax=aa^{-1}b=1b=b$.  To show uniqueness, let $x\in G$ be any element such that $ax=b$.  Then $a^{-1}b=a^{-1}ax=1x=x$, hence $x=a^{-1}b$.  The same argument goes for the equation $ya=b$; the solution is $y=ba^{-1}$.
\end{proof}

\noindent\textbf{EXPONENTIATION AND ORDER}\\

\noindent Let $G$ be a group, $a\in G$ and $n\in\mathbb Z$.  Earlier, if $n>0$, we defined $a^n=aa\dots a$, where there are $n$ operands.  For example, $a^2=aa$, $a^3=aaa$, etc.  If $n<0$, say $n=-k$ with $k>0$, then we define $a^n=a^{-1}a^{-1}\dots a^{-1}$, where there are $k$ operands.  Finally we set $a^0=1$.  We thus have a notion of exponentiation in $G$.  The familiar laws of exponents $a^{n+m}=a^na^m$ and $a^{nm}=(a^n)^m$ hold; see Exercise 2.

If $n$ is the smallest positive integer such that $a^n=1$, $n$ is called the \textbf{order of $a$}, and is denoted $|a|$; and $a$ is said to have \textbf{order $n$}.  If no such positive integer exists, $a$ is said to have \textbf{infinite order}.  Also, the size of the set $G$ is denoted $|G|$ and called the \textbf{order of $G$}.\\

\noindent\textbf{Proposition 1.2.} \emph{Let $G$ be a group and $a\in G$.  Then:} %1.2

(i) \emph{If $a$ has order $n$, then for $i,j\in\mathbb Z$, $a^i=a^j$ if and only if $i\equiv j\pmod n$.  In particular, $a^k=1$ if and only if $n\mid k$.}

(ii) \emph{If $a$ has infinite order, then for $i,j\in\mathbb Z$, $a^i=a^j$ if and only if $i=j$.}

\begin{proof}
(i) Suppose $i\equiv j\pmod n$.  Then $n\mid(i-j)$, so that $i-j=nk$ for some $k\in\mathbb Z$.  With that, $i=nk+j$, so that $a^i=a^{nk+j}=a^{nk}a^j=(a^n)^ka^j=1^ka^j=1a^j=a^j$.

Conversely, suppose $a^i=a^j$.  Then $a^{i-j}=a^ia^{-j}=a^i(a^j)^{-1}=1$.  By the Division Algorithm, one can write $i-j=nq+r$ where $0\leqslant r<n$.  Then $a^{i-j}=(a^n)^qa^r=a^r$.  Thus $a^r=1$.  Since $n$ is the \emph{smallest} positive integer such that $a^n=1$, we must have $r=0$.  Therefore $i-j=nq$ and $i\equiv j\pmod n$.

(ii) If $i=j$ then clearly $a^i=a^j$.  Conversely, suppose that $a^i=a^j$.  Then $1=a^i(a^j)^{-1}=a^{i-j}$.  If $i-j>0$, then some positive-integer power of $a$ would be $1$, contradicting the fact that $a$ has infinite order.  We get a similar contradiction if $i-j<0$, for then $1=a^{j-i}$ and $j-i>0$.  Therefore, $i-j=0$ and $i=j$.
\end{proof}

\noindent We conclude this section by introducing the product of two groups $G$ and $H$.  Suppose $*$ is the operation of $G$ and $\star$ is the operation of $H$.\footnote{The different operation symbols helps one understand that the construction is possible if the groups use different kinds of operations; e.g., if one uses addition and the other uses multiplication.}  Define an operation $\diamond$ on the Cartesian product $G\times H=\{(g,h):g\in G,h\in H\}$ via
$$(g_1,h_1)\diamond(g_2,h_2)=(g_1*g_2,h_1\star h_2)$$
Then it is readily verified that $G\times H$ is a group.  Its identity element is $(1_G,1_H)$ where $1_G$ is the identity element of $G$ and $1_H$ is the identity of $H$.  The inverse of $(g,h)\in G\times H$ is equal to $(g^{-1},h^{-1})$.

Moreover, $G\times H$ is abelian if and only if $G$ and $H$ are both abelian.  $G\times H$ is finite if and only if $G$ and $H$ are both finite, and in this case $|G\times H|=|G|~|H|$.

\subsection*{Exercises 1.1. (Definition and Examples of Groups)}
\begin{enumerate}
\item If $ab=1$ in a group, prove that $ba=1$.

\item Which of the following are groups under the given operation?

(a) $\mathbb Q_{>0}=\{x\in\mathbb Q:x>0\}$ under multiplication.

(b) $\mathbb R$ under addition.

(c) $\mathbb R$ under multiplication.

(d) $\mathbb R_{\ne 0}$ under multiplication.

(f) The set of even numbers under addition.

(g) The set of odd numbers under addition.

(h) $\mathbb R$ under the operation $a*b=a+2b$.

(i) The set of $2\times 2$ matrices with real entries, under addition.

(j) The set of singular $2\times 2$ matrices with real entries, under addition.

(k) The set of nonsingular $2\times 2$ upper triangular matrices with real entries, under multiplication.  An \textbf{upper triangular} matrix is a matrix all of whose entries on the bottom-left of the main diagonal are zero.

(l) The set of polynomials with real coefficients, under addition.

(m) The set of polynomials with real coefficients that have $17$ as a root, under addition.

(n) The set of polynomials with real coefficients of degree $6$, under addition.

(o) The set $\left\{\begin{bmatrix}a&b\\0&1\end{bmatrix}:a,b\in\mathbb R,a>0\right\}$ under multiplication.

(p) The set $\left\{\begin{bmatrix}a&a\\0&1\end{bmatrix}:a\in\mathbb R,a\ne 0\right\}$ under multiplication.

(q) $\mathbb R$ under the operation $a*b=a+b-ab$.

(r) $\mathbb R-\{1\}$ under the operation $a*b= a+b-ab$.

\item Let $G$ be a group and $a\in G$.  Show that: % You mentioned Hungerford's A. Alg. - an Intro., which is cited in the bibliography. ^^ If you insist on the acknowledgements section teasing him, I guess it will do that... in the fourth dimension.

(a) $a^{n+m}=a^na^m$.  [One or both of the exponents could be negative or zero.]

(b) $a^{nm}=(a^n)^m$.

(c) If $a,b\in G$ and $ab=ba$, then $(ab)^n=a^nb^n$.

(d) Show by example that part (c) may be false if $ab\ne ba$.

\item If $p$ is prime, $a^p=1$ and $a\ne 1$, then what is the order of $a$?

\item If $a\in G$ has order $nm$, show that $a^n$ has order $m$.

\item Suppose $a,b\in G$, $a\ne 1$, $b^5=1$ and $ba=a^2b$.  What is the order of $a$?  [Show by induction that $b^ka=a^{2^k}b^k$ for positive integers $k$.]

\item If $G$ is a finite group and $|G|$ is even, show that $G$ has an element of order $2$.  [The involution $x\mapsto x^{-1}$ on $G$ must have an even number of fixed points.  An \textbf{involution} on a set $X$ is a function $f:X\to X$ such that $f\circ f=1_X$.]

\item (a) If $a$ and $b$ have finite order and $ab=ba$, then $(ab)^{|a||b|}=1$.

(b) Show by example that part (a) may be false if $ab\ne ba$.

\item Let $G$ be a group of order $4$ in which no element has order $4$.

(a) Show that $G$ has no element of order $3$.  [If $|g|=3$, then the elements of $G$ are $g,g^2,g^3=1$ and a fourth element $d$.  Now which of these four elements is $gd$?]

(b) Explain why every $g\ne 1$ in $G$ has order $2$.

(c) Let the elements of $G$ be $1,a,b,c$ and write out the operation table for~$G$.

\item Let $G=\{(a,b):a,b\in\mathbb R,a\ne 0\}$ and define an operation on $G$ via $(a_1,b_1)*(a_2,b_2)=(a_1a_2,a_1b_2+b_1)$.  Show that $G$ is a group.  Is $G$ abelian?

\item Let $G$ be a group, and define a new operation $*$ on $G$ via $a*b=ba$.  Show that $G$ is also a group under this operation.

\item Let $G$ be a group and $c\in G$.  Define a new operation on $G$ via $a*b=acb$.  Show that $G$ is a group under this operation.

\item If $G$ is a nonabelian group, then $|G|\geqslant 6$.  [If $a,b\in G$ and $ab\ne ba$, show that the five elements of $H=\{1,a,b,ab,ba\}$ are all distinct.  Show that if $a^2=1$ then $aba\notin H$, and if $a^2\ne 1$ then $a^2\notin H$.]

\item If $a,b,c\in G$ and either $ab=ac$ or $ba=ca$, then $b=c$.

\item If $a,b,c\in G$, then the equation $axb=c$ has a unique solution for $x$ in $G$.

\item Let $G$ be a set equipped with an associative binary operation such that:

(i) There exists $1\in G$ such that $1a=a$ for all $a\in G$;

(ii) For each $a\in G$ there exists $d\in G$ such that $da=1$.

Then $G$ is a group under the operation.

\item Let $G$ be a set equipped with an associative binary operation such that (i) there exists $1\in G$ such that $1a=a$ for all $a\in G$, and (ii) for each $a\in G$ there exists $d\in G$ such that $ad=1$.  Is $G$ necessarily a group?  [Consider the case where the operation is defined by $a*b=b$ for all $a,b\in G$.]

\item Let $G$ be a nonempty set equipped with an associative binary operation such that for any $a,b\in G$, there exist $x,y\in G$ such that $ax=b$ and $ya=b$.  Then $G$ is a group.

\item (a) Let $G$ be a nonempty, finite set equipped with an associative binary operation which cancels: if $ab=ac$ or $ba=ca$, then $b=c$.  Then $G$ is a group.

(b) Show by example that part (a) may be false if $G$ is infinite.

\item If $G$ and $H$ are groups, $g\in G$ has order $n$ and $h\in H$ has order $m$, then what is the order of $(g,h)\in G\times H$?
\end{enumerate}

\subsection*{1.2. Subgroups}
\addcontentsline{toc}{section}{1.2. Subgroups}
Sometimes when given a group $G$, it is convenient to discuss a subset of only \emph{certain} elements of $G$.  We want such a set of elements to form a group itself under $G$'s operation; this can be thought of as a ``coarser'' group than $G$ itself.

For example, in the symmetric group $S(X)$, suppose some $x_0\in X$ is ``too precious'' to be moved anywhere else, and we let $G\subset S(X)$ consist of only the permutations $\sigma\in S(X)$ such that $\sigma(x_0)=x_0$.  Then $G$ is itself a group under function composition.  Indeed, if $\sigma,\tau\in G$ then $(\sigma\circ\tau)(x_0)=\sigma(\tau(x_0))=\sigma(x_0)=x_0$, hence $\sigma\circ\tau\in G$.  Function composition is associative, hence $G$'s operation is associative.  Since $1_X(x_0)=x_0$, $1_X\in G$ and $G$ has an identity element.  Finally if $\sigma\in G$ then $\sigma^{-1}\in G$ because $\sigma^{-1}(x_0)=\sigma^{-1}(\sigma(x_0))=(\sigma^{-1}\circ\sigma)(x_0)=x_0$.  Thus $G$ satisfies all four conditions that a group must satisfy.

Similarly, if $n$ is a positive integer, then $n\mathbb Z=\{na:a\in\mathbb Z\}$ is a subset of the additive group $\mathbb Z$ of integers.  Since $na+nb=n(a+b)$ for $a,b\in\mathbb Z$, the set $n\mathbb Z$ is closed under addition; similarly for the additive identity and inverses, because $0=n\cdot 0$ and $-na=n(-a)$ for $a\in\mathbb Z$.  Also, the addition is manifestly associative (and commutative).  Thus $n\mathbb Z$ is an abelian group under addition.  These two examples lead to the following definition.\\

\noindent\textbf{Definition.} \emph{A subset $H$ of a group $G$ is said to be a \textbf{subgroup} of $G$ if $H$ is itself a group under $G$'s operation.}\\

\noindent Note, by the way, that the additive group $\mathbb R$ of real numbers does \emph{not} have the multiplicative group $\mathbb R_{\ne 0}$ of nonzero real numbers as a subgroup.  Though it is true that $\mathbb R_{\ne 0}\subset\mathbb R$ as sets, one group uses addition as the operation and the other uses multiplication, so the subset does not inherit the same operation.  $H$ must inherit the same operation as $G$ in order to be considered a subgroup.\\

\noindent\textbf{Examples.}

(1) Note that any group $G$ has two canonical subgroups, the group $G$ itself, and the trivial subgroup $\{1_G\}$.  After all, $1_G1_G=1_G$ and $1_G^{-1}=1_G$.\\

(2) If $x_0\in X$, then the set $\{\sigma\in S(X):\sigma(x_0)=x_0\}$ is a subgroup of $S(X)$, as previously established.  Also, $n\mathbb Z$ is a subgroup of $\mathbb Z$.\\

(3) Let $GL_2(\mathbb R)$ be the multiplicative group of nonsingular $2\times 2$ matrices with real entries.  Then $\{A\in GL_2(\mathbb R):\det A=1\}$ is a subgroup, which is denoted $SL_2(\mathbb R)$ and called the \textbf{special linear group}.  After all, the formula $\det(AB)=(\det A)(\det B)$ holds for square matrices; if $\det A=\det B=1$, then $\det(AB)=1$, so $SL_2(\mathbb R)$ is closed under multiplication.  Since $\det I_2=1$, $SL_2(\mathbb R)$ has an identity element.  Finally if $\det A=1$, then $\det(A^{-1})=\det(A^{-1})\det A=\det(A^{-1}A)=\det I_2=1$; therefore the inverse of any matrix in $SL_2(\mathbb R)$ is in $SL_2(\mathbb R)$.

More generally, let $GL_n(\mathbb R)$ be the multiplicative group of nonsingular $n\times n$ matrices with real entries.  (The argument in Example (6) of Section 1.1 can be adapted to show that this is a group.)  Then $\{A\in GL_n(\mathbb R):\det A=1\}$ is a subgroup of $GL_n(\mathbb R)$, which is denoted $SL_n(\mathbb R)$.\\

(4) The subset $\{1,r,r^2,\dots,r^{n-1}\}$ of the dihedral group $D_n$ (Example (8) of Section 1.1) is a subgroup, as the reader can readily verify.  It consists of transformations of the regular $n$-gon for which the face-down side of the polygon remains face-down.  $\{1,d\}$ is also a subgroup of $D_n$, because $d^2=1$, and one can readily verify the group axioms.\\

\noindent It may be a surprise that many subsets are subgroups just because they satisfy a few properties.  Checking that a subset is a subgroup is usually not as cumbersome as one would expect from the start.  For example, associativity \emph{never} needs to be checked, because it will always hold, since it holds in the ambient group $G$.  The subgroup also never has an identity which is different from the identity of $G$.  In fact, we have the following proposition:\\

\noindent\textbf{Proposition 1.3.} \emph{Let $G$ be a group and $H\subset G$ be a subset.  Then $H$ is a subgroup if and only if:} %1.3

(i) \emph{$H$ is closed under the operation in $G$;}

(ii) \emph{$1_G\in H$ (and is the identity element of $H$);}

(iii) \emph{Whenever $a\in H$, $a^{-1}$ is also in $H$.}

\begin{proof}
Suppose $H$ is a subgroup.  Then $H$ is closed under the operation in $G$, by condition (i) in the definition of a group.  $H$ has some identity element $1_H$, and thus, for any $a\in H$, $1_Ha=a$.  Right multiplying both sides by $a^{-1}\in G$ gives $1_H=1_Haa^{-1}=aa^{-1}=1_G$.  Hence $1_H=1_G$ and $1_G\in H$.  Finally, if $a\in H$, then there exists $b\in H$ such that $ab=1_G$, since $H$ is a group.  Left multiplying both sides by $a^{-1}\in G$ yields $b=a^{-1}$ at once, therefore $a^{-1}\in H$.  Hence statements (i), (ii), (iii) hold.

Conversely, suppose $H$ is a subset satisfying statements (i), (ii), (iii).  Then $H$ is closed under the operation in $G$, so condition (i) in the definition of a group is satisfied.  Since $(ab)c=a(bc)$ holds for \emph{all} $a,b,c\in G$, it holds particularly if the elements happen to be in $H$.  Therefore $H$ satisfies condition (ii).  Since $1_G\in H$ and $1_Ga=a1_G=a$ for all $a\in G$ (and hence for all $a\in H$), $H$ has an identity element.  Finally, every element of $H$ has an inverse because whenever $a\in H$, $a^{-1}\in H$ and $aa^{-1}=a^{-1}a=1_G$.  Hence $H$ is a subgroup of $G$.
\end{proof}

\noindent Thus checking that a subset is a subgroup is reduced to checking three conditions based purely on the ambient group's structure.  In fact, if we already know $H$ to be nonempty, we do not need to explicitly check that $1_G\in H$, because it follows automatically from (i) and (iii).  Take any $a\in H$.  By (iii), $a^{-1}\in H$, hence by~(i), $aa^{-1}=1_G\in H$.  Thus we have proven:\\

\noindent\textbf{Proposition 1.4.} \emph{Let $G$ be a group and $H\subset G$ be a nonempty subset.  Then $H$ is a subgroup if and only if $H$ is closed under the operation of $G$ and whenever $a\in H$, $a^{-1}$ is also in $H$.}\\ %1.4

\noindent The situation is stronger yet when $H$ is finite.  In this case, all that needs to be checked is closure under the operation, as we now prove.\\

\noindent\textbf{Proposition 1.5.} \emph{Let $G$ be a group and $H\subset G$ be a nonempty, finite subset.  Then $H$ is a subgroup if and only if $H$ is closed under the operation of $G$.} %1.5

\begin{proof}
If $H$ is a subgroup, then $H$ is closed under the operation of $G$ by definition.  Conversely, suppose $H$ is closed under the operation of $G$.  Take any $a\in H$, we show that $a^{-1}\in H$; it will follow from Proposition 1.4 that $H$ is a subgroup of $G$.  Well, if $a$ has infinite order, then by Proposition 1.2(ii), the elements $a,a^2,a^3,\dots$ would all be distinct, and all of them would be in $H$ (by closure).  This contradicts the fact that $H$ is finite.  Therefore $a$ must have finite order, say $|a|=n$.  Consequently, $n-1\geqslant 0$, and $a^{n-1}=a^{-1}$ by Proposition 1.2(i).  If $n-1=0$ then $a=1_G$, and thus the hypothesis $a\in H$ immediately implies that $a^{-1}\in H$.  If $n-1>0$, then $a^{n-1}$ would be a positive power of $a$, hence be in $H$ by closure.  Therefore, $a^{-1}\in H$ as desired.
\end{proof}

\noindent\textbf{SUBGROUP GENERATED BY A SET}\\

\noindent Sometimes it helps to speak of a subgroup generated by a set.  If $S\subset G$ is any subset, we can yield the minimal subgroup containing it by multiplying its elements and their inverses together in all possible ways.  For example, if $G=\mathbb Q_{\ne 0}$ and $S=\{2\}$, then this subgroup consists of all powers of $2$ with positive and negative exponents.  The idea is made precise in the next proposition.\\

\noindent\textbf{Proposition 1.6 and Definition.} \emph{Let $S\subset G$ be a subset and let $H\subset G$ be the set of all products of finitely many elements of $S$ and their inverses.\footnote{If $S=\varnothing$, then $1$ is considered as a (vacuous) finite product.}  Then:} %1.6

(i) \emph{$H$ is a subgroup of $G$ and $H\supset S$;}

(ii) \emph{If $H'$ is any subgroup of $G$ containing $S$, then $H'\supset H$.}

(iii) \emph{$H$ is unique for properties (i) and (ii).}

\emph{$H$ is called the \textbf{subgroup of $G$ generated by $S$}.}

\begin{proof}
(i) Since the product of two finite products of elements of $S$ and their inverses is obviously another such finite product, $H$ is closed under the operation in $G$.  $1\in H$ because $1$ is a vacuous finite product (if $S\ne\varnothing$, then alternatively there exists $a\in S$, and then $1=aa^{-1}$).  Finally the fact that $H$ is closed under inverses follows from the facts that
$$(a_1a_2\dots a_k)^{-1}=a_k^{-1}\dots a_2^{-1}a_1^{-1},~~~~~~~~(a^{-1})^{-1}=a$$
(Proposition 1.1(iii), (iv)), and hence, the inverse of any finite product of elements of $S$ and their inverses is another such finite product.  Hence $H$ is a subgroup of $G$ by Proposition 1.3.

(ii) Since $H'$ contains $S$ but is closed under inverses and the operation in $G$, it follows immediately that $H'$ contains any finite product of elements of $S$ and their inverses.

(iii) Suppose $H_1$ also satisfies properties (i) and (ii).  Then $H_1$ is a subgroup of $G$ containing $S$, hence by Property (ii) for $H$, we have $H_1\supset H$.  But $H_1$ also satisfies property (ii) and $H$ is a subgroup of $G$ containing $S$, so that $H\supset H_1$.  Therefore $H_1=H$ and $H$ is unique.
\end{proof}

\noindent If the subgroup of $G$ generated by $S$ is $G$ itself, $S$ is said to \textbf{generate $G$}, or be a \textbf{generating set of $G$}.  Thus if $S$ generates $G$, then any subgroup of $G$ containing $S$ must be the whole group.  $G$ is said to be \textbf{cyclic} if it is generated by a single element (see Example (5) of Section 1.1).

Every $a\in G$ is contained in a cyclic subgroup.  Let $\left<a\right>$ be the subgroup of $G$ generated by $a$ alone.  Then $\left<a\right>$ is a cyclic subgroup of $G$, and it contains $a$.\\

\noindent\textbf{COSETS AND LAGRANGE'S THEOREM}\\

\noindent Let $H\subset G$ be a subgroup and $a\in G$.  Define $aH=\{ah:h\in H\}$.  The sets $aH$ are called \textbf{left cosets} of the subgroup $H$.  They are \emph{not} generally subgroups of $G$; however, they are equinumerous with $H$, and they partition the group $G$, as shown in the following proposition.\\

\noindent\textbf{Proposition 1.7.} \emph{Let $H\subset G$ be a subgroup.  Then:} %1.7

(i) \emph{The left cosets of $H$ are all equinumerous with $H$; i.e., $|aH|=|H|$ for all $a\in G$.}

(ii) \emph{$G$ is the union of the left cosets of $H$, and any two left cosets are either disjoint or identical.}

\begin{proof}
(i) Define $\varphi:H\to aH$ via $\varphi(h)=ah$.  Then $\varphi$ is well-defined because $h\in H$ implies that $ah\in aH$.  Moreover, every element of $aH$ is of the form $ah$ for some $h\in H$, and is therefore equal to $\varphi(h)$.  Therefore $\varphi$ is surjective.  $\varphi$ is injective due to the cancellation law (Exercise 14 of Section 1.1), thus if $\varphi(h_1)=\varphi(h_2)$, then $ah_1=ah_2$, and therefore $h_1=h_2$.  Hence $\varphi$ is a bijection and $|H|=|aH|$.

(ii) Since $a=a\cdot 1\in aH$ for each $a\in G$, the union of the left cosets is clearly $G$.  Now suppose $aH$ and $bH$ are two left cosets.  If $aH\cap bH=\varnothing$, then they are disjoint, so suppose $aH\cap bH\ne\varnothing$.  Let $c\in aH\cap bH$.  Then since $c\in aH$, $c=ah_1$ for some $h_1\in H$.  Also, since $c\in bH$ we have $c=bh_2$ for some $h_2\in H$.  Furthermore, $ah_1=bh_2$ (both are equal to $c$).  Left-multiplying both sides by $b^{-1}$ and right-multiplying by $h_1^{-1}$ yields $b^{-1}a=h_2h_1^{-1}\in H$.  Since $b^{-1}a\in H$, we conclude that for any $h\in H$, $ah=b(b^{-1}ah)$, and $b^{-1}ah\in H$, so that $ah\in bH$, showing $aH\subset bH$.  But also, since $H$ is closed under inverses, $(b^{-1}a)^{-1}=a^{-1}b\in H$.  It likewise follows that for any $h\in H$, $bh=a(a^{-1}bh)\in aH$, so that $bH\subset aH$.  Therefore $aH=bH$ and the cosets are identical.
\end{proof}

\noindent The number of left cosets of $H$ is called the \textbf{index of $H$ in $G$} and is denoted $[G:H]$.  Since $G$ is the disjoint union of the left cosets, and they are all equinumerous with $H$, it follows from set theory that $|G|=|H|~[G:H]$.  In particular, if $G$ is finite, this is a statement involving positive integers, and therefore\\ % You mention many people haven't heard of infinite cardinal arithmetic.  Thus I will remove such stuff from the exercises, but the other such text will be left as electives.

\noindent\textbf{Theorem 1.8.} (\textsc{Lagrange's Theorem}) \emph{If $G$ is a finite group and $H\subset G$ is a subgroup, then $|H|$ divides $|G|$.}\\

\noindent Similarly, if $H\subset G$ is a subgroup and $a\in G$, define $Ha=\{ha:h\in H\}$.  The $Ha$ are called \textbf{right cosets} of $H$.  Again, they are equinumerous with $H$ and these sets partition the group $G$; Proposition 1.7 and its proof can be adapted for right cosets.  If $G$ is finite, it is clear that the number of right cosets of $H$ is equal to the number of left cosets, because both are equal to $|G|/|H|$.  However, for infinite groups $G$, this is not so obvious, because division of cardinal numbers is not defined.  This caveat is easily remedied, as Exercise 7 shows that $aH\leftrightarrow Ha^{-1}$ is a well-defined bijection between the left cosets of $H$ and the right cosets.  Thus, the set of left cosets of $H$ is equinumerous with the set of right cosets; the cardinality of each of these sets is called the \textbf{index} of $H$ in $G$.

We conclude this section with a proposition which may seem ridiculously basic, but will be ubiquitous throughout this book.  It states that if $H\subset G$ is a subgroup of $G$, and you want to show that an element $a\in G$ is in $H$, you can multiply $a$ by elements you already know are in $H$ on either side, and then you need only show that the resulting element is in $H$.  In general, $G$ will seem to be a vastly complicated group, and it will be a lot more obvious that $H\subset G$ than that every element of $G$ is actually in $H$.  The proposition will make an element of $G$ easy to deal with.\\

\noindent\textbf{Proposition 1.9.} \emph{Let $G$ be a group, $H\subset G$ a subgroup.  If $a\in G$ and $x,y\in H$, then $a\in H$ if and only if $xay\in H$.}
\begin{proof}
If $a\in H$ then $xay\in H$ by closure.  Conversely, suppose $xay\in H$, say $xay=h$.  Then left-multiplying by $x^{-1}$ and right-multiplying by $y^{-1}$ yields $a=x^{-1}hy^{-1}\in H$.  Therefore, $a\in H$ as desired.
\end{proof}

\subsection*{Exercises 1.2. (Subgroups)}
\begin{enumerate}
\item Let $G$ be a group and $H\subset G$ be a subset.  Show that $H$ is a subgroup if and only if $H\ne\varnothing$ and for all $a,b\in H$, $ab^{-1}\in H$.

\item List all subgroups of the symmetric group $S_3$.  [There are six of them.]

\item In which of the following cases is $H$ a subgroup of $G$?

(a) $G=\mathbb Z,H=$ the even integers

(b) $G=\mathbb Z,H=$ the odd integers

(c) $G=\mathbb Z,H=$ the composite integers, along with $0$

(d) $G=\mathbb Q,H=\mathbb Z[1/2]=\{a/2^n:a,n\in\mathbb Z,n\geqslant 0\}$

(e) $G=\mathbb Q,H=$ fractions with denominators $\leqslant 17$

(f) $G=\mathbb R,H=$ irrational numbers along with $0$ %(g) $G=\mathbb R,H=$ algebraic numbers [You may assume that the sum of any two algebraic numbers is algebraic.  This fact is due to field theory and is irrelevant to the book, except for Section 2.4.]

(g) $G=D_n,H=\{1,d,rd,\dots,r^{n-1}d\}$

(h) $G=\mathbb Z/5\mathbb Z,H=\{\overline 0,\overline 2,\overline 4\}$

(i) $G=\mathbb Z/6\mathbb Z,H=\{\overline 0,\overline 2,\overline 4\}$

(j) $G=GL_2(\mathbb R),H=\{A\in G:\det A>0\}$

(k) $G=GL_2(\mathbb R),H=\{A\in G:\det A\in\mathbb Z\}$

(l) $G=S_5,H=\{\sigma\in S_n:\sigma(1)=1\}$

(m) $G=S_5,H=\{\sigma\in S_n:\sigma(3)=5\}$

(n) $G=S(X),H=\{\sigma\in S(X):\sigma(x)=x\text{ for all but finitely many }x\in X\}$.

\item (a) Show that the intersection of any family of subgroups of $G$ is a subgroup.

(b) Show that if $S\subset G$ is a subset, then the subgroup of $G$ generated by $S$ is the intersection of all subgroups containing $S$.

\item Give an example to show that the union of two subgroups of $G$ need not be a subgroup.

\item If $H$ and $K$ are subgroups of a finite group $G$ and $K\subset H$, then $K$ is a subgroup of $H$, and $[G:K]=[G:H][H:K]$.

\item If $H\subset G$ is a subgroup and $a,b\in G$, show that $aH=bH$ if and only if $Ha^{-1}=Hb^{-1}$.  [Show that $Ha^{-1}=\{x\in G:x^{-1}\in aH\}$.]

\item A group $G$ is said to be \textbf{finitely generated} if it is generated by some finite subset $X\subset G$.  For example, the additive group $\mathbb Z$ is finitely generated, because it is generated by $\{1\}$.  However, the multiplicative group $\mathbb Q_{\ne 0}$ is not finitely generated: if it were generated by $\{a_1/b_1,a_2/b_2,\dots,a_n/b_n\}$ with $a_k,b_k\in\mathbb Z,\gcd(a_k,b_k)=1$, then let $p_1,\dots,p_t$ be the distinct prime factors of the $a_i$ and the $b_i$.  Then $p_1p_2\dots p_t+1$ would be a product of these fractions and their inverses, which number theory shows is impossible.

(a) If $G$ and $H$ are groups, show that $G\times H$ is finitely generated if and only if both $G$ and $H$ are.

(b) Let $G$ be the subgroup of $GL_2(\mathbb R)$ generated by $\begin{bmatrix}1&1\\0&1\end{bmatrix}$ and $\begin{bmatrix}2&0\\0&1\end{bmatrix}$.  Then let $H\subset G$ consist of the matrices in $G$ with a $1$ in the upper-left hand corner.  Show that $H$ is a subgroup of $G$ but is not finitely generated.  Thus a subgroup of a finitely generated group need not be finitely generated.

\item Show that the following are equivalent for a subset $S\subset G$:

(i) $S\ne\varnothing$, and whenever $a,b,c\in S$, $ab^{-1}c\in S$.

(ii) $S$ is a left coset of some subgroup of $G$.

(iii) $S$ is a right coset of some subgroup of $G$.

\item If $H\subset G$ is a subgroup and $a\in G$, let $aHa^{-1}=\{aha^{-1}:h\in H\}$.

(a) Show that $aHa^{-1}$ is a subgroup of $G$.

(b) Two subgroups $H$ and $K$ of $G$ are said to be \textbf{conjugate} if $K=aHa^{-1}$ for some $a\in G$.  Prove that conjugacy of subgroups is an equivalence relation.

\item (a) Define $Z(G)=\{a\in G:ag=ga\text{ for all }g\in G\}$.  Show that $Z(G)$ is a subgroup of $G$, and is abelian.  $Z(G)$ is called the \textbf{center} of $G$.

(b) More generally, for each $a\in G$, define $C(a)=\{g\in G:ga=ag\}$.  Show that $C(a)$ is a subgroup of $G$, and that $Z(G)=\{a\in G:C(a)=G\}=\bigcap_{a\in G}C(a)$.  $C(a)$ is called the \textbf{centralizer} of $a$.

\item Let $a\in G$ be an element of finite order.

(a) Show that the order of $a$ is equal to the order of the cyclic subgroup $\left<a\right>$.  [Proposition 1.2(i).]

(b) If $G$ is finite, conclude that $|a|$ divides $|G|$, and therefore $a^{|G|}=1$.

(c) Use this to prove \emph{Fermat's Little Theorem}: if $p$ is prime and $p\nmid a$, then $a^{p-1}\equiv 1\pmod p$.  [Show that $(\mathbb Z/p\mathbb Z)^\times$, the set of nonzero congruence classes modulo $p$, is a group under multiplication.]

\item (a) Let $G$ be an abelian group, and $T$ the set of elements of $G$ of finite order.  Show that $T$ is a subgroup of $G$.  This subgroup $T$ is called the \textbf{torsion subgroup}.

(b) Show by example that part (a) may be false if $G$ is nonabelian.  [Consider $\begin{bmatrix}0&-1\\1&0\end{bmatrix}\begin{bmatrix}0&-1\\1&-1\end{bmatrix}$ in $GL_2(\mathbb R)$.]

\item (a) Let $a,b\in G$ such that $ab=ba$ and $\gcd(|a|,|b|)=1$.  Then $|ab|=|a||b|$.  [Show that the subgroup generated by $a$ and $b$ is equal to the cyclic subgroup generated by $ab$.  Use Lagrange's Theorem and Exercise 8(a) of Section 1.1.]

(b) If $G$ is a finite abelian group, and $a\in G$ has maximal order out of all elements of $G$, then for any $b\in G$, $|b|$ divides $|a|$. % Note: This is false in general if G is nonabelian!
[Suppose on the contrary that $|b|$ does not divide $|a|$.  Then there exists a prime $p$ whose multiplicity in $|b|$'s prime factorization is larger than that in $|a|$'s.  Write $|b|=p^rm$ and $|a|=p^sn$ with $r>s$ and $p\nmid m,n$.  Then Exercise 5 of Section 1.1 shows that $|b^m|=p^r$ and $|a^{p^s}|=n$.  By part (a), what is $|a^{p^s}b^m|$?] % Rem: this is the only tricky part of the proof that if F is a field and G is a finite subgroup of (F-{0},\cdot), then G is cyclic.  See Section 2.4's exercises
\end{enumerate}

\subsection*{1.3. Homomorphisms and Isomorphisms}
\addcontentsline{toc}{section}{1.3. Homomorphisms and Isomorphisms}
Some groups can be rather interesting lookalikes, and may seem to have matching operation tables.  For example, let $D_3$ be the dihedral group of an equilateral triangle, and let $S_3$ be the symmetric group.  Observe that there is a one-to-one correspondence between the elements of these groups.
\begin{center}
\begin{tabular}{cccccc}
$\begin{pmatrix}1&2&3\\1&2&3\end{pmatrix}$ &
$\begin{pmatrix}1&2&3\\2&3&1\end{pmatrix}$ &
$\begin{pmatrix}1&2&3\\3&1&2\end{pmatrix}$ &
$\begin{pmatrix}1&2&3\\1&3&2\end{pmatrix}$ &
$\begin{pmatrix}1&2&3\\2&1&3\end{pmatrix}$ &
$\begin{pmatrix}1&2&3\\3&2&1\end{pmatrix}$ \\
$\updownarrow$ & $\updownarrow$ & $\updownarrow$ & $\updownarrow$ & $\updownarrow$ & $\updownarrow$ \\
\includegraphics[scale=.3]{Tri1.png} &
\includegraphics[scale=.3]{TriR.png} &
\includegraphics[scale=.3]{TriR2.png} &
\includegraphics[scale=.3]{TriD.png} &
\includegraphics[scale=.3]{TriRD.png} &
\includegraphics[scale=.3]{TriR2D.png}
\end{tabular}
\end{center}
Notice that this is not only a one-to-one correspondence between the groups, but also that applying the group compositions to corresponding elements gives you corresponding elements.  For example, $r$ corresponds to $\begin{pmatrix}1&2&3\\2&3&1\end{pmatrix}$, $d$ corresponds to $\begin{pmatrix}1&2&3\\1&3&2\end{pmatrix}$, and the composition $rd$ also corresponds to the composition of the permutations, $\begin{pmatrix}1&2&3\\2&3&1\end{pmatrix}\circ\begin{pmatrix}1&2&3\\1&3&2\end{pmatrix}=\begin{pmatrix}1&2&3\\2&1&3\end{pmatrix}$.

These kinds of correspondences between groups are special, because they imply that the groups have the same operation tables (up to relabeling of the elements) and thus ``seem mechanically identical.''  We now carry over the concept to any two groups $G$ and $H$.  In order for a bijection $f:G\to H$ to have the property described above, we need to make sure that if $a\in G$ maps to $a'\in H$ and $b\in G$ maps to $b'\in H$, then $ab$ maps to $a'b'$.  In other words, the map $f$ must send $ab$ to $f(a)f(b)$, for any $a,b\in G$; i.e., $f(ab)=f(a)f(b)$.

Thus a bijective map $f:G\to H$ satisfying $f(ab)=f(a)f(b)$ for all $a,b\in G$ is exactly the kind of map which claims that two group structures match.  This leads to the following definition.  We also need a name for functions $f:G\to H$ that satisfy $f(ab)=f(a)f(b)$ but are not necessarily bijective.\\

\noindent\textbf{Definition.} \emph{If $G$ and $H$ are groups, a function $f:G\to H$ is said to be a \textbf{homomorphism} if $f(a*b)=f(a)*f(b)$ for all $a,b\in G$.  A homomorphism which is bijective is called an \textbf{isomorphism}.  If an isomorphism $G\to H$ exists, $G$ and $H$ are said to be \textbf{isomorphic}, denoted $G\cong H$.}\\

\noindent\textbf{Note.} We have temporarily returned to the ``$*$'' notation to emphasize that $G$ and $H$ may have different group operations.  For example, if $G$ is multiplicative and $H$ is additive, the definition reads $f(ab)=f(a)+f(b)$.\\

\noindent\textbf{Examples.}

(1) The correspondence mentioned in the beginning of this chapter is an isomorphism $S_3\to D_3$.  However, when $n>3$, there is \emph{no} isomorphism from $S_n$ to $D_n$, because $|D_n|=2n<n!=|S_n|$; there isn't even a bijective map from $D_n$ to $S_n$.  However, there is an injective homomorphism from $D_n$ to $S_n$; see Exercise 4.\\

(2) Linear algebra shows that $\det(AB)=(\det A)(\det B)$ for $n\times n$ matrices $A$ and $B$.  We thus have a homomorphism $\det:GL_n(\mathbb R)\to\mathbb R_{\ne 0}$, where $\mathbb R_{\ne 0}$ is the multiplicative group of nonzero real numbers.  Note that $\det$ is surjective because for any $a\in\mathbb R_{\ne 0}$, $\det\begin{bmatrix}a&0&\dots&0\\0&1&\dots&0\\\vdots&\vdots&\ddots&\vdots\\0&0&\dots&1\end{bmatrix}=a$.\\

(3) If $G$ is any group and $a\in G$, the map $f:\mathbb Z\to G$ given by $f(n)=a^n$ is a homomorphism.  This follows from the exponential law $a^{n+m}=a^na^m$ (Exercise 3(a) of Section 1.1).  However, $f$ is not generally injective or surjective.\\

(4) If $H$ is a subgroup of $G$, the inclusion map $\iota:H\to G$ given by $\iota(a)=a$ is an injective homomorphism.  The reasons are obvious.  $\iota$ is called the \textbf{inclusion map} from $H$ into $G$.\\

(5) Let $G$ and $H$ be groups.  Then $p_1:G\times H\to G$ given by $p_1(a,b)=a$ is a homomorphism.  After all, $p_1((a,b)(a',b'))=p_1(aa',bb')=aa'=p_1(a,b)p_1(a',b')$.  $p_1$ is also surjective, because for each $a\in G$, $a=p_1(a,1)$.  Similarly $p_2:G\times H\to H$ given by $p_2(a,b)=b$ is a surjective homomorphism.  These are called \textbf{projections}.\\

(6) For any groups $G$ and $H$, there is the \emph{trivial homomorphism} $f:G\to H$ given by $f(a)=1_H$ for all $a\in G$.\\

\noindent There are several basic properties about homomorphisms, just like there were about groups.  For example, one would expect a homomorphism to make the identity element correspond to the identity element.  It automatically does that, as we now show.\\

\noindent\textbf{Proposition 1.10.} \emph{Let $f:G\to H$ be a homomorphism of groups.  Then}

(i) \emph{$f(1_G)=1_H$.}

(ii) \emph{$f(a^{-1})=f(a)^{-1}$ for $a\in G$.}

(iii) \emph{If $a$ has finite order, then so does $f(a)$, and $|f(a)|$ divides $|a|$.}\\

\noindent The converse of (iii) is false: if $f(a)$ has finite order, $a$ does not necessarily have finite order.  For example, let $f:\mathbb Z\to\mathbb Z/5\mathbb Z$ be given by $f(a)=\overline a$.  Then $1\in\mathbb Z$ does not have finite order, but $f(1)=\overline 1$ has order $5$ in $\mathbb Z/5\mathbb Z$.
\begin{proof}
(i) Since $f$ is a homomorphism, $f(1_G)f(1_G)=f(1_G1_G)=f(1_G)=1_Hf(1_G)$.  Right multiplying both sides by $f(1_G)^{-1}$, we get $f(1_G)=1_H$.

(ii) $f(a)^{-1}$ is the \emph{unique} inverse of $f(a)$ in $H$; however, $f(a^{-1})$ is also an inverse of $f(a)$ because $f(a)f(a^{-1})=f(aa^{-1})=f(1_G)=1_H$ (by part (i)) and similarly $f(a^{-1})f(a)=1_H$.  Therefore $f(a^{-1})=f(a)^{-1}$.

(iii) Suppose $a$ has order $n>0$.  Then $a^n=1_G$.  Furthermore, by Exercise 9(a) below, $f(a)^n=f(a^n)=f(1_G)=1_H$.  Proposition 1.2 then implies that $f(a)$ has finite order and that $|f(a)|$ divides $n$.
\end{proof}

\noindent The concept of a group isomorphism may seem to be asymmetrical, since it is a map from $G$ to $H$ and not the other way around.  However, it is symmetrical.  If $f:G\to H$ is an isomorphism, then the bijective map $f$ has some inverse $f^{-1}$.  Moreover, $f^{-1}$ is also a bijection.  For any $a,b\in H$, $f^{-1}(ab)=f^{-1}(f(f^{-1}(a))f(f^{-1}(b)))=f^{-1}(f(f^{-1}(a)f^{-1}(b)))=f^{-1}(a)f^{-1}(b)$; thus $f^{-1}$ is an isomorphism $H\to G$.  Hence $G\cong H$ if and only if $H\cong G$.  In fact, isomorphism is an equivalence relation on the class of all groups (Exercise 1(b)).

Incidentally, we now have some criteria which can tell us whether or not two groups are isomorphic.  To show that groups $G$ and $H$ are isomorphic, simply find an isomorphism $G\to H$.  However, how can we show that they are not isomorphic?  It certainly doesn't suffice to find a map $G\to H$ which is not an isomorphism; for all we know, there could be a different function from the one we're talking about which \emph{is} an isomorphism.  However, Proposition 1.10 gives some facts that isomorphisms must satisfy.

For instance, if $f:G\to H$ is an isomorphism, then whenever $a\in G$ has finite order, $f(a)$ has finite order and $|a|=|f(a)|$.  After all, $|f(a)|$ divides $|a|$ by Proposition 1.10(iii), but also $|a|=|f^{-1}(f(a))|$ divides $|f(a)|$ because $f^{-1}$ is an isomorphism.  Thus an isomorphism $f$ must make elements of a given order correspond to elements of the same order.  If there is a positive integer $n$ such that $G$ and $H$ have different numbers of elements of order $n$, then we know that $G$ and $H$ cannot be isomorphic.

If $G\cong H$ and $G$ is abelian, then so is $H$.  For $x,y\in H$, we have $x=f(a)$ and $y=f(b)$ for some $a,b\in G$ (because $f$ is surjective); moreover $xy=f(a)f(b)=f(ab)=f(ba)=f(b)f(a)=yx$.  Similarly, if $H$ is abelian, then so is $G$.  An abelian group can never be isomorphic to a nonabelian group.

Any property of groups or group elements which is preserved by an isomorphism is called an \textbf{algebraic property}.  Thus the order of the group, the order of an element and a group being abelian are algebraic properties; however, whether a group contains the number $17$, or whether it contains a function as an element, is \emph{not} an algebraic property.  Exercise 3(c) shows that structure of the subgroups of $G$ is an algebraic property of $G$.  If two groups differ for a certain algebraic property, they can't be isomorphic.\\

\noindent\textbf{IMAGE OF A GROUP HOMOMORPHISM}\\

\noindent If $f:G\to H$ is a homomorphism, then $f(G)=\{f(x):x\in G\}$ is a subgroup of $H$.  After all, if $a,b\in f(G)$, say $a=f(x)$ and $b=f(y)$, then $ab=f(x)f(y)=f(xy)\in f(G)$, hence closure.  Since $f(1_G)=1_H$ by Proposition 1.10(i), $1_H\in f(G)$.  Finally, if $a\in f(G)$, say $a=f(x)$, then $a^{-1}=f(x)^{-1}=f(x^{-1})\in f(G)$.  Consequently, Proposition 1.3 implies that $f(G)$ is a subgroup of $H$.  $f(G)$ is called the \textbf{image} of $f$.

If $f$ is a \emph{surjective} homomorphism, then $f(G)=H$.  However, if $f$ is not surjective, then $f(G)$ is some proper subgroup $H'$ of $H$.  In this case, $f$ can be thought of as a function from $G$ to $H'$ (because for each $a\in G$, $f(a)\in H'$), and in this case it is a surjective homomorphism $G\to H'$.  Thus from any homomorphism, one induces a surjective homomorphism.\\

\noindent\textbf{CLASSIFICATION OF CYCLIC GROUPS}\\

\noindent We recall that a group is said to be \emph{cyclic} if it is generated by a single element.  It turns out that, up to isomorphism, a cyclic group is completely determined by its order.\\

\noindent\textbf{Proposition 1.11.} \emph{Let $G$ be a cyclic group, say $G=\left<a\right>$.}

(i) \emph{If $a$ has finite order, say $|a|=n$, then $G\cong\mathbb Z/n\mathbb Z$.}

(ii) \emph{If $a$ has infinite order, then $G\cong\mathbb Z$.}\\

\noindent We immediately conclude that (a) cyclic groups are abelian; and (b) if $a$ is any element of a group, the cyclic subgroup $\left<a\right>$ has the same order as $a$ (in particular, if $a$ has infinite order then $\left<a\right>$ is infinite). % "You already said this?" I don't remember previously saying that \left<a\right> and a have the same order
\begin{proof}
(i) Suppose $|a|=n$ and $G=\left<a\right>$.  Define $f:\mathbb Z/n\mathbb Z\to G$ by $f(\overline k)=a^k$.  First we must show that $f$ is well-defined: that is, if $j$ and $k$ are integers such that $\overline j=\overline k$ in $\mathbb Z/n\mathbb Z$, then $f(\overline j)=f(\overline k)$.  Well, if $\overline j=\overline k$ then $j\equiv k\pmod n$; hence $a^j=a^k$ by Proposition 1.2(i), so that $f(\overline j)=f(\overline k)$.  Thus $f$ is well-defined.  $f$ is also injective because if $f(\overline j)=f(\overline k)$, then $a^j=a^k$, hence $j\equiv k\pmod n$ by Proposition 1.2(i), so that $\overline j=\overline k$ in $\mathbb Z/n\mathbb Z$.  To show that $f$ is surjective, let $G'$ be the subgroup $f(\mathbb Z/n\mathbb Z)$ of $G$.  Then $a\in G'$ because $a=f(\overline 1)$.  Since $G$ is, however, \emph{generated} by $a$, we conclude that $G'=G$ and $f$ is surjective.  Finally $f$ is a homomorphism because
$$f(\overline j+\overline k)=f(\overline{j+k})=a^{j+k}=a^ja^k=f(\overline j)f(\overline k)$$
Therefore $f$ is an isomorphism $\mathbb Z/n\mathbb Z\to G$.

(ii) Suppose $a$ has infinite order and $G=\left<a\right>$.  Define $f:\mathbb Z\to G$ by $f(k)=a^k$.  Then $f$ is injective by Proposition 1.2(ii).  Again $f$ is surjective, because if $G'=f(\mathbb Z)$ then $a\in G'$, and hence $G'=G$ because $G$ is generated by $a$.  Finally, it is straightforward that $f$ is a homomorphism (see Example (3) beneath the definition of a homomorphism).  Thus $f$ is an isomorphism as desired.
\end{proof}

\subsection*{Exercises 1.3. (Homomorphisms and Isomorphisms)}
\begin{enumerate}
\item (a) Let $G_1,G_2,G_3$ be groups.  If $f:G_1\to G_2$ and $g:G_2\to G_3$ are group homomorphisms, show that $g\circ f$ is also a homomorphism.

(b) Show that isomorphism is an equivalence relation on the class of all groups.

\item\emph{(Cayley's Theorem.)} \---- Every group $G$ is isomorphic to a subgroup of $S(X)$ for some set $X$.  [Let $X$ be the set $G$ itself.  For each $a\in G$, let $T_a$ be the map $x\mapsto ax$ from $G$ to $G$.  Show that $T_a$ is in $S(G)$ (the set of bijective \emph{functions} from $G$ to $G$), and the map $\varphi:G\to S(G)$ given by $\varphi(a)=T_a$ is an injective homomorphism.]

\item Let $f:G\to H$ be a group homomorphism.

(a) If $G'$ is a subgroup of $G$, show that $f(G')=\{f(a):a\in G'\}$ is a subgroup of $H$.

(b) If $H'$ is a subgroup of $H$, show that the \textbf{inverse image} $f^{-1}(H')=\{a\in G:f(a)\in H'\}$ is a subgroup of $G$.

(c) If $f$ is an isomorphism, use parts (a) and (b) to obtain a bijection between the subgroups of $G$ and the subgroups of $H$.

\item For $n\geqslant 3$, show that there is a unique homomorphism $f:D_n\to S_n$ such that $f(r)=\begin{pmatrix}1&2&\dots&n-1&n\\2&3&\dots&n&1\end{pmatrix}$ and $f(d)=\begin{pmatrix}1&2&\dots&n-1&n\\n&n-1&\dots&2&1\end{pmatrix}$.  Then show that $f$ is injective, but not surjective for $n>3$.

\item If $a,b\in G$ and $ab=ba$, show that there is a unique homomorphism $f:\mathbb Z\times\mathbb Z\to G$ such that $f(1,0)=a$ and $f(0,1)=b$.

\item If $G$ is an abelian group and $n$ is a positive integer, show that the map $f:G\to G$ given by $f(a)=a^n$ is a homomorphism.  Show that this may be false if $G$ is nonabelian.

Furthermore, if $n=2$, show that $G$ is abelian if and only if $f$ is a homomorphism.  [If $n\geqslant 3$, it is possible for $f$ to be a homomorphism with $G$ nonabelian, but this is harder to prove.] % If p is an odd prime, take G = Heisenberg group of order p^3, and n = p.  Then G is nonabelian but f is constant map 1.

\item (a) If $X$ and $Y$ are sets such that $|X|=|Y|$, show that $S(X)\cong S(Y)$.

(b) Conclude that if $X$ is a finite set with $n$ elements, $S(X)\cong S_n$.

(c) If $X$ is a set, $x_0\in X$ and $G$ is the subgroup $\{\sigma\in S(X):\sigma(x_0)=x_0\}$ of $S(X)$, show that $G\cong S(X-\{x_0\})$.

(d) Now find a subgroup of $S_n$ isomorphic to $S_{n-1}$.

\item Let $\mathbb R$ be the additive group of real numbers and $\mathbb R_{>0}$ the multiplicative group of positive real numbers.  Define $f:\mathbb R\to\mathbb R_{>0}$ via $f(x)=e^x$.  Show that $f$ is an isomorphism.

\item (a) If $f:G\to H$ is a homomorphism, $a\in G$ and $n$ is a positive integer, show that $f(a^n)=f(a)^n$.  [Induction on $n$.]

(b) Now do part (a) where $n$ is any integer.

\item (a) Is $\mathbb Z/6\mathbb Z$ isomorphic to $\mathbb Z/2\mathbb Z\times\mathbb Z/3\mathbb Z$?

(b) Is $\mathbb Z/8\mathbb Z$ isomorphic to $\mathbb Z/2\mathbb Z\times\mathbb Z/4\mathbb Z$?

(c) Is $\mathbb Z/6\mathbb Z$ isomorphic to $S_3$?

\item Let $\mathbb C_{\ne 0}$ be the multiplicative group of nonzero complex numbers.  Show that $f:\mathbb C_{\ne 0}\to\mathbb R_{>0}$ given by $f(a+bi)=a^2+b^2$ is a surjective homomorphism of groups.

\item Show that the additive group $\mathbb R$ is not isomorphic to the multiplicative group $\mathbb R_{\ne 0}$.  [If $f:\mathbb R\to\mathbb R_{\ne 0}$ is an isomorphism, then $f(a)=-1$ for some $a\in\mathbb R$: now what is $f(a/2)$?]

\item Let $f:G\to H$ be a group homomorphism.

(a) If $f$ is surjective and $G$ is abelian, then $H$ is abelian.

(b) If $f$ is injective and $H$ is abelian, then $G$ is abelian.

(c) Show by example that part (a) (resp., (b)) may be false if $f$ is not surjective (resp., injective).

\item If $G$ is a cyclic group, show that every subgroup of $G$ is cyclic.  [Use Proposition 1.11.]

\item If $f:G\to H$ is a surjective homomorphism and $G$ is cyclic, then $H$ is cyclic.  [If $a\in G$ generates $G$, show that $f(a)$ generates $H$.]

\item Let $G$ be a group such that $|G|>1$ and the only subgroups of $G$ are $\{1\}$ and $G$ itself.  Prove that $G$ is cyclic of prime order.
\end{enumerate}

\subsection*{1.4. Normal Subgroups}
\addcontentsline{toc}{section}{1.4. Normal Subgroups}
In Section 1.3, we have shown that any homomorphism can be made into a surjective homomorphism by changing the target (i.e., if $f:G\to H$ is a homomorphism and $H'$ is the image of $f$ then $f$ can be thought of as a surjective homomorphism $G\to H'$).  One then asks the natural question of whether a homomorphism induces an \emph{injective} homomorphism.  The answer is yes, as we now see.

The intuitive idea behind this is modular arithmetic.  If $n$ is a positive integer and $a,b\in\mathbb Z$, then $a\equiv b\pmod n$ states that $n\mid(a-b)$, or what is the same thing, $a-b\in n\mathbb Z$.  The equivalence classes modulo $n$ can then be added (and multiplied, but that's not relevant here).  Here we are calling two elements of $\mathbb Z$ ``congruent'' if they differ by an element of the subgroup $n\mathbb Z$.  We now pass this idea over to arbitrary subgroups.

Let $H\subset G$ be a subgroup.  If $a,b\in G$, we say that $a$ and $b$ are \textbf{left congruent modulo $H$}, denoted $a\equiv b\pmod H$, if $a^{-1}b\in H$.  This is an equivalence relation: $a^{-1}a=1\in H$, hence $a\equiv a\pmod H$, proving reflexivity.  If $a\equiv b\pmod H$ then $a^{-1}b\in H$, so that $b^{-1}a=(a^{-1}b)^{-1}\in H$ and $b\equiv a\pmod H$, hence symmetry.  Finally if $a\equiv b\pmod H$ and $b\equiv c\pmod H$, then $a^{-1}c=(a^{-1}b)(b^{-1}c)\in H$, so that $a\equiv c\pmod H$ and we have transitivity.

Moreover, if $a,b,c\in G$ and $b\equiv c\pmod H$, then $ab\equiv ac\pmod H$, because $(ab)^{-1}ac=b^{-1}a^{-1}ac=b^{-1}c\in H$.  However, it is \emph{not} true in general that if $a\equiv b\pmod H$ and $c\equiv d\pmod H$, then $ac\equiv bd\pmod H$.  For example, suppose $G=D_4$ and $H$ is the subgroup $\{r_0,v\}$.  Then according to the following operation table:
\begin{center}
\begin{tabular}{c|cccccccc}
$\circ$ & $r_0$ & $r_1$ & $r_2$ & $r_3$ & $v$ & $t$ & $h$ & $d$\\\hline
$r_0$ & $r_0$ & $r_1$ & $r_2$ & $r_3$ & $v$ & $t$ & $h$ & $d$\\
$r_1$ & $r_1$ & $r_2$ & $r_3$ & $r_0$ & $d$ & $v$ & $t$ & $h$\\
$r_2$ & $r_2$ & $r_3$ & $r_0$ & $r_1$ & $h$ & $d$ & $v$ & $t$\\
$r_3$ & $r_3$ & $r_0$ & $r_1$ & $r_2$ & $t$ & $h$ & $d$ & $v$\\
$v$ & $v$ & $t$ & $h$ & $d$ & $r_0$ & $r_1$ & $r_2$ & $r_3$\\
$t$ & $t$ & $h$ & $d$ & $v$ & $r_3$ & $r_0$ & $r_1$ & $r_2$\\
$h$ & $h$ & $d$ & $v$ & $t$ & $r_2$ & $r_3$ & $r_0$ & $r_1$\\
$d$ & $d$ & $v$ & $t$ & $h$ & $r_1$ & $r_2$ & $r_3$ & $r_0$
\end{tabular}
\end{center}

* $r_2\equiv h\pmod H$, because $r_2^{-1}\circ h=r_2\circ h=v\in H$;

* $r_1\equiv d\pmod H$, because $r_1^{-1}\circ d=r_3\circ d=v\in H$;

* But $r_2\circ r_1\not\equiv h\circ d\pmod H$, because $(r_2\circ r_1)^{-1}\circ(h\circ d)=r_3^{-1}\circ r_1=r_1\circ r_1=r_2\notin H$.

Left congruence modulo $H$ is sometimes referred to as a \textbf{left congruence relation}, because it is an equivalence relation satisfying $b\equiv c\implies ab\equiv ac$.

An interesting thing about left congruence relations is that the equivalence classes of the relation are precisely the left cosets of $H$.  After all, for any $a\in G$,
\begin{align*}
\{x\in G:a\equiv x\pmod H\}&=\{x\in G:a^{-1}x\in H\}\\
&=\{x\in G:x=ah\text{ for some }h\in H\}\\
&=\{x\in G:x\in aH\}=aH.
\end{align*}
Therefore, for any $a,b\in G$ we have $aH=bH$ if and only if $a\equiv b\pmod H$.

By the same argument, we define $a$ and $b$ to be \textbf{right congruent modulo $H$}, denoted $a\equiv'b\pmod H$, if $ab^{-1}\in H$.  This is an equivalence relation, and whenever $a,b,c\in G$ and $b\equiv'c\pmod H$, $ba\equiv'ca\pmod H$.  Such a relation is called a \textbf{right congruence relation}.  Again, the equivalence classes of this relation are precisely the right cosets of $H$.

There are special kinds of subgroups $N$, which will prove very useful, for which left congruence and right congruence coincide.  In this case, it \emph{is} true that
\begin{center}
If $a\equiv b\pmod N$ and $c\equiv d\pmod N$, then $ac\equiv bd\pmod N$,
\end{center}
as Proposition 1.12 will prove.

Now, saying that left congruence and right congruence coincide is tantamount to saying that the left cosets of the subgroup coincide with the right cosets (because these are the equivalence classes of the relations).  We hereby give this kind of subgroup a name.\\

\noindent\textbf{Definition.} \emph{A subgroup $N$ of a group $G$ is said to be \textbf{normal} if $aN=Na$ for every $a\in G$.  This is commonly denoted $N\unlhd G$.}\\

\noindent\textbf{Examples.}

(1) For any group $G$, it is clear that $G$ and $\{1\}$ are normal subgroups.\\

(2) Let $G$ be an abelian group and $H$ a subgroup of $G$.  Then for each $a\in G$, $aH=Ha$ because $ah=ha$ for all $h\in H$.  Thus every subgroup of an abelian group is normal.  [The converse is false; see Exercise 6.]\\

(3) For any group $G$, let $C=Z(G)$ be the center of $G$ (see Exercise 11(a) of Section 1.2).  Now take any $a\in G$.  Then for every $g\in C$, $ag=ga$, by definition of the center.  Furthermore $aC=\{ag:g\in C\}=\{ga:g\in C\}=Ca$.  Hence $C$ is normal.  Similarly, any subgroup contained in $Z(G)$ is normal.\\

(4) Examples (2) and (3) are rather misleading, because in general, the condition $aN=Na$ does \emph{not} mean $an=na$ for all $n\in N$.  It merely means that for each $n\in N$, there exists a (possibly different) $n_1\in N$ such that $an=n_1a$.  For example, let $N$ be the cyclic subgroup $\left<r_1\right>=\{r_0,r_1,r_2,r_3\}$ of $D_4$.  Then $N$ is a normal subgroup by Exercise 4 below.  In particular, $vN=Nv$.  It is not true that $vr_1=r_1v$; however, we do have $vr_1=r_3v$ and $r_3\in N$, thus $vr_1\in Nv$.\\

(5) If $G$ and $H$ are groups, then $G\times 1=\{(g,1):g\in G\}$ and $1\times H=\{(1,h):h\in H\}$ are normal subgroups of $G\times H$.  After all, for $(a,b)\in G\times H$, $(a,b)(G\times 1)=\{(g,b):g\in G\}=(G\times 1)(a,b)$, and similarly for $1\times H$.  Note that $G\times 1\cong G$ and $1\times H\cong H$.  (Why?)\\

\noindent Normality is an important concept; however, the definition above may seem to be rather cumbersome.  Without a doubt we would like a simpler definition.  The next proposition gives a few alternate definitions.\\

\noindent\textbf{Proposition 1.12.} \emph{Let $N$ be a subgroup of $G$.  Then the following statements are equivalent:}

(i) \emph{$N$ is normal, i.e., $aN=Na$ for all $a\in G$.}

(ii) \emph{Left congruence modulo $N$ coincides with right congruence modulo $N$.}

(iii) \emph{Whenever $a\equiv b\pmod N$ and $c\equiv d\pmod N$, $ac\equiv bd\pmod N$.} % This is left congruence.  Right congruence would be \equiv'

(iv) \emph{$aNa^{-1}=N$ for all $a\in G$.}

(v) \emph{Whenever $n\in N$ and $a\in G$, $ana^{-1}\in N$.}
\begin{proof}
(i) $\iff$ (ii) because the left (resp., right) cosets are the equivalence classes of left (resp., right) congruence modulo $N$.

(ii) $\implies$ (iii). Since $a\equiv b\pmod N$ and $\equiv$ is a right congruence relation, $ac\equiv bc\pmod N$.  But also since $c\equiv d\pmod N$ and $\equiv$ is a left congruence relation, $bc\equiv bd\pmod N$.  Transitivity then implies that $ac\equiv bd\pmod N$.

(iii) $\implies$ (ii). Suppose $a,b,c\in N$ and $b\equiv c\pmod N$.  Then since $a\equiv a\pmod N$ (by reflexivity), the hypothesis implies that $ba\equiv ca\pmod N$.  Hence, left congruence modulo $N$ is a right congruence relation.  Moreover, it is clear that $N=\{a\in G:a\equiv 1\pmod N\}$; Exercise 5 then shows that left congruence modulo $N$ coincides with right congruence modulo $N$.

(i) $\iff$ (iv). Right-multiply both sides by either $a$ or $a^{-1}$.  Here we use the convention that for arbitrary \emph{subsets} $S\subset G$, $aS=\{as:s\in S\}$ and $Sa=\{sa:s\in S\}$, and we note that for $g,h\in G$, we have $(gh)S=g(hS)$, $S(gh)=(Sg)h$ and $1S=S=S1$.

(iv) $\implies$ (v). If $n\in N$ and $a\in G$, then $ana^{-1}\in aNa^{-1}=N$.

(v) $\implies$ (iv). Each element of $aNa^{-1}$ is of the form $ana^{-1},n\in N$.  By hypothesis $ana^{-1}\in N$; therefore $aNa^{-1}\subset N$.  Conversely, given $n\in N$, let $n_1=a^{-1}na$.  Then $n_1\in N$, because $n_1=a^{-1}n(a^{-1})^{-1}=bnb^{-1}$ where $b=a^{-1}$.  Moreover, $n=an_1a^{-1}\in aNa^{-1}$, so that $N\subset aNa^{-1}$.  Therefore $aNa^{-1}=N$ as desired.
\end{proof}

\noindent Thus any of the five conditions above can be used to prove that a subgroup is normal.  At this point we will mostly stick to condition (v), because it is a basic condition involving sets.  On rare occasions (e.g., Exercise 4), one of the other conditions is easier to handle.\\

\noindent\textbf{PRODUCTS OF SUBGROUPS}\\

\noindent If $H$ and $K$ are subgroups of a group $G$, define $HK=\{hk:h\in H,k\in K\}$.  Then $HK$ is \emph{not} necessarily a subgroup.  For example, if $G=D_4$, $H=\{r_0,v\}$ and $K=\{r_0,t\}$, then $H$ and $K$ are subgroups, but $HK=\{r_0,r_1,v,t\}$ is not a subgroup, because $r_1\in HK$ but $r_1\circ r_1=r_2\notin HK$.

It really shouldn't come across as a surprise that you can't just multiply subgroups together whenever you want.  After all, since the group multiplication doesn't commute, sophisticated products of elements of $H$ and $K$ are not necessarily of the form $hk$ with $h\in H$ and $k\in K$; they could look something like $h_1k_1h_2k_2\dots h_nk_n$, and then the factors cannot be rearranged in general into $(h_1\dots h_n)(k_1\dots h_n)$.

However, if one of the subgroups is normal, we have better luck.\\

\noindent\textbf{Proposition 1.13.} \emph{Let $H$ and $K$ be subgroups of a group $G$.}

(i) \emph{If $H$ or $K$ is normal, then $HK$ is a subgroup of $G$, and $HK=KH$.}

(ii) \emph{If $H$ and $K$ are both normal, then $HK$ is a normal subgroup.}
\begin{proof}
(i) Without loss of generality, we may assume $H$ is normal.  Clearly $1=1\cdot 1\in HK$.  Suppose $a,b\in HK$, say $a=h_1k_1$ and $b=h_2k_2$ with $h_1,h_2\in H$ and $k_1,k_2\in K$.  Then since $H$ is a normal subgroup, $k_1h_2\in k_1H=Hk_1$, so there exists $h'\in H$ such that $k_1h_2=h'k_1$.  Furthermore, $ab=h_1k_1h_2k_2=h_1(k_1h_2)k_2=h_1(h'k_1)k_2=(h_1h')(k_1k_2)\in HK$.  This proves closure.  Now suppose $a\in HK$, say $a=hk$ with $h\in H$ and $k\in K$.  Then $a\in Hk=kH$, so $a=kh^*$ for some $h^*\in H$.  Furthermore $a^{-1}=(h^*)^{-1}k^{-1}\in HK$.  Therefore $HK$ is a subgroup by Proposition 1.3.

The statement $HK=KH$ follows from the normality of $H$.  After all, if $a\in HK$, say $a=hk$, then $a\in Hk=kH\subset KH$, and therefore $HK\subset KH$.  The reverse inclusion is similar.

(ii) $HK$ is a subgroup by part (i).  Now suppose $a\in HK$ and $x\in G$.  Then write $a=hk$ with $h\in H,k\in K$.  By normality, $xhx^{-1}\in H$ and $xkx^{-1}\in K$.  Therefore $xax^{-1}=xhkx^{-1}=(xhx^{-1})(xkx^{-1})\in HK$.  Hence $HK$ is normal by Proposition 1.12.
\end{proof}

\subsection*{Exercises 1.4. (Normal Subgroups)}
\begin{enumerate}
\item If $N\subset G$ is a subgroup, show that $N$ is normal if and only if for all $a,b\in G$, $ab\in N$ implies $ba\in N$.

\item (a) Suppose that $N$ and $K$ are subgroups of $G$ with $N\subset K$.  If $N$ is a normal subgroup of $G$, then $N$ is a normal subgroup of $K$.

(b) If $N\subset G$ is a normal subgroup and $K\subset G$ is a subgroup, then $N\cap K$ is a normal subgroup of $K$.

\item Show that the intersection of a family of normal subgroups is normal.

\item Prove that any subgroup of $G$ of index $2$ is normal.  [If $[G:N]=2$, then the left cosets of $N$ are necessarily $N$ and $G-N$ (why?).  Same for the right cosets.]

\item Let $\equiv$ be a left (resp., right) congruence relation on a group $G$.  Show that $H=\{a\in G:a\equiv 1\}$ is a subgroup of $G$, and that $\equiv$ coincides with left (resp., right) congruence modulo $H$.

\item Here is an example of a nonabelian group in which every subgroup is normal.
Let $\mathbf 1,\mathbf i,\mathbf j,\mathbf k$ be the following $2\times 2$ complex-valued matrices:\\
$$\mathbf 1=\begin{bmatrix}1&0\\0&1\end{bmatrix}~~~~~~~~\mathbf i=\begin{bmatrix}i&0\\0&-i\end{bmatrix}~~~~~~~~\mathbf j=\begin{bmatrix}0&1\\-1&0\end{bmatrix}~~~~~~~~\mathbf k=\begin{bmatrix}0&i\\i&0\end{bmatrix}$$
(a) Show that (i) $\mathbf i^2=\mathbf j^2=\mathbf k^2=-\mathbf 1$, (ii) $\mathbf i\mathbf j=-\mathbf j\mathbf i=\mathbf k$, (iii) $\mathbf j\mathbf k=-\mathbf k\mathbf j=\mathbf i$, and (iv) $\mathbf k\mathbf i=-\mathbf i\mathbf k=\mathbf j$.

(b) Let $Q=\{\mathbf 1,-\mathbf 1,\mathbf i,-\mathbf i,\mathbf j,-\mathbf j,\mathbf k,-\mathbf k\}$.  Show that $Q$ is a nonabelian group under matrix multiplication.  [$Q$ is called the \textbf{quaternion group}.]

(c) Find the order of each element of $Q$.

(d) Now list all subgroups of $Q$, and show that they are all normal.

\item Suppose $K$ is a normal subgroup of $N$ and $N$ is a normal subgroup of $G$.  Is $K$ necessarily a normal subgroup of $G$?  [Consider the case $G=D_4,N=\{r_0,r_2,v,h\},K=\{r_0,v\}$.]

\item Let $f:G\to H$ be a homomorphism.  Prove or disprove:

(a) If $N$ is a normal subgroup of $G$ then $f(N)$ is a normal subgroup of $H$.

(b) If $N$ is a normal subgroup of $H$ then $f^{-1}(N)$ is a normal subgroup of $G$.

\item Let $G$ be a group, and let $[G,G]$ be the subgroup of $G$ generated by the set $\{aba^{-1}b^{-1}:a,b\in G\}$.  Show that $[G,G]$ is normal; more generally, show that any subgroup containing $[G,G]$ is normal.  [If $a\in[G,G]$ and $x\in G$, then $xax^{-1}=(xax^{-1}a^{-1})a\in[G,G]$.]  $[G,G]$ is called the \textbf{commutator subgroup} of $G$.

\item Let $H$ be a subgroup of $G$, not necessarily normal.  Define $N(H)=\{a\in G:aH=Ha\}$.  Show that $N(H)$ is a subgroup of $G$ containing $H$, and that $H$ is a normal subgroup of $N(H)$.  $N(H)$ is called the \textbf{normalizer} of $H$.  Note that $H$ is a normal subgroup of $G$ if and only if $N(H)=G$.

\item Let $H$ and $K$ be subgroups of $G$.  If $K\subset N(H)$, prove that $HK$ is a subgroup and $HK=KH$.  [Adapt the proof of Proposition 1.13(i).]

\item Let $H_1$ be a subgroup of $G_1$ and $H_2$ a subgroup of $G_2$.

(a) Show that $H_1\times H_2$ is a subgroup of $G_1\times G_2$.

(b) Show that $H_1\times H_2$ is normal if and only if $H_1$ is normal in $G_1$ and $H_2$ is normal in $G_2$.

\item\emph{(Internal Direct Product.)} \---- Suppose $N$ and $M$ are two normal subgroups of $G$, such that $G=NM$ and $N\cap M=\{1\}$.

(a) If $a\in N$ and $b\in M$, show that $ab=ba$.  [Verify that $a^{-1}b^{-1}ab$ must be in both $N$ and $M$.]

(b) Show that the map $f:N\times M\to G$ given by $f(a,b)=ab$ is an isomorphism.  Thus $G$ is naturally identified with the direct product $N\times M$, except that $N$ and $M$ are subgroups, rather than abstract operands.  This is called an \textbf{internal direct product} of $N$ and $M$.  By contrast, $N\times M$ is called the \textbf{external direct product} of $N$ and $M$.

\item (a) If $G$ is a group, let $\operatorname{Aut}(G)$ be the set of isomorphisms from $G$ to $G$.  Show that $\operatorname{Aut}(G)$ is a group under function composition.

(b) For each $x\in G$, show that $\chi_x:G\to G$ given by $\chi_x(a)=xax^{-1}$ is in $\operatorname{Aut}(G)$.  This is called an \textbf{inner automorphism}.

(c) Define $\varphi:G\to\operatorname{Aut}(G)$ via $\varphi(x)=\chi_x$.  Show that $\varphi$ is a homomorphism.

(d) Let $\operatorname{Inn}(G)$ be the image of $\varphi$.  Show that $\operatorname{Inn}(G)$ is a normal subgroup of $\operatorname{Aut}(G)$.

\item If $G$ is a group, let $S(G)$ be the group of permutations of the set $G$.  Then for each $a\in G$, let $T_a$ be the map $x\mapsto ax$ from $G$ to $G$.  By Exercise 2 of Section 1.3, the map $a\mapsto T_a$ is a homomorphism from $G$ to $S(G)$; therefore its image $G_L=\{T_a:a\in G\}$ is a subgroup of $S(G)$.  It is clear that $\operatorname{Aut}(G)$ is also a subgroup of $S(G)$.

(a) Show that $\operatorname{Aut}(G)\subset N(G_L)$.

(b) Now show that $N(G_L)=G_L\operatorname{Aut}(G)$, i.e., the product of the subgroups $G_L$ and $\operatorname{Aut}(G)$ \---- see Exercise 11.  [$N(G_L)$ is called the \textbf{holomorph} of $G$.]

\item\emph{(Semidirect Product.)} \---- Let $N$ and $K$ be groups, and $\varphi:K\to\operatorname{Aut}(N)$ a homomorphism.  Then let $G$ be the \emph{set} $N\times K$, equipped with the following binary operation:
$$(n_1,k_1)(n_2,k_2)=(n_1\varphi(k_1)(n_2),k_1k_2)$$
(a) Show that $G$ is a group under this operation.

(b) Let $\widehat N=\{(n,1):n\in N\}$ and $\widehat K=\{(1,k):k\in K\}$.  Then $\widehat N$ is a normal subgroup of $G$ isomorphic to $N$, and $\widehat K$ is a subgroup of $G$ isomorphic to $K$.

(c) $\widehat N\widehat K=G$ and $\widehat N\cap\widehat K=\{1\}$.

(d) If $N$ is identified with $\widehat N$ via $n\leftrightarrow(n,1)$ and $K$ is identified with $\widehat K$ via $k\leftrightarrow(1,k)$, then for each $k\in K$, $\varphi(k)$ is the automorphism $x\mapsto kxk^{-1}$ of $N$.

$G$ is denoted $N\rtimes_\varphi K$ (or sometimes $N\rtimes K$) and is called the \textbf{semidirect product of $N$ and $K$ with respect to $\varphi$}.

\item Let $G$ be a group, and $N$ and $K$ subgroups of $G$, such that $N$ is normal, $G=NK$ and $N\cap K=\{1\}$.

(a) Define $\varphi:K\to\operatorname{Aut}(N)$ via $\varphi(k)=\chi_k$, where $\chi_k:N\to N$ is the conjugation map $x\mapsto kxk^{-1}$.  Then $\varphi$ is a homomorphism.

(b) Now show that $G$ is isomorphic to the group $N\rtimes_\varphi K$ constructed in Exercise 16.
\end{enumerate}

\subsection*{1.5. Quotient Groups and Homomorphisms}
\addcontentsline{toc}{section}{1.5. Quotient Groups and Homomorphisms}
Now that we have the notion of a normal subgroup $N\subset G$, we can make the congruence classes (cosets) of $N$ into a group, just as in $\mathbb Z$, the congruence classes modulo $n$ form the group $\mathbb Z/n\mathbb Z$.

Let $G/N$ be the set of cosets of $N$.  Since $N$ is normal, $G/N$ can be regarded as either the set of left cosets or the set of right cosets, and it won't make a difference.  For definiteness we will think of $G/N$ as the set $\{aN:a\in G\}$ of left cosets of $N$.  Define a multiplication on $G/N$ via $(aN)(bN)=(ab)N$.

First we must show that this multiplication is well-defined; i.e., the product is independent of the particular way the cosets are represented.  Well, suppose $a,b,a',b'\in G$ such that $aN=a'N$ and $bN=b'N$.  Then $a\equiv a'\pmod N$ and $b\equiv b'\pmod N$, whence by Proposition 1.12, $ab\equiv a'b'\pmod N$.  Therefore $(ab)N=(a'b')N$, and thus $(aN)(bN)=(a'N)(b'N)$.  This proves that the multiplication is well-defined.

Hence we have established a binary multiplication on the set $G/N$, and now we show that $G/N$ is a group under this operation.  Closure under the operation is clear.  If $aN,bN,cN\in G/N$ then
\begin{align*}
((aN)(bN))(cN)&=((ab)N)(cN)=((ab)c)N\\&=(a(bc))N=(aN)((bc)N)=(aN)((bN)(cN));
\end{align*}
therefore the operation is associative.  The identity coset $1N$ serves as an identity because $(1N)(aN)=(1a)N=aN$, and likewise $(aN)(1N)=aN$.  Finally, if $aN\in G/N$, the coset $a^{-1}N$ is an inverse for $aN$ because $(aN)(a^{-1}N)=(aa^{-1})N=1N$, and likewise $(a^{-1}N)(aN)=1N$.  Therefore, $G/N$ is a group.

$G/N$ is called the \textbf{quotient group of $G$ by the normal subgroup $N$}.  In the special case where $G=\mathbb Z$ and $N=n\mathbb Z$, $G/N$ is the additive group $\mathbb Z/n\mathbb Z$ of congruence classes modulo $n$.  Note that since $G/N$ is the set of left cosets of $N$, when $N\subset G$ is a normal subgroup, then $|G/N|=[G:N]$, the index of $N$ in $G$.  (Recall that we defined the index of a subgroup to be the number of left cosets it has.)

In particular, $|G|=|N|~|G/N|$, and if $|G|$ is finite then $|G/N|=|G|/|N|$.  This explains the terminology ``quotient group.''

To show how quotient groups relate to injectivity of certain homomorphisms, we define for any homomorphism the following:\\

\noindent\textbf{Proposition 1.14 and Definition.} \emph{If $f:G\to H$ is a homomorphism, then the set $K=\{a\in G:f(a)=1_H\}$ is a normal subgroup of $G$.  $K$ is called the \textbf{kernel} of $f$, and is sometimes denoted $\ker f$.}
\begin{proof}
If $a,b\in K$, then $f(ab)=f(a)f(b)=1_H1_H=1_H$, hence $ab\in K$.  Proposition 1.10(i) implies $1_G\in K$.  If $a\in K$, then $f(a^{-1})=f(a)^{-1}=1_H^{-1}=1_H$ by Proposition 1.10(ii), therefore $a^{-1}\in K$.  Hence by Proposition 1.3, $K$ is a subgroup of $G$.

To show that $K$ is normal, let $a\in K$ and $x\in G$.  Then
$$f(xax^{-1})=f(x)f(a)f(x)^{-1}=f(x)1_Hf(x)^{-1}=f(x)f(x)^{-1}=1_H$$
Hence $xax^{-1}\in K$, so that $K$ is normal by Proposition 1.12.
\end{proof}

\noindent For instance, the homomorphism $a\mapsto\overline a$ from $\mathbb Z\to\mathbb Z/n\mathbb Z$ has kernel $n\mathbb Z$.  The homomorphism $\det:GL_n(\mathbb R)\to\mathbb R_{\ne 0}$ in Example (2) of Section 1.3 has kernel $SL_n(\mathbb R)=\{A\in GL_n(\mathbb R):\det A=1\}$.

Proposition 1.14 shows that the kernel of any homomorphism is a normal subgroup of $G$.  Conversely, every normal subgroup of $G$ is the kernel of some homomorphism:\\

\noindent\textbf{Proposition 1.15.} \emph{Let $N\subset G$ be a normal subgroup, and let $G/N$ be the quotient group.  Then the map $\pi:G\to G/N$ given by $\pi(a)=aN$ is a surjective homomorphism with kernel $N$.}\\

\noindent $\pi$ is called the \textbf{canonical epimorphism}, \textbf{quotient map} or \textbf{natural homomorphism} from $G$ to $G/N$.
\begin{proof}
To begin with, $\pi$ is a homomorphism because for $a,b\in G$,
$$\pi(ab)=(ab)N=(aN)(bN)=\pi(a)\pi(b)$$
$\pi$ is surjective because every element of $G/N$ is of the form $aN$ for $a\in G$, and $aN=\pi(a)$.  Finally, for any $a\in G$,
$$\pi(a)=1N\iff aN=1N\iff a\equiv 1\pmod N\iff a^{-1}1\in N\iff a\in N$$
Therefore, the kernel of $\pi$ is $N$.
\end{proof}

\noindent The fundamental thing about the kernel of a homomorphism is that it measures how far the homomorphism is from injectivity:\\

\noindent\textbf{Proposition 1.16.} \emph{Let $f:G\to H$ be a homomorphism with kernel $K$.  Then $K=\{1_G\}$ if and only if $f$ is injective.}
\begin{proof}
Suppose $K=\{1_G\}$.  If $f(a)=f(b)$, then $f(ab^{-1})=f(a)f(b)^{-1}=f(a)f(a)^{-1}=1_H$.  Hence $ab^{-1}\in K=\{1_G\}$, so that $ab^{-1}=1_G$ and $a=b$.  Therefore, $f$ is injective.  Conversely, suppose $f$ is injective.  If $c\in K$, then $f(c)=1_H$, so that $f(c)=f(1_G)$ by Proposition 1.10(i).  Therefore $c=1_G$ by injectivity.  Thus $K$ consists of the single element $1_G$.
\end{proof}

\noindent We now have enough materials to induce an injective homomorphism from any homomorphism.  The intuitive idea is this: if $f:G\to H$ is an isomorphism, then $H$ is an exact copy of $G$.  However, if $f:G\to H$ is merely a surjective homomorphism, $H$ is like a projection of $G$, which is induced by $G$ but with some of the information missing.  The kernel of $f$ then tells us how much information was lost.  By directly removing said information from $G$ ourselves, we get a copy of $H$.\\

\noindent\textbf{Theorem 1.17.} (\textsc{First Isomorphism Theorem}) \emph{If $f:G\to H$ is a surjective homomorphism with kernel $K$, then $G/K\cong H$.}
\begin{proof}
Define $\varphi:G/K\to H$ via $\varphi(aK)=f(a)$ for $aK\in G/K$.  First we must show that $\varphi$ is well-defined: suppose $aK=bK$.  Then $a\equiv b\pmod K$, so that $a^{-1}b\in K$, and therefore $f(a^{-1}b)=1_H$.  With that, $1_H=f(a^{-1}b)=f(a)^{-1}f(b)$ and hence $f(a)=f(b)$.  Therefore $\varphi(aK)=\varphi(bK)$ and $\varphi$ is well-defined.

Now if $aK,bK\in G/K$ and $\varphi(aK)=\varphi(bK)$, then $f(a)=f(b)$, whence $f(a^{-1}b)=f(a)^{-1}f(b)=1_H$ and $a^{-1}b\in K$, so that $a\equiv b\pmod K$ and $aK=bK$.  Therefore $\varphi$ is injective.  For any $h\in H$, we have $h=f(a)$ by surjectivity of $f$; consequently, $h=\varphi(aK)$, from which it follows that $\varphi$ is surjective.  Finally $\varphi$ is a homomorphism because
$$\varphi((aK)(bK))=\varphi((ab)K)=f(ab)=f(a)f(b)=\varphi(aK)\varphi(bK)$$
for $aK,bK\in G/K$.  Therefore $\varphi$ is an isomorphism $G/K\cong H$.
\end{proof}

\noindent There are several other isomorphism theorems in group theory.  They are outlined in Exercises 3-5 of this section.

At this point is it worth remarking that Theorem 1.17 gives an alternate way of classifying cyclic groups up to isomorphism (Proposition 1.11).  If $G=\left<a\right>$, then the map $f:\mathbb Z\to G$ given by $n\mapsto a^n$ is a surjective homomorphism.  If $K$ is its kernel, then $K$ is a subgroup of $\mathbb Z$, and number theory and the division algorithm show that either $K=0$ or $K=n\mathbb Z$ for some positive integer $n$.  If $K=0$, then $a$ has infinite order and $f$ is an isomorphism, so that $\mathbb Z\cong G$.  If $K=n\mathbb Z$, then $a$ has order $n$ (because $n$ is the smallest positive integer in $K$, i.e., the smallest positive integer such that $a^n=1$).  Moreover, Theorem 1.17 implies that $\mathbb Z/n\mathbb Z\cong G$.

\subsection*{Exercises 1.5. (Quotient Groups and Homomorphisms)}
\begin{enumerate}
\item A group $G$ is said to be \textbf{simple} if $G\ne\{1\}$ and the only normal subgroups of $G$ are $\{1\}$ and $G$ itself.  Suppose $G$ is a simple group and $f:G\to H$ is a surjective homomorphism.  Show that either $f$ is an isomorphism or $H=\{1\}$.

\item Let $N$ be a normal subgroup of $G$ of index $n$.  Show that $a^n\in N$ for every $a\in G$.  [Apply Exercise 12(b) of Section 1.2 to $aN\in G/N$.]

In particular, if $N$ is any subgroup of $G$ of index $2$, then $a^2\in N$ for every $a\in G$ (because $N$ must be normal by Exercise 4 of Section 1.4).

\item\emph{(Second Isomorphism Theorem.)} \---- Let $N$ and $K$ be subgroups of a group $G$, with $N$ normal.

(a) Define $f:K\to NK/N$ via $f(a)=aN$.  Show that $f$ is a surjective homomorphism with kernel $N\cap K$.

(b) Conclude that $K/(N\cap K)\cong NK/N$.

(c) If $G$ is finite, show that $|N|~|K|=|NK|~|N\cap K|$.  [Recall that $|G/N|=|G|/|N|$.]

\item\emph{(Subgroup Correspondence Theorem.)} \---- Let $N\subset K$ be subgroups of $G$, with $N$ normal in $G$.

(a) Show that $K/N=\{aN:a\in K\}$ is a subgroup of $G/N$.

(b) Every subgroup of $G/N$ is of the form $K/N$, with $K\supset N$ a subgroup of $G$.

(c) Show that $K\mapsto K/N$ is a one-to-one correspondence between subgroups of $G$ containing $N$ and subgroups of $G/N$.

(d) $K$ is normal in $G$ if and only if $K/N$ is normal in $G/N$.

\item\emph{(Third Isomorphism Theorem.)} \---- Let $N\subset M$ be normal subgroups of $G$.  Then $(G/N)/(M/N)\cong G/M$.  [Define $f:G/N\to G/M$ via $f(aN)=aM$; show that $f$ is a well-defined, surjective homomorphism with kernel $M/N$.]

\item If $G_1$ and $G_2$ are groups, and $N_1\subset G_1$ and $N_2\subset G_2$ are normal subgroups, then $N_1\times N_2$ is a normal subgroup of $G_1\times G_2$ by Exercise 12(b) of Section 1.4.  Show that $\frac{G_1\times G_2}{N_1\times N_2}\cong\frac{G_1}{N_1}\times\frac{G_2}{N_2}$.  [Show that $f:G_1\times G_2\to G_1/N_1\times G_2/N_2$ given by $f(a,b)=(aN_1,bN_2)$ is a surjective homomorphism with kernel $N_1\times N_2$.]

\item Let $N$ and $M$ be normal subgroups of a group $G$.  Define $f:G\to G/N\times G/M$ via $f(a)=(aN,aM)$.

(a) Show that $f$ is a homomorphism.

(b) Is $f$ necessarily surjective?  [Consider the case $G=\mathbb Z,N=2\mathbb Z,M=4\mathbb Z$.]

(c) What is the kernel of $f$?

\item (a) Let $G=GL_n(\mathbb R)$.  Show that $\mathbb R^\times=\{aI_n:a\ne 0\text{ in }\mathbb R\}$ is a normal subgroup of $G$.  The quotient group $GL_n(\mathbb R)/\mathbb R^\times$ is denoted $PGL_n(\mathbb R)$ and is called the \textbf{projective general linear group}.

(b) Define $GL^+_2(\mathbb R)=\{A\in GL_2(\mathbb R):\det(A)>0\}$.  Show that $GL^+_2(\mathbb R)$ is a subgroup of $GL_2(\mathbb R)$, and that $\mathbb R^\times\subset GL^+_2(\mathbb R)$ (where $\mathbb R^\times$ is defined as in part (a) with $n=2$).  Set $PGL^+_2(\mathbb R)=GL^+_2(\mathbb R)/\mathbb R^\times$.

(c) Define $PSL_2(\mathbb R)=SL_2(\mathbb R)/\{I_2,-I_2\}$.  [After all, $\pm I_2$ are the only matrices of the form $aI_2$ that are in $SL_2(\mathbb R)$.]

Show that $PGL^+_2(\mathbb R)\cong PSL_2(\mathbb R)$.

\item If $G$ is a group, let $\operatorname{Aut}(G)$ be the set of isomorphisms from $G$ to $G$.  Then $\operatorname{Aut}(G)$ is a group under function composition by Exercise 14(a) of Section 1.4.  $\operatorname{Aut}(G)$ is called the \textbf{automorphism group of $G$}.

For each $x\in G$, define $\chi_x:G\to G$ via $\chi_x(a)=xax^{-1}$.  Then by Exercise 14(c) of Section 1.4, $\varphi:G\to\operatorname{Aut}(G)$ given by $\varphi(x)=\chi_x$ is a homomorphism.  The image of homomorphism $\varphi$ is denoted $\operatorname{Inn}(G)$ and is called the group of \textbf{inner automorphisms of $G$}.  Exercise 14(d) of Section 1.4 shows that $\operatorname{Inn}(G)$ is a normal subgroup of $\operatorname{Aut}(G)$.  The quotient group $\operatorname{Aut}(G)/\operatorname{Inn}(G)$ is denoted $\operatorname{Out}(G)$ and called the group of \textbf{outer automorphisms of $G$}.

(a) Show that the kernel of $\varphi$ is the center $Z(G)$ of $G$.  Conclude that $G/Z(G)\cong\operatorname{Inn}~G$.

(b) Establish a sequence of homomorphisms
$$\{1\}\to Z(G)\overset{\subset}{\to}G\overset{\varphi}{\to}\operatorname{Aut}(G)\to\operatorname{Out}(G)\to\{1\}$$
and show that the kernel of each homomorphism is equal to the image of the previous one.  [Such a sequence is called an \textbf{exact sequence} of homomorphisms.  However, exact sequences are rather restrictive because the image of a homomorphism can be any subgroup, whereas the kernel must be a normal subgroup.  Usually exact sequences are dealt with in abelian groups (written additively), where all subgroups are normal.]

(c) Show that a subgroup $N$ is normal if and only if $f(N)=N$ for all $f\in\operatorname{Inn}(G)$.

A subgroup $K\subset G$ is said to be \textbf{characteristic} if $f(K)=K$ for all $f\in\operatorname{Aut}(G)$.  It is clear from part (c) that every characteristic subgroup is normal.  But the converse is false [$\mathbb Z\times\{0\}$ is a normal subgroup of $\mathbb Z\times\mathbb Z$ which is not characteristic].

(d) If $K$ is a characteristic subgroup of $N$ and $N$ is a normal subgroup of $G$, show that $K$ is a normal subgroup of $G$.  [Compare with Exercise 7 of Section 1.4.]

\item (a) Let $[G,G]\subset G$ be the commutator subgroup (Exercise 9 of Section 1.4).  Show that $G/[G,G]$ is abelian.

(b) Suppose $f:G\to H$ is a homomorphism with $H$ abelian.  Then the kernel of $f$ contains $[G,G]$.

(c) Now show that if $K\subset G$, then $K$ is normal and $G/K$ is abelian, if and only if $K\supset[G,G]$.

\item A group $G$ is said to be \textbf{solvable} if there exists a chain $G=G_0\supset G_1\supset\dots\supset G_t=\{1\}$ such that for each $0\leqslant i<t$, $G_{i+1}$ is a normal subgroup of $G_i$ and the quotient group $G_i/G_{i+1}$ is abelian.

(a) Define a sequence $G^{(0)},G^{(1)},G^{(2)},\dots$ via $G^{(0)}=G$, $G^{(n+1)}=[G^{(n)},G^{(n)}]$, the commutator subgroup of $G^{(n)}$.  Show that $G$ is solvable if and only if $G^{(n)}=\{1\}$ for some $n$.  [Use Exercise 10.]

(b) Show that every subgroup and every homomorphic image of a solvable group is solvable.

(c) If $N\subset G$ is a normal subgroup such that $N$ and $G/N$ are both solvable, show that $G$ is solvable.  [Use Exercise 4 to show that $G/N$ is solvable if and only if there is chain $G=G_0\supset G_1\supset\dots\supset G_t=N$ such that for each $0\leqslant i<t$, $G_{i+1}$ is normal in $G_i$ and $G_i/G_{i+1}$ is abelian.]

(d) Show that if $G$ and $H$ are solvable, so is $G\times H$.  [Verify that $G\times\{1\}\subset G\times H$ is a normal subgroup, $G\times\{1\}\cong G$ and $\frac{G\times H}{G\times\{1\}}\cong H$.  Now use part (c).]
\end{enumerate}

\subsection*{1.6. Group Actions}
\addcontentsline{toc}{section}{1.6. Group Actions}
A remarkable thing about groups is that every group can be viewed as a group of permutations.  [Exercise 2 of Section 1.3.]  Thus the study of group theory can be reduced to the study of permutations.  Though we will not do this systematically, we will look at how groups act on sets by permutations.

If $G$ is a group and $X$ is a set, a homomorphism $\rho:G\to S(X)$ is called an \textbf{action} of $G$ on $X$.  Each element of $G$ gets mapped to a permutation on the set $X$.  In this way one can view $G$ as a ``group of symmetries'' of $X$.  For example, let $D_n$ be the dihedral group of the regular $n$-gon, and $X$ be the set of vertices of the $n$-gon.  Then each transformation of $D_n$ permutes the vertices, thus yielding a function $D_n\to S(X)$.  Since it is clear that composing two transformations composes the permutations of the vertices, this function is also a homomorphism.\\

\noindent\textbf{Definition.} \emph{A \textbf{group action} of a group $G$ on a set $X$ is a group homomorphism $\rho:G\to S(X)$.}\\

\noindent We will now give a different definition and show that it is equivalent to the previous one.  If $G$ is a group and $X$ is a set, we take a map $\eta:(g,x)\mapsto g\cdot x$ from $G\times X\to X$ such that:

(i) $gh\cdot x=g\cdot(h\cdot x)$ for $g,h\in G,x\in X$;

(ii) $1\cdot x=x$ for all $x\in X$;

and we say $G$ \textbf{acts} on $X$ via the map $(g,x)\mapsto g\cdot x$.\\

\noindent\textbf{Note.} Many authors make the blunder of saying that ``$G$ acts on $X$ if there exists a function $(g,x)\mapsto g\cdot x$ satisfying [the above conditions].''  Such a function certainly exists for any group $G$ and set $X$; for example, one can define $g\cdot x=x$ for all $g\in G,x\in X$.  Since the notion of a group acting on a set is an algebraic structure and not a mere proposition, the definition of a group acting on a set has been given more carefully above.\\

\noindent To show that this definition of an action agrees with the one we have given, first observe that if $\rho:G\to S(X)$ is a homomorphism, then the map $(g,x)\mapsto\rho(g)(x)$ is a function $G\times X\to X$ satisfying conditions (i) and (ii) above.  Conversely, suppose $\eta:G\times X\to X$ is a function satisfying the conditions.  Then for each $g\in G$, let $\alpha_g:X\to X$ be defined by $\alpha_g(x)=\eta(g,x)$.  The $\alpha_g$ are functions from $X$ to $X$; we do not yet know if they are bijective.  However, the conditions in the definition imply that $\alpha_{gh}=\alpha_g\circ\alpha_h$ for $g,h\in G$, and that $\alpha_1=1_X$.  Moreover, we have for $g\in G$
$$\alpha_g\circ\alpha_{g^{-1}}=\alpha_{gg^{-1}}=\alpha_1=1_X$$
and similarly $\alpha_{g^{-1}}\circ\alpha_g=1_X$.  Thus $\alpha_g$ admits $\alpha_{g^{-1}}$ as a two-sided inverse, and is therefore bijective.  Hence $\alpha_g\in S(X)$ for all $g$, and $g\mapsto\alpha_g$ is a homomorphism $G\to S(X)$.  The reader can readily verify that these two conversions are inverses of one another.

Since $\alpha_g$ and $\alpha_{g^{-1}}$ are inverse functions, we take note that
\begin{center}
If $g\in G$ and $x,y\in X$, then $g\cdot x=y$ if and only if $g^{-1}\cdot y=x$.
\end{center}
Now for some examples of group actions.\\

\noindent\textbf{Examples.}

(1) If $X$ is the set of vertices of a regular $n$-gon, then $D_n$ acts on $X$ as previously established: if $g\in D_n$ and $x\in X$, then $g\cdot x$ is the vertex that $x$ is mapped to via the transformation $g$.\\

(2) $GL_n(\mathbb R)$ acts on $\mathbb R^n$ via multiplication of a matrix times a column vector, $(A,\vec v)\mapsto A\vec v$.  From the associativity of matrix multiplication and by virtue of the identity matrix, the conditions in the definition of a group action are satisfied.\\

(3) For any group $G$ and any set $X$, there is the \textbf{trivial action}, $g\cdot x=x$ for all $g\in G,x\in X$.\\

(4) If $X$ is any set, then $S(X)$ acts on $X$ in the obvious way, $\sigma\cdot x=\sigma(x)$.  Note that this action viewed in the first sense (i.e., homomorphism $G\to S(X)$) is the identity map $\sigma\mapsto\sigma$ on $S(X)$.\\

(5) Let $G$ be any group.  Then $G$ acts on $G$ by left multiplication, $g\cdot x=gx$.  After all, $(gh)x=g(hx)$ and $1x=x$.\\

(6) In general, $G$ does \emph{not} act on $G$ via $g\cdot x=xg$.\footnote{The ``$\cdot$'' symbol is important here because the group action uses a different operation from the group multiplication.}  Instead of $gh\cdot x=g\cdot(h\cdot x)$, we would have $gh\cdot x=h\cdot(g\cdot x)$ with this operation:
$$gh\cdot x=x(gh)=(xg)h=(g\cdot x)h=h\cdot(g\cdot x)$$
This is very different if $G$ is nonabelian.  However, $G$ does act on $G$ via $g\cdot x=xg^{-1}$.  This follows from Proposition 1.1(iii): $gh\cdot x=x(gh)^{-1}=x(h^{-1}g^{-1})=(xh^{-1})g^{-1}=g\cdot(xh^{-1})=g\cdot(h\cdot x)$.

This example shows why the action we have defined is often called a \textbf{left action}.\\

(7) $G$ also acts on $G$ via $g\cdot x=gxg^{-1}$.  After all, $gh\cdot x=ghx(gh)^{-1}=ghx(h^{-1}g^{-1})$ by Proposition 1.1(iii), and $ghx(h^{-1}g^{-1})=g(hxh^{-1})g^{-1}=g\cdot(h\cdot x)$.  Also, $1\cdot x=x$ is clear.  This action is called the \textbf{action by conjugation} in $G$, and $G$ is said to \textbf{act on $G$ by conjugation}.

Note, by the way, that if $N$ is any normal subgroup of $G$, then $G$ acts on $N$ by conjugation, because Proposition 1.12 implies that whenever $g\in G,x\in N$, we have $g\cdot x\in N$.\\

(8) Let $H$ be a subgroup of $G$, and $G/H$ the set of left cosets of $H$.  [$G/H$ is not a group if $H$ is not normal, but the notation is still common.]  Then $G$ acts on $G/H$ via $g\cdot aH=(ga)H$.  We leave it to the reader to verify that this is well-defined, and is an action of $G$ on the set $G/H$.  Similarly, $G$ acts on the set $G\backslash H$ of right cosets of $H$ by $g\cdot Ha=H(ag^{-1})$.\\

(9) Let $X$ be the set of subgroups of $G$.  Then $G$ acts on $X$ by conjugation: $g\cdot H=gHg^{-1}$.  Exercise 10(a) of Section 1.2 shows that $gHg^{-1}$ is a subgroup, and it is readily verified that this is an action.\\

(10) If $G$ acts on two sets $X$ and $Y$, then $G$ acts on the product $X\times Y$ via $g\cdot(x,y)=(g\cdot x,g\cdot y)$.  The reasons are obvious.  In particular, an action of $G$ on $X$ induces an action on $X\times X$.\\

(11) Suppose $G$ acts on a set $X$, and $H$ is a subgroup of $G$.  Then the map $(h,x)\mapsto h\cdot x$ from $H\times X\to X$ is certainly an action of $H$ on the set $X$.  It is called the \textbf{restriction of the action of $G$ to $H$}.

In particular, if $X$ is the set of subgroups of $G$, then $H$ acts on $X$ by conjugation.  $H$ also acts on any subset $X'\subset X$ satisfying the conditions $K\in X',h\in H\implies hKh^{-1}\in X'$.  In this case, we say that $H$ acts on $X'$ by conjugation.  Similarly, $H$ acts on $G$ (the set of \emph{elements} of $G$) by conjugation as well.\\

\noindent Now that we have the notion of a group action, let us define some terms revolving around it.  If $G$ acts on $X$ and $x\in X$, the \textbf{orbit} of $x$, denoted $\mathcal O_{X/G}(x)$ [or $\mathcal O_X(x)$ if $G$ is clear], is defined to be the set $\{g\cdot x:g\in G\}$.  Since $x=1\cdot x\in\mathcal O_X(x)$, the orbits of all the elements of $X$ are nonempty sets whose union is $X$.  Now we will show that:\\

\noindent\textbf{Proposition 1.18.} \emph{If $G$ acts on $X$, then any two orbits are either disjoint or identical.}\\

\noindent It will follow that the orbits partition the set $X$.
\begin{proof}
Take any $x,y\in X$.  If $\mathcal O_X(x)\cap\mathcal O_X(y)=\varnothing$, then they are disjoint, and we are done.  Now suppose that $\mathcal O_X(x)\cap\mathcal O_X(y)\ne\varnothing$.  Pick $z\in\mathcal O_X(x)\cap\mathcal O_X(y)$.  Then since $z\in\mathcal O_X(x)$, $z=g\cdot x$ for some $g\in G$.  Similarly $z=h\cdot y$ for some $y$.  Moreover, $y=h^{-1}\cdot z=h^{-1}\cdot(g\cdot x)=h^{-1}g\cdot x\in\mathcal O_X(x)$.  Furthermore, $\mathcal O_X(y)\subset\mathcal O_X(x)$, because each element of $\mathcal O_X(y)$ is of the form $a\cdot y$ with $a\in G$, and $a\cdot y=a\cdot(h^{-1}g\cdot x)=ah^{-1}g\cdot x$.  The same argument with the roles of $x$ and $y$ reversed shows that $x=g^{-1}h\cdot y$ and $\mathcal O_X(x)\subset\mathcal O_X(y)$.  Therefore, $\mathcal O_X(x)=\mathcal O_X(y)$, and the orbits are identical.
\end{proof}
Alternatively one can prove Proposition 1.18 by observing that the relation
\begin{center}
$x\sim y$ if there exists $g\in G$ such that $g\cdot x=y$
\end{center}
is an equivalence relation, and that the orbits are its equivalence classes.  [$1\cdot x=x$ so that $x\sim x$; we have symmetry since $g\cdot x=y\implies g^{-1}\cdot y=x$; and transitivity because $g\cdot x=y$ and $h\cdot y=z$ imply $hg\cdot x=z$.]

If $X\ne\varnothing$ and $\mathcal O_X(x)=X$ for some (and hence every) $x\in X$, the action is said to be \textbf{transitive}.  For instance, $S(X)$ acts transitively on $X$, because for any $x,y\in X$ there exists a permutation $\sigma\in S(X)$ such that $\sigma(x)=y$.  The group $D_n$ also acts transitively on the vertices of the regular $n$-gon, because any vertex can be moved to any other vertex.  $G$ acts transitively on $G$ by left multiplication, because for any $x,y\in G$, $(yx^{-1})x=y$.  $GL_n(\mathbb R)$ however, does \emph{not} act transitively on $\mathbb R^n$.  Its orbits are the sets $\{\vec 0\}$ and $\mathbb R^n-\{\vec 0\}$.

We emphasize that transitivity of the action does \emph{not} imply that the homomorphism $f:G\to S(X)$ is surjective.  It merely implies that the image $f(G)$ is large enough so that for any $x,y\in X$ there exists $\sigma\in f(G)$ sending $x\mapsto y$.  For example, the action of $G$ on $G$ is transitive, but the homomorphism $G\to S(G)$ is far from surjective; for $|G|=n$ implies $|S(G)|=n!$, which is much larger than $n$.  Exercise 2(e) effectively classifies transitive actions.

If $x\in X$, then the \textbf{stabilizer} of $x$ \---- denoted $\operatorname{Stab}(x)$ \---- is defined to be $\{g\in G:g\cdot x=x\}$.  It is the set of group elements that keep $x$ fixed.  This is a subgroup of $G$: if $g,h\in\operatorname{Stab}(x)$, then $gh\cdot x=g\cdot(h\cdot x)=g\cdot x=x$, so $gh\in\operatorname{Stab}(x)$.  $1\cdot x=x$ by condition (ii) in the definition of a group action, hence $1\in\operatorname{Stab}(x)$.  If $g\in\operatorname{Stab}(x)$, then $g^{-1}\cdot x=g^{-1}\cdot(g\cdot x)=g^{-1}g\cdot x=1\cdot x=x$, so $g^{-1}\in\operatorname{Stab}(x)$.

Observe that $\bigcap_{x\in X}\operatorname{Stab}(x)=\{g\in G:g\cdot x=x\text{ for all }x\in X\}$.  This is the set of group elements that fix every $x\in X$.  Exercise 4(a) shows that this is a normal subgroup; it is called the \textbf{kernel} of the action.  The action is said to be \textbf{faithful} if $\bigcap_{x\in X}\operatorname{Stab}(x)=\{1\}$.

If $\operatorname{Stab}(x)=G$, which means that $g\cdot x=x$ for all $g\in G$, then $x$ is said to be a \textbf{fixed point} of the action.  For example, $\vec 0$ is a fixed point of the action of $GL_n(\mathbb R)$ on $\mathbb R^n$.  Observe that $x$ is a fixed point if and only if $\mathcal O_X(x)=\{x\}$.\\%This is no accident, as we are about to see how the orbit and stabilizer of $x$ are related.\\

\noindent\textbf{Theorem 1.19.} (\textsc{Orbit-Stabilizer Theorem}) \emph{If $G$ acts on a set $X$ and $x\in X$, then $|\mathcal O_X(x)|~|\operatorname{Stab}(x)|=|G|$.}
\begin{proof}
Let $H=\operatorname{Stab}(x)$, and let $G/H$ be the set of left cosets of $H$.  Define a function $f:G/H\to\mathcal O_X(x)$ via $f(aH)=a\cdot x$.

First we show that $f$ is well-defined: if $aH=bH$, then $a\equiv b\pmod H$, so that $a^{-1}b\in H=\operatorname{Stab}(x)$ and hence $a^{-1}b\cdot x=x$.  Therefore $a\cdot x=a\cdot(a^{-1}b\cdot x)=aa^{-1}b\cdot x=b\cdot x$, so that $f(aH)=f(bH)$.  Hence $f$ is well-defined.

We claim that $f$ is bijective.

Suppose $f(aH)=f(bH)$.  Then $a\cdot x=b\cdot x$, hence $a^{-1}b\cdot x=a^{-1}\cdot(b\cdot x)=a^{-1}\cdot(a\cdot x)=a^{-1}a\cdot x=1\cdot x=x$.  Thus $a^{-1}b\in\operatorname{Stab}(x)=H$.  Hence $a\equiv b\pmod H$ and $aH=bH$.  Therefore, $f$ is injective.

Also, $f$ is surjective by definition of an orbit: if $y\in\mathcal O_X(x)$ then there exists $g\in G$ such that $g\cdot x=y$; with that, $y=f(gH)$.

Since $f$ is a bijection $G/H\to\mathcal O_X(x)$, it follows that $|G/H|=|\mathcal O_X(x)|$.  However, $|G/H|$ is merely the index $[G:H]$ of $H$ in $G$, and the remarks preceding Theorem 1.8 show that $|G|=|H|~[G:H]$.  Therefore $|G|=|H|~|\mathcal O_X(x)|$ as desired.
\end{proof}

\noindent For example, suppose $X$ is the set of vertices of a regular $n$-gon and $D_n$ acts on $X$.  Then $|X|=n$.  Since the action is transitive, $\mathcal O_X(x)=X$ for any $x\in X$, and hence $|\mathcal O_X(x)|=n$.  But also, $\operatorname{Stab}(x)$ consists of the identity and the reflection over the line through $x$ and the center of the polygon.  Hence $|\operatorname{Stab}(x)|=2$.  It follows from Theorem 1.19 that $|D_n|=2n$.  This is another proof that $|D_n|=2n$, a fact we have known since Section 1.1.\\

\noindent\textbf{BASIC TERMS AND PROPOSITIONS FOR LATER}\\

\noindent If $G$ acts on a set $X$, and $Y$ is a set (the set $Y$ does not have an action of $G$), then a function $f:X\to Y$ is said to be a \textbf{function on orbits} if $f|_{\mathcal O_X(x)}$ is constant for every $x\in X$; or equivalently, whenever $g\in G,x\in X$ we have $f(g\cdot x)=f(x)$.

For instance, suppose $G=GL_n(\mathbb R)$ acts on itself by conjugation.  Then $\det:G\to\mathbb R_{\ne 0}$ is a function on orbits, because for any $g,h\in G$, $\det(ghg^{-1})=\det(g)\det(h)\det(g)^{-1}=\det(h)$.  Since it is well-known in linear algebra that similar matrices have the same trace,\footnote{In fact, whenever $A,B$ are $n\times n$ matrices, not necessarily nonsingular, then $\operatorname{tr}(AB)=\operatorname{tr}(BA)$ because both are equal to $\sum_{j=1}^n\sum_{k=1}^nA_{jk}B_{kj}$.} the function $\operatorname{tr}:G\to\mathbb R$ is also a function on orbits.

Another basic example of a function on orbits is this: if $H$ is a subgroup of $G$, and $H$ acts on $G$ by left multiplication, then let $G\backslash H$ be the set of right cosets of $H$.  With that, $f:G\to G\backslash H$ given by $f(a)=Ha$ is a function on orbits.  (Why?) % Typical elements of the same orbit are g, hg with g\in G and h\in H.  f(g) = f(hg) because Hg = (Hh)g = Hhg. [The action isn't conjugation here]

When $f:X\to Y$ is a function on orbits, then one can think of $f$ as an attribute of certain elements of the set $X$, such that $G$ consists of ``symmetries'' with respect to that attribute: the attribute is invariant under the transformations in $G$.  This will be especially useful when $X$ consists of subsets of a space, and $G$ consists of the isometries of the space: then $G$'s transformations will preserve lengths and angles, hence these will be functions on orbits on the $G$-set $X$.

Now for a few basic propositions about functions on orbits.\\

\noindent\textbf{Proposition 1.20.} \emph{Suppose $G$ acts on $X$ and $X'\subset X$ is a subset satisfying this property:}

(*) \emph{For any $x\in X$, there exists $g\in G$ such that $g\cdot x\in X'$.}

\emph{If $Y$ is a set and $f_1,f_2:X\to Y$ are functions on orbits such that $f_1|_{X'}=f_2|_{X'}$, then $f_1=f_2$.}

\begin{proof}
Take any $x\in X$.  By (*), there exists $g\in G$ such that $g\cdot x\in X'$.  Since $f_1|_{X'}=f_2|_{X'}$, we have $f_1(g\cdot x)=f_2(g\cdot x)$.  However, $f_1(g\cdot x)=f_1(x)$ and $f_2(g\cdot x)=f_2(x)$ since they are functions on orbits; therefore $f_1(x)=f_1(g\cdot x)=f_2(g\cdot x)=f_2(x)$.  Moreover, $f_1(x)=f_2(x)$ for all $x\in X$, so $f_1=f_2$.
\end{proof}

\noindent\textbf{Proposition 1.21.} \emph{Suppose $G$ acts on $X$, $X'\subset X$ is a subset and $f:X'\to Y$ is a function to a set $Y$.  Furthermore, suppose that:}

(i) \emph{For any $x\in X$, there exists $g\in G$ such that $g\cdot x\in X'$.}

(ii) \emph{Whenever $x,y\in X'$ and $g\in G$ with $g\cdot x=y$, we have $f(x)=f(y)$.}

\emph{Then $f$ extends to a unique function on orbits $X\to Y$.}

\begin{proof}
Define $f^*:X\to Y$ as follows: for each $x\in X$, pick $g\in G$ such that $g\cdot x\in X'$, then set $f^*(x)=f(g\cdot x)$.

First we need to show that $f^*$ is well-defined, and the value of $f^*(x)$ is independent of which $g\in G$ is picked.  Well, suppose $g,h\in G$ such that $g\cdot x$ and $h\cdot x$ are both in $X'$.  Then, since $g\cdot x=gh^{-1}\cdot(h\cdot x)$, we have $f(g\cdot x)=f(h\cdot x)$ by condition (ii).  This proves that $f^*$ is well-defined.

Now, $f^*$ clearly extends $f$: after all, if $x\in X'$, then $1\cdot x\in X'$, and thus $f^*(x)=f(1\cdot x)=f(x)$ by definition of $f^*$.  Moreover, if $x\in X$, then there exists $g\in G$ such that $g\cdot x\in X'$.  Now for any $h\in G$, we have $gh^{-1}\cdot(h\cdot x)\in X'$, and thus $f^*(h\cdot x)=f(gh^{-1}\cdot(h\cdot x))=f(g\cdot x)=f^*(x)$.
Therefore $f^*$ is a function on orbits $X\to Y$ which extends $f$.

To show uniqueness, suppose $f':X\to Y$ is also a function on orbits extending $f$; then $f^*|_{X'}=f'|_{X'}=f$.  Now, $X'$ satisfies the property (*) in the hypothesis of Proposition 1.20, because that property is condition (i) here.  Thus Proposition 1.20 applies to show $f^*=f'$, and therefore $f^*$ is unique.
\end{proof}

\noindent It is convenient to be able to conclude that a function is a function on orbits just by casework on generators of $G$.  The following proposition shows that we can do this.\\

\noindent\textbf{Proposition 1.22.} \emph{Suppose $S\subset G$ is a generating set of $G$.  If $G$ acts on a set $X$, and $f:X\to Y$ is a function such that $f(g\cdot x)=f(x)$ for all $g\in S,x\in X$, then $f$ is a function on orbits.}

\begin{proof}
Let $H=\{g\in G:f(g\cdot x)=f(x)\text{ for all }x\in X\}$.  We claim that $H$ is a subgroup of $G$.

If $g,h\in H$, then for all $x\in X$, $f(gh\cdot x)=f(g\cdot(h\cdot x))=f(h\cdot x)=f(x)$; therefore $gh\in H$ and $H$ is closed.  Since $1\cdot x=x$ for all $x\in X$, we manifestly have $f(1\cdot x)=f(x)$; therefore, $1\in H$.  Finally, suppose $g\in H$; then for any $x\in X$, $f(g^{-1}\cdot x)=f(g\cdot(g^{-1}\cdot x))$ [because $g\in H$] $=f(gg^{-1}\cdot x)=f(1\cdot x)=f(x)$, from which $g^{-1}\in H$ follows.  Therefore $H$ is a subgroup by Proposition 1.3.

Now, $H\supset S$ by the hypothesis that $f(g\cdot x)=f(x)$ for all $g\in S,x\in X$.  Since $S$ generates $G$, it follows that $H=G$.  Therefore $f(g\cdot x)=f(x)$ for all $g\in G,x\in X$, and $f$ is a function on orbits.
\end{proof}

\noindent\textbf{BURNSIDE'S COUNTING THEOREM}\\

\noindent We conclude the chapter with a result in group theory and combinatorics, known as \textbf{Burnside's Counting Theorem}\footnote{It is alternatively known as the \emph{Cauchy-Frobenius Lemma}, or \emph{The Lemma that is not Burnside's}, because Cauchy and Frobenius established it before William Burnside.}.  It provides a simple way of counting mathematical objects, when taking symmetry into account.  Effectively, it enables one to compute the number of orbits of a group action, by using casework on the group elements, instead of the backbreaking casework on the set $X$ on which it acts.\\

\noindent\textbf{Theorem 1.23.} (\textsc{Burnside's Counting Theorem}) \emph{Suppose a finite group $G$ acts on a finite set $X$.  Then the number of orbits of the action is equal to $\frac 1{|G|}\sum_{g\in G}|\{x\in X:g\cdot x=x\}|$.}\\

\noindent It is worth remarking that without this theorem, it is not so clear that the expression in the statement is even an integer (because of the denominator of $|G|$).  However, we will prove that the expression is an integer, as well as implying that it is the number of orbits.
\begin{proof}
Let $S=\{(g,x)\in G\times X:g\cdot x=x\}$.  We find $|S|$ in two ways.  On the one hand, we have:
$$|S|=\sum_{g\in G}|\{x\in X:g\cdot x=x\}|$$
After all, for each $g\in G$, set $S_g=\{(g,x):x\in X\}\cap S$.  (Note that $g$ occurs free in this expression, whereas it did not in the definition of $S$.)  It is clear that $S=\bigsqcup_{g\in G}S_g$ (this means that $S=\bigcup_{g\in G}S_g$ and $g\ne h\implies S_g\cap S_h\ne\varnothing$, and hence the $S_g$'s partition $S$), so that $|S|=\sum_{g\in G}|S_g|$.  But also $|S_g|=|\{x\in X:g\cdot x=x\}|$, because $x\mapsto(g,x)$ is clearly a bijection from the set $\{x\in X:g\cdot x=x\}$ to $S_g$.  Hence, $|S|=\sum_{g\in G}|\{x\in X:g\cdot x=x\}|$.

On the other hand, we also have
$$|S|=\sum_{x\in X}|\{g\in G:g\cdot x=x\}|=\sum_{x\in X}|\operatorname{Stab}(x)|$$
by pretty much the same argument, but this time taking $S^x=\{(g,x):g\in G\}\cap S$, then noting that $S=\bigsqcup_{x\in X}S^x$ and $|S^x|=|\operatorname{Stab}(x)|$.  Thus we have
$$\frac 1{|G|}\sum_{g\in G}|\{x\in X:g\cdot x=x\}|=\frac 1{|G|}|S|=\frac 1{|G|}\sum_{x\in X}|\operatorname{Stab}(x)|=\sum_{x\in X}\frac{|\operatorname{Stab}(x)|}{|G|}$$
However, by the Orbit-Stabilizer Theorem (1.19), $|\mathcal O_X(x)|~|\operatorname{Stab}(x)|=|G|$ for every $x\in X$, and therefore
$$\sum_{x\in X}\frac{|\operatorname{Stab}(x)|}{|G|}=\sum_{x\in X}\frac 1{|\mathcal O_X(x)|}$$
Let $\mathcal O_1,\mathcal O_2,\dots,\mathcal O_k$ be the (distinct) orbits of the action.  Since they partition $X$, any summation over the elements of $X$ can be considered as a summation over the orbits, where each summand runs through the elements of the orbit.  Thus
$$\sum_{x\in X}\frac 1{|\mathcal O_X(x)|}=\sum_{j=1}^k\sum_{x\in\mathcal O_j}\frac 1{|\mathcal O_X(x)|}=\sum_{j=1}^k\sum_{x\in\mathcal O_j}\frac 1{|\mathcal O_j|}=\sum_{j=1}^k|\mathcal O_j|\frac 1{|\mathcal O_j|}=\sum_{j=1}^k1=k$$
Therefore $\frac 1{|G|}\sum_{g\in G}|\{x\in X:g\cdot x=x\}|$ is equal to $k$, the number of orbits, as desired.
\end{proof}

\noindent As an application of Burnside's Counting Theorem, we will solve the following problem.

\emph{Suppose there are $n$ available colors, where $n$ is a positive integer.  Each vertex of a square is to be colored with one of these colors.  Two colorings are considered to be the same if one can be obtained from the other through rotating/flipping the square.  How many different ways are there to color the vertices?}

If we were not taking rotations and flips into consideration, the answer would obviously be $n^4$, because each of the four vertices has one of $n$ possible colors.  Let $X$ be the set of all vertex colorings of a fixed square; then $|X|=n^4$.  However, $D_4$ acts on $X$ by applying a transformation to a square with the colored vertices; here is an example of what the rotation $r_1$ does to a square with four different colors:
\begin{center}\includegraphics[scale=.4]{ColoredSquares.png}\end{center}
Since two colorings are considered the same if and only if they're in the same orbit of the group action, the question is effectively asking for the number of orbits of this action.  It is hard to do this directly, because there are many special cases.  What if two adjacent colors are the same?  What if two colors across the diagonal are the same?  Three colors?  The orbit sizes vary for exactly these reasons (because, e.g., if two colors across the diagonal are the same, the stabilizer contains a diagonal reflection).

Thanks to Burnside's Counting Lemma, this is rather easy, even with the number of available colors being a variable.

For each $g\in D_4$, we find $|\{x\in X:g\cdot x=x\}|$, the number of colorings fixed by $g$.  If $g=r_0$, which is the identity, clearly every element of $X$ is fixed, so in this case the number is $n^4$.  If $g=r_1$, then a coloring of the square is fixed by $g$ if and only if all four vertices have the same color, as can be visualized easily.  In this case there are $n$ colorings \---- one for each available color.  Similarly, for $g=r_3$ the number of fixed colorings is $n$.  But for $g=r_2$ (rotation by a half-turn), opposite vertices get swapped, and so a coloring is fixed whenever each pair of opposite vertices is in one color.  The pairs need not have the same color as each other.  Since there are two choices of color, there are $n^2$ colorings fixed by $r_2$.

$v$ swaps the top two vertices and swaps the bottom two vertices; thus there are $n^2$ colorings fixed by $v$, because these are the colorings where the top two vertices have the same color and the bottom two vertices have the same color.  Similarly $h$ fixes $n^2$ of the colorings in $X$.  However, $t$ fixes $n^3$ of them, because $t$ swaps two vertices and leaves the other two fixed; the latter two vertices can have any colors, yet the two swapped vertices must have the same color, in order for the coloring to be fixed by $t$.  Similarly, $d$ fixes $n^3$ of the elements of $X$.

Thus, the number of orbits of the action is equal to
\begin{align*}
\frac 1{|D_4|}\sum_{g\in D_4}|\{x\in X:g\cdot x=x\}|&=\frac 18(\overset{\begin{matrix}r_0\\\downarrow\end{matrix}}{n^4}
+\overset{\begin{matrix}r_1\\\downarrow\end{matrix}}{n}
+\overset{\begin{matrix}r_2\\\downarrow\end{matrix}}{n^2}
+\overset{\begin{matrix}r_3\\\downarrow\end{matrix}}{n}
+\overset{\begin{matrix}v\\\downarrow\end{matrix}}{n^2}
+\overset{\begin{matrix}t\\\downarrow\end{matrix}}{n^3}
+\overset{\begin{matrix}h\\\downarrow\end{matrix}}{n^2}
+\overset{\begin{matrix}d\\\downarrow\end{matrix}}{n^3})\\&=\frac{n^4+2n^3+3n^2+2n}8
\end{align*}
There are $\frac{n^4+2n^3+3n^2+2n}8$ ways to color the vertices of the square.  Note that one can use pure number theory (not Burnside's Lemma) to show that this expression is an integer whenever $n$ is an integer.  Here is a table of this expression for the first few small values of $n$.
\begin{center}
\begin{tabular}{c|c}
Number of colors & Number of colorings\\\hline
$1$ & $1$\\
$2$ & $6$\\
$3$ & $21$\\
$4$ & $55$\\
$5$ & $120$\\
$6$ & $231$\\
$7$ & $406$
\end{tabular}
\end{center}
For example, if $n=1$, there is only one way to color the square because there is only one color that can be used.  If $n=2$, there are six colorings of the square, as shown below:
\begin{center}\includegraphics[scale=.3]{Colorings.png}\end{center}
This same tactic can be used to answer various similar combinatorical questions.  See Exercises 16-17, as well as Exercise 8 of Section 2.7.

\subsection*{Exercises 1.6. (Group Actions)}
\begin{enumerate}
\item Let $G$ be a group acting on a set $X$.

(a) If $x\in X$ and $y\in\mathcal O_X(x)$, then $\operatorname{Stab}(y)$ and $\operatorname{Stab}(x)$ are conjugate subgroups.

(b) Every conjugate subgroup of $\operatorname{Stab}(x)$ is of the form $\operatorname{Stab}(y)$ for some $y\in\mathcal O_X(x)$.

\item Suppose $G$ is a group and $X$ and $Y$ are two sets on which $G$ acts.  A \textbf{$G$-equivariant map} is a function $\varphi:X\to Y$ such that for all $g\in G,x\in X$, $\varphi(g\cdot x)=g\cdot\varphi(x)$.

(a) If $\varphi:X\to Y$ is a $G$-equivariant map and $x\in X$, then $\varphi(\mathcal O_X(x))=\mathcal O_Y(\varphi(x))$, and hence, $\varphi$ maps orbits to orbits.

(b) If $\varphi:X\to Y$ is a $G$-equivariant map and $x\in X$, then $\operatorname{Stab}(x)\subset\operatorname{Stab}(\varphi(x))$, but it may be a proper inclusion.

(c) If $\varphi,\psi:X\to Y$ are $G$-equivariant maps, then $\{x\in X:\varphi(x)=\psi(x)\}$ is a union of orbits.

(d) Now suppose $X'\subset X$ and $\eta:X'\to Y$ is a function.  Furthermore, suppose that:

~~~~(i) For all $x\in X$, there exists $g\in G$ such that $g\cdot x\in X'$;

~~~~(ii) Whenever $x,y\in X'$ and $y=g\cdot x$, $\eta(y)=g\cdot\eta(x)$.

Then $\eta$ extends to a unique $G$-equivariant map $X\to Y$.

(e) An \textbf{isomorphism} (of $G$-sets) $X\to Y$ is a bijective $G$-equivariant map.  If $G$ acts transitively on $X$, show that there is a subgroup $H$ and an isomorphism between $X$ and the set $G/H$ of left cosets of $H$ on which $G$ acts by left multiplication.  [Let $H$ be the stabilizer of some element of $X$.]

\item Complete this inductive proof that $|S_n|=n!$.  The statement is clear for $n=1$.  Now suppose inductively that $|S_{n-1}|=(n-1)!$.  Then $S_n$ acts transitively on the set $\{1,2,\dots,n\}$ in the obvious way.  Show that the stabilizer of the element $n$ is isomorphic to $S_{n-1}$.  Now use the Orbit-Stabilizer Theorem.

\item Suppose a group $G$ acts on a set $X$.

(a) Let $N=\bigcap_{x\in X}\operatorname{Stab}(x)=\{g\in G:g\cdot x=x\text{ for all }x\in X\}$.  Show that $N$ is a normal subgroup of $G$.

(b) Show that $G/N$ acts on $X$ via $gN\cdot x=g\cdot x$.

(c) More generally, let $\mathcal O$ be an orbit of the action.  Show that $\bigcap_{x\in\mathcal O}\operatorname{Stab}(x)$ is a normal subgroup of $G$.

(d) If $N$ is an \emph{abelian} normal subgroup of $G$, show that $G/N$ acts on $N$ by conjugation.

\item\emph{(Factorial formula for binomial coefficients.)} \---- Let $0\leqslant k\leqslant n$ be integers, and $X$ the set of $k$-element subsets of $\{1,2,\dots,n\}$.  Then $|X|={n\choose k}$ by definition.  Let $S_n$ act on $X$ via $\sigma\cdot\{a_1,\dots,a_k\}=\{\sigma(a_1),\dots,\sigma(a_k)\}$ for $\sigma\in S_n$ and $a_1,\dots,a_k$ distinct elements of $\{1,2,\dots,n\}$.

(a) Show that the action is transitive.

(b) Show that the stabilizer of the set $\{1,2,\dots,k\}$ is isomorphic to $S_k\times S_{n-k}$.

(c) Conclude that $|X|=\frac{n!}{k!(n-k)!}$.

\item\emph{(Another proof of Lagrange's Theorem.)} \---- Let $G$ be a finite group, and $H$ a subgroup of $G$.  Then $H$ acts on $G$ by left multiplication.

(a) Explain why the stabilizer of any $x\in G$ is trivial.

(b) Show that $|G|=n|H|$, where $n$ is the number of orbits of the action.  Conclude that $|H|$ divides $|G|$.

\item Suppose $|G|=p^n$ where $p$ is prime.  If $X$ is a finite set on which $G$ acts, and $X_0\subset X$ is the set of fixed points of the action, then $|X|\equiv|X_0|\pmod p$.  [What are all possible sizes of orbits in the action?]

\item Suppose $|G|=p^n$ where $p$ is prime and $n>0$.  Then $Z(G)\ne 1$.  [$G$ acts on itself by conjugation.]

\item\emph{(Wilson's Theorem.)} \---- If $p$ is prime, prove that $(p-1)!\equiv -1\pmod p$.  [$\mathbb Z/2\mathbb Z$ acts on the set $(\mathbb Z/p\mathbb Z)^\times=\{\overline 1,\overline 2,\dots,\overline{p-1}\}$ by the involution $x\mapsto x^{-1}$.  The product of any orbit of size $2$ is equal to $1$ (why?), so the product of the elements of $(\mathbb Z/p\mathbb Z)^\times$ is equal to the product of the involution's fixed points.]

\item\emph{(Cauchy's Theorem.)} \---- Let $G$ be a finite group, and $p$ a prime divisor of $|G|$.  We show that $G$ contains an element of order $p$.

(a) Let $X=\{(a_1,a_2,\dots,a_p):a_1,\dots,a_p\in G,a_1\dots a_p=1\}$.  Show that $|X|=|G|^{p-1}$, hence is divisible by $p$.

(b) If $(a_1,a_2,\dots,a_p)\in X$, then $(a_2,\dots,a_p,a_1)\in X$.  (In other words, $X$ is closed under ``rotation'' of the tuple.)

(c) The cyclic group $\mathbb Z/p\mathbb Z$ acts on $X$ via rotations, i.e., if $k\in\mathbb Z/p\mathbb Z$ then $k\cdot(a_1,a_2,\dots,a_p)=(a_{k+1},\dots,a_p,a_1,\dots,a_k)$.

(d) Conclude that $G$ has an element of order $p$.  [$(1,1,\dots,1)\in X$ is a fixed point of the action.  By Exercise 7, there must be another fixed point.]

\item\emph{(Class equation.)} \---- If $G$ is a finite group, show that one has
$$|G|=|Z(G)|+[G:C(a_1)]+\dots+[G:C(a_m)],$$
where $Z(G)$ is the center of $G$, and each $a_i\in G-Z(G)$, so that $C(a_i)$, the centralizer of $a_i$ (from Exercise 11(b) of Section 1.2), is a proper subgroup.
[$G$ acts on itself by conjugation.  Think of $|G|$ as the sum of the sizes of the orbits.  Then use the Orbit-Stabilizer Theorem.]

\item\emph{(Sylow Theorems.)} \---- Let $G$ be a finite group.

(a) If $p$ is prime and $p^n$ divides $|G|$, then $G$ has a subgroup of order $p^n$.  [Use complete induction on $|G|$.  Use Exercise 11 to write $|G|=|Z(G)|+[G:C(a_1)]+\dots+[G:C(a_m)]$.  If there exists $i$ such that $p$ does not divide $[G:C(a_i)]$, then $p^n$ divides $|C(a_i)|$.  Thus by induction, $C(a_i)$ has a subgroup of order $p^n$.  Contrariwise, if $p$ divides every $[G:C(a_i)]$, then $p$ divides $|Z(G)|=|G|-\sum_{i=1}^n[G:C(a_i)]$.  By Cauchy's Theorem, $Z(G)$ has an element $a$ of order $p$.  Since $a$ is central, $\left<a\right>$ is a normal subgroup, and $G/\left<a\right>$ has a subgroup of order $p^{n-1}$ by induction; now take the inverse image of that subgroup through the homomorphism $G\to G/\left<a\right>$.]

If $p$ is prime, a \textbf{Sylow $p$-subgroup} of $G$ is defined to be a subgroup whose order is the largest power of $p$ dividing $G$.  In other words, if $|G|=p^nm$ with $p\nmid m$, a Sylow $p$-subgroup is a subgroup of order $p^n$.  Note that by part (a), $G$ has at least one Sylow $p$-subgroup.

(b) If $Q$ is a Sylow $p$-subgroup, $|x|=p^k$ and $xQx^{-1}=Q$, then $x\in Q$. [The hypothesis $xQx^{-1}=Q$ states that $x\in N(Q)$, where $N(Q)$ is the normalizer of $Q$.  Since $Q$ is a normal subgroup of $N(Q)$, $N(Q)/Q$ is a well-defined group.  Explain why $p\nmid|N(Q)/Q|$ and the order of $xQ\in N(Q)/Q$ is a power of $p$; then use Lagrange's Theorem.]

(c) Any two Sylow $p$-subgroups of $G$ are conjugate.  [If $P$ and $Q$ are Sylow $p$-subgroups, then let $G$ act on the set $X$ of conjugates of $Q$ by conjugation.  Then the Orbit-Stabilizer Theorem implies that $|X|=[G:\operatorname{Stab}(Q)]\mid[G:\operatorname{Stab}(Q)][\operatorname{Stab}(Q):Q]=[G:Q]=m$.  Therefore $p$ does not divide $|X|$.  If $P$ acts on $X$ by conjugation, then Exercise 7 implies that the action has at least one fixed point.  Now use part (b) to show that the fixed point is equal to $P$.]

(d) The number of Sylow $p$-subgroups divides $|G|$ and is $\equiv 1\pmod p$.  [By part (a) there exists a Sylow $p$-subgroup $P$.  Now, if $X$ is the set of conjugates of $P$, then by part (c) $X$ is the set of all Sylow $p$-subgroups.  If $G$ acts on $X$ by conjugation, then $|X|=[G:\operatorname{Stab}(P)]$ by the Orbit-Stabilizer Theorem, hence $|X|$ divides $|G|$.  Moreover, if $P$ acts on $X$ by conjugation, part (b) shows that the only fixed point of the action is $P$; now use Exercise 7.]

\item\emph{(Symmetric and Alternating Groups.)} \---- Let $X$ be a finite set, $|X|\geqslant 2$.  If $x_1,x_2,\dots,x_n\in X$ are distinct elements, then the permutation in $S(X)$ given by
$$x_k\mapsto x_{k+1},1\leqslant k<n,~~~~~~~~x_n\mapsto x_1,~~~~~~~~x\mapsto x\text{ for }x\notin\{x_1,\dots,x_n\}$$
is denoted $(x_1x_2\dots x_n)$ and is called an \textbf{$n$-cycle}.\footnote{$(x_1x_2\dots x_n)$ is the same permutation as $(x_2\dots x_nx_1)$.  In the notation, we will allow cycles to rotate without being considered different from what they were before.}  For example, if $X=\{1,2,3,4,5,6\}$, then $(1325)$ is the permutation
$$\begin{pmatrix}1&2&3&4&5&6\\3&5&2&4&1&6\end{pmatrix}$$
(Observe that any $1$-cycle is the identity permutation on one element.  We drop these from the notation.)  Two cycles $(x_1x_2\dots x_n)$ and $(y_1y_2\dots y_m)$ are said to be \textbf{disjoint} if $x_i\ne y_j$ for all $1\le i\le n,1\le j\le m$.

(a) Show that if $\sigma$ and $\tau$ are disjoint cycles then $\sigma\tau=\tau\sigma$.

(b) Show that every $\sigma\in S(X)$ is a product of disjoint cycles in a unique way.  [$S(X)$ acts on $X$ in the obvious way.  Now restrict this action to the cyclic subgroup $\left<\sigma\right>$.  Each orbit then gives rise to a cycle.]

(c) A 2-cycle is called a \textbf{transposition}.  Thus a transposition is a permutation of the form $(x_1x_2)$ (with $x_1,x_2\in X$ distinct), which sends $x_1\mapsto x_2$, $x_2\mapsto x_1$ and $x\mapsto x$ for $x\ne x_1,x_2$.  Show that every $\sigma\in S(X)$ is a product of transpositions.  [Verify that $(x_1x_2\dots x_n)=(x_1x_n)(x_1x_{n-1})\dots(x_1x_2)$.  Now use part (b).]

(d) Show that $1_X$ cannot be written as the product of an odd number of transpositions.  [Suppose $1_X=\tau_{2k+1}\dots\tau_2\tau_1$, where the $\tau_i$ are transpositions and $k$ is as small as possible.  Clearly $k$ can't be zero.  Now let $c$ be any element of $X$ which occurs somewhere in these transpositions.  Consider the \emph{rightmost} transposition $\tau_r$ in which $c$ occurs; i.e., $\tau_r=(cd)$ and $c$ does not occur in $\tau_1,\dots,\tau_{r-1}$.  If $r=2k+1$, we would have $1_X(c)=d$, which is impossible since $d\ne c$.  Thus we must have $r<2k+1$, and depending on whether $c$ or $d$ or both is in $\tau_{r+1}$, the pair $\tau_{r+1}\tau_r$ must look like one of these (with $x,y,c,d$ distinct elements of $X$):
$$(xy)(cd)~~~~~~~~(xd)(cd)~~~~~~~~(cy)(cd)~~~~~~~~(cd)(cd)$$
Verify that $(xy)(cd)=(cd)(xy)$, $(xd)(cd)=(xc)(xd)$, and $(cy)(cd)=(cd)(dy)$.  Thus in any of the first three cases, you can substitute the pair of transpositions to move the rightmost transposition in which $c$ occurs one transposition to the left.  You can repeat this process as long as the rightmost transposition with $c$ satisfies either of the first three cases.  Eventually the fourth case must be reached, otherwise we would reach a product where the rightmost transposition involving $c$ is at the very left, yielding the same impossibility as before.  However in the fourth case, since $(cd)(cd)=1_X$, the two transpositions can be deleted, yielding $1_X$ as the product of $2k-1=2(k-1)+1$ transpositions.  This contradicts that $k$ is as small as possible.]

(e) Show that no $\sigma\in S(X)$ can be written both as the product of an even number of transpositions, and as the product of an odd number of transpositions.  [If $\sigma=\tau_{2j}\dots\tau_2\tau_1$ and $\sigma=\tau'_{2k+1}\dots\tau'_2\tau'_1$, then $1_X=\tau_1\tau_2\dots\tau_{2j}\tau'_{2k+1}\dots\tau'_2\tau'_1$: this contradicts part (d).]

(f) Define a permutation in $S(X)$ to be \textbf{even} if it is the product of an even number of transpositions, and \textbf{odd} if it is the product of an odd number of transpositions.  By (c) every element of $S(X)$ is either even or odd, and by (e) no element of $S(X)$ is both even and odd.  Define $s:S(X)\to\mathbb Z/2\mathbb Z$ via $s(\sigma)=0$ if $\sigma$ is even and $1$ if $\sigma$ is odd.  Show that $s$ is a surjective homomorphism.  The kernel of $s$ is denoted $A(X)$ and is called the \textbf{alternating group with respect to $X$}.

If $X=\{1,2,\dots,n\}$, then $A(X)$ is alternatively denoted $A_n$, just as $S(X)$ is denoted $S_n$.

(g) $A(X)$ is generated by the $3$-cycles.  [Every element of $A(X)$ is the product of an even number of transpositions.  Now verify that $(ab)(ab)=1_X$, $(ab)(ac)=(acb)$ and $(ab)(cd)=(adb)(adc)$.]

(h) $A(X)$ is the only subgroup of $S(X)$ of index $2$.  [If $N\subset S(X)$ is a subgroup of index $2$, then Exercise 2 of Section 1.5 shows that $\sigma^2\in N$ for all $\sigma\in S(X)$.  Thus $N$ contains all the $3$-cycles (why?).  Use part (g).]

\item Recall that a group $G$ is \textbf{solvable} if there exists a chain $G=G_0\supset G_1\supset\dots\supset G_t=\{1\}$ such that for each $0\leqslant i<t$, $G_{i+1}$ is normal in $G_i$ and $G_i/G_{i+1}$ is abelian.  [Exercise 11 of Section 1.5.]

For $n\geqslant 5$, show that $S_n$ is not solvable.  [Suppose $S_n=G_0\supset G_1\supset\dots\supset G_t=\{1\}$ is a chain satisfying the above requirements.  Then $aba^{-1}b^{-1}\in G_{k+1}$ for all $a,b\in G_k$, by Exercise 10(c) of Section 1.5.  If $(abc)$ is any $3$-cycle, then there exist $u,v\in\{1,\dots,n\}$ other than $a,b,c$ (because $n\geqslant 5$), and it can be verified that
$$(auc)(cbv)(auc)^{-1}(cbv)^{-1}=(abc)$$
Thus it follows from induction that $G_k$ contains all the $3$-cycles for all $k\geqslant 0$.  In particular, $G_t=\{1\}$ contains all the $3$-cycles: contradiction.]

\item If Lagrange's Theorem (1.8) had a converse, it would probably go like this: ``If $G$ is a finite group and $n$ is a divisor of $|G|$, then $G$ has a subgroup of order $n$.''  Show that this is false by showing that $A_4$ is a group of order $12$ with no subgroup of order $6$.  [If $N\subset A_4$ and $|N|=6$, then $[A_4:N]=2$.  Hence $\sigma^2\in N$ for all $\sigma\in A_4$.  But does $N$ have enough room for all these elements?]

\item (a) Let $n$ be a positive integer.  If there are $n$ available colors, then how many ways are there to color the six vertices of a regular hexagon, if rotating and flipping the hexagon is not considered to change the coloring?

(b) If $p$ is prime and there are $n$ available colors, then how many ways are there to color the vertices of a regular $p$-gon?

\item An \textbf{(undirected) graph} is a pair $(V,E)$ where $V$ is a finite set, and $E$ is an irreflexive, symmetric relation on $V$.  (In other words $(x,x)\notin E$ for all $x\in V$, but if $(x,y)\in E$ then $(y,x)\in E$.)  $V$ is the set of \textbf{vertices} and $E$ is the set of \textbf{edges}.

Two graphs $(V,E)$ and $(V',E')$ are said to be \textbf{isomorphic} if there is a bijection $f:V\to V'$ such that for $x,y\in V$, $(x,y)\in E$ if and only if $(f(x),f(y))\in E'$.

(a) How many nonisomorphic graphs with $4$ vertices are there?  [Verify that if $V$ is a given set with $|V|=4$, then $S(V)$ acts on the set of graphs of the form $(V,E)$.  To avoid having to do $4!=24$ separate additions, partition $S(V)$ into its conjugacy classes (i.e., orbits when it acts on itself by conjugation).  Verify that two permutations in $S(V)$ are conjugate if and only if, when writing one of them as a product of disjoint cycles, the labels can be replaced with other elements of $V$ to get the other permutation.  Then use Burnside's lemma.]

(b) How many nonisomorphic graphs with $5$ vertices are there?

(c) How many nonisomorphic graphs with $6$ vertices are there?

%(c) Define a \textbf{directed graph} to be a pair $(V,E)$ where $V$ is a finite set, and $E$ is an irreflexive relation on $V$ which may not be symmetric.  Now answer parts (a) and (b) with ``directed graph'' in place of ``graph.''
\end{enumerate}







































\chapter{Euclidean Geometry}

Euclidean geometry is the first kind of geometry to have been discovered.  Geometric facts were discovered by people throughout the world over thousands of years.  The Alexandrian Greek mathematician Euclid collected some of these facts into an influential series of books around 300 B.C.  Euclid's method consisted of taking some axioms and using them to prove statements.  We will present the axioms in Section 2.1.  Much later, however, several other kinds of geometry were discovered.  Like Euclid's, they are logically self-consistent.  We will study all these geometries by using algebra, rather than merely taking axioms for them.  Thus from the second section of this chapter onwards, we will deal with Euclidean space algebraically.  It will be clear how this geometry connects with the axiomatic one described at first.

\subsection*{2.1. Axiomatic Plane Geometry}
\addcontentsline{toc}{section}{2.1. Axiomatic Plane Geometry}
When mathematicians mention the ``plane,'' we naturally think of the coordinate plane: two axes, $x$ and $y$, and a swarm of points around them, $\mathbb R^2$.  However, a true geometer does not actually use the concrete plane; in fact, Euclid used \emph{no} axes, no $x$ and $y$, and not even a fixed unit of length.  An axiomatic system is used.

Imagine picking up some whipped cream from the ice cream parlor, spreading it out on a table, and enforcing certain portions of the whipped cream to be ``lines.''  Suppose you are also the boss of the whipped cream and are giving the lines some rules to follow; for example, no two distinct lines can intersect more than once.  With enough rules, you are in the setting of axiomatic plane geometry, even though you did not actually start with a plane.

Thus we imagine a set where elements are called ``points,'' and certain subsets are called ``lines.''  Unlike usual mathematics, we shall denote points by the \emph{capital} letters $A$, $B$, $C$, etc.; this is a standard in plane geometry.  We shall list axioms and use them to prove basic propositions.

The following axioms revolve around lines, line segments and rays.  There are many ways to give axioms for Euclidean geometry.  In the late 1800s, several mathematicians proposed axioms that were more complete than Euclid's (such as Hilbert).  One system will be given here.\\

\noindent\textbf{Axiom 2.1.}

(i) \emph{Two points determine a line; i.e., given any two points $A\ne B$ there is a unique line passing through both $A$ and $B$.  We refer to this line as $\overset{\longleftrightarrow}{AB}$.}

(ii) \emph{If $A,B,C$ are points on a common line, either $C$ is \textbf{between} $A$ and $B$ or it isn't.  The set of points between $A$ and $B$ is denoted $\overline{AB}$.  Moreover, (a) $A,B\in\overline{AB}\subset\overset{\longleftrightarrow}{AB}$; (b) for any points $C$ and $D$ in $\overline{AB}$ we have $\overline{CD}\subset\overline{AB}$; (c) whenever $A,B,C$ are distinct points on the same line, exactly one of them is between the other two; (d) if $A$ and $B$ are in a line $\ell$ then there exists a point $C\in\ell$ such that $C\ne A$ and $A$ is between $C$ and $B$; (e) $\overline{AA}=\{A\}$; and (f) when $B$ is between $A$ and $C$, $\overline{AC}=\overline{AB}\cup\overline{BC}$.}

\emph{If $A\ne B$, then $\overline{AB}$ is called the \textbf{line segment} from $A$ to $B$.  Note that each line segment is contained in a unique line; by (i), there is a unique line containing $A$ and $B$, and by (ii) (a), it contains the whole segment.}

(iii) \emph{Two points $A,B$ also determine a \textbf{ray}, the set of points $C\in\overset{\longleftrightarrow}{AB}$ such that either $C$ is between $A$ and $B$, or else $B$ is between $A$ and $C$.  This ray is denoted $\overset{\longrightarrow}{AB}$.}\\

\noindent The three concepts are intuitively illustrated in the diagram below.  (The arrows indicate indefinite continuation of the stroke.)
\begin{center}\includegraphics[scale=.6]{LineSegmentRay.png}\end{center} % I guess I'll make it bigger, these revisions will wreck the paging anyway...
Note that if $A$ and $B$ are points, then $\overset{\longleftrightarrow}{AB}=\overset{\longleftrightarrow}{BA}$ and $\overline{AB}=\overline{BA}$; however, $\overset{\longrightarrow}{AB}\ne\overset{\longrightarrow}{BA}$.  After all, by (d) of Axiom 2.1(ii) there exists $C$ such that $B$ is strictly in between $A$ and $C$, and then $C\in\overset{\longrightarrow}{AB}$ but $C\notin\overset{\longrightarrow}{BA}$.

There are several important consequences of (ii).  First, if $A,B,C$ are points such that $A$ is between $B$ and $C$, and $B$ is between $A$ and $C$, then $A=B$.  Indeed, $A,B,C$ cannot all be distinct (if they were, then by part (c) of (ii), no more than one of $A,B,C$ would be between the other two), hence either $A=B$, $B=C$ or $A=C$.  If $B=C$, then $A$ would be between $C$ and $C$, hence $A\in\overline{CC}=\{C\}$ (by (e)) and hence $A=C=B$.  Similarly, $A=C$ implies $A=B$; thus we have $A=B$ in all three cases.

It further follows that if $A,B,C$ are points on a line with $B$ between $A$ and $C$, $\overline{AB}\cap\overline{BC}$ consists of only the point $B$.  After all if $D\in\overline{AB}\cap\overline{BC}$, then $D$ is between $A$ and $B$, but also $D$ is between $B$ and $C$.  Since $D\in\overline{AB}\subset\overline{AC}$ (by part (b) of (ii)), $D$ is between $A$ and $C$.  Moreover, $B\in\overline{AC}=\overline{AD}\cup\overline{DC}$ (by (f)), so either $B\in\overline{AD}$ or $B\in\overline{DC}$.  If $B\in\overline{AD}$, then $B$ is between $A$ and $D$, and $D$ is also between $A$ and $B$; therefore $B=D$ by the argument of the preceding paragraph.  Similarly if $B$ is between $D$ and $C$ then $B=D$.  Thus $D=B$ follows from the assumption that $D\in\overline{AB}\cap\overline{BC}$, which means that set consists of only $B$.

Finally, the points $A,B$ can be recovered from the line segment $\overline{AB}$.  After all, suppose $\overline{AB}=\overline{A'B'}$; then it is clear that $A,B,A',B'$ all lie on the same straight line.  Since $B'$ is between $A$ and $B$, we have by (f) of Axiom 2.1(ii) that $B'\in\overline{AA'}\cup\overline{A'B}$.  If $B'\in\overline{AA'}$ then $B$ is between $A$ and $A'$, but also $A'$ is between $A$ and $B$; therefore we have $A'=B$.  Moreover, $\overline{AB}=\overline{BB'}$; this jointly implies that $A$ is between $B$ and $B'$ and that $B'$ is between $B$ and $A$, and therefore $A=B'$.  Hence in this case, the (unordered) set $\{A,B\}$ of points used to define $\overline{AB}$ coincides with $\{A',B'\}$.  Similarly, one can show that $B'\in\overline{A'B}$ implies that $B=B'$ and $A=A'$.  Thus we have proven\\

\noindent\textbf{Proposition 2.2 and Definition.} \emph{If $A,B$ are points, the set $\{A,B\}$ is determined by the line segment $\overline{AB}$.  These points $A$ and $B$ are called the \textbf{endpoints} of $\overline{AB}$.  If $A=B$ (which implies $\overline{AB}=\{A\}$ by (e)), the segment is said to be \textbf{degenerate}; otherwise it is called \textbf{nondegenerate}.}\footnote{Unless otherwise specified we shall always assume line segments to be nondegenerate.}\\

\noindent Now we prove basic facts about rays: first we claim that if $B$ is between $A$ and $C$, $\overset{\longrightarrow}{AB}=\overset{\longrightarrow}{AC}$.  For suppose $D\in\overset{\longrightarrow}{AB}$.  Then either $D$ is between $A$ and $B$ (in which case $D\in\overline{AB}\subset\overline{AC}\subset\overset{\longrightarrow}{AC}$), or else $B$ is between $A$ and $D$ (in this case, if $A$ is between $C$ and $D$ then $B\in\overline{AD}\cap\overline{AC}=\{A\}$ a contradiction; hence either $C$ is between $A,D$ or $D$ is between $A,C$, implying $D\in\overset{\longrightarrow}{AC}$).  This proves that $\overset{\longrightarrow}{AB}\subset\overset{\longrightarrow}{AC}$.  Conversely, if $E\in\overset{\longrightarrow}{AC}$, then either $C$ is between $A$ and $E$ (in which case $B\in\overline{AC}\subset\overline{AE}$ and $B$ is between $A$ and $E$), or else $E$ is between $A$ and $C$ (in this case $E\in\overline{AB}\cup\overline{BC}\subset\overset{\longrightarrow}{AB}\cup\overline{BC}$ \---- and $E\in\overline{BC}$ implies $B\in\overline{AC}=\overline{AE}\cup\overline{EC}$ and hence $B\in\overline{AE}$ because $B\in\overline{EC}$ implies $B=E$); therefore, $E\in\overset{\longrightarrow}{AB}$.

Now if $\overset{\longrightarrow}{AB}=\overset{\longrightarrow}{A'B'}$ (with $A\ne B,A'\ne B'$), then we have $A=A'$, but (by the preceding paragraph) it does \emph{not} necessarily follow that $B=B'$.  For, suppose on the contrary that $A\ne A'$.  Then since $A'\in\overset{\longrightarrow}{AB}$, then the preceding paragraph entails $\overset{\longrightarrow}{AB}=\overset{\longrightarrow}{AA'}$ (because either $A'$ is between $A,B$ or $B$ is between $A,A'$, and the claim can be applied to either case).  Similarly, since $A\in\overset{\longrightarrow}{A'B'}$ we have $\overset{\longrightarrow}{A'B'}=\overset{\longrightarrow}{A'A}$.  Therefore $\overset{\longrightarrow}{AA'}=\overset{\longrightarrow}{A'A}$, which contradicts what we have established in the first paragraph after the statement of Axiom 2.1.  Hence\\

\noindent\textbf{Proposition 2.3 and Definition.} \emph{If $A\ne B$ are points, then the point $A$ is completely determined by the ray $\overset{\longrightarrow}{AB}$.  $A$ is called the \textbf{endpoint} of $\overset{\longrightarrow}{AB}$.}\\

\noindent Note that a ray has only one endpoint, even though a line segment has two (and a line has none).\\

\noindent\textbf{LENGTH OF LINE SEGMENTS}\\

\noindent For any line segment $\overline{AB}$ we shall associate a real number, which we denote $AB$ and call the \textbf{length} of $AB$.  We also call this real number the \textbf{distance} between the points $A$ and $B$.  The following axioms will be posed for this notion.\\

\noindent\textbf{Axiom 2.4.} (i) \emph{If $\rho$ is a ray with endpoint $A$, and $r$ is a positive real number, there exists a point $B$ on $\rho$ such that $AB=r$.}

(ii) \textsc{(Segment Addition Postulate)} \emph{If $B$ is between $A$ and $C$, then $AC=AB+BC$.}

(iii) \emph{If $A\ne B$ then $AB>0$.}\\

The first part guarantees that all real numbers are possible distances.  The second is a well-known axiom in plane geometry: your intuition most likely caught it at once, even though it does not logically follow from anything else we have stated.  The third does the basic job of saying that two distinct points have positive distance from one another.

Note that the point $B$ in part (i) is unique: if $B$ and $B'$ are both points on $\rho$ such that $AB=AB'=r$, then (since $B'\in\overset{\longrightarrow}{AB}$), either $B'$ is between $A$ and $B$, or else $B$ is between $A$ and $B'$.  In the former case, part (ii) entails $AB=AB'+B'B$; i.e., $r=r+B'B$, which implies $B'B=0$.  By part (iii) we conclude $B'=B$.  The same argument works if $B$ is between $A$ and $B'$.

Also, applying part (ii) with $A=B=C$ entails that $AA=0$ for any point $A$.  Combining this with (iii), we get that \emph{a line segment is degenerate if and only if its length is zero}.

We say that two line segments $\overline{AB}$ and $\overline{CD}$ are \textbf{congruent} (denoted $\overline{AB}\cong\overline{CD}$) if $AB=CD$.  This is manifestly an equivalence relation on the line segments.  Note, however, that lengths of rays and lines are \emph{not} defined (why?).

If $\overline{AB}$ is any (nondegenerate) line segment, let $r=\frac 12(AB)$.  Then by part (i), there is a point $M$ on $\overset{\longrightarrow}{AB}$ such that $AM=r$.  Furthermore, if $B$ were between $A$ and $M$, then we would have
$$r=AM=AB+BM\geqslant AB=2r$$
which is a contradiction because $r>0$.  Therefore, $M$ must be between $A$ and $B$ (by definition of the ray).  Furthermore, since $AB=2r$, the Segment Addition Postulate immediately entails that $MB=r$ as well.  Since $AM=MB=r$, we have $\overline{AM}\cong\overline{MB}$.  The reader can readily verify that $M$ is the \emph{unique} point on $\overline{AB}$ satisfying this condition.  This point is called the \textbf{midpoint} of the line segment $\overline{AB}$.\\

\noindent\textbf{PARALLEL POSTULATE}\\

\noindent Let $\ell_1$ and $\ell_2$ be two distinct lines.  Observe that Axiom 2.1(i) alone implies that $\ell_1$ and $\ell_2$ intersect in at \emph{most} one point: for if $A,B$ were two different points in $\ell_1\cap\ell_2$, then there would be multiple lines (such as $\ell_1,\ell_2$) passing through both $A$ and $B$, contradicting Axiom 2.1(i).  However, the lines may not intersect at all.

Lines $\ell_1$ and $\ell_2$ are said to be \textbf{parallel}, denoted $\ell_1\parallel\ell_2$, if they do not intersect.  Intuitively this implies that they go in exactly the same direction (due to their infinitude on both sides, if they went in even slightly different directions, they would eventually approach on one side).  The concept is illustrated below.
\begin{center}\includegraphics[scale=.4]{ParallelIntersecting.png}\end{center}
Thus, we find it believable that given any point not on a line $\ell$, there is exactly one line through the point that is parallel to $\ell$.\\

\noindent\textbf{Axiom 2.5.} \textsc{(Parallel Postulate)} \emph{Given a line $\ell$ and a point $A\notin\ell$, there exists a unique line through $A$ parallel to $\ell$.}\\

\noindent There is an interesting story regarding this axiom, where many mathematicians tried to prove it from the other axioms.  However, they failed to find any way to prove the parallel postulate without using any proposition that already depends on the parallel postulate.  After many years, their attempts to prove this statement led to the discovery of other types of geometry where this statement is not true.  Examples are projective, hyperbolic and spherical geometry, which will be covered in Chapters 3, 4 and 5 respectively. % I'm especially surprised this "revision" is in red: I gave examples, didn't need to give *all* of them.

If $\ell_1$ and $\ell_2$ are not parallel, however, their intersection consists of exactly one point $A$.  In this case, we have (set theoretically) $\ell_1\cap\ell_2=\{A\}$.  By abuse of notation, we shall allow ourselves to say $\ell_1\cap\ell_2=A$, and that $A$ is the intersection of the lines $\ell_1,\ell_2$.\\

\noindent\textbf{ANGLES}\\

\noindent Notice how despite a ray having a unique endpoint, there are generally many rays with a given endpoint (they can point in many directions).  We now introduce an important concept which revolves around this.\\

\noindent\textbf{Definition.} \emph{An \textbf{angle} is an unordered pair of two rays with the same endpoint.  If $A,B,C$ are points with $A\ne B,C$, the angle made up of $\overset{\longrightarrow}{AB}$ and $\overset{\longrightarrow}{AC}$ is denoted $\angle BAC$, or just $\angle A$ if the context is clear.}\\

\noindent Here is an illustration of $\angle BAC$:
\begin{center}\includegraphics[scale=.4]{AngleBAC.png}\end{center}
In the notation $\angle BAC$, it is crucial that the common endpoint (which in this case is $A$) is written in between the other two endpoints.  The angle $\angle ABC$ is \emph{not} the same as $\angle BAC$: instead, it is the angle made up of rays $\overset{\longrightarrow}{BA}$ and $\overset{\longrightarrow}{BC}$.  However, $\angle CAB=\angle BAC$ (why?).

A point $D$ is said to be \textbf{inside} $\angle BAC$ if there exists a line $\ell$ through $D$ such that: (a) $\ell$ intersects $\overset{\longrightarrow}{AB}$ in a point $E$, (b) $\ell$ intersects $\overset{\longrightarrow}{AC}$ in a point $F$, and (c) $D$ is between $E$ and $F$.  Note that this is impossible if $A,B,C$ all lie on one line $\ell_1$ and $D$ does not lie on this line: in this case, $E$ and $F$ would have two distinct lines going through them (namely, $\ell_1$ and $\ell$), contradicting Axiom 2.1(i).

Just like the length of a line segment, we shall associate with each angle a real number, which we call its \textbf{measure}.  We denote the measure of $\angle BAC$ as $m\angle BAC$, or just $m\angle A$ if the context is clear.  However, we will not usually write this as a real number.  After all, an angle is usually measured in radians in mathematics, but when studying pure geometry, people are usually more satisfied dealing with the \emph{degree} measures of the angles.  For example, there are $360$ degrees in a circle, but $2\pi$ radians.  Thus we equip ourselves with the following convention:
\begin{center}
\textbf{Throughout this book, for $\alpha\in\mathbb R$, $\alpha^\circ$ is defined to be $\frac{\alpha\pi}{180}$.  This is the (radian) measure of the angle which is $\alpha$ degrees.}
\end{center}
For example, $180^\circ=\pi$, and $90^\circ=\frac{\pi}2$.\\

\noindent\textbf{Axiom 2.6.} (i) \emph{If $\overset{\longrightarrow}{AB}$ is a ray, and $\alpha$ is a real number with $0^\circ<\alpha<180^\circ$, there exist two distinct rays $\overset{\longrightarrow}{AC},\overset{\longrightarrow}{AC'}$ such that $m\angle BAC=m\angle BAC'=\alpha$.}

(ii) \emph{If $\overset{\longrightarrow}{AB}\ne\overset{\longrightarrow}{AC}$ then $m\angle BAC>0$.}

(iii) \textsc{(Straight Angle)} \emph{If $A,B,C$ lie on one line with $A$ between $B$ and $C$, $m\angle BAC=180^\circ$.}

(iv) \textsc{(Angle Addition Postulate)} \emph{If either point $D$ is inside $\angle BAC$ or $A,B,C$ lie on one line with $A$ between $B$ and $C$, then $m\angle BAC=m\angle BAD+m\angle DAC$.}\\

\noindent Several remarks are in order.  Firstly, in part (i), the two rays can be thought of as being on either side of $\overset{\longrightarrow}{AB}$.  After all, if $C'$ were inside $\angle BAC$, we would have $m\angle BAC=m\angle BAC'+m\angle C'AC$ by (iv); i.e., $\alpha=\alpha+m\angle C'AC$, so that $m\angle C'AC=0$.  Therefore by (ii) $\overset{\longrightarrow}{AC}=\overset{\longrightarrow}{AC'}$, a contradiction.  Similarly, $C$ cannot be inside $\angle BAC'$.  Thus the insides of $\angle BAC$ and $\angle BAC'$ are non-overlapping, and are seen to be on ``opposite sides of $\overset{\longrightarrow}{AB}$.''

Secondly, if $A,B,C$ lie on one line with $A$ between $B$ and $C$ (as shown in the diagram below), $\angle BAD$ and $\angle DAC$ are said to form a \textbf{linear pair}.
\begin{center}\includegraphics[scale=.4]{LinearPair.png}\end{center}
In this case (iii) and (iv) jointly imply that $m\angle BAD+m\angle DAC=180^\circ$.  We say that two angles are \textbf{supplementary} if their measures add to $180^\circ$.  Thus, any two angles in a linear pair are supplementary.

Two angles $\angle BAC,\angle B'A'C'$ are said to be \textbf{congruent} if $m\angle BAC=m\angle B'A'C'$.  Again, this is an equivalence relation on the angles.

An angle $\angle BAC$ is said to be a \textbf{right angle} if $m\angle BAC=90^\circ$.  Note that this is equivalent to the angle being supplementary to itself.  Moreover, \emph{the two angles in a linear pair are congruent if and only if one of them is a right angle} (in which case the other one is as well).

Suppose $\ell_1$ and $\ell_2$ are intersecting lines with $\ell_1\cap\ell_2=A$.  Let $B$ and $C$ be points in $\ell_1$ with $A$ strictly between (they exist by (d) of Axiom 2.1(ii)).  Likewise, let $D$ and $E$ be points in $\ell_2$ with $A$ strictly between.  The angles $\angle DAC$ and $\angle BAE$, illustrated below, are called \textbf{vertical angles}.\footnote{The word ``vertical'' does not refer to a line being parallel to the $y$-axis in this context.  Rather, the angles are half of a full turn from one another, and ``vert-'' is a prefix for \emph{turn}.}
\begin{center}\includegraphics[scale=.4]{VerticalAngles.png}\end{center}
Moreover, since angles in a linear pair are supplementary, we have the following proposition\\

\noindent\textbf{Proposition 2.7.} \emph{Vertical angles are congruent.}\\
\begin{proof}
In the above diagram, $\angle BAD$ and $\angle DAC$ form a linear pair, and therefore $m\angle BAD+m\angle DAC=180^\circ$.  Similarly, since $\angle DAB$ and $\angle BAE$ form a linear pair (based on the line $\ell_2$), $m\angle DAB+m\angle BAE=180^\circ$.  Consequently
$$m\angle DAC=180^\circ-m\angle DAB=m\angle BAE$$
and $\angle DAC\cong\angle BAE$ as desired.
\end{proof}

\noindent If one (and hence all) of the angles in the diagram are right angles, the lines are said to be \textbf{perpendicular}, denoted $\ell_1\perp\ell_2$.  We pause now to give another postulate similar to Axiom 2.5.  This statement can actually be proved instead of being taken as an axiom, but the book will ignore this fact.\\ % The argument in the revisions would be great, except the triangle stuff hasn't been covered yet

\noindent\textbf{Proposition 2.8.} \textsc{(Perpendicular Postulate)} \emph{Given a line $\ell$ and a point $A$, there exists a unique line through $A$ perpendicular to $\ell$.}\\

\noindent Note that this is \emph{not} an immediate consequence of Axiom 2.6(i) because the point $A$ may not be on the line $\ell$.

If $D$ is inside $\angle BAC$ and $\angle BAD\cong\angle DAC$, the ray $\overset{\longrightarrow}{AD}$ is called an \textbf{angle bisector} of $\angle BAC$.  The existence and uniqueness of such a ray are left to the reader (Exercise 1).\\

\noindent\textbf{PARALLEL LINES AND TRANSVERSALS}\\

\noindent An important aspect of geometry revolves around a line going through two parallel lines.  They are illustrated below, with the horizontal lines parallel, and the lowercase Greek letters denoting the angles.
\begin{center}\includegraphics[scale=.4]{ParallelTransversal.png}\end{center}
There are several pieces of terminology associated with this diagram.s

* The slanted line is called a \textbf{transversal} going through the parallel lines.

* Angles $\alpha$ and $\zeta$ are called \textbf{corresponding angles}, since they are on the same side of both the transversal and the parallel lines, so they are intuitively pointing in the same direction in the plane.  ($\beta$ and $\theta$ are also corresponding angles.)

* Angles $\alpha$ and $\theta$ are called \textbf{corresponding exterior angles}, since they are both outside the parallel lines (``exterior''), yet they are on the same side of the transversal (hence ``corresponding'').

* Angles $\alpha$ and $\eta$ are called \textbf{alternate exterior angles}, since they are both outside the parallel lines, but on different sides of the transversal.

* Angles $\beta$ and $\zeta$ are called \textbf{corresponding interior angles}, and $\beta$ and $\gamma$ are called \textbf{alternate interior angles}, likewise.

A fundamental axiom which invovles parallel lines and their transversals is\\

\noindent\textbf{Axiom 2.9.} \emph{If $\ell_1\parallel\ell_2$ and $\ell_3$ is a transversal, then corresponding angles are congruent.}\\

\noindent The intuition is that $\ell_2$ has been obtained by translating line $\ell_1$ straight along $\ell_3$ without doing any bending or stretching.

Originally Euclid made Axiom 2.9 unnecessary, by giving the parallel postulate as: ``if a straight line falling on two straight lines make the interior angles on the same side less than two right angles, the two straight lines, if produced indefinitely, meet on that side on which the angles are less than two right angles.''  After all, if this axiom were placed and $\ell_1\parallel\ell_2$, the corresponding interior angles would have to sum to $\geqslant 180^\circ$ (otherwise $\ell_1$ and $\ell_2$ would intersect by the given axiom).  But the same is true for the corresponding interior angles on the \emph{other} side of the transversal.  Since linear pair angles are supplementary, all four of these angles together add to exactly $360^\circ$; hence each pair of corresponding interior angles adds to \emph{exactly} $180^\circ$; i.e., be supplementary.  It then follows that corresponding angles are congruent.

Going back to Axiom 2.9, let us prove:\\

\noindent\textbf{Proposition 2.10.} \emph{If $\ell_1,\ell_2$ are lines (not necessarily parallel) and $\ell_3$ is a transversal, the following statements are equivalent:}

(i) \emph{$\ell_1\parallel\ell_2$.}

(ii) \emph{Corresponding angles are congruent.}

(iii) \emph{Corresponding exterior angles are supplementary.}

(iv) \emph{Alternate exterior angles are congruent.}

(v) \emph{Corresponding interior angles are supplementary.}

(vi) \emph{Alternate interior angles are congruent.}
\begin{proof}
(i) $\implies$ (ii) is Axiom 2.9.

(ii) $\implies$ (i). Let $A=\ell_3\cap\ell_1$.  By Axiom 2.5, there is a unique line\----call it $\ell'$\----through $A$ parallel to $\ell_2$.  As $\ell'$ and $\ell_2$ are parallel lines with $\ell_3$ as a transversal, Axiom 2.9 entails that they have congruent corresponding angles.  Hence (by the hypothesis (ii)), the angle between $\ell'$ and $\ell_3$ facing in a particular direction is congruent to the angle between $\ell_1$ and $\ell_3$ facing in that direction (because they both correspond to the same angle between $\ell_2,\ell_3$).  Axiom 2.6(iv) implies that the angle between $\ell',\ell_1$ is zero, which yields $\ell'=\ell_1$ by Axiom 2.6(ii).  Therefore, $\ell_1$ (being $\ell'$) is parallel to $\ell_2$.

The equivalence of the five statements (ii) - (vi) easily follows from angles in linear pairs being supplementary.
\end{proof}

\noindent\textbf{TRIANGLES}\\

\noindent Triangles are a commonly discussed concept in plane geometry, as there are various interesting types of triangles and properties about them.  A \textbf{triangle} consists of three points $A,B,C$ that do not lie on the same line, as well as the three line segments $\overline{AB},\overline{BC},\overline{AC}$ connecting these points.  The points $A,B,C$ are called the \textbf{vertices} (singular \emph{vertex}), and the line segments are called the \textbf{edges} (or \textbf{sides}).
\begin{center}\includegraphics[scale=.4]{Triangle.png}\end{center}
Notice that each vertex has a canonical angle: for example, at vertex $A$ is the angle $\angle BAC$ (or just $\angle A$).  These three angles are called the \textbf{angles of the triangle}.  The sides $\overline{AB},\overline{AC}$ are said to be \textbf{adjacent} to vertex $A$ (because they touch it), and $\overline{BC}$ is said to be \textbf{opposite} $A$.  A point $D$ is said to be \textbf{inside} the triangle if it is inside all three of the angles.

A triangle with vertices $A,B,C$ is denoted $\triangle ABC$.\\

\noindent\textbf{Proposition 2.11.} \emph{The measures of the angles of a triangle add to $180^\circ$.}
\begin{proof}
Given any triangle, Axiom 2.5 guarantees that there is a line through a vertex of the triangle parallel to the opposite side (i.e., parallel to the line that contains the opposite side), as illustrated below.
\begin{center}\includegraphics[scale=.4]{Triangle180.png}\end{center}
Observe that angles $\alpha',\alpha$ are alternate interior angles of two parallel lines and a transversal (obtained by extending two sides of the triangle).  By Proposition 2.10, the angles are congruent, i.e., $\alpha'=\alpha$.  Similarly $\beta'=\beta$.  By Axiom 2.6 (iii) and (iv), $\alpha'+\gamma+\beta'=180^\circ$.  It follows from these results that $\alpha+\beta+\gamma=180^\circ$ as desired.
\end{proof}

With this, we can classify triangles in terms of their angles.  We recall that an angle is a \textbf{right angle} if its measure is equal to $90^\circ$.  We further define an angle to be \textbf{acute} if its measure is $<90^\circ$, and \textbf{obtuse} if its measure is $>90^\circ$.  Note that in an arbitrary triangle, if any angle is either obtuse or right, the other two angles must be acute (by Proposition 2.11).  Thus \emph{every triangle has at least two acute angles}.  We say that a triangle is:

* An \textbf{obtuse triangle} if it has an obtuse angle;

* A \textbf{right triangle} if it has a right angle;

* An \textbf{acute triangle} if all three angles are acute.

If $\triangle ABC$ is a right triangle with $\angle B$ the right angle, then sides $\overline{AB}$ and $\overline{BC}$ are called the \textbf{legs}, and side $\overline{AC}$ (the one opposite the right angle) is called the \textbf{hypotenuse}.  An important fact about right triangles will be covered at the end of the section.

Triangles can also be classified in terms of their side lengths, but before looking into this, let us first introduce the concept of congruence of triangles.  Two triangles $\triangle ABC,\triangle A'B'C'$ are said to be \textbf{congruent} (denoted $\triangle ABC\cong\triangle A'B'C'$) if the following six statements hold:
$$\overline{AB}\cong\overline{A'B'},~~~~~~\overline{BC}\cong\overline{B'C'},~~~~~~\overline{CA}\cong\overline{C'A'},$$
$$\angle BAC\cong\angle B'A'C',~~~~~~\angle ABC\cong\angle A'B'C',~~~~~~\angle ACB\cong\angle A'C'B'$$
In other words, congruent triangles have all their side lengths and angles matching up.  This is an equivalence relation on triangles.  However, exercise care in the fact though $\triangle BAC$ is the same triangle as $\triangle ABC$, the statement $\triangle BAC\cong\triangle A'B'C'$ is \emph{not} equivalent to $\triangle ABC\cong\triangle A'B'C'$.  Indeed, the former statement entails that $\overline{BC}\cong\overline{A'C'}$, unlike the latter.  This example shows that
\begin{center}
\textbf{When saying triangles are congruent, the order in which the vertices are listed is crucial: $\triangle ABC\cong\triangle A'B'C'$ specifically indicates that $\overline{AB}$ corresponds to $\overline{A'B'}$, etc.}
\end{center}
We will start by rewriting our definition of congruent triangles, because in geometry proofs, it is a statement which deserves a title.\\

\noindent\textbf{Definition 2.12.} \textsc{(Corresponding Parts of Congruent Triangles are Congruent / CPCTC)} \emph{If $\triangle ABC\cong\triangle A'B'C'$, then $\overline{AB}\cong\overline{A'B'}$, $\overline{BC}\cong\overline{B'C'}$, $\overline{CA}\cong\overline{C'A'}$, $\angle A\cong\angle A'$, $\angle B\cong\angle B'$ and $\angle C\cong\angle C'$.}\\

\noindent Knowing triangles to be congruent is thus quite a treasure, as it yields many consequences.  Thus it is useful to know two triangles are congruent without knowing in advance that each one of the six statements in Proposition 2.12 are true.  We set up some axioms for this.\\

\noindent\textbf{Axiom 2.13.} (i) \textsc{(Side-side-side / SSS)} \emph{If $\overline{AB}\cong\overline{A'B'}$, $\overline{BC}\cong\overline{B'C'}$, $\overline{CA}\cong\overline{C'A'}$, then $\triangle ABC\cong\triangle A'B'C'$.}

(ii) \textsc{(Side-angle-side / SAS)} \emph{If $\overline{AB}\cong\overline{A'B'}$, $\overline{AC}\cong\overline{A'C'}$, and $\angle A\cong\angle A'$, then $\triangle ABC\cong\triangle A'B'C'$.}

(iii) \textsc{(Hypotenuse-leg / HL / RHS)} \emph{If $\angle B,\angle B'$ are right angles, $\overline{AB}\cong\overline{A'B'}$ and $\overline{AC}\cong\overline{A'C'}$, then $\triangle ABC\cong\triangle A'B'C'$.}

(iv) \textsc{(Angle-side-angle / ASA)} \emph{If $\angle A\cong\angle A'$, $\overline{AB}\cong\overline{A'B'}$ and $\angle B\cong\angle B'$, then $\triangle ABC\cong\triangle A'B'C'$.}\\

\noindent You can realize the intuition behind each of these axioms by using pencil, paper, a ruler and a protractor.  Observe that ``SAS'' (with the A in the middle) indicates that both sides being compared between the triangles are adjacent to the angle.  Likewise, ``ASA'' indicates that both angles are adjacent to the side.  If there were ``SSA'' congruence, it would go like this:
\begin{center}
If $\overline{AB}\cong\overline{A'B'}$, $\overline{BC}\cong\overline{B'C'}$, and $\angle C\cong\angle C'$, then $\triangle ABC\cong\triangle A'B'C'$.\footnote{This time only \emph{one} side being compared is adjacent to the angle, just like there's only one S touching the A in ``SSA.''}
\end{center}
However, \emph{that statement may be false} \---- see Exercise 5.  Thus SSA is not generally a valid way to prove congruence.  However, SSA is valid if the angles being compared are right angles; this is hypotenuse-leg congruence (Axiom 2.13(iii)).

Also, AAS is valid, and actually follows from Axiom 2.13:\\

\noindent\textbf{Proposition 2.14.} \textsc{(Angle-angle-side / AAS)} \emph{If $\angle A\cong\angle A'$, $\angle B\cong\angle B'$ and $\overline{BC}\cong\overline{B'C'}$, then $\triangle ABC\cong\triangle A'B'C'$.}
\begin{proof}
By Proposition 2.11 we have
$$m\angle C=180^\circ-m\angle A-m\angle B=180^\circ-m\angle A'-m\angle B'=m\angle C'$$
and therefore $\angle C\cong\angle C'$.  Since $\angle B\cong\angle B'$ and $\overline{BC}\cong\overline{B'C'}$, Axiom 2.13(iv) then implies $\triangle ABC\cong\triangle A'B'C'$.
\end{proof}

\noindent With that, we give a new classification of triangles.  We say that a triangle is:

* \textbf{Equilateral} if all three sides are congruent;

* \textbf{Isosceles} if it has two congruent sides;

* \textbf{Scalene} if no two sides are congruent.

Note that under our convention, equilateral triangles are considered isosceles.  Hence a triangle is scalene if and only if it is not isosceles.  Observe that\\

\noindent\textbf{Proposition 2.15.} \textsc{(Isosceles Triangle Theorem)} \emph{If $\triangle ABC$ is a triangle, then $\overline{AB}\cong\overline{AC}$ if and only if $\angle B\cong\angle C$.}
\begin{proof}
By Proposition 2.12 and Axiom 2.13(i), we have that $\overline{AB}\cong\overline{AC}$ if and only if $\triangle ABC\cong\triangle ACB$.  (For, $\overline{AB}\cong\overline{AC}$ jointly implies $\overline{AB}\cong\overline{AC},\overline{BC}\cong\overline{CB},\overline{CA}\cong\overline{BA}$.)  By Proposition 2.12 and Axiom 2.13(iv), however, $\triangle ABC\cong\triangle ACB$ if and only if $\angle B\cong\angle C$.  Thus the statement in the theorem is proved.
\end{proof}

\noindent We conclude that a triangle is equilateral if and only if all three \emph{angles} are congruent (because congruence of angles is equivalent to congruence of the opposite sides by Proposition 2.15).  By Proposition 2.11, it follows that an equilateral triangle necessarily has $60^\circ$ angles.  Moreover, equilateral triangles are acute, but arbitrary isosceles triangles can be either acute, right, or obtuse (can you find examples?).  Similarly a triangle is scalene if and only if no two angles are congruent.\\

\noindent\textbf{SIMILARITY OF TRIANGLES}\\

\noindent Earlier we have described SSS, SAS, HL, ASA, AAS for proving congruence of triangles.  We may now observe that AAA is false in general: if $\angle A\cong\angle A',\angle B\cong\angle B',\angle C\cong\angle C'$, that does not imply $\triangle ABC\cong\triangle A'B'C'$.  Indeed, look at the diagram below, where a line parallel to $\overset{\longleftrightarrow}{BC}$ is constructed which intersects $\overline{AB}$ and $\overline{AC}$ in the respective points $D,E$.
\begin{center}\includegraphics[scale=.4]{SimilarTriangles.png}\end{center}
$\angle A\cong\angle A$ obviously, $\angle ADE\cong\angle B$ (because they are corresponding angles, see Proposition 2.10), and $\angle AED\cong\angle C$.  However, $\triangle ABC\not\cong\triangle ADE$ because $\overline{AB}\not\cong\overline{AD}$; the former segment is longer than the latter.

Your intuition probably tells you that $\triangle ABC$ and $\triangle ADE$ have corresponding side lengths in equal proportions, i.e., $\frac{AD}{AB}=\frac{DE}{BC}=\frac{AE}{AC}$.  We shall enforce this in a new concept.\\

\noindent\textbf{Definition.} \emph{Two triangles $\triangle ABC$ and $\triangle A'B'C'$ are said to be \textbf{similar}, denoted $\triangle ABC\sim\triangle A'B'C'$, if $\angle A\cong\angle A',\angle B\cong\angle B',\angle C\cong\angle C'$ and $\frac{AB}{A'B'}=\frac{BC}{B'C'}=\frac{CA}{C'A'}$.}\\

\noindent Just like congruence, the order in which the vertices are listed is crucial to the statement semantics.

Now we give the analogues of Proposition 2.12 and Axiom 2.13 for similarity of triangles.\\

\noindent\textbf{Proposition 2.16.} \emph{If $\triangle ABC\sim\triangle A'B'C'$, then $\angle A\cong\angle A'$, $\angle B\cong\angle B'$, $\angle C\cong\angle C'$, and $\frac{AB}{A'B'}=\frac{BC}{B'C'}=\frac{CA}{C'A'}$.}\\

\noindent\textbf{Axiom 2.17.} (i) \textsc{(Side-side-side / SSS)} \emph{If $\frac{AB}{A'B'}=\frac{BC}{B'C'}=\frac{CA}{C'A'}$, then $\triangle ABC\sim\triangle A'B'C'$.}

(ii) \textsc{(Side-angle-side / SAS)} \emph{If $\frac{AB}{A'B'}=\frac{AC}{A'C'}$ and $\angle A\cong\angle A'$, then $\triangle ABC\sim\triangle A'B'C'$.}

(iii) \textsc{(Hypotenuse-leg / HL / RHS)} \emph{If $\angle B,\angle B'$ are right angles and $\frac{AB}{A'B'}=\frac{AC}{A'C'}$, then $\triangle ABC\sim\triangle A'B'C'$.}

(iv) \textsc{(Angle-angle / AA)} \emph{If $\angle A\cong\angle A'$ and $\angle B\cong\angle B'$, then $\triangle ABC\sim\triangle A'B'C'$.}\\

\noindent Note that (iv) implies that any two triangles with identical angles are similar.  It is the most common way to prove triangles are similar, because ratios seldom arise in pure geometry without coming from similar triangles first.  Similarity of triangles will occur in many examples, such as Ceva's theorem (Exercise 16).\\

\noindent\textbf{CIRCLES}\\

\noindent Circles are a common geometrical concept, and also show up in many places in real life.  We thus introduce them here.

If $r$ is a positive real number and $O$ is a point, the set of points $A$ such that $AO=r$ is called a \textbf{circle}.  $O$ is called the \textbf{center} of the circle and $r$ is called its \textbf{radius}.
\begin{center}\includegraphics[scale=.4]{Circle.png}\end{center}
The center and radius are uniquely determined by that set of points (Exercise 12).

The technical name for the set of points on the circle is the \textbf{arc} of the circle.  If $A$ is on the arc, note that $\overline{OA}$ is a line segment whose length is the radius $r$ by definition.  $\overline{OA}$ is called a \textbf{radius} of the circle.\footnote{Understand the difference between \emph{a} radius and \emph{the} radius; the former is a line segment from the center to a point on the arc, and the latter is a real number which is the length of such a segment.}  A line segment whose endpoints are both on the arc of the circle is called a \textbf{chord}.  A chord which contains the center is called a \textbf{diameter}.

Our first observation is\\

\noindent\textbf{Proposition 2.18.} (i) \emph{Let $\omega$ be a circle with center $O$ and radius $r$.  If $\overline{AB}$ is a diameter of the circle, then $AB=2r$.}
\begin{proof}
$O\in\overline{AB}$ by definition.  Hence by the segment addition postulate, $AO+OB=AB$.  But $AO$ and $OB$ are both equal to the radius $r$, hence we conclude $AB=2r$.
\end{proof}
Thus, just like all radii have length $r$, every diameter has length $2r$.  We may as well refer to the real number $2r$ as \emph{the diameter} of the circle (just like $r$ is the radius).

Next, we see that two different circles have at most \emph{two} intersection points.  After all, let $\omega$ be the circle centered at $O$ with radius $r$, and $\omega'$ the circle centered at $O'$ with radius $r'$.  If $O=O'$, then the circles must be disjoint, for if $A$ were in their intersection, then we would have $r=OA=O'A=r'$ and hence the circles would be the same.  So assume $O\ne O'$.

If $A,A'$ are intersection points of the circles, then $OA=OA'=r$ and $O'A=O'A'=r'$, and therefore $\triangle OAO'\cong\triangle OA'O'$ by SSS congruence (Axiom 2.13(i)).  Hence by CPCTC, $\angle AOO'\cong\angle A'OO'$.  Therefore the radii $\overline{AO},\overline{A'O}$ meet $\overline{OO'}$ at the same angle; in other words, every radius of $\omega$ going to an intersection point has makes same angle with $\overline{OO'}$.  From Axiom 2.6 it follows that there are at most two such radii, hence at most two intersection points.

We now give a criterion for the intersection points to exist.  Using real analysis, it can be proven without being taken as an axiom, but that will take us too far afield.\\

\noindent\textbf{Proposition 2.19.} \emph{Let $\omega$ be a circle with center $O$ and radius $r$, and $\omega'$ a circle with center $O'$ and radius $r'$.  Set $s=OO'$.  If each of the numbers $r,r',s$ is less than the sum of the other two, then $\omega$ and $\omega'$ have two intersection points.}\\

\noindent The situation when one of the numbers $r,r',s$ is greater than or equal to the sum of the other two is covered in Exercise 15.

It is natural to ask how much a line $\ell$ can intersect a circle $\omega$.  Again the intersection consists of at most two points; this can be proven by constructing the line $\ell'$ through $O$ perpendicular to $\ell$, then seeing that every radius which meets a point in the intersection of $\ell$ and $\omega$ must make the same angle with $\ell$.  If $\ell\cap\omega$ consists of exactly one point, $\ell$ is said to be \textbf{tangent} to $\omega$; if $\ell\cap\omega$ consists of two points, $\ell$ is said to be a \textbf{secant}.

We observe that if $\ell$ is tangent to $\omega$ at a point $A$, then $\ell\perp\overline{OA}$: Let $B$ be a point in $\ell$ other than $A$.  If $m\angle OAB<90^\circ$, let $\alpha=180^\circ-2m\angle OAB$.  Now construct a ray with endpoint $O$ whose points are inside $\angle OAB$ and which makes an angle of $\alpha$ with $\overline{OA}$.  If it meets $\ell$ at a point $C$, then applying Proposition 2.11 to $\triangle OAC$ entails that $m\angle OCA=m\angle OAC[=m\angle OAB]$.  Hence $\overline{OA}\cong\overline{OC}$ by Proposition 2.15, so that $OC=r$ and $C$ is also an intersection point of $\ell$ and $\omega$; contradiction because $\ell$ is tangent to $\omega$.  The same contradiction arises if $m\angle OAB>90^\circ$; this time let $B'$ be a point in $\ell$ with $A$ in between $B,B'$, and then $m\angle OAB'<90^\circ$.  Therefore $m\angle OAB=90^\circ$, making $\angle OAB$ a right angle.

If $A,A'$ are two points on the arc of a circle, then the set of points on the arc that are inside $\angle AOA'$ is called a \textbf{minor arc} of the circle, and is denoted $\measuredangle AA'$; and its angle measure is defined to be $m\angle AOA'$.  [Note that the minor arc does not exist if $A,O,A'$ lie on one line.]  Similarly, the set of points that are outside the angle (along with $A$ and $A'$) is called a \textbf{major arc}.  The angle addition postulate then entails that if $B$ is in an arc $\measuredangle AA'$, we have $m\measuredangle AA'=m\measuredangle AB+m\measuredangle BA'$.

Now suppose $A,B,C$ are distinct points on the arc.  The set of points on the arc inside $\angle BAC$ is called the \textbf{intercepted arc}, and it measures twice the measure of $\angle BAC$, as we now prove.\\

\noindent\textbf{Proposition 2.20.} \emph{If $A,B,C$ are distinct points on the arc of a circle, then $m\angle BAC=\frac 12m\measuredangle BC$.}
\begin{proof}
We shall prove the case where the arc is minor and the center $O$ is inside $\angle BAC$.
\begin{center}\includegraphics[scale=.4]{IntArc.png}\end{center}
Since $\overline{OB}$ and $\overline{OA}$ are both radii, they have length $r$, hence $\overline{OB}\cong\overline{OA}$.  By Proposition 2.15 applied to $\triangle OAB$, $\angle OAB\cong\angle OBA$.  Similarly, $\angle OAC\cong\angle OCA$.  We also have that (by Proposition 2.11), 
$$m\angle BOA+m\angle OAB+m\angle OBA=180^\circ=m\angle BOA+2m\angle OAB$$
$$m\angle COA+m\angle OAC+m\angle OCA=180^\circ=m\angle COA+2m\angle OAC$$
and by extending line $\overset{\longleftrightarrow}{OA}$ and using the angle addition postulate, we get that $m\angle BOC+m\angle COA+m\angle BOA=360^\circ$.  Therefore it follows that
\begin{align*}
m\angle BAC & = m\angle BAO+m\angle OAC \\
& = \frac 12(180^\circ-m\angle BOA)+\frac 12(180^\circ-m\angle COA)\\
& = 180^\circ - \frac 12(m\angle BOA+m\angle COA)\\
& = 180^\circ - \frac 12(360^\circ - m\angle BOC)\\
& = 180^\circ - 180^\circ + \frac 12m\angle BOC\\
& = \frac 12m\angle BOC=\frac 12m\measuredangle BC.
\end{align*}
As for the case where the arc is major or $O$ is outside $\angle BAC$, the proof is similar and left to the reader.
\end{proof}

\noindent\textbf{THE PYTHAGOREAN THEOREM}\\

\noindent We conclude this section with an important theorem concerning the side lengths of a right triangle.\\

\noindent\textbf{Theorem 2.21.} \textsc{(The Pythagorean Theorem)} \emph{If $\triangle ABC$ is a right triangle with $\angle B$ the right angle, then $(AB)^2+(BC)^2=(AC)^2$.}
\begin{proof}
Construct the line through $B$ perpendicular to $\overline{AC}$, and let $D$ be its intersection point with $\overline{AC}$:
\begin{center}\includegraphics[scale=.4]{PythTheorem.png}\end{center}
We start by observing $\angle ACB\cong\angle BCD$ (they are the same angle) and $\angle ABC\cong\angle BDC$ (since both are right angles).  By AA similarity (Axiom 2.17(iv)), $\triangle ABC\sim\triangle BDC$.  Therefore by Axiom 2.16, $\frac{AC}{BC}=\frac{BC}{CD}$.  Multiplying both sides of this equation by $(BC)(CD)$, $(AC)(CD)=(BC)^2$.

A similar argument shows that $\triangle ABC\sim\triangle ADB$, and hence $\frac{AC}{AB}=\frac{AB}{AD}$ and $(AC)(AD)=(AB)^2$.  Therefore,
$$(AB)^2+(BC)^2=(AC)(AD)+(AC)(CD)=(AC)(AD+DC)=(AC)^2,$$
as desired.
\end{proof}

\subsection*{Exercises 2.1. (Axiomatic Plane Geometry)}
\begin{enumerate}
\item Prove that any angle (with measure $<180^\circ$) has a unique angle bisector.

\item Let $\overline{AB}$ be a line segment.  If $M$ is the midpoint of $\overline{AB}$, then a line is said to \textbf{bisect} the segment $\overline{AB}$ if it goes through $M$.  By Proposition 2.8, there is a unique line $\ell$ perpendicular to $\overline{AB}$ that bisects $\overline{AB}$; $\ell$ is called the \textbf{perpendicular bisector} of $\overline{AB}$.  If $C$ is any point, prove that $\overline{AC}\cong\overline{BC}$ if and only if $C\in\ell$.  Thus, the perpendicular bisector of a line segment is the locus of points equidistant from each of the endpoints of the line segment.

\item Let $A$ be a point and $\ell$ be a line.  By Proposition 2.8 there is a unique line $\ell'$ containing $A$ such that $\ell'\perp\ell$.  If $B=\ell\cap\ell'$, show that $AB$ is the minimum distance from $A$ to any point on $\ell$.  This number is called \textbf{the distance from the point $A$ to the line $\ell$.}

\item A \textbf{quadrilateral} consists of four points $A,B,C,D$ such that (a) no three of them lie on the same line, and (b) $\overline{AB}\cap\overline{CD}=\overline{BC}\cap\overline{DA}=\varnothing$; along with the four segments $\overline{AB},\overline{BC},\overline{CD},\overline{DA}$.  (\emph{Caution}: (b) does not mean $\overset{\longleftrightarrow}{AB}$ is parallel to $\overset{\longleftrightarrow}{CD}$!  It states that $\overline{AB}$ and $\overline{CD}$ are set theoretically disjoint as line \emph{segments}; the lines containing them could still intersect.)
\begin{center}\includegraphics[scale=.4]{Quadrilateral.png}\end{center}
The four points $A,B,C,D$ are called \textbf{vertices}, and the four line segments are called \textbf{edges} / \textbf{sides}.  The quadrilateral is denoted as \emph{quadrilateral $ABCD$}.\footnote{It is necessary for the letters to be spelled out in the order the line segments travel; for example, the quadrilateral can also be referred to as $BCDA$ or $DCBA$, but not as $ABDC$, because $B$ isn't connected to $D$ by a line segment.}  Just like in a triangle, each vertex has its canonical angle (e.g., $\angle BAD$ for $A$).

Show that the measures of the angles of a quadrilateral add to $360^\circ$.  [Constructing $\overline{BD}$ divides the quadrilateral into two triangles; now use Proposition 2.11.]

\item Construct a circle whose center is the point $A$, then let $C$ and $D$ be distinct points on the circle's arc.  Let $B$ be an arbitrary point on $\overset{\longrightarrow}{DC}$, with $C$ strictly between $B$ and $D$.  Construct segments $\overline{AB},\overline{AC},\overline{AD},\overline{BD}$ as shown below.
\begin{center}\includegraphics[scale=.4]{SSArefute.png}\end{center}
Then $\overline{AC}\cong\overline{AD}$, $\overline{AB}\cong\overline{AB}$ and $\angle ABC\cong\angle ABD$, but $\triangle ABC\not\cong\triangle ABD$.  These triangles thus constitute a counterexample to the assertion of SSA congruence.

\item Given a quadrilateral $ABCD$, prove that the following are equivalent.

(i) $\overset{\longleftrightarrow}{AB}\parallel\overset{\longleftrightarrow}{CD}$ and $\overset{\longleftrightarrow}{BC}\parallel\overset{\longleftrightarrow}{DA}$.

(ii) $\overline{AB}\cong\overline{CD}$ and $\overline{BC}\cong\overline{DA}$.

(iii) $\angle A\cong\angle C$ and $\angle B\cong\angle D$.

Such a quadrilateral is called a \textbf{parallelogram} (indeed, (i) states that opposite sides are parallel).  [Construct line segment $\overline{BD}$ to divide the quadrilateral into two triangles.]

\item If $ABCD$ is a quadrilateral, the segments $\overline{AC}$ and $\overline{BD}$ are called the \textbf{diagonals} of the quadrilateral.  Show that $ABCD$ is a parallelogram if and only if the diagonals bisect each other (i.e., each diagonal goes through the midpoint of the other one).

\item If $ABCD$ is a quadrilateral, let $A',B',C',D'$ be the midpoints of $\overline{AB},\overline{BC},\overline{CD},\overline{DA}$ respectively.  Prove that quadrilateral $A'B'C'D'$ is a parallelogram.  [First show that each side of this quadrilateral is parallel to one of the diagonals of quadrilateral $ABCD$.]

\item Suppose $ABCD$ is a quadrilateral.

(a) If $\overset{\longleftrightarrow}{AB}\parallel\overset{\longleftrightarrow}{CD}$ and $\angle A\cong\angle C$, prove that $ABCD$ is a parallelogram.

(b) If $\overset{\longleftrightarrow}{AB}\parallel\overset{\longleftrightarrow}{CD}$ and $\overline{AB}\cong\overline{CD}$, prove that $ABCD$ is a parallelogram.

(c) If $\overline{AB}\cong\overline{CD}$ and $\overset{\longleftrightarrow}{BC}\parallel\overset{\longleftrightarrow}{DA}$, is $ABCD$ necessarily a parallelogram?  [Think of Exercise 5.]

\item\emph{(Trigonometric functions.)} \---- Let $\triangle ABC$ be a right triangle with $\angle B$ the right angle.  Observe that the ratio $\frac{BC}{AC}$ is completely determined by the real number $m\angle A$ and not by the right triangle: for if $\triangle A'B'C'$ is another right triangle with $\angle B'$ the right angle and $m\angle A=m\angle A'$, AA similarity entails $\triangle ABC\sim\triangle A'B'C'$, and hence $\frac{BC}{AC}=\frac{B'C'}{A'C'}$.  Similarly, the ratios $\frac{AB}{AC}$ and $\frac{BC}{AB}$ are completely determined by $m\angle A$.  Define
$$\sin(m\angle A)=\frac{BC}{AC},~~~~\cos(m\angle A)=\frac{AB}{AC},~~~~\tan(m\angle A)=\frac{BC}{AB}$$
Then suppose $0^\circ<\alpha<90^\circ$.  Show that:

(a) $(\sin\alpha)^2+(\cos\alpha)^2=1$.  [Use Theorem 2.21.]

(b) $\frac{\sin\alpha}{\cos\alpha}=\tan\alpha$.

(c) $\sin(90^\circ-\alpha)=\cos\alpha$ and $\tan(90^\circ-\alpha)=\frac 1{\tan\alpha}$.  [If $\triangle ABC$ is a right triangle with $\angle B$ the right angle, then $m\angle A+m\angle C=90^\circ$ by Proposition 2.11.]

(d) $\tan 45^\circ=1$ and $\sin 30^\circ=\frac 12$.  [Construct a right triangle whose legs have equal length.  Then take an equilateral triangle and construct the perpendicular from one vertex to the opposite side, dividing it into two right triangles.]

(e) Use part (d) to find the $\sin\alpha,\cos\alpha,\tan\alpha$ for $\alpha=30^\circ,45^\circ,60^\circ$.

\item Here are some rather interesting \emph{inequalities} revolving around triangles.

(a) If $\triangle ABC$ is a triangle, prove that $AB<AC$ if and only if $m\angle C<m\angle B$.  Thus, the correspondence between sides and opposite angles is order-preserving.  [To prove $\Rightarrow$, let $D$ be the point on $\overset{\longrightarrow}{AC}$ such that $AD=AB$, then use Proposition 2.15 on $\triangle ABD$, as well as Proposition 2.11.  The direction $\Leftarrow$ then follows from Proposition 2.15 and the law of trichotomy.]

(b) \emph{(Triangle inequality.)} \----  If $\triangle ABC$ is a triangle, prove that $AB+BC>AC$.  [By Proposition 2.8, there is a unique line $\ell$ through $B$ perpendicular to $\overset{\longleftrightarrow}{AC}$.  Let $D=\ell\cap\overset{\longleftrightarrow}{AC}$.  If $D$ is between $A$ and $C$, then part (a) entails that $AB>AD$ and $BC>DC$; now use the segment addition postulate.  If $D$ is not between $A$ and $C$, the proof should be easier.]

(c) \emph{(Hinge theorem.)} \---- Let $\triangle ABC$ and $\triangle A'B'C'$ be triangles.  If $\overline{AB}\cong\overline{A'B'}$ and $\overline{AC}\cong\overline{A'C'}$, then $m\angle A<m\angle A'$ if and only if $BC<B'C'$.  [If $m\angle A<m\angle A'$, start by establishing a point $D$ such that $\triangle ADC\cong\triangle A'B'C'$.  With that $\overline{AB}\cong\overline{AD}$ (why?).  Now let the angle bisector of $\angle BAD$ meet $\overline{CD}$ at the point $E$.  By SAS, $\triangle ABE\cong\triangle ADE$.  Therefore $\overline{BE}\cong\overline{DE}$, so that $B'C'=DC=DE+EC=BE+EC>BC$ by part (b).  The converse follows from Axiom 2.13 and the law of trichotomy.]
\begin{center}\includegraphics[scale=.4]{HingeTheorem.png}\end{center}

\item Let $\omega$ be the circle centered at $O$ with radius $r$.  By Exercise 2, the perpendicular bisector of any chord passes through $O$.  Use this to show that $O$ can be recovered from the set of points in the arc of $\omega$.  Moreover, $r$ can also be recovered, because if $A$ is a point in the arc then $AO=r$.

\item A chord of a circle is a diameter if and only if it has the maximum possible length among all the chords of the circle.  [Exercise 11(b) may help.]

\item Let $\omega$ be the circle centered at $O$ with radius $r$, and let $A$ be a point on the circle's arc.  Then there is exactly one line that is tangent to $\omega$ at $A$.  [Consider the line through $A$ perpendicular to $\overline{OA}$.]

\item Let $\omega$ be the circle centered at $O$ with radius $r$, and $\omega'$ the circle centered at $O'$ with radius $r'$.  Set $s=OO'$.

(a) If one of $r,r',s$ is strictly greater than the sum of the other two, then $\omega$ and $\omega'$ do not intersect at all.

(b) If one of $r,r',s$ is equal to the sum of the other two, then $\omega$ and $\omega'$ intersect in exactly one point.  [That point must be on $\overset{\longleftrightarrow}{OO'}$.]

\item\emph{(Ceva's theorem.)} \---- Let $\triangle ABC$ be a triangle.  Suppose $A'\in\overline{BC},B'\in\overline{CA},C'\in\overline{AB}$ are points that are not any of the vertices of the triangle.  Then line segments $\overline{AA'},\overline{BB'},\overline{CC'}$ intersect in a common point if and only if $\frac{AB'}{B'C}\frac{CA'}{A'B}\frac{BC'}{C'A}=1$.
[To prove $\Rightarrow$, let $O$ be the common point of intersection of $\overline{AA'},\overline{BB'},\overline{CC'}$.  Construct line segments through $O$ parallel to the sides of the triangle.  Now use similarity of triangles, starting with the observation that the three triangles shaded below are similar to $\triangle ABC$.  To prove $\Leftarrow$, let $O=\overline{AA'}\cap\overline{BB'}$ and let $D$ be the point where $\overset{\longrightarrow}{CO}$ meets $\overline{AB}$.  Use the already proven $\Rightarrow$ direction to show that $\frac{BD}{DA}=\frac{BC'}{C'A}$.  Then $D=C'$ follows easily.]
\begin{center}\includegraphics[scale=.4]{Ceva.png}\end{center}

\item Let $\triangle ABC$ be a triangle.  We shall introduce four kinds of centers of the triangle.  Prove the following.

(a) The perpendicular bisectors of the sides of the triangle intersect in a common point.  This point is called the \textbf{circumcenter} of the triangle.  [First explain why any two of them must intersect.  Then use Exercise 2 to show that their intersection point is also contained in the third one.]

(b) If $O$ is the circumcenter, then $\overline{AO}\cong\overline{BO}\cong\overline{CO}$.  If $r$ is the length of these line segments, the circle centered at $O$ with radius $r$ is the unique circle passing through $A,B,C$.  [We know it is unique because distinct circles intersect in at most two points.]  This is called the \textbf{circumcircle} or \textbf{circumscribed circle} of the triangle.

(c) The angle bisector of an angle is the locus/set of points inside the angle whose distances to the two lines are equal (see Exercise 3).  Use this to show that the angle bisectors of the angles of the triangle intersect in a common point.  This point is called the \textbf{incenter} of the triangle.

(d) If $I$ is the incenter, then part (c) shows that the distance from $I$ to each of the three sides is equal.  Let $r$ be this common distance; then the circle centered at $I$ with radius $r$ is the unique circle tangent to the sides $\overline{AB},\overline{BC},\overline{CA}$.  This is called the \textbf{incircle} or \textbf{inscribed circle}.

(e) A \textbf{median} of the triangle is a line segment from a vertex to the midpoint of the opposite side.  By Ceva's theorem, the medians intersect in a common point.  This point is called the \textbf{centroid} of the triangle.

Let $A'$ be the midpoint of $\overline{BC}$, $B'$ the midpoint of $\overline{CA}$ and $C'$ the midpoint of $\overline{AB}$.  Let $O$ be the centroid (i.e., $\overline{AA'}\cap\overline{BB'}\cap\overline{CC'}$).  Then $\frac{AO}{OA'}=2$.  [First use SAS similarity (Axiom 2.17(ii)) to show that $\triangle B'A'C\sim\triangle ABC$ and $\frac{AB}{A'B'}=2$.  Moreover, $\angle CB'A'\cong\angle CAB$ by Proposition 2.16, and hence $\overline{A'B'}\parallel\overline{AB}$ by Proposition 2.10.  Now use AA similarity to show that $\triangle OA'B'\sim\triangle OAB$, and the rest is easy.]%[Construct $\triangle A'B'C'$ and use similarity of triangles, starting from the observation that $\triangle A'B'C'\sim\triangle ABC$ and the medians of $\triangle ABC$ where they are visible in $\triangle A'B'C'$ are medians of $\triangle A'B'C'$.]

(f) An \textbf{altitude} of the triangle is a line passing through a vertex perpendicular to the opposite side.  Show that the altitudes intersect in a common point.  [Through each vertex, construct the line parallel to the opposite side.  These three lines form a larger triangle, whose perpendicular bisectors are the altitudes of the original triangle.]  This point is called the \textbf{orthocenter}.

It turns out that the orthocenter, centroid and circumcenter always lie on one line: this fact will be covered in Section 2.2.

(g) If $\ell_1$ is the altitude from vertex $A$, $\ell_2$ is the median from vertex $A$, $\ell_3$ is the angle bisector from vertex $A$, and $\ell_4$ is the perpendicular bisector of $\overline{BC}$, then the following are equivalent:

~~~~(i) $\overline{AB}\cong\overline{AC}$.

~~~~(ii) $\ell_1,\ell_2,\ell_3,\ell_4$ are all the same line.

~~~~(iii) At least two of $\ell_1,\ell_2,\ell_3,\ell_4$ are the same line.

\item\emph{(Angle bisector theorem.)} \---- Let $\triangle ABC$ be a triangle.  Suppose the angle bisector of $\angle A$ meets $\overline{BC}$ at point $D$.  Show that $\frac{AB}{AC}=\frac{BD}{CD}$.  [Construct perpendicular line segments from $B$ and $C$ to $\overset{\longrightarrow}{AD}$, and use similar triangles.]

\item Let $ABCD$ be a quadrilateral.  Show that the following are equivalent:

~~~~(i) The four points $A,B,C,D$ all lie on one circle;

~~~~(ii) $\angle ACB\cong\angle ADB$;

~~~~(iii) $\angle ABC$ and $\angle CDA$ are supplementary angles.

Such a quadrilateral is said to be \textbf{cyclic}.  [(i) $\implies$ (ii), (iii): Use Proposition 2.20.  (ii) (resp., (iii)) $\implies$ (i): Let $\omega$ be the circumscribed circle of $\triangle ABC$, and suppose $\overset{\longrightarrow}{AD}$ meets $\omega$ at point $D'$.  Then the hypothesis and (i) $\implies$ (ii) (resp., (iii)) jointly imply that $\angle ADB\cong\angle AD'B$ (resp., $\angle CDA\cong\angle CD'A$); use Proposition 2.10 to conclude that $D=D'$.]

\item (a) If $B$ and $C$ are distinct points on a circle, let $\ell$ be the tangent line to the circle through point $B$, and let $A$ be a point of $\ell$ other than $B$.  Then $m\angle ABC=\frac 12m\measuredangle BC$, where the arc is taken as the set of points inside $\angle ABC$.  [Use Propositions 2.11 and 2.15 on $\triangle BOC$ (with $O$ the center of the circle), along with the observed fact that $\ell\perp\overline{OB}$.]

(b) In the diagram below, where $\overset{\longleftrightarrow}{AB}$ is tangent to the circle, show that $(AB)^2=(AC)(AD)$.
\begin{center}\includegraphics[scale=.3]{TangentSecant.png}\end{center}
[$m\angle ADB=\frac 12m\measuredangle BC$ by Proposition 2.20, and $m\angle ABC=\frac 12m\measuredangle BC$ by part (a).  Therefore AA similarity implies $\triangle ABC\sim\triangle ADB$.]

\item\emph{(Intersecting chords theorem.)} \---- Suppose $\overline{AC}$ and $\overline{BD}$ are two chords of circle $\omega$ which intersect in a point $E$.  Then $(AE)(EC)=(BE)(ED)$.  [$\angle ACB\cong\angle ADB$ by Exercise 19, and hence $\angle ECB\cong\angle ADE$.  Also $\angle AED\cong\angle BEC$ because they are vertical angles.  It follows from AA similarity that $\triangle AED\sim\triangle BEC$.]

\item\emph{(Ptolemy's theorem.)} \---- Suppose $ABCD$ is a cyclic quadrilateral.  Then $(AB)(CD)+(BC)(DA)=(AC)(BD)$.  [Start by establishing a point $K\in\overline{AC}$ such that $\angle ABK\cong\angle DBC$.  By Exercise 19, $\angle BAK=\angle BAC\cong\angle BDC$.  Therefore $\triangle ABK\sim\triangle DBC$ by AA similarity.  Now explain why $\angle KBC\cong\angle ABD$, and use this fact to show that $\triangle KBC\sim\triangle ABD$.  Therefore $\frac{AK}{AB}=\frac{DC}{DB}$ and $\frac{KC}{BC}=\frac{AD}{BD}$.  Cross-multiplying, $(AK)(BD)=(AB)(CD)$ and $(KC)(BD)=(BC)(AD)$.  Therefore, adding the two equations, $(AK)(BD)+(KC)(BD)=(AB)(CD)+(BC)(AD)$.  Yet $(AK)(BD)+(KC)(BD)=(AC)(BD)$ by the segment addition postulate.] %http://mypages.iit.edu/~maslanka/Ptolemy.pdf

\item What is wrong with the following ``proof'' that every triangle is isosceles?\footnote{Maxwell 1959, Chapter II, Section 1.}

Let $\triangle ABC$ be a triangle; we show that $\overline{AB}\cong\overline{AC}$.  Construct the angle bisector $\ell_1$ of $\angle A$ and the perpendicular bisector $\ell_2$ of $\overline{BC}$, letting $P$ be the midpoint of $\overline{BC}$.  If $\ell_1\parallel\ell_2$, then $\ell_1\perp\overset{\longleftrightarrow}{BC}$ which means the angle bisector of $\angle A$ coincides with the altitude, making $\overline{AB}\cong\overline{AC}$ by Exercise 17(g).
Thus we may assume $\ell_1\not\parallel\ell_2$.

Let $O$ be the point $\ell_1\cap\ell_2$.  Construct $\overline{OB}$ and $\overline{OC}$.  By Proposition 2.8 there is a line through $O$ perpendicular to $\overline{AB}$; let it intersect $\overline{AB}$ at point $Q$ and construct $\overline{OQ}$.  Similarly let $R$ be the point on $\overline{AC}$ such that $\overline{OR}\perp\overline{AC}$, as shown below.
\begin{center}\includegraphics[scale=.4]{ITFallacy.png}\end{center}
(The diagram is not drawn to scale, so as to not reveal the error.)

Now, $\angle AQO\cong\angle ARO$ (both are right angles), $\angle QAO\cong\angle RAO$ (by definition of an angle bisector) and $\overline{AO}\cong\overline{AO}$.  Hence by AAS, $\triangle AQO\cong\triangle ARO$.  Hence by CPCTC (Proposition 2.12), $\overline{AQ}\cong\overline{AR}$ and $\overline{OQ}\cong\overline{OR}$.

Observe now that $\angle OPB\cong\angle OPC$ (both are right angles), $\overline{OP}\cong\overline{OP}$ and $\overline{BP}\cong\overline{PC}$ (because $P$ is the midpoint of $\overline{BC}$).  By SAS, $\triangle OPB\cong\triangle OPC$.  By CPCTC, $\overline{OB}\cong\overline{OC}$.

Finally, consider triangles $\triangle OQB$ and $\triangle ORC$.  We have shown $\overline{OQ}\cong\overline{OR}$ and $\overline{OB}\cong\overline{OC}$ in the previous two paragraphs; also, $\angle OQB,\angle ORC$ are both right angles.  By HL, $\triangle OQB\cong\triangle ORC$.  By CPCTC, $\overline{QB}\cong\overline{RC}$ follows.

Since $\overline{AQ}\cong\overline{AR}$ and $\overline{QB}\cong\overline{RC}$, we get
$$AB=AQ+QB=AR+RC=AC$$
and therefore $\overline{AB}\cong\overline{AC}$ as desired.
\end{enumerate}

\subsection*{2.2. Giving the Plane Coordinates}
\addcontentsline{toc}{section}{2.2. Giving the Plane Coordinates}
In the previous section, we have gone through a basic course of axiomatic plane geometry.  In this section, we shall introduce coordinate axes, thereby turning the aforementioned constructions (line, line segment, ray, angle, triangle, circle) into algebraic figures.  We will then stick to the coordinate setting for the rest of the book.

We start by letting $\chi_1$ and $\chi_2$ be two perpendicular lines, which we call \emph{axes}.  Let $O=\chi_1\cap\chi_2$.  We will henceforth call $O$ the \textbf{origin} of the plane, or of our coordinate system.  Finally, let $A\in\chi_1$ and $B\in\chi_2$ be points distinct from $O$.  (We will use them to specify which semiaxes are positive.)

Now suppose $P$ is an arbitrary point in the plane.  We derive an ordered pair $(x,y)\in\mathbb R^2$ as follows.  By Proposition 2.8, there is a line through $P$ perpendicular to $\chi_1$; let it intersect $\chi_1$ at point $P'$.  Now $O,A,P'$ are all on $\chi_1$: either $O$ is between $A$ and $P'$ or it isn't.  If $O$ is between $A$ and $P'$, we set $x=-(OP')$; otherwise, we set $x=OP'$.  Likewise, suppose the line through $P$ perpendicular to $\chi_2$ meets $\chi_2$ at $P^*$.  If $O$ is between $B$ and $P^*$, then $y=-(OP^*)$, otherwise $y=OP^*$.
\begin{center}\includegraphics[scale=.4]{CoordPlane.png}

\emph{The coordinates in this example are $(-3,2)$.  Note that $x$ is negative because $O$ is between $A$ and $P'$.}\end{center}

\noindent This gives us a one-to-one correspondence between our Euclidean plane and $\mathbb R^2$:\\

\noindent\textbf{Lemma 2.22.} \emph{If $A,B,C$ are arbitrary points that lie on a line, then $\overline{BC}\subset\overline{AB}\cup\overline{AC}$.}
\begin{proof}
If $A$ is between $B$ and $C$, then $\overline{BC}=\overline{AB}\cup\overline{AC}$ by Axiom 2.1(ii), hence the lemma holds in this case.  If $B$ is between $A$ and $C$, then $\overline{BC}\subset\overline{AC}$ by Axiom 2.1(ii) so that $\overline{BC}\subset\overline{AB}\cup\overline{AC}$.  Similarly if $C$ is between $A$ and $B$.
\end{proof}
\noindent\textbf{Proposition 2.23.} \emph{The map $\varphi:P\mapsto(x,y)$ defined above is bijective.}
\begin{proof}
Suppose $\varphi(P)=\varphi(Q)=(x,y)$, say.  Then construct the line through $P$ perpendicular to $\chi_1$ and let $P'$ be its intersection point.  Similarly, let $Q'$ be the intersection point of the line through $Q$ perpendicular to $\chi_1$.  Then by definition, $OP'=|x|=OQ'$.

We claim that $P'=Q'$: by Axiom 2.4, this is immediate if $P'$ is between $O$ and $Q'$ (in this case $OQ'=OP'+P'Q'$ and hence $P'Q'=0$, implying $P'=Q'$), or if $Q'$ is between $O$ and $P'$.  So let us assume $O$ is between $P'$ and $Q'$.  If $x=0$, then $O=P'=Q'$ is clear so assume $x\ne 0$.  If $x$ is negative, then $O$ is between $A$ and $P'$ (that is the criterion for a negative coordinate value), and also $O$ is between $A$ and $Q'$; from this it follows that $O\in\overline{P'A}\cap\overline{AQ'}$.  If $A$ is between $P'$ and $Q'$ this implies $\overline{P'A}\cap\overline{AQ'}=\{A\}$ by the observations after Axiom 2.1, and hence $O=A$, a contradiction.  If $P'$ is between $A$ and $Q'$ then $O\in\overline{AP'}\cap\overline{P'Q'}=\{P'\}$, hence $O=P'$ and $x=0$ which doesn't hold water; similarly we cannot have $Q'$ between $A$ and $P'$.  Therefore the assumption that $x$ is negative leads to a contradiction; hence $x$ is positive.  Therefore $O$ is neither between $A$ and $P'$, nor between $A$ and $Q'$.  This implies $O\notin\overline{AP'}\cup\overline{AQ'}$.  Yet $O\in\overline{P'Q'}\subset\overline{AP'}\cup\overline{AQ'}$ by Lemma 2.22.  This is a contradiction, and hence it is impossible that $O$ is between $P'$ and $Q'$ and $x\ne 0$.  Since we've proven $P'=Q'$ in all other cases, we may conclude $P'=Q'$.

Furthermore this implies that $P$ and $Q$ lie on the same line perpendicular to $\chi_1$ (the perpendicular line going through $P'=Q'$); let $\ell_1$ be such a line.  The same argument shows that $P$ and $Q$ lie on the same line $\ell_2$ perpendicular to $\chi_2$.  Then $\ell_1\perp\ell_2$ (why?), and $P$ and $Q$ are both equal to the unique intersection point of $\ell_1$ and $\ell_2$.  Hence $P=Q$, and $\varphi$ is injective.

To show surjectivity, suppose $(x,y)\in\mathbb R^2$.  If $x\geqslant 0$, let $P'$ be the point on $\overset{\longrightarrow}{OA}$ such that $OP'=x$ (see Axiom 2.4).  In this case $O$ is not between $P'$ and $A$ if $O\ne P'$; indeed, if $O$ were between $P'$ and $A$, then (since $P'\in\overset{\longrightarrow}{OA}$), either $A$ is between $O$ and $P'$ (in which case $O=A$, contradiction), or else $P'$ is between $O$ and $A$ (in which case $O=P'$).  On the other hand, if $x<0$, let $A'$ be a point on $\overset{\longleftrightarrow}{OA}$ such that $O$ is strictly between $A'$ and $A$, then let $P'$ be the point on $\overset{\longrightarrow}{OA'}$ such that $OP'=|x|$.  With that, $O$ \emph{is} between $P'$ and $A$ \---- after all, $O\in\overline{AA'}\subset\overline{AP'}\cup\overline{A'P'}$ by Lemma 2.22, and if $O\in\overline{A'P'}$ then (since $P'\in\overset{\longrightarrow}{OA'}$), either $P'$ is between $O$ and $A'$ (which implies $O=P'$ and $|x|=OP'=0$, contradiction because $x<0$ strictly), or else $A'$ is between $O$ and $P'$ (which implies $O=A'$, another contradiction).  Thus $O$ is necessarily in $\overline{AP'}$.

Likewise, if $y\geqslant 0$, let $P^*$ be the point on $\overset{\longrightarrow}{OB}$ such that $OP^*=x$ (which implies, like in the last paragraph, that $O$ is not between $P^*$ and $B$ if $O\ne P^*$); and if $y<0$, let $B'$ be a point on $\overset{\longrightarrow}{OB}$ such that $O$ is strictly between $B'$ and $B$, then let $P^*$ be the point on $\overset{\longrightarrow}{OB'}$ such that $OP^*=|y|$ (which implies $O$ is between $P^*$ and $B$).  Now let $\ell_1$ be the line through $P'$ perpendicular to $\chi_1$; let $\ell_2$ be the line through $P^*$ perpendicular to $\chi_2$; and let $P=\ell_1\cap\ell_2$.  The reader can readily verify that $\varphi(P)=(x,y)$, and therefore $\varphi$ is surjective.
\end{proof} % Remark: Most of the hassle in this proof involves distinguishing positive from negative coordinates.  After all, length of a line segment is never negative

\noindent Now that we have identified our axiomatic plane with $\mathbb R^2$, we wish to find algebraic formulas for the constructions of the previous chapter.  Let us start with lines.  First note that if $P=(x,y)$, then $P\in\chi_1$ if and only if $y=0$, and $P\in\chi_2$ if and only if $x=0$.  (Verifications left to the reader.)  Hence, the origin $O=(0,0)$.\\

\noindent\textbf{Proposition 2.24.} \textsc{(Distance Formula)} \emph{Let $P_1$ and $P_2$ be points in the plane (where the plane is identified with $\mathbb R^2$).}

(i) \emph{If $P_1=(x_1,y)$ and $P_2=(x_2,y)$, then $P_1P_2=|x_1-x_2|$.}

(ii) \emph{If $P_1=(x,y_1)$ and $P_2=(x,y_2)$, then $P_1P_2=|y_1-y_2|$.}

(iii) \emph{If $P_1=(x_1,y_1)$ and $P_2=(x_2,y_2)$, then $P_1P_2=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$.}\\

\noindent Formulas like Proposition 2.24 may seem familiar.  However, we will go through their proofs in detail because the methods will not work in the non-Euclidean geometries we will study in Chapters 3-5.  For example, the argument in the proof of (i) with $P_1P_2Q_2Q_1$ will \emph{not} work in the other geometries, because this quadrilateral will not have four right angles.
\begin{proof}
(i) Let $\ell$ be the line through $P_1$ perpendicular to $\chi_2$.  We first observe that $\ell\cap\chi_2=(0,y)$: the $x$-coordinate is zero because the point is on $\chi_2$ (see note before the proposition).  As for the $y$-coordinate, the trick is to observe in the definition of the coordinates of a point, that the $y$-coordinate of a point is completely determined by the particular line perpendicular to $\chi_2$ that goes through the point.  But this line is $\ell$ for both the points $P_1$ and $\ell\cap\chi_2$ (as they both lie on the line); hence, the two points have the same $y$-coordinate.  Since $P_1=(x_1,y)$, it follows that $y$ is the $y$-coordinate of $\ell\cap\chi_2$ and that $\ell\cap\chi_2=(0,y)$.

Likewise if $\ell'$ is the line through $P_2$ perpendicular to $\chi_2$, then $\ell'\cap\chi_2=(0,y)$.  Hence if $\ell$ and $\ell'$ are distinct lines, they would be parallel by Proposition 2.10 (both are perpendicular to $\chi_2$), which contradicts $(0,y)\in\ell\cap\ell'$.  Thus $\ell=\ell'$, and this line passes through both $P_1$ and $P_2$.

Now let $\ell_1$ be the line through $P_1$ perpendicular to $\chi_1$, and $\ell_2$ the line through $P_2$ perpendicular to $\chi_1$.  If $Q_k=\ell_k\cap\chi_1$ for $k=1,2$, then adapting the argument of the first paragraph of the proof shows that $Q_k=(x_k,0)$.  Observe that $\overset{\longleftrightarrow}{P_kQ_k}=\ell_k$, $\overset{\longleftrightarrow}{Q_1Q_2}=\chi_1$ and $\overset{\longleftrightarrow}{P_1P_2}=\ell$.  Repeated application of Exercise 4 of the previous section shows that quadrilateral $P_1P_2Q_2Q_1$ has four right angles.  Therefore $\overline{P_1P_2}\cong\overline{Q_1Q_2}$ by Exercise 6 of the previous section, and we need only show that $Q_1Q_2=|x_1-x_2|$.

If either $Q_1=O$ or $Q_2=O$, the statement is obvious, hence we may assume $Q_1,Q_2\ne O$.  If $Q_1$ is between $Q_2$ and $O$, then $Q_1Q_2=OQ_2-OQ_1=|x_2|-|x_1|$ by the segment addition postulate.  In this case, if $O$ is between $Q_1$ and $A$, then $O\in\overline{Q_1A}\subset\overline{Q_1Q_2}\cup\overline{Q_2A}$, and hence $O$ is between $Q_2$ and $A$ (because $O\in\overline{Q_1Q_2}$ implies $O$ is between $Q_1$ and $Q_2$, hence $O=Q_1$, contradiction); and conversely, if $O$ is between $Q_2$ and $A$, then $O\in\overline{Q_2A}\subset\overline{Q_2Q_1}\cup\overline{Q_1A}$ likewise implying $O$ is between $Q_1$ and $A$.  Thus we have proven that
$$x_2\leqslant 0\iff O\text{ between }Q_2\text{ and }A\iff O\text{ between }Q_1\text{ and }A\iff x_1\leqslant 0$$
with that, it is easy to see through casework on the signs of $x_1,x_2$ that $|x_2-x_1|$ is the absolute value of $|x_2|-|x_1|$.  Yet $|x_2|-|x_1|=Q_1Q_2\geqslant 0$ (as we showed earlier), which further entails $|x_2-x_1|=|x_2|-|x_1|$.  Hence $Q_1Q_2=|x_2-x_1|$ in this case.

If $Q_2$ is between $Q_1$ and $O$, repeat the same argument with the roles of $Q_1,Q_2$ exchanged.  So suppose $O$ is between $Q_1$ and $Q_2$.  Then $O\in\overline{Q_1Q_2}\subset\overline{Q_1A}\cup\overline{Q_2A}$, so either $O$ is between $Q_1$ and $A$, or else it is between $Q_2$ and $A$.  We claim that this is an \emph{exclusive} disjunction: for, if $O$ were both between $Q_1$ and $A$, and between $Q_2$ and $A$, then we would have $O\in\overline{Q_1A}\cap\overline{Q_2A}$.  If $A$ is between $Q_1$ and $Q_2$ this intersection is $\{A\}$, hence $O=A$, contradiction.  If $Q_1$ is between $A$ and $Q_2$, then $O\in\overline{Q_1Q_2}\cap\overline{Q_1A}=\{Q_1\}$, another contradiction; similarly if $Q_2$ is between $A$ and $Q_1$.  Thus, we have that:
\begin{center}
Either $O$ is between $Q_1$ and $A$, or $O$ is between $Q_2$ and $A$, but not both.
\end{center}
Therefore either $x_1\leqslant 0$ or $x_2\leqslant 0$, but not both.  Simple casework then shows that $|x_1-x_2|=|x_1|+|x_2|$.  Yet since $O$ is between $Q_1$ and $Q_2$, we have $Q_1Q_2=OQ_1+OQ_2=|x_1|+|x_2|=|x_1-x_2|$ as desired.

(ii) Repeat part (i) with the roles of $x$ and $y$ exchanged, along with the roles of $\chi_1$ and $\chi_2$.

(iii) Let $P^*$ be the point $(x_2,y_1)$.  Let $\ell_1$ be the line through $(0,y_1)$ perpendicular to $\chi_2$; the argument used in part (i) entails that $\ell_1$ contains any point of the form $(x,y_1)$; hence $P_1,P^*\in\ell_1$.  Similarly, if $\ell_2$ is the line through $(x_2,0)$ perpendicular to $\chi_1$, then $P^*,P_2\in\ell_2$.  Hence since $\ell_1\perp\ell_2$, $\triangle P_1P^*P_2$ is a right triangle with $\angle P^*$ the right angle.  By Theorem 2.21, $(P_1P^*)^2+(P^*P_2)^2=(P_1P_2)^2$.  Yet $P_1P^*=|x_1-x_2|$ (by part (i)), and $P^*P_2=|y_1-y_2|$.  It follows that $(P_1P^*)^2+(P^*P_2)^2=(x_1-x_2)^2+(y_1-y_2)^2$, and hence $P_1P_2=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$ as desired.
\end{proof}
\noindent Now that we have the distance formula, we shall establish a fundamental inequality for verifying the equation of a line.\\

\noindent\textbf{Lemma 2.25.} \emph{If $a,b,c,d\in\mathbb R$ (with $c,d$ not both zero), then $\sqrt{a^2+b^2}+\sqrt{c^2+d^2}\geqslant\sqrt{(a+c)^2+(b+d)^2}$, and equality holds if and only if there is a constant $\lambda\geqslant 0$ such that $a=\lambda c,b=\lambda d$.}
\begin{proof}
Our goal is to reformulate this inequality into a more familiar one:
$$\sqrt{a^2+b^2}+\sqrt{c^2+d^2}\geqslant\sqrt{(a+c)^2+(b+d)^2}$$
Squaring both sides of the equation,
$$a^2+b^2+c^2+d^2+2\sqrt{(a^2+b^2)(c^2+d^2)}\geqslant(a+c)^2+(b+d)^2$$
Subtracting $a^2+b^2+c^2+d^2$, and using the binomial theorem on the right-hand side,
$$2\sqrt{(a^2+b^2)(c^2+d^2)}\geqslant 2ac+2bd$$
Dividing by $2$ and squaring,
$$(a^2+b^2)(c^2+d^2)\geqslant(ac+bd)^2$$
This statement is a well-known inequality in mathematics called the \emph{Cauchy-Schwarz inequality}.  It can be proven by noting that the quadratic polynomial $(cx-a)^2+(dx-b)^2$ sends real numbers to nonnegative real numbers, so its discriminant must be $\leqslant 0$ (otherwise it would have two real roots and its graph would cross the $x$-axis) \---- but straightforward computation shows the discriminant to be $4[(ac+bd)^2-(a^2+b^2)(c^2+d^2)]$.  Since our steps are reversible, we get the inequality in the lemma.

Now note that equality holds if and only if the discriminant is zero, which means that $(cx-a)^2+(dx-b)^2$ has a multiple root $\lambda$ on the real line.  Hence, we have $c\lambda-a=d\lambda-b=0$ (the only nonnegative real numbers that add to zero are zero themselves), and therefore $a=\lambda c$ and $b=\lambda d$.  If this holds and $\sqrt{a^2+b^2}+\sqrt{c^2+d^2}=\sqrt{(a+c)^2+(b+d)^2}$, then $|\lambda|\sqrt{c^2+d^2}+\sqrt{c^2+d^2}=|\lambda+1|\sqrt{c^2+d^2}$, hence $|\lambda|+1=|\lambda+1|$, which holds if and only if $\lambda\geqslant 0$.
\end{proof}
\noindent\textbf{Proposition 2.26.} \emph{Let $\ell$ be a line.}

(i) \emph{If $O\in\ell$, then there exist $a,b\in\mathbb R$, not both zero, such that $\ell=\{(x,y):ax+by=0\}$.}

(ii) \emph{In the general case, there exist $a,b,c\in\mathbb R$ such that $a$ and $b$ are not both zero and $\ell=\{(x,y):ax+by=c\}$.}

(iii) \emph{Conversely, if $\ell'=\{(x,y):ax+by=c\}$ with $a,b$ not both zero, then $\ell'$ is a line.}
\begin{proof}
(i) Let $P$ be a point of $\ell$ other than $O$.  Write $P=(b,-a)$ with $a,b\in\mathbb R$ \---- since $P\ne O$, $a$ and $b$ are not both zero.  We claim that $\ell=\{(x,y):ax+by=0\}$.

First suppose $Q=(x,y)$ is a point such that $ax+by=0$.  Then letting
$$u=\left\{\begin{array}{c l}-y/a\text{ if }a\ne 0\\x/b\text{ if }a=0\end{array}\right.$$
we have that $x=ub$ and $y=-ua$.  Note that by Proposition 2.24,
$$OQ=\sqrt{(ub)^2+(-ua)^2}=|u|\sqrt{a^2+b^2}$$
$$QP=\sqrt{(b-ub)^2+(-a+ua)^2}=|1-u|\sqrt{a^2+b^2}$$
$$OP=\sqrt{b^2+(-a)^2}=\sqrt{a^2+b^2}$$
Thus if $0\leqslant u\leqslant 1$ then $OQ+QP=OP$, and hence $Q$ must lie on the line $\ell$ containing $O$ and $P$ (otherwise we would have $OQ+QP>OP$ by Exercise 11(b) of the previous section).  If $u>1$ then $OQ=OP+PQ$, hence likewise $Q\in\ell$.  Similarly $u<0$ entails $PQ=PO+OQ$.

Conversely, suppose $Q=(x,y)\in\ell$.  Then Proposition 2.24 entails that
$$OQ=\sqrt{x^2+y^2}$$
$$OP=\sqrt{b^2+(-a)^2}=\sqrt{a^2+b^2}$$
$$QP=\sqrt{(b-x)^2+(-a-y)^2}=\sqrt{(x-b)^2+(y+a)^2}$$
Since $O,P,Q\in\ell$, at least one of the three points is between the other two.  If $Q$ is between $O$ and $P$, $OQ+QP=OP$ by the segment addition postulate, and hence
$$\sqrt{x^2+y^2}+\sqrt{(b-x)^2+(-a-y)^2}=\sqrt{a^2+b^2}=\sqrt{(x+b-x)^2+(y-a-y)^2};$$
from which Lemma 2.25 implies that either $b-x=-a-y=0$ (in which case $ax+by=0$ is clear), or else there is a constant $\lambda\geqslant 0$ such that $x=\lambda(b-x),y=\lambda(-a-y)$.  In the latter case $x=\frac{\lambda b}{\lambda+1}$ and $y=\frac{-\lambda a}{\lambda+1}$, and hence $ax+by=0$ is checked.

If $O$ is between $P$ and $Q$, or $Q$ is between $P$ and $O$, Lemma 2.25 can be likewise used to show $ax+by=0$.  We leave the verifications to the reader.

(ii) Suppose $P=(u,v)\in\ell$ is arbitrary.  Define $\mathcal T:\mathbb R^2\to\mathbb R^2$ via $\mathcal T(x,y)=(x-u,y-v)$.  By the distance formula (2.24) it is clear that whenever $A,B,A',B'$ are points such that $\mathcal T(A)=A'$ and $\mathcal T(B)=B'$, we have $A'B'=AB$.  Therefore $\mathcal T$ preserves lines by Exercise 2(a).  Hence $\ell'=\mathcal T(\ell)$ is a line that contains the origin (since $O=\mathcal T(P)$ and $P\in\ell$).  By part (i), there are $a,b\in\mathbb R$, not both zero, such that $\ell'=\{(x,y):ax+by=0\}$.  With that, it follows by definition of $\mathcal T$ and $\ell'$, that $\ell=\{(x,y):ax+by=c\}$, where $c$ is the real number $au+bv$; we leave the details to the reader.

(iii) It is clear that $\ell'$ contains at least two points: take
$$A=\left\{\begin{array}{c l}\left(0,\frac cb\right)\text{ if }b\ne 0\\\left(\frac ca,0\right)\text{ if }b=0\end{array}\right.$$
and $B$ to be the translation of $A$ by the vector $(b,-a)$; then $A$ and $B$ are both on $\ell'$.  By Axiom 2.1(i), there is a unique line $\ell$ containing $A$ and $B$.  By part (ii), $\ell$ is of the form $a_1x+b_1y=c_1$ with $a_1,b_1$ not both zero.  Now, the linear system
$$ax+by=c,~~~~a_1x+b_1y=c_1$$
has at least two solutions ($A$ and $B$), hence its coefficient matrix is singular.  Since both rows are nonzero, they are hence scalar multiples of one another; i.e., there exists $\lambda\in\mathbb R$ such that $a_1=\lambda a$ and $b_1=\lambda b$.  Multiplying the first equation by $\lambda$ then entails $c_1=\lambda c$.  Hence the equations are scalar multiples and have the same locus, so that $\ell'=\ell$ and $\ell'$ is a line.
\end{proof}

\noindent At this point it should be clear how line segments and rays are formulated.  For example, if $A=(1,0)$ and $B=(0,2)$, then the ray $\overset{\longrightarrow}{AB}$ is given by the equations
$$2x+y=2,y\geqslant 0.$$
With the distance formula in hand, it is easy to figure out angle measures.\\

\noindent\textbf{Proposition 2.27.} \emph{Suppose $A=(x_1,y_1)$ and $B=(x_2,y_2)$ are points $\ne O$.  Then $\cos(m\angle AOB)=\frac{x_1x_2+y_1y_2}{\sqrt{(x_1^2+y_1^2)(x_2^2+y_2^2)}}$.}\\

\noindent Incidentally this gives the formula for the cosine of the measure of \emph{any} angle $\angle ACB$, whether $C=O$ or not.  After all, one can adapt the proof of Proposition 2.26(b) specifically with $P=C$, and use Exercise 2(c).

Knowing the cosine of the measure, though, one can immediately find the measure (cosine is bijective from $[0^\circ,180^\circ]$ to $[-1,1]$)
\begin{proof} % Remark: This argument is applicable to any number of dimensions.  But in the general case \ell is a hyperplane.
Let $\ell$ be the line given by $x_1x+y_1y=x_1^2+y_1^2$ (i.e., $ax+by=c$ with $a=x_1,b=y_1,c=x_1^2+y_1^2$).  Then manifestly $A\in\ell$.  It turns out that $A$ is the point on $\ell$ closest to $O$, because if $P=(x,y)\in\ell$ then $x_1x+y_1y=x_1^2+y_1^2$, and therefore
$$(x^2+y^2)-(x_1^2+y_1^2)=(x^2+y^2)-2(x_1^2+y_1^2)+(x_1^2+y_1^2)$$
$$=(x^2+y^2)-2(x_1x+y_1y)+(x_1^2+y_1^2)=(x^2-2xx_1+x_1^2)+(y^2-2yy_1+y_1^2)$$
$$=(x-x_1)^2+(y-y_1)^2\geqslant 0,$$
from which it follows that $\sqrt{x^2+y^2}\geqslant\sqrt{x_1^2+y_1^2}$; i.e., $OA\leqslant OP$.  Therefore, $\overline{OA}\perp\ell$ by Exercise 3 of the previous section.

If $\ell$ is parallel to $\overset{\longleftrightarrow}{OB}$, then $\angle BOA$ is a right angle (why?)  Moreover, $\overset{\longleftrightarrow}{OB}$ is the line through $O$ parallel to $\ell$ and hence it is given by the equation $x_1x+y_1y=0$.  Since $B=(x_2,y_2)$ and is in this line, we get $x_1x_2+y_1y_2=0$, hence $\frac{x_1x_2+y_1y_2}{\sqrt{(x_1^2+y_1^2)(x_2^2+y_2^2)}}=0=\cos 90^\circ=\cos(m\angle AOB)$.

Now suppose $\ell$ is not parallel to $\overset{\longleftrightarrow}{OB}$.  Then $x_1x_2+y_1y_2\ne 0$ (if $x_1x_2+y_1y_2$ were zero, then the line $x_1x+y_1y=0$ \---- which is parallel to $\ell$ \---- would contain both $B$ and the origin, hence be $\overset{\longleftrightarrow}{OB}$).  Since it is clear from Proposition 2.26 that $\overset{\longleftrightarrow}{OB}=\{(kx_2,ky_2):k\in\mathbb R\}$, we have $Q=\overset{\longleftrightarrow}{OB}\cap\ell=(rx_2,ry_2)$ for some $r\in\mathbb R$.  Since this point is in $\ell$, $x_1(rx_2)+y_1(ry)_2=x_1^2+y_1^2$, from which $r=\frac{x_1^2+y_1^2}{x_1x_2+y_1y_2}$ follows.\footnote{At the beginning of the paragraph we ascertained that the division is defined, by showing $x_1x_2+y_1y_2\ne 0$.}

Now, $\overline{OA}\perp\ell$, and $\overline{AQ}\subset\ell$.  Therefore, $\triangle OAQ$ is a right triangle with $\angle A$ the right angle.  If $O$ is not between $Q$ and $B$, then $\overset{\longrightarrow}{OB}=\overset{\longrightarrow}{OQ}$, so we may conclude $\angle AOB=\angle AOQ$.  By definition of $\cos$ (Exercise 10 of Section 2.1), $\cos(m\angle AOQ)=\frac{AO}{OQ}$.  Now, by the distance formula,
$$AO=\sqrt{x_1^2+y_1^2}$$
$$OQ=\sqrt{(rx_2)^2+(ry_2)^2}=|r|\sqrt{x_2^2+y_2^2}$$
Now $r\geqslant 0$ (because $r<0$ would easily entail that $QB=QO+OB$, and therefore $O$ is between $Q$ and $B$), hence the $|r|$ may be changed to just $r$.  We conclude that
$$\cos(m\angle AOB)=\frac{\sqrt{x_1^2+y_1^2}}{r\sqrt{x_2^2+y_2^2}}=\frac{(x_1x_2+y_1y_2)\sqrt{x_1^2+y_1^2}}{(x_1^2+y_1^2)\sqrt{x_2^2+y_2^2}}=\frac{x_1x_2+y_1y_2}{\sqrt{(x_1^2+y_1^2)(x_2^2+y_2^2)}}$$
concluding this case.

If $O$ is between $Q$ and $B$, we get $\cos(m\angle AOB)=-\frac{AO}{OQ}$ this time around (because $\angle AOB$ and $\angle AOQ$ are supplementary angles).  We leave it to the reader to verify that in this case $r$ (and hence $x_1x_2+y_1y_2$) is negative, and so $OQ=-r\sqrt{x_2^2+y_2^2}$ and cancellation of minus signs yields the result.
\end{proof}

\noindent\emph{Remark.} When $\vec v$ and $\vec w$ are nonzero vectors, and $\theta$ is the angle between them (when they are touching tail-to-tail), then it is learned in precalculus that $\cos\theta=\frac{\vec v\cdot\vec w}{\|\vec v\|\|\vec w\|}$.  This is exactly what has been proven in Proposition 2.27 for two-dimensional vectors with $\vec v=x_1\hat i+y_1\hat j$ and $\vec w=x_2\hat i+y_2\hat j$.  The proof can be adapted to apply to two vectors in $d$-dimensional space for any positive integer $d$, but in this case $\ell$ is a hyperplane given by an equation.  $d$-dimensional space will be further studied in Section 2.5.\\

\noindent After our work on distances and anles, triangles are self-explanatory, and will be studied more in the next section anyway.  Thus, we shall go straight into circles.

We recall that if $P$ is a point and $r$ is a positive real number, the circle centered at $P$ with radius $r$ is the set of points $A$ such that $AP=r$.  If $P=(x_c,y_c)$ and $A=(x,y)$, then by the distance formula (Proposition 2.24(c)), $AP=\sqrt{(x-x_c)^2+(y-y_c)^2}$.  Hence, saying $AP=r$ is tantamount to saying that $(x-x_c)^2+(y-y_c)^2=r^2$.  Thus,
\begin{center}
The equation for the circle with radius $r$ centered at $P=(x_c,y_c)$ is $(x-x_c)^2+(y-y_c)^2=r^2$.
\end{center}
We can actually use this to prove that a circle and a line intersect in at most two points.  Let $\omega$ be the circle with center $(x_c,y_c)$ and radius $r$, and let $\ell$ be the line $ax+by=c$.  Then if $(x,y)\in\omega\cap\ell$ then
$$(x-x_c)^2+(y-y_c)^2=r^2$$
$$ax+by=c$$
If $b\ne 0$, we can derive $y=\frac{c-ax}b$ from the second equation.  Moreover, we can substitute this into the first equation:
$$(x-x_c)^2+\left(\frac{c-ax}b-y_c\right)^2=r^2$$
Basic algebra shows that this is a quadratic equation in $x$, where $a,b,c,x_c,y_c,r$ are regarded as constants.  Thus there are most two possible values of $x$ for the intersection point.  Given $x$, however, $y$ is determined (because $y=\frac{c-ax}b$), so that the circle and line intersect in at most two points.  If $b=0$, then $a\ne 0$ and the argument can be repeated with $x=\frac{c-by}a$.

Similarly, two distinct circles intersect in at most two points.  For, if $\omega$ is the circle with center $(x_c,y_c)$ and radius $r$, and $\omega'$ is the circle with center $(x_c',y_c')$ and radius $r'$, an intersection point $(x,y)\in\omega\cap\omega'$ must satisfy:
$$(x-x_c)^2+(y-y_c)^2=r^2$$
$$(x-x_c')^2+(y-y_c')^2=(r')^2$$
If $x_c=x_c'$ and $y_c=y_c'$, then $r\ne r'$ (because the circles are distinct); in this case the equations are incompatible and there are no intersection points.  If either $x_c\ne x_c'$ or $y_c\ne y_c'$, subtracting the equations gives
$$2(x_c'-x_c)x+(x_c^2-(x_c')^2)+2(y_c'-y_c)y+(y_c^2-(y_c')^2)=r^2-(r')^2$$
What is essential is that the $x^2$ and $y^2$ terms have both dropped out.  This is the equation for the line $ax+by=c$ where $a=2(x_c'-x_c)$, $b=2(y_c'-y_c)$ and $c=r^2-(r')^2-(x_c^2-(x_c')^2)-(y_c^2-(y_c')^2)$.  Thus any intersection point of the circles is also an intersection point of either circle and the line, so therefore there are at most two.

The reader should take the time to look through the axioms in Chapter 2.1 and try to prove that the concrete plane $\mathbb R^2$ satisfies each one.  Thus our axiomatic plane geometry can be realized in an algebraic setting.

\subsection*{Exercises 2.2. (Giving the Plane Coordinates)}
\begin{enumerate}
\item Let $P=(x_0,y_0)$, and let $\ell$ be the line given by $ax+by=c$ ($a,b$ not both zero).  Show that the distance from $P$ to $\ell$ (Exercise 3 of the previous section) is $\frac{|ax_0+by_0-c|}{\sqrt{a^2+b^2}}$.

\item Let $\mathcal T:\mathbb R^2\to\mathbb R^2$ be a function that preserves distances, i.e., whenever $\mathcal T(A)=A'$ and $\mathcal T(B)=B'$ we have $A'B'=AB$.

(a) $\mathcal T$ preserves lines.  [If $A$ and $B$ are distinct points, then for any $C$, the segment addition postulate and Exercise 11(b) of Section 2.1 jointly imply that $C\in\overset{\longleftrightarrow}{AB}$ if and only if one of $AB,BC,AC$ is equal to the sum of the other two.]

(b) $\mathcal T$ preserves line segments and rays.

(c) $\mathcal T$ preserves angles.  [If $\triangle ABC$ is a triangle, and $\mathcal T(A)=A',\mathcal T(B)=B',\mathcal T(C)=C'$, then $\triangle ABC\cong\triangle A'B'C'$ by SSS congruence.]

Such a $\mathcal T$ is called an \textbf{isometry}; these will be studied in more detail in Section 2.6.

\item\emph{(Some more trigonometry.)} \---- Recall the functions $\sin,\cos,\tan$ from Exercise 10 of Section 2.1.  Exercises (a)-(g) assume that $\alpha,\beta$ and any other angles that the trigonometric functions are applied to are strictly between $0^\circ$ and $90^\circ$.

(a) $\cos(\alpha-\beta)=\cos\alpha\cos\beta+\sin\alpha\sin\beta$. [Apply Proposition 2.27 to $A=(\cos\alpha,\sin\alpha)$ and $B=(\cos\beta,\sin\beta)$, first noting that $\overline{OA}$ is a line segment of length $1$ making an angle of $\alpha$ with the $x$-axis.]

(b) $\sin(\alpha+\beta)=\sin\alpha\cos\beta+\cos\alpha\sin\beta$. [By Exercise 10(c) of Section 2.1, $\sin(\alpha+\beta)=\cos((90^\circ-\alpha)-\beta)$; now use part (a).]

(c) $\sin(\alpha-\beta)=\sin\alpha\cos\beta-\cos\alpha\sin\beta$.  [Write $\sin\alpha=\sin((\alpha-\beta)+\beta)$.  Then expand using parts (a) and (b), and consider using Exercise 10(a) of Section 2.1.]

(d) $\cos(\alpha+\beta)=\cos\alpha\cos\beta-\sin\alpha\sin\beta$.

(e) $\sin(2\alpha)=2\sin\alpha\cos\alpha$ and $\cos(2\alpha)=\cos^2\alpha-\sin^2\alpha$.

(f) $\cos(\alpha/2)=\sqrt{\frac{1+\cos\alpha}2}$ and $\sin(\alpha/2)=\sqrt{\frac{1-\cos\alpha}2}$.  [Use (e) and Exercise 10(a) of Section 2.1.]

(g) $\tan(\alpha+\beta)=\frac{\tan\alpha+\tan\beta}{1-\tan\alpha\tan\beta}$ and $\tan(\alpha-\beta)=\frac{\tan\alpha-\tan\beta}{1+\tan\alpha\tan\beta}$.  Also, $\tan(2\alpha)=\frac{2\tan\alpha}{1-\tan^2\alpha}$ and $\tan(\alpha/2)=\sqrt{\frac{1-\cos\alpha}{1+\cos\alpha}}=\frac{\sin\alpha}{1+\cos\alpha}$.  [Use Exercise 10(b) of Section 2.1.]

(h) Using the results from Exercise 10(e) of Section 2.1, find $\sin\alpha$, $\cos\alpha$, $\tan\alpha$ for $\alpha=15^\circ,75^\circ$.

We extend the trigonometric functions to all real numbers through the following defining properties.  $\sin 0^\circ=0$, $\cos 0^\circ=1$, $\sin(\alpha+90^\circ)=\cos\alpha$, $\cos(\alpha+90^\circ)=-\sin\alpha$, $\tan\alpha=\frac{\sin\alpha}{\cos\alpha}$.  Observe that $\sin,\cos$ are periodic with a period of $360^\circ$ (in other words $\sin(\alpha+360^\circ)=\sin\alpha$ for all $\alpha$), and $\tan$ is periodic with a period of $180^\circ$.  However, $\tan$ is not defined at the numbers $(180k+90)^\circ$ for $k\in\mathbb Z$: the cosine of such numbers is zero.

(i) Do parts (a)-(c) of Exercise 10 of Section 2.1, and parts (a)-(g) of this problem, for these extended trigonometric functions.  [For (f) and (g) of this problem, the square roots could be either positive or negative.]

(j) Show that $\sin(-\alpha)=-\sin\alpha$, $\cos(-\alpha)=\cos\alpha$ and $\tan(-\alpha)=-\tan\alpha$.  Also show that $\tan(\alpha+90^\circ)=-\frac 1{\tan\alpha}$.

\item\emph{(Sine and cosine laws.)} \---- Suppose $\triangle ABC$ is a triangle.  Let $a=BC,b=CA,c=AB$, as shown below.
\begin{center}\includegraphics[scale=.4]{Triangle_abc.png}\end{center}
(Note that $a$ is the length of the side opposite vertex $A$, etc.)

(a) Show that $\frac a{\sin m\angle A}=\frac b{\sin m\angle B}=\frac c{\sin m\angle C}$.  This is known as the \textbf{law of sines}.  [If $\overline{AP}$ is the altitude from vertex $A$ (i.e., $P\in\overset{\longleftrightarrow}{BC}$ and $\overline{AP}\perp\overline{BC}$), explain why $AP=b\sin m\angle C=c\sin m\angle B$.]

(b) Let $\omega$ be the circumscribed circle of the triangle (Exercise 17(b) of Section 2.1), and let $O$ be the center and $r$ be the radius of $\omega$.  Then $\frac a{\sin m\angle A}=2r$.  [If $A'$ is the point $\ne B$ where $\overset{\longrightarrow}{BO}$ meets $\omega$, then $\angle BA'C\cong\angle BAC$ by Exercise 19 of Section 2.1.  Thus the problem reduces to showing that $\frac a{\sin m\angle BA'C}=2r$.  To do this, note that $\overline{A'B}$ is a diameter of the circle, and show that $\triangle A'BC$ is a right triangle with $\angle C$ the right angle.]

Since the same argument will show that $\frac b{\sin m\angle B}=2r$, etc., this actually proves the law of sines along with a stronger fact.

(c) Show that $c^2=a^2+b^2-2ab\cos m\angle C$.  This is known as the \textbf{law of cosines}.  [First suppose $m\angle B,m\angle C<90^\circ$.  If $\overline{AD}$ is the altitude from vertex $A$, let $m=CD$, $d=AD$ and $n=DB$.  Then by the Pythagorean Theorem and segment addition postulate, $d^2+m^2=b^2$, $d^2+n^2=c^2$ and $m+n=a$.  Hence $c^2=d^2+n^2=b^2-m^2+n^2=b^2+(n+m)(n-m)=b^2+a(a-2m)=a^2+b^2-2am$.  Yet $m=b\cos m\angle C$ (why?).  Similar arguments can be used for $m\angle B$ or $m\angle C\geqslant 90^\circ$.]  Note that since $\cos 90^\circ=0$, the Pythagorean Theorem is the special case where $\angle C$ is a right angle.

(d) Use these results to conclude that if $a,b,c$ are \emph{any} positive real numbers such that $a+b>c$, $b+c>a$ and $c+a>b$, there exists a triangle \---- unique up to congruence \---- with side lengths $a,b,c$.

\item\emph{(Euler's line.)} \---- Let $\triangle ABC$ be a triangle.  We shall show that the orthocenter, centroid and circumcenter are all on one line.  This line is called \textbf{Euler's line}.

Since the translations $(x,y)\mapsto(x+u,y+v)$ preserve distances and hence also (by Exercise 2) lines and angles, they preserve all the constructions we will deal with in this problem.  Thus, we may apply one to this triangle to assume its circumcenter is the origin $O$.  With that, $\overline{OA}\cong\overline{OB}\cong\overline{OC}$.  Thus, if $A=(x_1,y_1),B=(x_2,y_2),C=(x_3,y_3)$ then $x_1^2+y_1^2=x_2^2+y_2^2=x_3^2+y_3^2$.

(a) Show that the centroid of $\triangle ABC$ is $\left(\frac{x_1+x_2+x_3}3,\frac{y_1+y_2+y_3}3\right)$.  [First verify that the midpoint of $\overline{BC}$ is $\left(\frac{x_2+x_3}2,\frac{y_2+y_3}2\right)$, then form the line segment from this point to $A$ to get a median.  Show that this median contains $\left(\frac{x_1+x_2+x_3}3,\frac{y_1+y_2+y_3}3\right)$.  Then observe that the argument can be repeated with the roles of $A,B,C$ exchanged.]

(b) Show that the orthocenter of $\triangle ABC$ is $\left(x_1+x_2+x_3,y_1+y_2+y_3\right)$.  [Let $P=\left(x_1+x_2+x_3,y_1+y_2+y_3\right)$.  Then $\overset{\longleftrightarrow}{AP}$ is given by the equation $(y_2+y_3)x-(x_2+x_3)y=(y_2+y_3)x_1-(x_2+x_3)y_1$.  Also, verify that $\overset{\longleftrightarrow}{BC}$ is the line $(y_3-y_2)x-(x_3-x_2)y=y_3x_2-x_3y_2$.  Now let $\ell_1$ be the line $(y_2+y_3)x-(x_2+x_3)y=0$ and $\ell_2$ the line $(y_3-y_2)x-(x_3-x_2)y=0$; these are lines through the origin parallel to $\overset{\longleftrightarrow}{AP}$ and $\overset{\longleftrightarrow}{BC}$ respectively.  If $R=(x_2+x_3,y_2+y_3)$ and $S=(x_3-x_2,y_3-y_2)$, observe that $R\in\ell_1,S\in\ell_2$; then use Proposition 2.27 to show that $\angle ROS$ is a right angle.  Conclude that $\ell_1\perp\ell_2$, and hence $\overline{AP}\perp\overline{BC}$ and $P$ is on the altitude from $A$.]

(c) Conclude that if $O$ is the circumcenter, $E$ the centroid and $H$ the orthocenter, then $O,E,H$ all lie on one line, and $OE=\frac 12(EH)$.

\item\emph{(Feuerbach's 9-point circle.)} \---- Let $\triangle ABC$ be a triangle.  Consider the following nine points:

[1] The foot of each altitude (i.e., the point where the altitude from each vertex intersects the opposite side);

[2] The midpoint of each side (i.e., the foot of each median);

[3] The midpoint of each line segment connecting a vertex to the orthocenter.

Our goal is to prove that these nine points all lie on the arc of one circle.  This circle is called \textbf{Feuerbach's 9-point circle}.

By using a translation as in Exercise 2, we may assume that the orthocenter of $\triangle ABC$ is the origin $O$.  Now suppose $\overline{AA'},\overline{BB'},\overline{CC'}$ are the altitudes of the triangle.  Finally, let $\omega$ be the circumscribed circle of the triangle.

(a) Let $P$ be the point on $\overset{\longrightarrow}{OA'}$ such that $OP=2(OA')$; in this case $P\ne O$ and $A'P=A'O$.  Show that $P$ is on the arc of $\omega$.  [First explain why $\triangle BOA'\cong\triangle BPA'$, and use this to show that $\triangle BOC\cong\triangle BPC$.  Hence $\angle BPC\cong\angle BOC$ by CPCTC.  The fact that each altitude is perpendicular to the opposite side yields (please supply the details) that $\angle BOC$ and $\angle BAC$ are supplementary angles.  Hence $\angle BPC$ and $\angle BAC$ are supplementary angles; now use Exercise 19 of Section 2.1.]

(b) Now if $M$ is the midpoint of $\overline{BC}$, let $Q$ be the point on $\overset{\longrightarrow}{OM}$ such that $OQ=2(OM)$.  Show that $Q$ is on the arc of $\omega$.  [First show that $\triangle BOM\cong\triangle CQM$, then that $\triangle BOC\cong\triangle CQB$.  Then carry on as in part (a).]

(c) Now show that taking any of the nine points in [1]-[3], and multiplying both coordinates by $2$, yields a point on the arc of $\omega$.  [The foot $A'$ of altitude $\overline{AA'}$ yields the point $P$ of part (a).  The midpoint of $\overline{BC}$ yields the point $Q$ of part (b).  And the points in [3] yield the vertices of the triangle.]

(d) Conclude that if $\omega$ has center $(x_0,y_0)$ and radius $r$, then the nine points in [1]-[3] lie on the circle with center $(x_0/2,y_0/2)$ and radius $r/2$.

\item Let $\omega$ be a circle with radius $r$.  If $\overline{AB}$ is a chord, which intercepts an arc of angle $\alpha$, then $AB=r\sqrt{2-2\cos\alpha}$.  [If $M$ is the midpoint of $\overline{AB}$, then the line through $M$ perpendicular to $\overline{AB}$ \---- which is the perpendicular bisector \---- passes through $O$.  This implies $\triangle OMB$ is a right triangle with $\angle M$ the right angle.  Moreover, $\triangle OMB\cong\triangle OMA$ (why?); use this to show that $m\angle MOB=\alpha/2$.  Now use Exercise 3(f).]

\item\emph{(Stewart's theorem.)} \---- Let $\triangle ABC$ be a triangle, $D\in\overline{BC}$.  Then let $a=BC,b=CA,c=AB,d=AD,n=CD,m=DB$ as in the diagram below.  Note that $n+m=a$ by the segment addition postulate.
\begin{center}\includegraphics[scale=.4]{StewartsTheorem.png}\end{center}
Show that $a(mn+d^2)=b^2m+c^2n$.  [Let $\theta=m\angle ADC$ and $\varphi=m\angle ADB$.  By the law of cosines (Exercise 4(c)), $b^2=d^2+n^2-2dn\cos\theta$, and $c^2=d^2+m^2-2dm\cos\varphi$.  Yet since $\angle ADC,\angle ADB$ form a linear pair, they are supplementary angles, and therefore $\cos\varphi=-\cos\theta$.]

This useful statement in plane geometry has a clever mnemonic for memorization.  Indeed, it can be alternatively written as $man+dad=bmb+cnc$ \---- which can be remembered as ``a man and his dad put a bomb in the sink.''

\item\emph{(Conic sections.)} \---- (a) Let $a>b>0$ be fixed real numbers, and let $\gamma$ be the set of points $(x,y)$ such that $\frac{x^2}{a^2}+\frac{y^2}{b^2}=1$.  Then $\gamma$ is called an \textbf{ellipse}.  Observe that it has $(\pm a,0)$ and $(0,\pm b)$ as its leftmost, rightmost, highest and lowest points.  (Under the assumption that $a>b$), the line segment from $(-a,0)$ to $(a,0)$ is called the \textbf{major axis}, and the line segment from $(b,0)$ to $(-b,0)$ is called the \textbf{minor axis}.

Let $c=\sqrt{a^2-b^2}$.  Let $P$ be the point $(c,0)$ and $\ell$ the line $x=\frac{a^2}c$.  Show that a point $Q$ is on the ellipse if and only if the distance from $Q$ to $P$ is exactly $c/a$ times the distance from $Q$ to $\ell$.  [$P$ is called a \textbf{focus} (plural ``foci'') of the ellipse, and $\ell$ is called a \textbf{directrix} (plural ``directrices'').  The factor $c/a$ \---- which is a real number strictly between $0$ and $1$ \---- is called the \textbf{eccentricity} of the ellipse.]

(b) If $P'=(-c,0)$ and $\ell'$ is the line $x=-\frac{a^2}c$, it likewise follows that $Q$ is on the ellipse if and only if $QP'$ is $c/a$ times the distance from $Q$ to $\ell'$.  Use this to show that $PQ+P'Q=2a$ for every point $Q$ on the ellipse.

(c) Suppose $p>0$ is a fixed real number.  Let $\zeta$ be the set of points $(x,y)$ such that $x^2=4py$.  Then $\zeta$ is called a \textbf{parabola}.  Observe that $\zeta$ has $O=(0,0)$ as its lowest point, but it has no points farthest to the left, right or above.  (Indeed, for any real number $x$ whatsoever, there is a unique $y$ \---- namely $\frac{x^2}{4p}$ \---- such that $(x,y)\in\zeta$.  Moreover, these values of $y$ have no upper bound.)

If $P$ is the point $(0,p)$ and $\ell$ is the line $y=-p$, a point $Q$ is on the parabola if and only if the distance from $Q$ to $P$ is equal to the distance from $Q$ to $\ell$.  [$P$ is called the \textbf{focus} of the parabola, and $\ell$ is called its \textbf{directrix}.  Observe that the factor which relates the distances \---- which was $c/a$ in part (a) \---- is equal to $1$ this time.  Thus the eccentricity of any parabola is $1$.]

(d) Now let $a,b>0$ be fixed real numbers, and let $\eta$ be the set of points $(x,y)$ such that $\frac{y^2}{b^2}-\frac{x^2}{a^2}=1$.  $\eta$ is called a \textbf{hyperbola}.  It does not have ``farthest'' points in any direction, but it does have $(0,\pm b)$ as two \emph{local} highest and lowest points (because there are no points in $\eta$ with $-b<y<b$).  It is composed of two separate curves, one with each of these points.  The line going through the points $(0,\pm b)$ (which is the $y$-axis in this case) is called the \textbf{transverse axis}.

Note that $\frac{x^2}{a^2}-\frac{y^2}{b^2}=1$ also gives a hyperbola, but this time it has $(\pm a,0)$ as its local leftmost and rightmost points, and therefore the $x$-axis as its transverse axis.  However, in this problem, we shall specifically deal with the hyperbola $\eta$ (given by $\frac{y^2}{b^2}-\frac{x^2}{a^2}=1$).

Let $c=\sqrt{a^2+b^2}$.  Let $P$ be the point $(0,c)$ and $\ell$ the line $y=\frac{b^2}c$.  Show that a point $Q$ is on the hyperbola if and only if the distance from $Q$ to $P$ is exactly $c/b$ times the distance from $Q$ to $\ell$.  [$P$ is called a \textbf{focus} of the hyperbola and $\ell$ is called a \textbf{directrix}, just like the case for the ellipse.  The factor $c/b$ \---- which is a real number $>1$ this time \---- is called the \textbf{eccentricity}.]

(e) Adapt part (b) to show that if $P'=(0,-c)$, then for every point $Q$ on the hyperbola, $|PQ-P'Q|=2b$.
\end{enumerate}

\subsection*{2.3. Triangles, Polygons and Tilings}
\addcontentsline{toc}{section}{2.3. Triangles, Polygons and Tilings}
We recall triangles from the previous two sections.  They are structures obtained by taking three points and joining them with line segments in all possible ways.  They can be either acute, right or obtuse.  They can be either equilateral, isosceles or scalene.

First, we will study something that we have not considered in Sections 2.1 or 2.2 \---- namely, how much ``space'' is inside the triangle.  This is captured in a notion called \emph{area}, which is quite important throughout mathematics.  We will avoid delving into calculus in this section, and only deal with areas of basic figures.\\

\noindent\textbf{Definition.} \emph{Let $\triangle ABC$ be a triangle.  If the altitude from vertex $A$ meets $\overset{\longleftrightarrow}{BC}$ at point $P$, then the \textbf{area} of $\triangle ABC$ is defined to be $\frac 12(AP)(BC)$.}\\

\noindent Intuitively, $\overline{AP}$ is called the \textbf{height} of the triangle and $\overline{BC}$ is called the \textbf{base}.  The area can thus be remembered as ``half the base times the height.''

You may have noticed that this notion of area appears to be ambiguous, because it appears to depend on which vertex we take the altitude from.  However, we claim that this is not the case:\\

\noindent\textbf{Lemma 2.28.} \emph{Suppose $\triangle ABC$ is a triangle.  If the altitude from vertex $B$ meets $\overset{\longleftrightarrow}{AC}$ at point $D$, and the altitude from vertex $C$ meets $\overset{\longleftrightarrow}{AB}$ at point $E$, then $(BD)(AC)=(CE)(AB)$.  Therefore, the area of a triangle is well-defined.}
\begin{center}\includegraphics[scale=.3]{TriangleArea.png}\end{center}
\begin{proof}
$\angle BAD\cong\angle CAE$ (because they're the same angle) and $\angle ADB\cong\angle AEC$ (both are right angles).  By AA similarity, $\triangle ADB\sim\triangle AEC$.  By Proposition 2.16, $\frac{AB}{BD}=\frac{AC}{CE}$, hence $(BD)(AC)=(CE)(AB)$ as desired.
\end{proof}

\noindent Moreover, we need the following lemma to define areas of quadrilaterals and other kinds of figures.\\

\noindent\textbf{Lemma 2.29.} \emph{Suppose $\triangle ABC$ is a triangle.  If $D\in\overline{BC}$, as shown below, then $\operatorname{Area}(\triangle ABD)+\operatorname{Area}(\triangle ADC)=\operatorname{Area}(\triangle ABC)$.}
\begin{center}\includegraphics[scale=.3]{AreaAdding.png}\end{center}
\begin{proof}
If $\ell$ is the line through $A$ perpendicular to $\overset{\longleftrightarrow}{BC}$, and $P=\ell\cap\overset{\longleftrightarrow}{BC}$, we have, by definition,
$$\operatorname{Area}(\triangle ABD)=\frac 12(AP)(BD)$$
$$\operatorname{Area}(\triangle ADC)=\frac 12(AP)(DC)$$
$$\operatorname{Area}(\triangle ABC)=\frac 12(AP)(BC)$$
Yet $\frac 12(AP)(BD)+\frac 12(AP)(DC)=\frac 12(AP)(BD+DC)=\frac 12(AP)(BC)$ by the segment addition postulate; hence the statement in the lemma.
\end{proof}

\noindent\textbf{Definition}. \emph{If $n\geqslant 3$ is an integer, an \textbf{$n$-gon} (or \textbf{polygon} when $n$ is not specified) is defined to be an ordered $n$-tuple of points $A_1,A_2,\dots,A_n$ (called \textbf{vertices}) equipped with the $n$ line segments $\overline{A_1A_2},\dots,\overline{A_{n-1}A_n},\overline{A_nA_1}$ (called \textbf{edges} or \textbf{sides}), such that no two of the line segments intersect each other unless they share a vertex.}\\

\noindent Note that two line segments not intersecting does \emph{not} imply that they are parallel; the \emph{lines} that contain the segments could still intersect one another.

Some basic examples have already been covered in the chapter; e.g., if $n=3$, an $n$-gon is a triangle.  (In this case the last condition is superfluous because any two sides share a vertex.)  If $n=4$, an $n$-gon is a quadrilateral (Exercise 4 of Section 2.1).  The reader is assumed to be familiar with the names of polygons for other values of $n$ (\textbf{pentagon} for $n=5$, \textbf{hexagon} for $n=6$, \textbf{heptagon} for $n=7$, etc.)

Each vertex of an $n$-gon has its canonical angle (if $1<j<n$, the angle at $A_j$ is $\angle A_{j-1}A_jA_{j+1}$; the angle at $A_1$ is $\angle A_nA_1A_2$ and the angle at $A_n$ is $\angle A_{n-1}A_nA_1$).  A polygon is said to be \textbf{convex} if there exists a point $P$, which is not on any of the sides (nor equal to any of the vertices), and is simultaneously inside all $n$ angles.  The difference is illustrated below.
\begin{center}\includegraphics[scale=.5]{Convex.png}\end{center}
Throughout this book we will be restricting ourselves to convex polygons.  If $A_1A_2\dots A_n$ is a convex $n$-gon ($n\geqslant 4$), we define an \textbf{edge triangle} to be any of the following triangles:
$$\triangle A_nA_1A_2,~~~~\triangle A_{n-1}A_nA_1,~~~~\triangle A_{j-1}A_jA_{j+1}\text{ for }1<j<n.$$
Thus an edge triangle is the triangle obtained from three consecutive vertices.  Clearly it shares two sides with the $n$-gon; but the third side is new.  Moreover, we get a convex $(n-1)$-gon upon forgetting the middle vertex (for example, if $\triangle A_{j-1}A_jA_{j+1}$ is constructed, we get the $(n-1)$-gon $A_1\dots A_{j-1}A_{j+1}\dots A_n$).  The proof that this is really a convex $(n-1)$-gon is left to the reader.

We recall (Proposition 2.11) that the measures of the angles of a triangle add to $180^\circ$.  We are now in a position where we can add the measures of the angles of \emph{any} polygon:\\

\noindent\textbf{Proposition 2.30.} \emph{If $n\geqslant 3$, the measures of the angles of an $n$-gon add to $180(n-2)^\circ$.}\\

\noindent For instance, the measures of the angles of a pentagon add to $540^\circ$ (special case where $n=5$); the measures of the angles of a hexagon add to $720^\circ$.
\begin{proof}
We shall use induction on $n$.  The case $n=3$ has been covered in Proposition 2.11.

Now suppose inductively that $n\geqslant 4$, and the measures of the angles of an $(n-1)$-gon add to $180(n-3)^\circ$.  Let $A_1A_2\dots A_n$ be an $n$-gon.  Construct the edge triangle $\triangle A_1A_2A_3$ and the corresponding $(n-1)$-gon $A_1A_3\dots A_n$.  By Proposition 2.11, the angles of the triangle add to $180^\circ$, and by the induction hypothesis, the angles of the $(n-1)$-gon add to $180(n-3)^\circ$.  However, observe that the set of all angles of these two polygons altogether is obtained by taking the set of angles of the $n$-gon, and then splitting $\angle A_1$ and $\angle A_3$ each into two adjacent angles.  By the angle addition postulate, the sum of each pair of adjacent angles is equal to the original angle of the $n$-gon; thus the sum of the $n$ angles is also the sum of the $n+2$ angles of the triangle and $(n-1)$-gon.  Hence this sum is $180^\circ+180(n-3)^\circ=180(n-2)^\circ$.
\end{proof}

\noindent If $A_1A_2\dots A_n$ is an $n$-gon with $n\geqslant 4$, we define its \textbf{area} \---- through recursion on $n$ \---- via taking an edge triangle and adding its area to the area of the induced $(n-1)$-gon:
$$\operatorname{Area}(A_1A_2\dots A_n)=\operatorname{Area}(\triangle A_1A_2A_3)+\operatorname{Area}(A_1A_3\dots A_n).$$
As in Lemma 2.28, we must show that this is well-defined, and independent of the particular edge triangle chosen.\\

\noindent\textbf{Lemma 2.31.} \emph{If $n\geqslant 3$, then the area of an $n$-gon is well-defined.}
\begin{proof}
The case $n=3$ was proved in Lemma 2.28.

We first tackle the case $n=4$.  Suppose $ABCD$ is a quadrilateral.  Then the recursive definition has only two possible choices:\footnote{We use the notation ``$:=$''\----which indicates setting a variable to a value in computer science\----so that the reader does not assume $\operatorname{Area}(\triangle ABC)+\operatorname{Area}(\triangle CDA)=\operatorname{Area}(\triangle BCD)+\operatorname{Area}(\triangle DAB)$ a priori.}
$$\operatorname{Area}(ABCD):=\operatorname{Area}(\triangle ABC)+\operatorname{Area}(\triangle CDA);$$
$$\operatorname{Area}(ABCD):=\operatorname{Area}(\triangle BCD)+\operatorname{Area}(\triangle DAB);$$
and we must show that they give the same value.  Let $P=\overline{AC}\cap\overline{BD}$ (this point exists by the convexity).  Then $\operatorname{Area}(\triangle ABC)=\operatorname{Area}(\triangle ABP)+\operatorname{Area}(\triangle PBC)$ by Lemma 2.29.  Similar summations can be derived for the other three triangles on the right-hand sides of the above equations.  We wind up concluding that both sums are equal to $\operatorname{Area}(\triangle APB)+\operatorname{Area}(\triangle BPC)+\operatorname{Area}(\triangle CPD)+\operatorname{Area}(\triangle DPA)$, thus they are equal.

Now (using complete induction), suppose $n\geqslant 5$ and the area of a $k$-gon is well-defined for all $3\leqslant k<n$.  Suppose the area of an $n$-gon $A_1\dots A_n$ is defined using the edge triangle $\triangle A_1A_2A_3$.  Then it suffices to show that the area of $A_1\dots A_n$ is invariant under changing the edge triangle to the adjacent one $\triangle A_2A_3A_4$; as every edge triangle can be obtained by starting at $\triangle A_1A_2A_3$ and switching to an adjacent edge triangle a finite number of times.  In other words, we need only show
$$\operatorname{Area}(\triangle A_1A_2A_3)+\operatorname{Area}(A_1A_3\dots A_n)=\operatorname{Area}(\triangle A_2A_3A_4)+\operatorname{Area}(A_1A_2A_4\dots A_n).$$
The trick is to consider quadrilateral $A_1A_2A_3A_4$.  By the previous case where $n=4$, we get
\begin{equation}\tag{*}
\operatorname{Area}(\triangle A_1A_2A_3)+\operatorname{Area}(\triangle A_3A_4A_1)=\operatorname{Area}(\triangle A_2A_3A_4)+\operatorname{Area}(\triangle A_4A_1A_2).
\end{equation}
Also, by the definition of area,
$$\operatorname{Area}(A_1A_3\dots A_n)=\operatorname{Area}(\triangle A_1A_3A_4)+\operatorname{Area}(A_1A_4\dots A_n)$$
$$\operatorname{Area}(A_1A_2A_4\dots A_n)=\operatorname{Area}(\triangle A_1A_2A_4)+\operatorname{Area}(A_1A_4\dots A_n)$$
(the area of the $(n-2)$-gon $A_1A_4\dots A_n$ can be taken, since we are using \emph{complete} induction).  Thus, adding $\operatorname{Area}(A_1A_4\dots A_n)$ to both sides of (*) gives the desired statement.
\end{proof}
\noindent At this point, the reader should be able to find certain areas easily.  For example, if $ABCD$ is a quadrilateral with four right angles (which implies $AB=CD$ and $BC=DA$ by Exercise 6 of Section 2.1), then the area of $ABCD$ is $(AB)(BC)$.  [This follows because the area of a right triangle is one-half the product of the lengths of its legs.]  Such a quadrilateral is called a \textbf{rectangle}.

Notice that the angle measures of an arbitrary polygon do \emph{not} determine the side lengths up to ratios (unless $n=3$).  For example, the rectangle can have its side lengths in any ratio whatsoever \---- just let $a,b>0$ be fixed real numbers and set $A=(0,0),B=(0,b),C=(a,b),D=(a,0)$.

Likewise the side lengths do not determine the angle measures.  If $0^\circ<\alpha<180^\circ$, then the points $A=(0,0),B=(1,0),C=(1+\cos\alpha,\sin\alpha),D=(\cos\alpha,\sin\alpha)$ determine a quadrilateral $ABCD$ where all four side lengths equal $1$.  (This quadrilateral is called a \textbf{rhombus}.)  However, the fact that the side lengths equal $1$ does not determine the measure $\alpha$ (which can be taken arbitrarily).  We thus introduce several new notions.\\

\noindent\textbf{Definition.} \emph{An $n$-gon $A_1A_2\dots A_n$ is said to be:}

(i) \emph{\textbf{Equilateral} if $\overline{A_1A_2}\cong\dots\cong\overline{A_{n-1}A_n}\cong\overline{A_nA_1}$;}

(ii) \emph{\textbf{Equiangular} if $\angle A_1\cong\angle A_2\cong\dots\cong\angle A_n$;}

(iii) \emph{\textbf{Regular} if it is both equilateral and equiangular.}
\begin{center}\includegraphics[scale=.4]{EquiRegular.png}\end{center}

\noindent Note by the way, that if $n=3$ then (i) $\iff$ (ii) by Proposition 2.15.  Since (iii) is the conjunction of (i) and (ii), we conclude that every equilateral triangle is regular, and also that every equiangular triangle is regular.  However, this is not true for polygons with more sides: for instance, a rhombus satisfies (i) but not (ii), and a rectangle satisfies (ii) but not (i).

It turns out that if $A_1A_2\dots A_n$ is an equiangular $n$-gon, then we can determine the (common) angle measure.  Let $\alpha=m\angle A_1=\dots=m\angle A_n$.  Then Proposition 2.30 entails $n\alpha=180(n-2)^\circ$.  Therefore, $\alpha=\left(\frac{180(n-2)}n\right)^\circ$.  Thus we've proven that
\begin{center}
\textbf{If $A_1A_2\dots A_n$ is an equiangular $n$-gon, then each of its angles measures $\left(\frac{180(n-2)}n\right)^\circ$.}
\end{center}
In particular, a regular $n$-gon has an interior angle measure of $\left(\frac{180(n-2)}n\right)^\circ$.  We can take some sample values of $n$ to see that an equilateral triangle has angle measure $60^\circ$, a regular quadrilateral (henceforth called a \textbf{square}) has angle measure $90^\circ$, a regular pentagon has angle measure $108^\circ$, and a regular hexagon has angle measure $120^\circ$.\\

\noindent\textbf{CONGRUENCE AND SIMILARITY}\\

\noindent As in the case for triangles, $n$-gons $A_1\dots A_n$ and $A'_1\dots A'_n$ are said to be \textbf{congruent} (denoted $A_1\dots A_n\cong A'_1\dots A'_n$) if $\overline{A_jA_{j+1}}\cong\overline{A'_jA'_{j+1}}$ for $1\leqslant j<n$, $\overline{A_nA_1}\cong\overline{A'_nA'_1}$, and $\angle A_j\cong\angle A'_j$ for $1\leqslant j<n$.  And $A_1\dots A_n$ and $A'_1\dots A'_n$ are said to be \textbf{similar} (denoted $A_1\dots A_n\sim A'_1\dots A'_n$) if $\frac{A_1A_2}{A'_1A'_2}=\frac{A_2A_3}{A'_2A'_3}=\dots=\frac{A_{n-1}A_n}{A'_{n-1}A'_n}=\frac{A_nA_1}{A'_nA'_1}$, and $\angle A_j\cong\angle A'_j$ for $1\leqslant j<n$.

Most of the congruence and similarity theorems for triangles do not hold for arbitrary polygons.  However, since a regular $n$-gon has angle measure $\left(\frac{180(n-2)}n\right)^\circ$ \---- which depends only on $n$ \---- it follows that any two regular $n$-gons are similar.\\

\noindent\textbf{CENTER OF A REGULAR POLYGON AND CIRCLES}\\

\noindent Now let $A_1\dots A_n$ be a regular $n$-gon.  Let $\alpha=\left(\frac{180(n-2)}n\right)^\circ$, the angle measure of the polygon.

Suppose, for each $1\leqslant j\leqslant n$, $\ell_j$ is the angle bisector of $\angle A_j$.  We claim these angle bisectors all meet at one common point: let $O$ be $\ell_1\cap\ell_2$, the intersection of the angle bisectors of $\angle A_1$ and $\angle A_2$.  Then $\angle OA_1A_2$ and $\angle OA_2A_1$ both have measure $\alpha/2$ (why?).  Hence, $\angle OA_1A_2\cong\angle OA_2A_1$, thus $\overline{OA_1}\cong\overline{OA_2}$ by Proposition 2.15.  But $m\angle OA_2A_3=\alpha/2$ as well, and therefore $\angle OA_1A_2\cong\angle OA_2A_3$.  Finally, $\overline{A_1A_2}\cong\overline{A_2A_3}$ since this is a regular polygon.  By SAS congruence, $\triangle OA_1A_2\cong\triangle OA_2A_3$; by CPCTC $\angle OA_3A_2\cong\angle OA_2A_1$ and therefore $m\angle OA_3A_2=\alpha/2$ and $\overset{\longleftrightarrow}{OA_3}$ is the angle bisector of $\angle A_3$ (hence $O\in\ell_3$).

Now repeat this argument: $m\angle OA_2A_3=m\angle OA_3A_2=m\angle OA_3A_4$, and therefore $\overline{OA_2}\cong\overline{OA_3}$, and $\overline{A_2A_3}\cong\overline{A_3A_4}$, therefore $\triangle OA_2A_3\cong\triangle OA_3A_4$ and $m\angle OA_4A_3=\alpha/2$, making $\overset{\longleftrightarrow}{OA_4}$ the angle bisector of $\angle A_4$, so that $O\in\ell_4$.  The argument repeats successively and shows that $O\in\ell_j$ for all $j$.  This point $O$ is unique and is called the \textbf{center} of the regular polygon.

Since the argument above shows $\overline{OA_1}\cong\overline{OA_2}\cong\dots\cong\overline{OA_n}$, we can let $r$ be the common value of these line segments.  Then the circle centered at $O$ with radius $r$ \emph{contains all the vertices of the polygon} (why?).  This circle is called the \textbf{circumscribed circle} of the regular polygon.

On the other hand, for each $1\leqslant j<n$ let $A'_j$ be the midpoint of $\overline{A_1A_2}$; then let $A'_n$ be the midpoint of $\overline{A_nA_1}$.  Then by SSS congruence, $\triangle OA'_1A_1\cong\triangle OA'_1A_2$, so that $\angle OA'_1A_1\cong\angle OA'_1A_2$.  Since these angles form a linear pair, they are right angles.  Moreover, it can be shown that $\overline{OA'_1}\cong\overline{OA'_2}\cong\dots\cong\overline{OA'_n}$: for starters, since $\angle OA_1A_2\cong\angle OA_2A_3$, we have $\angle OA_1A'_1\cong\angle OA_2A'_2$.  Also $\overline{OA_1}\cong\overline{OA_2}$ and $\overline{A_1A'_1}\cong\overline{A_2A'_2}$ (both have length one-half the side length of the polygon).  By SAS congruence $\triangle OA_1A'_1\cong\triangle OA_2A'_2$; hence $\overline{OA'_1}\cong\overline{OA'_2}$ follows from CPCTC.  Repeating this argument shows the rest of the $\overline{OA'_j}$ to be congruent as well.

If $r'$ is the common value of the segments $\overline{OA'_j}$, then the circle centered at $O$ with radius $r'$ is tangent to every side of the polygon (because $\overline{OA'_j}\perp\overline{A_jA_{j+1}}$).  This is called the \textbf{inscribed circle} of the regular polygon.

Thus, every regular polygon (like arbitrary triangles) has a circumscribed circle and an inscribed circle.  However, arbitrary polygons need not have them.  For instance, a quadrilateral has a circumscribed circle if and only if it is cyclic (Exercise 19 of Section 2.1).

Going in the other direction, let $n$ be an integer $\geqslant 3$ and $\omega$ a circle with center $O$.  We construct $n$ radii of $\omega$ as follows: Let $\overline{OA_1}$ be an arbitrary radius.  If $\alpha=(360/n)^\circ$, then by Axiom 2.6(i) there exists a ray with endpoint $O$ making an angle of $\alpha$ with $\overset{\longrightarrow}{OA_1}$.  Suppose it meets $\omega$ at $A_2$.  Now for $\overset{\longrightarrow}{OA_2}$ there are \emph{two} rays coming from $O$ making an angle of $\alpha$ \---- one of them is $\overset{\longrightarrow}{OA_1}$, so let the other one meet $\omega$ at $A_3$.  Likewise, let the ray making an angle of $\alpha$ from $\overset{\longrightarrow}{OA_3}$ (other than $\overset{\longrightarrow}{OA_2}$) meet $\omega$ at $A_4$.  Continue this process to define $A_5,\dots,A_n$.

Since $\alpha=m\angle A_1A_2=m\angle A_2A_3=\dots=m\angle A_{n-1}A_n$, but also $m\angle A_1A_2+\dots+m\angle A_{n-1}A_n+m\angle A_nA_1=360^\circ$ (the angles close around a point), we have that $m\angle A_nA_1=\alpha$ as well.  The case $n=8$ is illustrated down below.
\begin{center}\includegraphics[scale=.4]{WheelSpokes.png}\end{center}
With that, it follows that $A_1\dots A_n$ is a regular $n$-gon: Since $\angle A_1OA_2\cong\angle A_2OA_3$ (both have measure $\alpha$) and $\overline{OA_1}\cong\overline{OA_2}\cong\overline{OA_3}$ (they are radii), we have $\triangle A_1OA_2\cong\triangle A_2OA_3$ by SAS, and therefore $\overline{A_1A_2}\cong\overline{A_2A_3}$.  Similar arguments show all the triangles $\triangle A_jOA_{j+1}$ ($1\leqslant j<n$) and $\triangle A_nOA_1$ are congruent, and hence all sides of the $n$-gon to be congruent.  Moreover, the congruence $\triangle A_nOA_1\cong\triangle A_1OA_2\cong\triangle A_2OA_3$ shows that, by CPCTC,
\begin{align*}
m\angle A_nA_1A_2 & =m\angle A_nA_1O+\angle OA_1A_2 \\
& =m\angle A_1A_2O+\angle OA_2A_3 = m\angle A_1A_2A_3
\end{align*}
so that $\angle A_nA_1A_2\cong\angle A_1A_2A_3$; and similar arguments show all $n$ angles of the $n$-gon to be congruent.

Since $A_1,\dots,A_n$ are all in $\omega$, $\omega$ is the circumscribed circle of the $n$-gon and $O$ is the $n$-gon's center.  Similarly, we get a regular polygon for which $\omega$ is the \emph{inscribed} circle by taking the line tangent to $\omega$ at each $A_j$.  We leave the verifications to the reader.\\

\noindent\textbf{TILINGS}\\

\noindent We conclude this section by showing how polygons can tile the plane.  Polygonal tilings are seen many places, such as on bathroom floors, or in bee's honeycomb.  By a \emph{tiling} we mean that the polygons along with their interiors\footnote{The interior, of course, means the set of points inside all angles of the polygon.  These points exist as long as the polygon is convex.} unite to form the plane, and that the intersection of any two polygons is either (i) empty, (ii) a single vertex of each one or (iii) an entire edge of each one.  (For example, we do not allow the vertex of any polygon to be in the middle of an edge of another, nor do we allow any two polygons to have overlapping interior.)  The polygons are called the \textbf{faces} of the tiling, and the edges (resp., vertices) of the polygons are called the \textbf{edges} (resp., \textbf{vertices}) of the tiling.

The first question is, if $n$ is an integer $\geqslant 3$, when can the plane be tiled by just regular $n$-gons?  For starters, the $n$-gons are all congruent (as we will see).  Every vertex must have at least three faces around it (because the angle measures are strictly $<180^\circ$).  Let $\alpha=\left(\frac{180(n-2)}n\right)^\circ$, the interior angle measure of an $n$-gon.  Let $k$ be the number of faces ($n$-gons) around a certain vertex; then the angles around that vertex \---- each of which measures $\alpha$ \---- must add to a total of $360^\circ$ to circulate the point.  This entails $k\alpha=360^\circ$.  Thus the tiling will only exist if $\alpha$ is an exact divisor of $360^\circ$.

If $n=3$ then $\alpha=60^\circ$; in this case $360^\circ=6\alpha$, so the tiling will work with $6$ equilateral triangles around each vertex; see below.  If $n=4$ then $\alpha=90^\circ$, so that $360^\circ=4\alpha$ and one can fit $4$ squares around each vertex.  If $n=5$ then $\alpha=108^\circ$: in this case the tiling does not exist, because $k\alpha=360^\circ$ implies $3<k<4$, which means that three regular pentagons will not be enough to circulate the vertex, and a fourth pentagon will cause an overlap (of interiors).  If $n=6$ then $\alpha=120^\circ$, so $3$ regular hexagons can be fit around each vertex.

Finally, there is never a tiling for $n\geqslant 7$, because in this case $\alpha>120^\circ$, making it impossible to have three faces around a vertex.  Thus the plane can be tiled by regular $n$-gons alone if and only if $n=3$, $4$ or $6$:
\begin{center}\includegraphics[scale=.4]{Tilings1.png}\end{center}
Though these are the only tilings involving just \emph{one} type of regular polygon; there are many other tilings involving two or more types of regular polygons; the first two tilings below are examples.
\begin{center}\includegraphics[scale=.4]{Tilings2.png}\end{center}
The first of these tilings is made up of two distinct types of regular polygons, a square and an octagon; and the second is made up of three distinct types of regular polygons, a triangle, a square and a hexagon.  The third tiling, however, is made up of pentagons \---- not \emph{regular} pentagons; rather, congruent copies of the pentagon with vertices $(-1,0),(1,0),(2,2),(0,3),(-2,2)$.  It is worth noting that the first two tilings have the same pattern of faces around each vertex (e.g., the first tiling has two octagons and one square at each vertex); and each vertex of the third tiling has either four right angles or three angles, and every three-angle vertex has the same angle measures. %% About the bibliography thing, I don't see myself needing to add a source to the biblio, if I didn't use it to write this nor was I aware of it.  The Giant Golden Book of Mathematics is the main book where I remember learning the regular tilings, if that is worth citing.

\subsection*{Exercises 2.3. (Triangles, Polygons and Tilings)}
\begin{enumerate}
\item Let $\triangle ABC$ be a triangle.

(a) Show that its area is equal to $\frac 12(AB)(AC)\sin m\angle A$.  [Let the altitude from vertex $B$ meet $\overset{\longleftrightarrow}{AC}$ at point $P$.  Then explain why $BP=AB\sin m\angle A$.]

(b) If $A=(0,0)$, $B=(x_1,y_1)$ and $C=(x_2,y_2)$, show that the area is equal to $\frac 12\big|x_1y_2-x_2y_1\big|$.  [Use part (a) and Exercise 3(c) of Section 2.2.]

\item If $\triangle ABC$ is a triangle with $A=(x_1,y_1),B=(x_2,y_2),C=(x_3,y_3)$, show that the area of $\triangle ABC$ is
$$\frac 12\big|x_1(y_2-y_3)+x_2(y_3-y_1)+x_3(y_1-y_2)\big|$$
[Verify that the above expression is invariant under translations (i.e., $(x,y)\mapsto(x+u,y+v)$ for $u,v$ fixed).  Use this to assume $A=(0,0)$.]

\item\emph{(Sine sum formula.)} \---- Here is another proof that if $\alpha,\beta$ are angles, $\sin(\alpha+\beta)=\sin\alpha\cos\beta+\cos\alpha\sin\beta$.
\begin{center}\includegraphics[scale=.4]{SineSum2.png}\end{center}
Let $\triangle ADB$ and $\triangle ADC$ be right triangles, adjacent as above, with $m\angle CAD=\alpha$ and $m\angle DAB=\beta$.

(a) Show that the area of $\triangle ADC$ is $\frac 12ab\sin\alpha\cos\beta$.  [First explain why the area of a right triangle is one-half the product of the lengths of its legs.]

(b) Show that the area of $\triangle ADB$ is $\frac 12ab\cos\alpha\sin\beta$.

(c) By Lemma 2.29, the area of $\triangle ABC$ is $\frac 12ab\sin\alpha\cos\beta+\frac 12ab\cos\alpha\sin\beta=\frac 12ab(\sin\alpha\cos\beta+\cos\alpha\sin\beta)$.  Now use Exercise 1(a) to conclude.

\item (a) The area of an equilateral triangle with side length $s$ is equal to $\frac{\sqrt 3}4s^2$.

(b) The area of a square with side length $s$ is equal to $s^2$.

\item Suppose $A_1\dots A_n\sim A'_1\dots A'_n$ are similar $n$-gons with $r=\frac{A_1A_2}{A'_1A'_2}$.  [$r$ is called the \textbf{similarity ratio of $A_1\dots A_n$ to $A'_1\dots A'_n$}.]  Show that $\frac{\operatorname{Area}(A_1\dots A_n)}{\operatorname{Area}(A'_1\dots A'_n)}=r^2$.

\item Let $\triangle ABC$ be a triangle, $P$ a point inside the triangle.  Let $\overset{\longrightarrow}{AP}$ meet side $\overline{BC}$ at point $A'$, let $\overset{\longrightarrow}{BP}$ meet $\overline{AC}$ at $B'$, and let $\overset{\longrightarrow}{CP}$ meet $\overline{AB}$ at $C'$.  Then $\frac{PA'}{AA'}+\frac{PB'}{BB'}+\frac{PC'}{CC'}=1$.  [First show that $\frac{PA'}{AA'}=\frac{\operatorname{Area}(\triangle BPC)}{\operatorname{Area}(\triangle BAC)}$.]

\item A \textbf{diagonal} of an $n$-gon is defined to be a line segment between two distinct vertices which is not a side.  Show that an $n$-gon has $\frac{n(n-3)}2$ diagonals.  [In particular, a triangle has no diagonals; a quadrilateral has $2$ diagonals; a pentagon has $5$ diagonals; a hexagon has $9$ diagonals.]

If the $n$-gon is regular, how many diagonals have the same lengths?  What are these lengths?

\item Let $ABCD$ be a square of side length $s$.  Show that the length of a diagonal of the square is $s\sqrt 2$.

\item Let $ABCDE$ be a regular pentagon of side length $s$, and let $\omega$ be its circumscribed circle.

(a) Show that the five diagonals of the pentagon are all congruent.

(b) Show that the length of a diagonal is equal to $s\frac{1+\sqrt 5}2$.  [Use Ptolemy's theorem (Exercise 22 of Section 2.1) on the quadrilateral $ABCD$.  Note that if $\phi=\frac{1+\sqrt 5}2$ then $\phi$ has the special property that $\phi^2=\phi+1$ \---- this number is called the \textbf{golden ratio}.]

\item\emph{(Area of a circle.)} \---- Let $\omega$ be a circle with center $O$, radius $r$.  Though we have not given a definition of the area of a circle, this exercise uses approximations togive a reasonable formula for (or bounds on) the area.

(a) If $n\geqslant 3$, let $A_1\dots A_n$ be a regular $n$-gon having $\omega$ as a circumscribed circle.  Then $A_1\dots A_n$ has area $\frac{nr^2}2\sin((360/n)^\circ)$.  [Split the $n$-gon into the triangles $\triangle A_jOA_{j+1}$ ($1\leqslant j<n$), $\triangle A_nOA_1$; then use Exercise 1(a).]

(b) Since making $n$ larger and larger makes the region inside the polygon closer and closer to that of the circle (why?), we now take the limit as $n\to\infty$ to obtain the area of the circle.  To do this we first convert the angle to a radian measure \---- i.e., we think of the area as $\frac{nr^2}2\sin(2\pi/n)$.  Show that $\lim_{n\to\infty}\frac{nr^2}2\sin(2\pi/n)=\pi r^2$, and hence the circle has area $\pi r^2$.  [Substitute $h=1/n$ in the expression, and use L'H\^opital's Rule to find the limit as $h\to 0$.]

\item\emph{(Pythagorean Theorem.)} \---- Here is an alternative proof of the Pythagorean Theorem (2.21) using areas.  Suppose $\triangle ABC$ is a right triangle with $\angle B$ the right angle, $AB=a,BC=b,AC=c$.  Take a square of side length $a+b$, and construct four line segments creating four congruent copies of $\triangle ABC$ as below.
\begin{center}\includegraphics[scale=.4]{PythTheoremAlt.png}\end{center}
(a) Explain why the shaded area has area $c^2$.

(b) $\operatorname{Area}(\triangle ABC)=\frac 12ab$, and the area of the big square of side length $a+b$ is $(a+b)^2=a^2+2ab+b^2$.  Use this to show that the shaded area has area $a^2+b^2$.

\item This exercise gives many examples of tilings made up of two or more types of regular polygons.  Try a hand at constructing each of these tilings, either using pencil and paper or computer graphics software.  [All the polygons have the same side length in each tiling.]

(a) Octagons and squares, with two octagons and one square at each vertex

(b) Hexagons and triangles, with one hexagon and one triangle at each edge

(c) Dodecagons\footnote{A dodecagon is a $12$-sided polygon.} and triangles, with two dodecagons and one triangle at each vertex

(d) Triangles, squares and hexagons, with one triangle, two squares and one hexagon at each vertex (the squares do not share any edges)

(e) Dodecagons, hexagons and squares, with one of each kind of polygon at each vertex

(f) Hexagons and triangles, with one hexagon and four triangles at each edge [This tiling has no orientation-reversing symmetry.]

(g) Triangles and squares, with three triangles and two squares at each vertex (the squares do not share any edges)

(h) Triangles and squares, with three triangles and two squares at each vertex (but this time, the two squares at each vertex share an edge)
\end{enumerate}

\subsection*{2.4. Straightedge and Compass Constructions}
\addcontentsline{toc}{section}{2.4. Straightedge and Compass Constructions}
One of the most beautiful things about plane geometry is the ability to draw mathematically accurate figures.  Many tools can be used to draw these figures, ranging from binder edges, compasses, rulers, or even a loop of string.\footnote{An ellipse can be drawn by placing tacks at the foci and putting a loop of string around them \---- see Exercise 9 of Section 2.2.}  In this section, we will particularly focus on the most basic kind of geometric tools: the straightedge and compass.

We will assume we are given an infinite-sized sheet of paper, a writing utensil with unlimited ink or graphite, a \textbf{straightedge} (a hard object which can be used to construct lines, such as the edge of a binder), and a \textbf{compass} (which can be used to construct perfect circles).  [In real life, situations are obviously different, but we would like to stick to the mathematical setting where supplies are unlimited.]

Our ambition in this section is to establish what kinds of things can be constructed, purely with the use of a straightedge and compass.  For starters, there are only three kinds of \emph{deterministic} constructions we can do with these tools.

(A) Given two distinct points, drawing the line between them (using the straightedge);

(B) Given distinct points $O$ and $P$, drawing the circle centered at $O$ with radius $OP$ (so that $P$ is on the arc \---- this is done by setting the compass point to $O$ and the graphite to $P$ then twirling it around);

(C) Taking intersection points of lines/circles with other lines/circles.

In addition, we would like to do a few \emph{nondeterministic} constructions:

(D) Given a point, one can construct a line through it going in an arbitrary direction.  However, without the use of other constructions, it cannot be controlled to a particular direction.  [Remember, you only have a straightedge and compass.] % Yes, you can pick any Q \ne P and draw \overset{\longleftrightarrow}{PQ}.  It's just that without other constructions, you can't put it in an exact/precise direction you described with words.

(E) One can draw a circle with a possibly arbitrary center or radius.  But the center/radius cannot be controlled without the use of the other constructions.  [Note that one might be able to control a \emph{relation} between the center and radius without controlling either one separately, if they start with the arc (compass' graphite) on a constructed point.]

(F) A point can be taken on a line or circle.  However, without other constructions, you cannot control its exact position with your mind \---- you can only declare it to be on the line or circle.

We also assume that we can copy a basic construction accurately on a different spot of the paper:

(G) Given a line segment $\overline{AB}$, the compass radius can be adjusted to the length $AB$.  Thus, one can construct other line segments with the same length, given a point on a line (or possibly not given a line).  In other words, given $C$, and any line $\ell$ through $C$, one can construct $D$ on $\ell$ so that $CD=\ell$.

(H) Given an angle $\angle BAC$, one can construct angles of measure $m\angle BAC$ anywhere on the paper, given any line/segment/ray (or possibly not given any).

It is a bit tricky to grasp the exact concepts.  Any segment length can come ``randomly'' (for example, if you just draw a line and mark two points on it), but there are only certain segment lengths that you can think of, and do some of the aforementioned valid steps, to \emph{guarantee} a line segment of that length.  To delve into this concept, you will first need to (for normalization purposes) draw a nondegenerate line segment, and declare its length to be $1$.  [This is called the \textbf{unit segment}.]

We identify $\mathbb R^2$ with $\mathbb C$ by the correspondence $(a,b)\leftrightarrow a+bi$.  We now define\\

\noindent\textbf{Definition.} \emph{A real number $r$ is said to be \textbf{constructible} if one can apply steps (A)-(H) in such a way that will guarantee them a line segment of length $|r|$.  A complex number $z$ is said to be \textbf{constructible} if its real and imaginary parts are both constructible.}\\

\noindent The importance of constructible complex numbers will be revealed later on.

As a basic example of the definition, $1$ is constructible (because we declared it to be the length of the unit segment).  $0$ is clearly constructible too (because if $A$ is any point then $0=AA$).  Moreover, $2$ is constructible \---- if you let $\ell$ be a line and $A,B,C\in\ell$ be points such that $AB=BC=1$ (possible by constructibility of $1$ by (G)), we have $AC=2$ by the segment addition postulate.

The best way to classify constructible numbers is to digress and introduce an algebraic concept called a field.\\

\noindent\textbf{FIELDS}\\

\noindent A field is essentially a ``restricted'' number system where the four basic arithmetic operations (addition, subtraction, multiplication and division) are all possible.  It is assumed to consist of complex numbers; this includes real numbers and rational numbers.  We now give a formal definition.\\

\noindent\textbf{Definition.} \emph{A \textbf{subfield of $\mathbb C$} (or a \textbf{field} in this section) is a subset $F$ of the set of complex numbers $\mathbb C$ such that:}

(i) \emph{Whenever $a,b\in F$, $a+b\in F$ and $ab\in F$.}

(ii) \emph{$0$ and $1$ are in $F$.}

(iii) \emph{Whenever $a\in F$, we have $-a\in F$, and if $a\ne 0$ then $1/a\in F$.}\\

\noindent Effectively, this states that $F$ is a subgroup of the additive group $\mathbb C$, and that $F-\{0\}$ is a subgroup of the multiplicative group $\mathbb C_{\ne 0}$.

For example, $\mathbb Q,\mathbb R,\mathbb C$ are all fields.  $\mathbb Q(\sqrt 2)=\{x+y\sqrt 2:x,y\in\mathbb Q\}$ is a field as well; here are the main things to check to show this:
\begin{center}
$(x+y\sqrt 2)(x'+y'\sqrt 2)=(xx'+2yy')+(xy'+yx')\sqrt 2$;

If $x+y\sqrt 2\ne 0$ then $x^2-2y^2\ne 0$,\footnote{If $x^2-2y^2=0$, then $y$ must be zero, otherwise we would have $x/y=\pm\sqrt 2$, contradicting the irrationality of $\sqrt 2$.  With $y=0$, we obviously have $x=0$ as well, so $x+y\sqrt 2=0$.} and $\frac 1{x+y\sqrt 2}=\frac x{x^2-2y^2}-\frac y{x^2-2y^2}\sqrt 2$.
\end{center}
In $\mathbb Q(\sqrt 2)$, every element can uniquely be written as $x+y\sqrt 2$ with $x,y\in\mathbb Q$.  (The uniqueness follows from the fact that if $x+y\sqrt 2=0$, we must have $x=y=0$.)  This may sound like a familiar concept: in linear algebra, we learn that if vectors $\vec v_1,\vec v_2,\dots,\vec v_n\in\mathbb R^n$ form a basis, then every element of $\mathbb R^n$ is uniquely expressible as $c_1\vec v_1+\dots+c_n\vec v_n$, $c_1,\dots,c_n\in\mathbb R$ (the spanning of the vectors implies existence, and we have uniqueness due to linear independence).  We now generalize this to various fields.\\

\noindent\textbf{Definition.} \emph{Let $F\subset K$ be fields, and let $u_1,\dots,u_n\in K$.  Then:}

(i) \emph{$\{u_1,\dots,u_n\}$ is said to \textbf{span $K$} over $F$ if every element of $K$ can be written as $c_1u_1+\dots+c_nu_n$ with the $c_j\in F$.}

(ii) \emph{$\{u_1,\dots,u_n\}$ is said to be \textbf{linearly independent} over $F$ provided that whenever $c_1u_1+\dots+c_nu_n=0$ with the $c_j\in F$, we have $c_1=c_2=\dots=c_n=0$.  The set is said to be \textbf{linearly dependent} if it is not linearly independent.}

(iii) \emph{$\{u_1,\dots,u_n\}$ is said to be a \textbf{basis} (plural ``bases'') of $K$ over $F$ if it both spans $K$ and is linearly independent over $F$.}

\emph{If $F=\mathbb Q$, then $K$ is said to be a \textbf{number field} if it has a finite basis over $\mathbb Q$.}\\

\noindent\textbf{Examples.}

(1) The set $\{1,\sqrt 2\}$ is a basis of $\mathbb Q(\sqrt 2)$ over $\mathbb Q$, as we have previously established.  On the other hand, consider the set $\{2,7\sqrt 2,1+\sqrt 2\}$.  This set spans $\mathbb Q(\sqrt 2)$ over $\mathbb Q$, because any $x+y\sqrt 2,x,y\in\mathbb Q$ can be written as $\frac x2(2)+\frac y7(7\sqrt 2)$.  However, it is \emph{not} linearly independent, because there exist linear combinations that equal zero without the coefficients being zero, for example:
$$7(2)+2(7\sqrt 2)-14(1+\sqrt 2)=0$$
(2) Consider the subset $\{\sqrt 2,\sqrt 5\}$ of $\mathbb R$.  If $x\sqrt 2+y\sqrt 5=0$ for $x,y\in\mathbb Q$, then $y$ must be zero (otherwise, we would have $-x/y=\sqrt{5/2}$, and elementary number theory shows that $\sqrt{5/2}$ is irrational).  Therefore $0=x\sqrt 2+0\sqrt 5=x\sqrt 2$ from which $x=0$ follows.  This proves $x=y=0$, and therefore, $\{\sqrt 2,\sqrt 5\}$ is linearly independent over $\mathbb Q$.  However, $\{\sqrt 2,\sqrt 5\}$ does not span $\mathbb R$ over $\mathbb Q$ because, for example, there is no way to write $\sqrt 3\in\mathbb R$ as $x\sqrt 2+y\sqrt 5$ with $x,y\in\mathbb Q$; see Exercise 1.\\

\noindent Now we introduce the concept of dimension.  The statements for Lemma 2.31 and Propositions 2.32-2.33 are slightly unorthodox, since they are usually for a vector space $V$ over $F$,\footnote{This means that $(V,+,0)$ is an abelian group, and there is a ``scalar multiplication'' $(a,x)\mapsto ax$ from $F\times V\to V$ such that $a(x+y)=ax+ay$, $(a+b)x=ax+bx$, $(ab)x=a(bx)$ and $1x=x$.  If $F\subset K$ are fields, then $K$ is clearly an example.} rather than a field.  But since fields are the only vector spaces involved in this section, we will omit the details.\\

\noindent\textbf{Lemma 2.31.} \emph{Let $F\subset K$ be fields, and $u_1,\dots,u_n\in K$.}

(i) \emph{If $v\in K$ is a linear combination of $u_1,\dots,u_n$, then $\{v,u_1,\dots,u_n\}$ is linearly dependent over $F$.}

(ii) \emph{$\{u_1,\dots,u_n\}$ is linearly dependent if and only if there exists $1\leqslant k\leqslant n$ and $c_1,\dots,c_{k-1}\in F$ such that $u_k=c_1u_1+\dots+c_{k-1}u_{k-1}$.  In other words, an ordered list of elements of $K$ is linearly dependent if and only if some element is a linear combination of the preceding ones.}
\begin{proof}
(i) because $v=c_1u_1+\dots+c_nu_n$ implies that $1v-c_1u_1-\dots-c_nu_n=0$, and the coefficient $1$ is nonzero.

(ii) If $u_k=c_1u_1+\dots+c_{k-1}u_{k-1}$, then we clearly have linear dependence, because
$$c_1u_1+\dots+c_{k-1}u_{k-1}+(-1)u_k+0u_{k+1}+\dots+0u_n=0$$
and the coefficient $-1$ is nonzero.  Conversely, suppose $u_1,\dots,u_n$ are linearly dependent.  Then there exist $a_1,\dots,a_n\in F$, such that $a_1u_1+\dots+a_nu_n=0$ but the $a_j$ are not all zero.  Let $k$ be the largest integer such that $a_k\ne 0$.  Then we have $a_{k+1}=\dots=a_n=0$, hence $a_1u_1+\dots+a_ku_k=0$ and
$$u_k=-\frac{a_1}{a_k}u_1-\dots-\frac{a_{k-1}}{a_k}u_{k-1}$$
as desired.
\end{proof}

\noindent In Example (1) above, notice that the spanning set has more elements than the linearly independent set first mentioned.  This is no accident, as we show that if $F\subset K$ are fields, any linearly independent subset of $K$ cannot contain any more elements than a spanning subset.\\

\noindent\textbf{Proposition 2.32.} \emph{Suppose $F\subset K$ are fields.  If $\{v_1,\dots,v_n\}\subset K$ spans $K$ over $F$, and $\{u_1,\dots,u_m\}\subset K$ is linearly independent over $F$, then $m\leqslant n$.}
\begin{proof}
First we shall use induction on $k$ to show that if $1\leqslant k\leqslant\min(m,n)$, there exist $n-k$ elements of $\{v_1,\dots,v_n\}$, say $v_{j_1},\dots,v_{j_{n-k}}$, such that $\{u_1,\dots,u_k,v_{j_1},\dots,v_{j_{n-k}}\}$ spans $K$ over $F$.

If $k=1$, then note that $u_1$ \---- like every element of $K$ \---- is a linear combination of $v_1,\dots,v_n$.  By Lemma 2.31(i), $\{u_1,v_1,\dots,v_n\}$ is linearly dependent.  By Lemma 2.31(ii) one of the elements \---- say $v_j$ \---- is a linear combination of the preceding ones, say $v_j=au_1+c_1v_1+\dots+c_{j-1}v_{j-1}$.  With that, the set $\{u_1,v_1,\dots,v_{j-1},v_{j+1},\dots,v_n\}$ obtained by removing $v_j$ still spans $K$ over $F$, because every element of $K$ is a linear combination of $v_1,\dots,v_n$, and then in such a linear combination, we can replace $v_j$ with $au_1+c_1v_1+\dots+c_{j-1}v_{j-1}$ and expand the result.  This concludes the case where $k=1$.

Now suppose inductively that $k<\min(m,n)$ and there are $n-k$ elements $v_{j_1},\dots,v_{j_{n-k}}$, such that $\{u_1,\dots,u_k,v_{j_1},\dots,v_{j_{n-k}}\}$ spans $K$.  Then $u_{k+1}$ is a linear combination of those elements, and hence $\{u_1,\dots,u_k,u_{k+1},v_{j_1},\dots,v_{j_{n-k}}\}$ is linearly dependent.  Therefore, some element is a linear combination of the preceding ones.  This element cannot be one of the $u$'s (because that would imply that $\{u_1,\dots,u_m\}$ is linearly dependent), hence it must be one of the $v$'s, and we can, as before, remove that $v$ and still have a spanning set.  This spanning set consists of $u_1,\dots,u_{k+1}$ and $n-(k+1)$ $v$'s, completing this inductive step.

If $m>n$, then the statement in the first paragraph holds for $k=n$, which means $\{u_1,\dots,u_n\}$ spans $K$ over $F$.  In this case, $u_m$ is a linear combination of $u_1,\dots,u_n$: contradiction, because $\{u_1,\dots,u_m\}$ is linearly independent.
\end{proof}
\noindent\textbf{Proposition 2.33.} \emph{Suppose $F\subset K$ are fields.  Then any two (finite) bases have the same number of elements.}
\begin{proof}
If $\{v_1,\dots,v_n\}$ and $\{u_1,\dots,u_m\}$ are bases, then two applications of Proposition 2.32 yield $m\leqslant n$ and $n\leqslant m$.  Hence, $m=n$.
\end{proof}
\noindent Thus, the number of elements of a basis of $K$ over $F$ is independent of the particular basis.  We hereby have the following definition.\\

\noindent\textbf{Definition.} \emph{Suppose $F\subset K$ are fields.  If there exists a (finite) basis of $K$ over $F$, then the number of elements in a basis of $K$ is denoted $[K:F]$ and called the \textbf{dimension} of $K$ over $F$.  Such a field is said to be \textbf{finite-dimensional}.  If no (finite) basis exists, then $K$ is said to be \textbf{infinite-dimensinal} over $F$.}\\

\noindent For example, $[\mathbb Q(\sqrt 2):\mathbb Q]=2$ because $\{1,\sqrt 2\}$ is a basis of $\mathbb Q(\sqrt 2)$ over $\mathbb Q$.  However, $\mathbb R$ is infinite-dimensional over $\mathbb Q$ \---- Exercise 2 provides a strategy for proving this.  Also, the number fields are precisely the fields which are finite-dimensional over $\mathbb Q$.

Now for a few basic facts about dimension.\\

\noindent\textbf{Proposition 2.34.} \emph{Suppose $F\subset K$ are fields.  Then $[K:F]=1$ if and only if $K=F$.}
\begin{proof}
Suppose $[K:F]=1$.  Then $K$ has some single-element basis, say $\{u\}$.  Since $\{u\}$ spans $K$ over $F$, we can write, for instance $1=cu$ with $c\in F$.  This entails that $u=1/c\in F$.  With that, clearly $au\in F$ for all $a\in F$, which means $K=F$.

Conversely, if $K=F$, then $\{1\}$ is a single-element basis of $K$ over $F$.
\end{proof}
\noindent\textbf{Proposition 2.35.} \emph{Suppose $F\subset K\subset L$ are fields with $L$ finite-dimensional over $K$, and $K$ finite-dimensional over $F$.  Then $L$ is finite-dimensional over $F$ and $[L:F]=[L:K][K:F]$.}
\begin{proof}
Suppose $[L:K]=n$ and $[K:F]=m$; we show that $[L:F]=nm$.

Let $\{v_1,\dots,v_n\}$ be a basis of $L$ over $K$, and $\{u_1,\dots,u_m\}$ is a basis of $K$ over $F$.  Then consider the set $S=\{v_ju_k:1\leqslant j\leqslant n,1\leqslant k\leqslant m\}\subset L$.  Observe that it contains $nm$ elements (they are all distinct because if $v_ju_k=v_{j'}u_{k'}$ with $(j,k)\ne(j',k')$, then $u_kv_j-u_{k'}v_{j'}=0$ \---- yet $u_k,u_{k'},u_k-u_{k'}$ are nonzero elements of $K$, so this contradicts the linear independence of the $v$'s over $K$).

Our aim is to show that $S$ is a basis of $L$ over $F$.  Firstly, take any $x\in L$.  Since $v_1,\dots,v_n$ span $L$ over $K$, we can write $x=c_1v_1+\dots+c_nv_n$ with the $c_j\in K$.  Moreover, since $u_1,\dots,u_m$ span $K$ over $F$, every $c_j$ can be written as a linear combination, say $c_j=a_{j1}u_1+\dots+a_{jm}u_m$.  With that,
$$x=\sum_{j=1}^n\sum_{k=1}^m a_{jk}v_ju_k$$
hence $x$ is a linear combination of the elements of $S$ with coefficients in $F$, so that $S$ spans $L$ over $F$.

Now suppose $\sum_{j=1}^n\sum_{k=1}^m a_{jk}v_ju_k=0$ with the $a_{jk}\in F$.  Then
$$\sum_{j=1}^n\left(\sum_{k=1}^m a_{jk}u_k\right)v_j=0,$$
which means that every $\sum_{k=1}^m a_{jk}u_k=0$ by the linear independence of the $v$'s over $K$.  Moreover, for each $j$, $a_{j1}u_1+\dots+a_{jm}u_m=0$, which means that (since the $u$'s are linearly independent over $F$), $a_{j1}=\dots=a_{jm}=0$.  Therefore all the $a_{jk}$ (for any $j,k$) are zero, hence $S$ is linearly independent over $F$, and is therefore a basis of $L$ over $F$.
\end{proof}
\noindent If $F$ is a subfield of $\mathbb C$ and $u\in\mathbb C$, then we define $F(u)$ to be the intersection of all fields containing both $F$ and $u$.  This is clearly a field, and it is the smallest field containing those things.  The aforementioned field $\mathbb Q(\sqrt 2)$ is the special case with $F=\mathbb Q,u=\sqrt 2$.

The next proposition covers the essential kinds of fields built upon one another in geometric constructions.\\

\noindent\textbf{Proposition 2.36.} \emph{Let $F\subset K$ be fields.  Then $[K:F]=2$ if and only if $K=F(\sqrt u)$ for some $u\in F,\sqrt u\notin F$.}
\begin{proof}
Suppose $[K:F]=2$.  Then $F\subsetneq K$ by Proposition 2.34; let $x$ be an element of $K-F$.  Then no three-element subset of $K$ can be linearly independent over $F$ (since there is a two-element basis, so that would contradict Proposition 2.32).  In particular, $\{x^2,x,1\}$ is not linearly independent, which means we have an equation
$$ax^2+bx+c1=0$$
with $a,b,c\in F$, not all zero.  If $a=b=0$ then $c=0$ clearly follows, a contradiction; and if $a=0$ and $b\ne 0$, then $x=-c/b$, another contradiction since $x\notin F$.  Therefore, we must have $a\ne 0$.  By dividing the above equation by $a$, we may write (with $p=b/a,q=c/a$),
$$x^2+px+q=0$$
Now let $u=(x+p/2)^2$.  Then $\sqrt u=\pm(x+p/2)$, which is not in $F$ (because $\sqrt u\in F$ implies $x\in F$ easily).  However, $u=x^2+px+\frac{p^2}4=\frac{p^2}4-q\in F$.  We claim that $K=F(\sqrt u)$; indeed, if we let $K'=F(\sqrt u)$ then $F\subsetneq K'\subset K$.  As $[K:F]=2$, a prime, Exercise 4 implies that we must have $K'=K$.  This proves $K=F(\sqrt u),u\in F,\sqrt u\notin F$.

Conversely, let $K=F(\sqrt u)$ for $u\in F,\sqrt u\notin F$.  Then $\{1,\sqrt u\}$ is a basis of $K$ over $F$: indeed, one can check that for $x,y,x',y'\in F$:
\begin{center}
$(x+y\sqrt u)(x'+y'\sqrt u)=(xy+ux'y')+(xy'+yx')\sqrt u$;
If $x+y\sqrt u\ne 0$ then $x^2-uy^2\ne 0$, and $\frac 1{x+y\sqrt u}=\frac x{x^2-uy^2}-\frac y{x^2-uy^2}\sqrt u$.
\end{center}
With this aid, it is easily proven that $K'=\{x+y\sqrt u:x,y\in F\}$ is a field containing $F$ and $\sqrt u$: hence, since $K$ is the \emph{smallest} such field, we have $K\subset K'$.  Yet also $K'\subset K$ because $K$ manifestly contains elements of the form $x+y\sqrt u$ with $x,y\in F$.  Hence, $K=K'$, and $\{1,\sqrt u\}$ spans $K'$ (i.e., $K$) over $F$.  Linear independence over $F$ is a straightforward exercise, using the fact that $\sqrt u\notin F$.  Therefore $[K:F]=2$.
\end{proof}

\noindent\textbf{FINDING CONSTRUCTIBLE NUMBERS}\\

\noindent We have finally developed enough material to classify constructible numbers.  We have previously established the constructibility of $0$ and $1$ \---- as well as $2$ by putting two line segments together.  Thus our intuition tells us that the sum of two numbers is constructible.  This is indeed the case:\\

\noindent\textbf{Proposition 2.37.} \emph{If $a$ and $b$ are constructible numbers, so are $a+b$ and $a-b$.}
\begin{proof}
It suffices to assume $a,b$ are real numbers.  The statement will then follow for complex numbers by considering their real and imaginary parts separately.

We will prove the case where $a,b\geqslant 0$; the case where either $a$ or $b$ is negative should be worked out by the reader.  In this case, let $\ell$ be a line, $A$ a point on $\ell$, $B$ a point on $\ell$ such that $AB=a$ [constructible by rule (G)], and $C$ a point on $\ell$ such that $BC=b$ and $B$ is between $A$ and $C$.  Then $AC=a+b$, hence $a+b$ is constructible.
\begin{center}\includegraphics[scale=.4]{AddConstr.png}\end{center}
As for $a-b$, arrange for $AB=a$ and $BC=b$ again, but this time $B$ should \emph{not} be between $A$ and $C$.  In other words, either $A$ is between $B,C$ or $C$ is between $A,B$.  In this case $AC=|a-b|$, which implies $a-b$ is constructible.
\end{proof}
\noindent Similarly, multiplication and division can be worked out:\\

\noindent\textbf{Lemma 2.38.} (i) \emph{If $\ell$ is a line and $A$ is a point, then the line through $A$ perpendicular to $\ell$ can be constructed.}

(ii) \emph{If $\ell$ is a line and $A$ is a point not on $\ell$, then the line through $A$ parallel to $\ell$ can be constructed.}
\begin{proof}
(i) Using the compass, construct a circle centered at $A$ whose radius is sufficiently large for the arc to intersect $\ell$ in two points $B,C$.  Then $\overline{AB}\cong\overline{AC}$ (both are radii of the circle).  Now fix a compass radius $r$ and construct circles $\omega_1,\omega_2$ centered at $B$ and $C$ respectively, with radius $r$: this radius must be large enough for $\omega_1,\omega_2$ to intersect in two points $P,Q$.

Since $BP=CP=BQ=CQ=r$, Exercise 2 of Section 2.1 implies that $P$ and $Q$ are on the perpendicular bisector of $\overline{BC}$, whence $\overset{\longleftrightarrow}{PQ}$ is the perpendicular bisector of $BC$.  But since $\overline{AB}\cong\overline{AC}$, the perpendicular bisector is also the altitude of $\triangle ABC$ from vertex $A$, by Exercise 17(g) of Section 2.1, hence contains $A$.  Thus, the (constructible) line $\overset{\longleftrightarrow}{PQ}$ is perpendicular to $\overline{BC}$ (hence to $\ell$) and passes through $A$, completing this part.

(ii) By part (i), one can construct the line $\ell_1$ through $A$ perpendicular to $\ell$.  By part (i) again, one can construct the line $\ell_2$ through $A$ perpendicular to $\ell_1$.  With that, $\ell_2$ goes through $A$ and $\ell_2\parallel\ell$ by Proposition 2.10.
\end{proof}
\noindent\textbf{Proposition 2.39.} \emph{If $a$ and $b$ are constructible numbers, then $ab$ is constructible, and if $b\ne 0$ then $a/b$ is constructible.}
\begin{proof}
Again we may assume $a,b$ are real numbers, in view of the statements
$$(x+yi)(x'+y'i)=(xx'-yy')+(xy'+yx')i$$
$$x'+y'i\ne 0\implies\frac{x+yi}{x'+y'i}=\frac{xx'+yy'}{(x')^2+(y')^2}+\frac{yx'-xy'}{(x')^2+(y')^2}i$$
and the fact that we have already proven sums and differences of constructible numbers to be constructible.

Moreover, since $a$ is constructible if and only if $|a|$ is, we may assume $a$ and $b$ are both $\geqslant 0$.

Let $\overline{AB}$ be a line segment of length $a$, and consider the ray $\overset{\longrightarrow}{AB}$.  Let $P$ be the point on this ray such that $AP=1$.  (We already know that $1$ is constructible.)  Now, let $C$ be a point such that $PC=b$ and $A,C,P$ don't lie on one line; then construct the line through $B$ parallel to $\overset{\longleftrightarrow}{PC}$ (Lemma 2.38).  Finally, let $D$ be the intersection of this line with $\overset{\longleftrightarrow}{AC}$, as shown below.
\begin{center}\includegraphics[scale=.4]{MultConstr.png}\end{center}
Since $\overset{\longleftrightarrow}{PC}\parallel\overset{\longleftrightarrow}{BD}$, $\angle ACP\cong\angle ADB$ by Proposition 2.10.  Since $\angle CAP\cong\angle DAB$ clearly, AA similarity entails $\triangle ACP\sim\triangle ADB$.  Hence $\frac{CP}{AP}=\frac{DB}{AB}$, i.e., $b=\frac{DB}a$.  Therefore $DB=ab$, and $ab$ is constructible.

To show that $a/b$ is constructible for $b\ne 0$, the argument is similar; this time let $AB=b$, let $P$ be the point on $\overset{\longrightarrow}{AB}$ with $AP=1$, then pick $D$ first so that $DB=a$, and let $C$ be the intersection of $\overset{\longleftrightarrow}{AD}$ and the line through $P$ parallel to $\overset{\longleftrightarrow}{DB}$.  Then $CP=a/b$.
\end{proof}

\noindent From Propositions 2.37 and 2.39 (along with the constructibility of $0$ and $1$), we conclude that \textbf{the constructible numbers form a field}.  This field we shall denote as $\mathscr C$.  $\mathscr C$ is actually more than just a field, as the square root of a constructible number is constructible:\\

\noindent\textbf{Proposition 2.40.} \emph{If $a$ is a constructible number, then so is $\sqrt a$.}
\begin{proof}
Again we may assume $a$ is a real number, in view of the formula
$$\sqrt{x+yi}=\sqrt{\frac{\sqrt{x^2+y^2}+x}2}\pm i\sqrt{\frac{\sqrt{x^2+y^2}-x}2}$$
for $x,y\in\mathbb R$.

Moreover, since $a\geqslant 0\implies\sqrt{-a}=i\sqrt a$, we may assume $a\geqslant 0$.

Let $\ell$ be a line, $P$ a point on $\ell$.  Construct the circle $\omega$ with center $P$ and radius $\frac{1+a}2$ (which is constructible).  Let $A$ and $B$ be the intersection points of $\omega$ and $\ell$.  Observe that $\overline{AB}$ is a diameter (because it contains $P$, the center of the circle), hence has length $1+a$.  Now, construct the point $C\in\ell$ such that $AC=1$ and $C$ is between $A$ and $B$.  By the segment addition postulate, $BC=a$.  Finally, construct the line through $C$ perpendicular to $\ell$ (Lemma 2.38) and let $D$ be its intersection point with $\omega$:
\begin{center}\includegraphics[scale=.4]{SqrtConstr.png}\end{center}
(To avoid confusion, the point $P$ has not been marked in the diagram.)

We claim that $DC=\sqrt a$, from which the constructibility of $\sqrt a$ will follow.  Since $\overline{AB}$ is a diameter, $m\measuredangle AB=180^\circ$, and therefore $m\angle ADB=90^\circ$ by Proposition 2.20.  Hence, $\angle ADB\cong\angle ACD$ and obviously $\angle DAC\cong\angle BAD$, which implies $\triangle ADC\sim\triangle ABD$ by AA similarity.  A similar argument shows that $\triangle ABD\sim\triangle DBC$.  Therefore, it easily follows that $\triangle ADC\sim\triangle DBC$.  Moreover, $\frac{DC}{AC}=\frac{BC}{DC}$; in other words $DC=\frac a{DC}$, which equates to $DC=\sqrt a$.
\end{proof}
\noindent In view of this, we define a field $F$ to be \textbf{quadratically closed} if it is closed under taking square roots; i.e., $a\in F$ implies $\sqrt a\in F$.\footnote{``Quadratically closed'' more directly means that whenever $b,c\in F$, the polynomial $x^2+bx+c$ has roots in $F$.  Using the quadratic formula, it is clear that these definitions are equivalent.}  We have just shown that the field $\mathscr C$ of constructible numbers is quadratically closed.  The surprising result of the matter is that $\mathscr C$ is actually the \emph{smallest} quadratically closed field containing $\mathbb Q$: in other words, if $F$ is a quadratically closed field containing $\mathbb Q$, then $\mathscr C\subset F$.  To prove this, we shall start with the following observation:
\begin{center}
\textbf{A real number $r$ is constructible if and only if steps (A)-(C) alone (the determinstic steps) can be used to guarantee a line segment of length $|r|$.}
\end{center}
For example, in the proof of Proposition 2.39, one can specifically arrange for $\overline{AC}$ to have a certain length, thus using purely deterministic steps.

The intuition behind this observation is that any nondeterministic constructions will lead to segments where you cannot tell the length unless the segment is made in ``deterministic ways.''  It is a rather tricky statement.

In the coordinate plane, if points $(0,0)$ and $(1,0)$ are marked, then Exercise 6 shows that a point is constructible if and only if it is a constructible number when considered as a point in the complex plane, (where $(0,0)=0$ and $(1,0)=1$).  More generally, if $F$ is a field, we define a point $(x,y)$ to be \textbf{constructible with respect to $F$} if $x+yi\in F$.  (Thus the constructible points are precisely the points constructible with respect to $\mathscr C$.)

We define a line to be \textbf{constructible with respect to $F$} if it contains at least two points in $F$.  We define a circle to be \textbf{constructible with respect to $F$} if its center is in $F$, and at least one point on its arc is in $F$.  Our first lemma is\\

\noindent\textbf{Lemma 2.41.} \emph{Suppose $F$ is a subfield of $\mathbb C$ containing the imaginary unit $i$, and there is a geometric construction in the complex plane made up of points, lines and circles which are constructible with respect to $F$.  If one of steps (A)-(C) is applied, the resulting line/circle/point is constructible with respect either to $F$, or to $F(\sqrt u)$ for some $u\in F$.}
\begin{proof}
(A) is easy: if you draw a line through two points in $F$, this line is constructible with respect to $F$.  (B) is likewise clear.

As for (C), we shall temporarily think of the elements of $F$ as ordered pairs rather than complex numbers.  If $P$ is the intersection of two lines
$$ax+by=c,~~~~a'x+b'y=c',~~~~a,b,c,a',b',c'\in F$$
then we must have $ab'-ba'\ne 0$ (otherwise the lines would either be parallel or coincide).  With that, elementary linear algebra shows that $P=\left(\frac{cb'-bc'}{ab'-ba'},\frac{ac'-ca'}{ab'-ba'}\right)$, and clearly both its coordinates are in $F$.  Therefore, the point $P$ is in $F$ (since $i\in F$).

Now suppose $P=(x,y)$ is an intersection point of a line and a circle:
$$ax+by=c,~~~~(x-x_c)^2+(y-y_c)^2=r^2,~~~~a,b,c,x_c,y_c,r\in F$$
If $b\ne 0$ then $y=\frac{c-ax}b$, which implies
$$(x-x_c)^2+\left(\frac{c-ax}b-y_c\right)^2=r^2$$
$$\left(1+\frac{a^2}{b^2}\right)x^2+2\left(\frac{ay_c}b-x_c-\frac{ac}{b^2}\right)x+\left(x_c^2+y_c^2+\frac{c^2}{b^2}-\frac{2cy_c}b-r^2\right)=0$$
This is a quadratic equation in $x$, where the leading coefficient is nonzero (because $a,b\in\mathbb R$, hence $1+a^2/b^2\geqslant 1$).  Let $\mathfrak a,\mathfrak b,\mathfrak c$ be the respective coefficients of $x^2,x,1$; then we have $\mathfrak ax^2+\mathfrak bx+\mathfrak c=0$.  With that, the well-known quadratic formula entails $x=\frac{-\mathfrak b\pm\sqrt{\mathfrak b^2-4\mathfrak a\mathfrak c}}{2\mathfrak a}$, and either way we have $x\in F(\sqrt u)$ where $u=\mathfrak b^2-4\mathfrak a\mathfrak c\in F$.  Since $y=\frac{c-ax}b$, $y\in F(\sqrt u)$ follows at once.  Hence $x+yi\in F(\sqrt u)$, and $P$ is constructible with respect to $F(\sqrt u)$.

If $b=0$, then $a\ne 0$; repeat the above argument with the roles of $x,y$ swapped, the roles of $x_c,y_c$ swapped and the roles of $a,b$ swapped.

Finally, if $P$ is an intersection of two circles,
$$(x-x_c)^2+(y-y_c)^2=r^2,~~~~x_c,y_c,r\in F$$
$$(x-x_c')^2+(y-y_c')^2=(r')^2,~~~~x_c',y_c',r'\in F$$
then subtracting the equations yields a line that must also contain $P$:
$$2(x_c'-x_c)x+2(y_c'-y_c)y=r^2-(r')^2-(x_c^2-(x_c')^2)-(y_c^2-(y_c')^2)$$
($2(x_c'-x_c)$ and $2(y_c'-y_c)$ are not both zero, because $x_c'=x_c$ and $y_c'=y_c$ implies that $r^2=(r')^2\implies r=r'$ from the first two equations, hence the circles coincide.)

Thus, by what we have proven before $P$ is constructible with respect to some $F(\sqrt u)$.
\end{proof}

\noindent We thus have our desired result:\\

\noindent\textbf{Theorem 2.42.} \emph{If $F$ is a quadratically closed subfield of $\mathbb C$, then $\mathscr C\subset F$.}
\begin{proof}
First note that $i\in F$ because $i=\sqrt{-1}$, so Lemma 2.41 can be applied to $F$.

Since $F$ is quadratically closed, $F(\sqrt u)=F$ for all $u\in F$.  Hence Lemma 2.41 implies that whenever points, lines and circles are constructible with respect to $F$, applying any of steps (A)-(C) results in figures still constructible with respect to $F$.  Yet the first two marked points ($(0,0),(1,0)$) are manifestly constructible with respect to $F$.  It follows from induction that all constructible points, lines and angles are constructible with respect to $F$.  Since $\mathscr C$ is the set of constructible points (in the complex plane), we conclude that $\mathscr C\subset F$.
\end{proof}

\noindent From Theorem 2.42, the reader can readily verify that if $x\in\mathscr C$, then there exists a chain of (number) fields
$$\mathbb Q=F_0\subset F_1\subset\dots\subset F_n$$
such that $x\in F_n$, and for each $1\leqslant k\leqslant n$, $F_k=F_{k-1}(\sqrt u)$ for some $u\in F_{k-1}$.  (The aim is to show that the set of $x\in\mathbb C$ satisfying this condition is a quadratically closed field.)

Moreover, $[F_n:\mathbb Q]=[F_n:F_{n-1}]\dots[F_2:F_1][F_1:F_0]$ by Proposition 2.35.  Since $F_k=F_{k-1}(\sqrt u)$, $[F_k:F_{k-1}]$ is either equal to $1$ or $2$.  Moreover, it follows that \textbf{$[F_n:\mathbb Q]$ is a power of $2$}, because it is the product of numbers, each of which is either $1$ or $2$.  Also, since $x\in F_n$, $\mathbb Q(x)\subset F_n$, and hence $[F_n:\mathbb Q]=[F_n:\mathbb Q(x)][\mathbb Q(x):\mathbb Q]$.  Therefore, $[\mathbb Q(x):\mathbb Q]$ is a divisor of $[F_n:\mathbb Q(x)]$.  Since any divisor of a power of $2$ is also a power of $2$, we conclude
\begin{center}
\textbf{If $x$ is a constructible number then $[\mathbb Q(x):\mathbb Q]$ is a power of $2$.} %% Yes, \mathbb Q(x) has been formally defined in this context, because $F(u)$ has been defined.
\end{center}
However, the converse is false: it is possible for $[\mathbb Q(x):\mathbb Q]$ to be a power of $2$ without $x$ being constructible \---- see Exercise 8.\\

\noindent We conclude this section by describing some possible/impossible straightedge and compass constructions.
\begin{itemize}
\item One can construct the perpendicular bisector of a line segment $\overline{AB}$.  The idea imitates the proof of Lemma 2.38(i): if $r$ is a fixed sufficiently-large radius, then draw circles of radius $r$ centered at $A$ and $B$.  The line through their two intersection points is the perpendicular bisector of $\overline{AB}$.  Moreover, since the perpendicular bisector intersects $\overline{AB}$ in its midpoint, the midpoint of any line segment can also be constructed.

\item The angle bisector of an angle is also constructible; see Exercise 7.

\item For this and the next few examples, we will use the fact that if $\zeta\in\mathbb C$ is the root of a polynomial $p(x)=x^d+a_{d-1}x^{d-1}+\dots+a_1x+a_0$ which is irreducible over $\mathbb Q$, then $\{1,\zeta,\dots,\zeta^{d-1}\}$ is a basis of $\mathbb Q(\zeta)$ over $\mathbb Q$, and hence $[\mathbb Q(\zeta):\mathbb Q]=d$.  This follows from algebra and proving it here would take us too far afield.

 Suppose $n\geqslant 0$ is an integer such that $p=2^{2^n}+1$ is prime.  [Such a $p$ is called a \textbf{Fermat prime}.]  Then a regular $p$-gon is constructible.  This fact follows once we can prove that $\zeta=e^{2\pi i/p}$\----a primitive $p$-th root of unity\----is a constructible complex number.  (For then $1,\zeta,\zeta^2,\dots,\zeta^{p-1}$ are constructible, and their vertices form a regular $p$-gon.)

Here, we will use some results from Galois theory, without proving them.  Firstly, $x^{p-1}+\dots+x^2+x+1$ is an irreducible polynomial over $\mathbb Q$ with $\zeta$ as a root, which means that $[\mathbb Q(\zeta):\mathbb Q]=p-1=2^{2^n}$.  Moreover, $\mathbb Q(\zeta)$ is a Galois extension of $\mathbb Q$ (this means it is generated by \emph{all} roots of that polynomial), so that the Galois group\footnote{If $F\subset K$ are fields, then the set of permutations $\sigma\in S(K)$ such that $\sigma(a+b)=\sigma(a)+\sigma(b),\sigma(ab)=\sigma(a)\sigma(b)$ for all $a,b\in K$, and $\sigma(c)=c$ for all $c\in F$, is a group under function composition, called the \textbf{Galois group} of $K$ over $F$.} has order $2^{2^n}$.  The Sylow theorems (Exercise 12 of Section 1.6) imply that there is a chain of subgroups whose orders are $2^k,k=0,\dots,2^n$.  By the Fundamental Theorem of Galois Theory, the subgroups of the Galois group are in an order-reversing bijection with the fields lying between $K$ and $F$, hence there exist fields
$$\mathbb Q=F_0\subset F_1\subset\dots\subset F_{2^n-1}\subset F_{2^n}=K$$
such that every $[F_k:F_{k-1}]=2$.  Using Proposition 2.36, it follows that every element of $K$, in particular $\zeta$, is constructible.

As special cases, it follows that the regular pentagon, $17$-gon, $257$-gon and $65537$-gon are constructible.  It is not known whether there are any Fermat primes larger than $65537$.

\item The regular heptagon ($7$ sides), on the other hand, is \emph{not} constructible.  If it were constructible, then it would be possible to construct an angle of $(360/7)^\circ$, and therefore, the complex number $\zeta=e^{2\pi i/7}$ would be constructible (verify!).  Since $x^6+\dots+x+1$ is an irreducible polynomial with $\zeta$ as a root, $[\mathbb Q(\zeta):\mathbb Q]=6$.  Since $6$ is not a power of $2$, we conclude that $\zeta$ can't be constructible.

%http://mathworld.wolfram.com/CubeDuplication.html
\item\emph{(Duplicating the Cube / Delian Problem.)} \---- There is a curious Greek story involving a cubical altar and a plague.\footnote{Weisstein, Eric W.~``Cube Duplication.''~From \emph{MathWorld}\----A Wolfram Web Resource.}  While Athenians had the plague, an Athenian leader asked the Oracle at Delos for advice.  The god Apollo told him, through the Oracle, to double the size of the altar in attempt to end the disease.  By ``size,'' of course, he meant the \emph{volume} of the cube.  He was also particularly picky and did not want the altar's size to be larger than he specified.  This led to the following geometric problem: Given a cube, construct (with straightedge and compass) another cube with exactly twice the volume.

Of course, this question is ill-posed with what we have gone through during the chapter, because we are dealing with a coordinate \emph{plane}, not $3$-space.  However, one can construct a cube by constructing its net (see below), then cutting it out, folding along the edges and taping.  Thus, we will assume a cube of side length $s>0$ is constructible if and only if $s$ is a constructible number.
\begin{center}\includegraphics[scale=.4]{CubeNet.png}\end{center}
Now, take a cube of side length $s$.  If we were able to construct another cube with exactly twice the volume, the volume of the second cube would be $2s^3$, hence its side length would be $\sqrt[3]{2s^3}=s\sqrt[3]2$.  With that, $s$ and $s\sqrt[3]2$ are both constructible, and therefore so is $\sqrt[3]2$.  But this does not hold water, as $\sqrt[3]2$ is a root of the polynomial $x^3-2$, and this polynomial is irreducible over $\mathbb Q$.  This makes $[\mathbb Q(\sqrt[3]2):\mathbb Q]=3$, not a power of $2$.

The confirmation that duplicating the cube is impossible took around 2000 years after the aforementioned Greek story.  It was first proven by Descartes in 1637.

\item\emph{(Trisecting the angle.)} \---- Given an angle $\angle BAC$ with measure $\alpha$, Axiom 2.6 implies that there exist points $P,Q$ inside $\angle BAC$, such that $m\angle BAP=m\angle QAC=\alpha/3$, and therefore (by the angle addition postulate) $m\angle PAQ=\alpha/3$ as well.  $\overset{\longrightarrow}{AP}$ and $\overset{\longrightarrow}{AQ}$ are said to \textbf{trisect} the angle.

An ancient problem concerning the straightedge and compass is: given an arbitrary angle, can you construct the rays that trisect it?  We know we can bisect the angle (and this had already been known to the geometers); however, with the material in this chapter, we can answer our question with ``no'': straightedge and compass cannot trisect an arbitrary angle.  [Note, however, that trisection of an angle is possible with \emph{ruler} and compass: see Exercise 9.]

Suppose trisection of an angle \emph{were} possible.  It is clear that an equilateral triangle is constructible (let $\overline{AB}$ be a line segment of length $1$, then construct circles of radius $1$ centered at $A$ and $B$ and see where they intersect); hence, a $60^\circ$ angle is constructible.  Under the assumption that angles can be trisected, it follows that a $20^\circ$ angle can be constructed.  Hence by Exercise 6, $\cos 20^\circ$ is a constructible real number.  However, if $\alpha$ is real number, then the results of Exercise 3 of Section 2.2 can be used to show
$$\cos(3\alpha)=4(\cos\alpha)^3-3\cos\alpha$$
Moreover if $\alpha=20^\circ$ and $x=\cos\alpha$, then $4x^3-3x=\cos(3\alpha)=\cos(60^\circ)=\frac 12$.  Hence, $8x^3-6x-1=0$.  That polynomial can be seen to be irreducible over $\mathbb Q$ (because it has degree $3$ and it has no rational roots).  Consequently $[\mathbb Q(x):\mathbb Q]=3$, hence $x$ is not constructible.  This proves that $\cos 20^\circ$ is not constructible, and we have a contradiction.

\item\emph{(Squaring the circle.)} \---- Another ancient problem is this: given a circle, can a square be constructed with exactly the same area as the circle?  There is an interesting piece of history that Babylonian mathematicians were trying to use squares to approximate the area of a circle.  Later, Anaxagoras and Hippocrates of Chios tried tackling the problem of squaring the circle.  They were not successful.  The answer to whether squaring the circle was possible was not found until 1882, when the Lindemann-Weierstrass Theorem confirmed $\pi$ to be transcendental.\footnote{The Lindemann-Weierstrass Theorem states that if $x_1,\dots,x_n$ are distinct algebraic numbers, then $e^{x_1},\dots,e^{x_n}$ are linearly independent over the field of algebraic numbers.  Taking $x_1=0,x_2=x$, it follows that $e^x$ is transcendental whenever $x$ is a nonzero algebraic number.  In particular, if $\pi$ were algebraic then $e^{i\pi}$ would be transcendental: absurd, because Euler discovered that $e^{i\pi}=-1$.}

Let $\omega$ be a circle of radius $r$.  Then $\omega$ has area $\pi r^2$ (Exercise 10 of Section 2.3).  If it were possible to construct a square of the same area, the square would have side length $\sqrt{\pi r^2}=r\sqrt{\pi}$.  Moreover, line segments of length $r$ and $r\sqrt{\pi}$ would both be present, entailing constructibility of $\sqrt{\pi}$.  However, since $\pi$ is transcendental, $\mathbb Q(\sqrt{\pi})$ is infinite-dimensional over $\mathbb Q$ (since for any $n>0$ whatsoever, $1,\pi,\pi^2,\dots,\pi^n$ are linearly independent over $\mathbb Q$).  Therefore $\sqrt{\pi}$ is certainly not constructible, and we have a contradiction.
\end{itemize}
\subsection*{Exercises 2.4. (Straightedge and Compass Constructions)}
Unless otherwise specified, $F$ and $K$ denote subfields of $\mathbb C$ with $F\subset K$.
\begin{enumerate}
\item Show that $\sqrt 3$ cannot be written as $x\sqrt 2+y\sqrt 5$ with $x,y\in\mathbb Q$.  [Suppose $\sqrt 3=x\sqrt 2+y\sqrt 5$ with $x,y\in\mathbb Q$.  By squaring both sides and noting that $\sqrt{10}$ is irrational, conclude that either $x=0$ or $y=0$.  Now there is an easy contradiction.]

\item Show that the following statements are equivalent:

~~~~(i) $K$ is infinite-dimensional over $F$.

~~~~(ii) For every positive integer $n$, there exist linearly independent elements $v_1,\dots,v_n\in K$.

~~~~(iii) There is no finite set which spans $K$ over $F$.

\item Suppose $F\subset K\subset L$ are fields with $L$ finite-dimensional over $F$.  Show that $L$ is finite-dimensional over $K$ and $K$ is finite-dimensional over $F$ (from which Proposition 2.35 entails $[L:F]=[L:K][K:F]$).  [A basis of $L$ over $F$ spans $L$ over $K$; therefore $L$ is finite-dimensional over $K$ by Exercise 2.  If $K$ were infinite-dimensional over $F$, then Exercise 2 would imply that there are arbitrary large subsets of $K$ which are linearly independent over $F$ \---- but these are also subsets of $L$ \---- use Proposition 2.32 to get a contradiction.]

\item If $[K:F]$ is prime, show that there is no field $E$ such that $F\subsetneq E\subsetneq K$.

\item If $[K:F]=n$ and $u_1,\dots,u_n\in K$, show that $u_1,\dots,u_n$ span $K$ if and only if they are linearly independent.

\item\emph{(Constructibility of points, lines and angles.)} \---- A point $P$ is said to be \textbf{constructible} if there is a way to apply steps (A)-(H) to guarantee that you will land on exactly that point.

In the following exercises, assume you are dealing with a coordinate plane on the sheet of paper, and (for normalization purposes), only the points $(0,0)$ and $(1,0)$ are marked.

(a) Show that a point $(x,y)$ is constructible if and only if $x,y$ are both constructible real numbers.  Conclude that if the coordinate plane is the complex plane, then the constructible points are precisely the constructible numbers.

A line $\ell$ is said to be \textbf{constructible} if it contains at least two distinct constructible points.

(b) Show that $\ell$ is constructible if and only if $\ell$ is given by an equation $ax+by=c$ with $a,b,c\in\mathscr C$, $a,b$ not both zero.  Conclude that if $\ell$ is constructible and $A$ is a constructible point, the lines through $A$ parallel and perpendicular to $\ell$ are both constructible.

An angle $\angle BAC$ is said to be \textbf{constructible} if $\overset{\longleftrightarrow}{AB}$ and $\overset{\longleftrightarrow}{AC}$ are constructible lines (i.e., both rays of the angle determine constructible lines).

(c) If $0^\circ\leqslant\alpha\leqslant 180^\circ$ is a real number, show that the following statements are equivalent:

~~~~(i) There exists a constructible angle whose measure is $\alpha$.

~~~~(ii) $\sin\alpha$ is constructible (as a real number).

~~~~(iii) $\cos\alpha$ is constructible.

~~~~(iv) Either $\tan\alpha$ is constructible, or else $\alpha=90^\circ$ (making the tangent undefined).

In this case we say that $\alpha$ is a \textbf{constructible angle measure}.  This is very different from being a constructible real number: for instance, $\pi/4=45^\circ$ is a constructible angle (its tangent is $1$), but $\pi/4$ is transcendental, hence not a constructible real number.

\item If $\angle BAC$ is an angle, then its angle bisector can be constructed with straightedge and compass.  [Use the compass to draw a circular arc $\omega$, centered at $A$, which goes through both $\overset{\longrightarrow}{AB}$ and $\overset{\longrightarrow}{AC}$.  Let $B'$ and $C'$ be the points where $\omega$ meets those rays respectively.  Then $\overline{AB'}\cong\overline{AC'}$; both have length the radius of $\omega$.  Now, fix a particular radius $r$ for the compass, and draw circular arcs of radius $r$ centered at $B'$ and $C'$: make sure they intersect.  If $P$ is their intersection point, then $\overline{B'P}\cong\overline{C'P}$ because both have length $r$.  Now show that $\overset{\longrightarrow}{AP}$ is the angle bisector of $\angle BAC$.]

\item Let $\zeta$ be a root in $\mathbb C$ of the polynomial $x^4-4x+2$.  This polynomial is irreducible over $\mathbb Q$,\footnote{This follows from Eisenstein's criterion; we will not go into details here.} and hence, $[\mathbb Q(\zeta):\mathbb Q]=4$, which is a power of $2$.  However, we claim that $\zeta$ is \emph{not} constructible.

(a) If $a,b,c,d,r\in\mathbb Q$ and $(ax^3+bx^2+cx+d)^2\equiv r\pmod{x^4-4x+2}$, show that $a=b=c=0$.  [If $a\ne 0$ or $b\ne 0$, expand the left-hand side, then use the polynomial Division Algorithm to derive a contradiction.  If $a=b=0$, it should be clear that $c=0$.]

(b) If $u\in\mathbb Q$ such that $v=\sqrt u\in\mathbb Q(\zeta)$, show that $v\in\mathbb Q$.  [$\{1,\zeta,\zeta^2,\zeta^3\}$ is a basis of $\mathbb Q(\zeta)$ over $\mathbb Q$, so one may write $v=a\zeta^3+b\zeta^2+c\zeta+d$ with $a,b,c,d\in\mathbb Q$.  Explain why $(ax^3+bx^2+cx+d)^2\equiv u\pmod{x^4-4x+2}$, and use part (a).]

(c) Conclude that there is no field $\mathbb Q\subset F\subset\mathbb Q(\zeta)$ such that $[F:\mathbb Q]=2$, and therefore $\zeta$ is not constructible.

\item\emph{(Trisection of an angle with ruler and compass.)} \---- In this problem, instead of a straightedge, you are given a ruler.  This time you are allowed to mark points on the ruler's edge for later usage.  This gives the ability to do more constructions than just with the straightedge and compass.  For example, you can trisect the angle; the following construction is due to Archimedes.

Let $\angle BAC$ be an angle.  Construct a circle $\omega$ centered at $A$, as shown below, and assume $B,C$ are on the circle's arc.  Extend line $\overset{\longleftrightarrow}{AC}$.  Place the ruler against $\overset{\longleftrightarrow}{AB}$, and mark the locations of $A$ and $B$ on the ruler.  Now the ruler has two marks, at a distance of $AB$.

Adjust the ruler so that one mark is on the arc of $\omega$, the other mark is on $\overset{\longleftrightarrow}{AC}$, and the ruler meets point $B$ (away from the marks).  [Your intuition should tell you that this can be done.]  Then, trace the ruler, resulting in a line through $B$ as shown below.  Let it meet the arc at $D$ and let it meet $\overset{\longleftrightarrow}{AC}$ at $E$.  We have $DE=AB$ because $D,E$ are at the marks on the ruler.
\begin{center}\includegraphics[scale=.4]{AngleTrisection.png}\end{center}
Show that $m\angle DEC=\frac 13m\angle BAC$.  [Use the angle addition postulate and Propositions 2.11 and 2.15.]  Thus, by copying $\angle DEC$ into $\angle BAC$ at both rays, the angle is trisected.

\item This exercise introduces the concept of an (abstract) field.  A \textbf{field} is a set $F$ equipped with two binary operations $+,\cdot$ (called \emph{addition} and \emph{multiplication} respectively), such that:

~~~~(i) $F$ is an abelian group under $+$, with identity element $0$;

~~~~(ii) $F-\{0\}$ is an abelian group under $\cdot$, with identity element $1$;

~~~~(iii) For all $a,b,c\in F$, $a(b+c)=ab+ac$ [distributivity of multiplication over addition].

There are examples of fields which are not subfields of $\mathbb C$.  For instance, if $p$ is prime, $\mathbb Z/p\mathbb Z$ under modular arithmetic is a field, ((ii) follows from a statement mentioned in Exercise 12(c) of Section 1.2).  Note that the definitions imply that $0$ and $1$ denote distinct elements of $F$ (distinct since $1\in F-\{0\}$ by (ii)).  Now for a few exercises.

(a) If $F$ is a field and $a,b\in F$, then $a\cdot 0=0$, and $a(-b)=-(ab)$.  [(iii) implies that $x\mapsto ax$ is a homomorphism from the additive group $F$ to itself; now use Proposition 1.10.]  Conclude that $a(b-c)=ab-ac$ for $a,b,c\in F$.

(b) If $K$ is a field, a \textbf{subfield} of $K$ is a subset $F$ such that $F$ is a subgroup of the additive group $K$, and $F-\{0\}$ is a subgroup of the multiplicative group $K-\{0\}$.  In this case, $F$ is itself a field under the restrictions of the operations.

If $F$ is a subfield of $K$, imitate the material in this section to define what it means for a subset of $K$ to span/be linearly independent/be a basis over $F$.  Then prove Propositions 2.31-2.35 in this setting, giving a definition of $[K:F]$ after proving Proposition 2.33.

(c) Suppose $K$ is a field and $F$ is a subfield such that $[K:F]=2$.  If $1+1\ne 0$ in $F$, imitate Proposition 2.36 to prove that $K=F(v)$ for some $v\in K$ such that $v^2\in F$.

(d) Show by example that part (c) may be false if $1+1=0$ in $F$.  [There is a unique field with $4$ elements; find it, and then take it to be $K$.]

A \textbf{polynomial} in $F$ is a formal linear combination of the symbols $1,x,x^2,x^3,\dots$ with coefficients in $F$.  For example, $ax^2+bx+c$ is a polynomial when $a,b,c\in F$, and so is $ax^2+(c-a)$.  They can be added and multiplied the normal way you would add/multiply polynomials:
$$(a_nx^n+\dots+a_1x+a_0)+(b_nx^n+\dots+b_1x+b_0)=(a_n+b_n)x^n+\dots+(a_1+b_1)x+(a_0+b_0)$$
$$(a_nx^n+\dots+a_1x+a_0)(b_mx^m+\dots+b_1x+b_0)=$$
$$(a_nb_m)x^{n+m}+(a_nb_{m-1}+a_{n-1}b_m)x^{n+m-1}+\dots+(a_2b_0+a_1b_1+a_0b_2)x^2+(a_1b_0+a_0b_1)x+(a_0b_0)$$
and essentially the same rules we are familiar with (such as associativity of multiplication) hold for the polynomials.  We shall admit only \emph{finite} linear combinations (infinite linear combinations of $1,x,x^2,x^3,\dots$ are called \emph{power series}).

The set of polynomials in $F$ is denoted $F[x]$.  [Note that this is not a field; for example, $x$ has no multiplicative inverse, so (ii) fails.]

(e) Let $K$ be a field and $F$ a subfield.  Then every polynomial $f\in F[x]$ induces a function $\varphi_f:K\to K$, which applies a polynomial with $x$ as the input.  For instance, if $f=ax^2+bx+c$ with $a,b,c\in F$, then for every $u\in K$, $\varphi_f(u)=au^2+bu+c$.  The element $u\in K$ is said to be a \textbf{root} of $f$ if $\varphi_f(u)=0$.

If $u\in K$, show that $f(x)$, regarded as a polynomial in $K$, can be written as $g(x)(x-u)+r$ with $g(x)\in K[x],r\in K$.  [Imitate the polynomial Division Algorithm.]  Furthermore, show that $r=\varphi_f(u)$.

(f) Show that if $f(x)=a_nx^n+\dots+a_1x+a_0$ with $a_n\ne 0$ (in this case we say $n$ is the \textbf{degree} of $f$), then $f$ has at most $n$ roots in $K$.  [If $u$ is a root of $f$ in $K$, then $f(x)=g(x)(x-u)$ with $g(x)\in K[x]$ by part (e); now note that $g(x)$ has degree $n-1$ and use induction.]

(g) If $F$ is a field and $G$ is a finite subgroup of the multiplicative group $F-\{0\}$, show that $G$ is cyclic.  [Let $n=|G|$, and let $a\in G$ be an element of maximal order out of all elements of $G$, say $|a|=d$.  By Exercise 14(b) of Section 1.2, the order of every element of $G$ divides $d$.  Hence all elements of $G$ are roots of the polynomial $x^d-1\in F[x]$ (why?); use part (f).]
\end{enumerate}

\subsection*{2.5. Euclidean $n$-space} % Sections 2.1-2.4 are 65 pages. XD I find that rather heavy for half of a chapter.  Guess it doesn't matter, I've seen lots of books that'll obviously be bigger than this one.  This *is* a first draft, after all; if judging mathematicians have a better idea of a chapter layout, I'm willing to go with it.
\addcontentsline{toc}{section}{2.5. Euclidean $n$-space}
In the first half of the chapter, we have studied plane geometry.  In other words, we have dealt with points and lines in $\mathbb R^2$.  However, in this section, we shall introduce higher dimensions of Euclidean space.  Some of the concepts to be introduced will be important for the next few chapters.

If $n>1$ is an integer, we consider $\mathbb R^n$ to be \textbf{Euclidean $n$-space}.  We do not regard it as an $n$-dimensional vector space: right now, its origin does not have any more importance than the other points.  (This is sometimes referred to as ``affine space.'')

If $a,b\in\mathbb R^n$, we define the \textbf{distance} from $a$ to $b$, denoted $\rho(a,b)$ or $\|a-b\|$, to be the magnitude of the vector from $a$ to $b$.  This still makes sense even though $\mathbb R^n$ is not regarded as a vector space.  If $a=(u_1,\dots,u_n)$ and $b=(v_1,\dots,v_n)$, this is $\sqrt{(u_1-v_1)^2+\dots+(u_n-v_n)^2}$.

We now introduce the concept of a $k$-plane and a $k$-sphere:\\

\noindent\textbf{Definition.} \emph{Let $0\leqslant k<n$.}

(i) \emph{A \textbf{$k$-plane} refers to a translation of a $k$-dimensional vector subspace of $\mathbb R^n$.  In other words, $P$ is a $k$-plane if there exist linearly independent vectors $\vec v_1,\dots,\vec v_k\in\mathbb R^n$ and a vector $\vec w\in\mathbb R^n$ such that}
$$P=\{\vec w+c_1\vec v_1+\dots+c_k\vec v_k:c_1,\dots,c_k\in\mathbb R\}.$$
\emph{A $0$-plane is called a \textbf{point}; a $1$-plane is called a \textbf{line}; a $2$-plane is called a \textbf{plane}; and an $(n-1)$-plane in $\mathbb R^n$ is called a \textbf{hyperplane}.}

(ii) \emph{A \textbf{hypersphere} (or $(n-1)$-sphere) in $\mathbb R^n$ is the set of points with a given positive distance from a given point; what is the same thing, a set given by an equation}
$$(x_1-x_{c_1})^2+(x_2-x_{c_2})^2+\dots+(x_n-x_{c_n})^2=r^2$$
\emph{where $x_{c_1},x_{c_2},\dots,x_{c_n},r\in\mathbb R$ and $r>0$.  $(x_{c_1},\dots,x_{c_n})\in\mathbb R^n$ is called the \textbf{center} of the hypersphere, and $r$ is called its \textbf{radius}.}

\emph{If $k<n-1$, then a \textbf{$k$-sphere} is defined to be the intersection of a hypersphere with a $(k+1)$-plane, provided that this intersection contains more than one point.  A $1$-sphere is called a \textbf{circle}, and a $2$-sphere is called a \textbf{sphere}.}\\

\noindent A $3$-sphere is alternatively called a ``glome''\footnote{Weisstein, Eric W.~``Glome.''~From \emph{MathWorld}\----A Wolfram Web Resource.} %http://mathworld.wolfram.com/Glome.html
\---- however, we shall avoid this terminology so that the reader is not befuddled.

We recall from Section 2.1 that two points determine a line.  They do so also in $\mathbb R^n$: indeed, if $P\ne Q$ are points in $\mathbb R^n$, the unique line going through them is $\{tP+(1-t)Q:t\in\mathbb R\}$, where $P$ and $Q$ are regarded as vectors.

A set of points is said to be \textbf{collinear} if there exists one line containing them.  For example, two points are always collinear (two distinct points determine a line); yet three points are not in general collinear.  In $\mathbb R^2$, three points form a triangle if and only if they are not collinear.  Similarly, a set of points is said to be \textbf{coplanar} if there exists one plane containing them, and $k$-\textbf{coplanar} if there exists one $k$-plane containing them.

Now for some basic facts:\\

\noindent\textbf{Proposition 2.43.} \emph{In $\mathbb R^3$:}

(i) \emph{Any two distinct planes are either disjoint (in which case we say they are \textbf{parallel}) or intersect in a line.}

(ii) \emph{Three points that are not collinear determine a plane.}

\begin{proof}
(i) follows from elementary results in linear algebra.  If $P_1$ and $P_2$ are planes such that $P_1\cap P_2\ne\varnothing$, we can apply a translation of $\mathbb R^3$ to assume $0\in P_1\cap P_2$.  With that, $P_1$ and $P_2$ are distinct $2$-dimensional vector subspaces of $\mathbb R^3$, which means that $P_1\cap P_2\subsetneq P_1$.  Hence, $\dim_{\mathbb R}(P_1\cap P_2)<\dim_{\mathbb R}(P_1)=2$.  If $\dim_{\mathbb R}(P_1\cap P_2)=0$ then $P_1\cap P_2=0$, hence we have $\mathbb R^3\supset P_1\oplus P_2$; taking the dimension of both sides gives $3\geqslant 2+2=4$, contradiction.  Therefore $\dim_{\mathbb R}(P_1\cap P_2)=1$ and $P_1\cap P_2$ is a line.

(ii) Suppose $a=(x_0,y_0,z_0),b=(x_1,y_1,z_1),c=(x_2,y_2,z_2)$ are noncollinear points in $\mathbb R^3$.  Then the vectors $\vec v=(x_1-x_0,y_1-y_0,z_1-z_0)$ and $\vec w=(x_2-x_0,y_2-y_0,z_2-z_0)$ are linearly independent (if, for instance, $\vec w=k\vec v$ for $k\in\mathbb R$ then $a,b,c$ lie in the line through $a$ parallel to $\vec v$).  Moreover, one can let $P$ be the plane spanned by $\vec v,\vec w$; more specifically, let $\vec u$ be the cross product $\vec v\times\vec w$ and then $P$ is given by $\vec u\cdot\vec x=0$.  With that, the plane through $a$ parallel to $P$ contains $a,b,c$.

To show that this plane is unique, suppose $P'$ is a different plane containing $a,b,c$.  Then $P\cap P'$ is either empty or a line by (i), and it contains $a,b,c$: contradiction, because those three points are noncollinear.
\end{proof}

\noindent Similarly, it can be shown that in $\mathbb R^4$, four points that are not coplanar determine a hyperplane.  Also, in $\mathbb R^3$, the intersection of a plane with a sphere is either empty, a point or a circle (Exercise 1); when the intersection is a point, the plane is said to be \textbf{tangent} to the sphere.  Various other basic facts, such as the possibilities for the intersection of a $k$-sphere with a $c$-sphere in $\mathbb R^n$, or the determination of a hypersphere from points (Exercise 2), can also be shown.  However, we will not list them here and prove them; we will leave the verifications to the reader so that he or she can get a good glimpse of the concept.

\subsection*{Exercises 2.5. (Euclidean $n$-space)}
\begin{enumerate}
\item In $\mathbb R^3$, the intersection of a plane with a sphere is either empty, a point, or a circle.  Furthermore, if it is a point $P$, then $\overset{\longleftrightarrow}{OP}$ is perpendicular to the plane.

\item (a) If $0<k<n$, then every $(k-1)$-sphere is contained in a unique $k$-plane.

(b) Suppose $0<k<n$; let $\omega$ be a $(k-1)$-sphere and $p$ a point outside the $k$-plane containing $\omega$.  Then there is a unique $k$-sphere containing $\omega$ and $p$.

(c) In $\mathbb R^n$, let $p_1,\dots,p_{n+1}$ be points that do not lie in one hyperplane.  Show that there is a unique hypersphere containing all of the points.  [First show that for any $0<k<n$, no $k+1$ of the points can lie in one $(k-1)$-plane; in particular, e.g., no three of the points can be collinear.]

\item\emph{(Sylvester–Gallai theorem.)} \---- Suppose $k\geqslant 3$ and $x_1,\dots,x_k\in\mathbb R^2$ are distinct points, not all collinear.  Show that there exists a line (going infinitely in both directions) which goes through exactly two of those points.  [Since the points are not all collinear, there exist triangles which use these points as vertices.  Now consider the altitudes of these triangles.  One of the altitudes has minimal length, in other words there is a triangle $\triangle ABC$ with $A,B,C\in\{x_1,\dots,x_k\}$ such that the altitude from vertex $A$ has the smallest length out of all the altitudes of all the triangles (why?).  Show that $B$ and $C$ are the only points among $x_1,\dots,x_k$ that $\overset{\longleftrightarrow}{BC}$ goes through.]

\item Let $\vec v,\vec w\in\mathbb R^n$ be vectors, say $\vec v=(x_1,\dots,x_n)$ and $\vec w=(y_1,\dots,y_n)$.  Then the \textbf{dot product} $\vec v\cdot\vec w$ is defined to be $x_1y_1+\dots+x_ny_n$.

(a) $\vec v\cdot\vec v=\|\vec v\|^2$, where $\|\vec v\|$ is the magnitude of the vector $\vec v$.

(b) We define $\vec v,\vec w$ to be \textbf{perpendicular} / \textbf{orthogonal}, denoted $\vec v\perp\vec w$, provided that $\vec v\cdot\vec w=0$.  If $\vec v\perp\vec w$, show the Pythagorean identity $\|\vec v+\vec w\|^2=\|\vec v\|^2+\|\vec w\|^2$.  [Use part (a) and the bilinearity of the dot product.]

(c) More generally, we define the angle between $\vec v$ and $\vec w$ to be $\theta=\cos^{-1}\frac{\vec v\cdot\vec w}{\|\vec v\|\|\vec w\|}$, which is between $0^\circ$ and $180^\circ$.  [This makes sense in view of Exercise 4(c) of Section 2.2.]  Show that $\|\vec v-\vec w\|^2=\|\vec v\|^2+\|\vec w\|^2-2\|\vec v\|\|vec w\|\cos\theta$; this is the new version of the law of cosines from Exercise 8 of Section 2.2.

(d) If $\vec w\ne\vec 0$, define $\operatorname{proj}_{\vec w}\vec v=\frac{\vec v\cdot\vec w}{\vec w\cdot\vec w}\vec w$.  Then $\operatorname{proj}_{\vec w}\vec v$ is the unique vector $\vec u$ such that $\vec u=k\vec w$ for some $k\in\mathbb R$, and $(\vec v-\vec u)\perp\vec w$.  $\vec u$ is called the \textbf{orthogonal projection of $\vec v$ onto $\vec w$}.

Now suppose that $\vec v,\vec w\in\mathbb R^3$, say $\vec v=(x_1,y_1,z_1)$ and $\vec w=(x_2,y_2,z_2)$.  Then we define the \textbf{cross product} $\vec v\times\vec w$ as $(y_1z_2-y_2z_1,z_1x_2-z_2x_1,x_1y_2-x_2y_1)$.

(e) The cross product is bilinear, and satisfies $\vec v\times\vec v=\vec 0$ and $\vec v\times\vec w=-\vec w\times\vec v$.

(f) $\vec u\cdot(\vec v\times\vec w)=\vec v\cdot(\vec w\times\vec u)=\vec w\cdot(\vec u\times\vec v)$.  [Show that $\vec u\cdot(\vec v\times\vec w)$ is the determinant of the matrix $\begin{bmatrix}\leftarrow\vec u\rightarrow\\\leftarrow\vec v\rightarrow\\\leftarrow\vec w\rightarrow\end{bmatrix}$.]  Conclude that $\vec v$ and $\vec w$ are both perpendicular to $\vec v\times\vec w$.

(g) $\vec u\times(\vec v\times\vec w)+\vec v\times(\vec w\times\vec u)+\vec w\times(\vec u\times\vec v)=\vec 0$.  [Since the left-hand side is linear in each of the three vectors, you need only show this for $\vec u,\vec v,\vec w\in\{\vec e_1,\vec e_2,\vec e_3\}$.]  This is called the \textbf{Jacobi identity}, and incidentally proves that $\mathbb R^3$ is a Lie algebra over $\mathbb R$ with the cross product.

(h) If $\theta$ is the angle between $\vec v$ and $\vec w$ when they are touched tail-to-tail, $\|\vec v\times\vec w\|=\|\vec v\|\|\vec w\||\sin\theta|$.  [Show that $\|\vec v\|^2\|\vec w\|^2=(\vec v\cdot\vec w)^2+\|\vec v\times\vec w\|^2$.  Then use part (c) and Exercise 10(a) of Section 2.1.]

(i) Now show that $\vec u=\vec v\times\vec w$ is the unique vector such that (1) $\vec u$ is perpendicular to both $\vec v$ and $\vec w$; (2) $\|\vec u\|$ is equal to the area of the parallelogram with sides $\vec v$ and $\vec w$; (3) when you place your right index finger along $\vec v$ and your right middle finger along $\vec w$ (without stretching them apart $>180^\circ$), $\vec u$ points away from your right palm.  [Show that the area of a parallelogram $ABCD$ (in the plane) is equal to $(AB)(AC)\sin m\angle A$.  Then use part (h).  Note that (3) is sometimes called the \emph{right-hand rule}.]
\end{enumerate}

\subsection*{2.6. Isometries of Euclidean $n$-space}
\addcontentsline{toc}{section}{2.6. Isometries of Euclidean $n$-space}
In the previous section, Euclidean $n$-space was introduced, along with $k$-spheres and $k$-planes.  We shall now cover the important concept of its symmetry; this will enable us to find a wide variety of other sorts of geometric figures in space.  An isometry is a distance-preserving function.  As we will eventually see, such a function must necessarily preserve lines, angles and circles.  We shall study them as they relate to other branches of mathematics, like linear algebra.

At this time we shall think of elements of $\mathbb R^n$ as vectors.\\

\noindent\textbf{Definition}. \emph{A function $T:\mathbb R^n\to\mathbb R^n$ is called an \textbf{isometry} if $\|T(\vec x)-T(\vec y)\|=\|\vec x-\vec y\|$ for all $\vec x,\vec y\in\mathbb R^n$.}\\ % Distance has been defined in Section 2.5 now ^^

\noindent The main step to pinning down isometries is to introduce a distance-preserving matrix as follows. An $n\times n$ matrix $A$ is said to be \textbf{orthogonal} if $A^TA=I_n$, where $A^T$ is the transpose of $A$.  The set of all orthogonal $n\times n$ matrices is denoted $O(n)$.

We shall start by noting the relation of the transpose to the dot product in linear algebra: if $\vec v,\vec w\in\mathbb R^n$ are viewed as $n\times 1$ matrices (column vectors), then $\vec v\cdot\vec w=\vec v^T\vec w$.  This is straightforward from the definition of the dot product.  Moreover, if $A$ is an $n\times n$ matrix,
$$A\vec v\cdot\vec w=\vec v\cdot A^T\vec w$$
because $A\vec v\cdot\vec w=(A\vec v)^T\vec w=(\vec v^TA^T)\vec w=\vec v^T(A^T\vec w)=\vec v\cdot A^T\vec w$.  Now we can show\\

\noindent\textbf{Lemma 2.44.} \emph{If $A$ is an $n\times n$ matrix with real entries, the following are equivalent:}

(i) \emph{$A\in O(n)$.}

(ii) \emph{$A\vec v\cdot A\vec w=\vec v\cdot\vec w$ for all $\vec v,\vec w\in\mathbb R^n$.}

(iii) \emph{$\|A\vec v\|=\|\vec v\|$ for all $\vec v\in\mathbb R^n$.}

(iv) \emph{$A\vec e_j\cdot A\vec e_j=1$ and $A\vec e_j\cdot A\vec e_k=0$ whenever $j\ne k$.}\footnote{By $\vec e_j$, of course, we mean the vector $(0,0,\dots,\overset{\begin{matrix}j\\\downarrow\end{matrix}}1,\dots,0,0)$ where the $j$-th component is $1$ and the rest are $0$.}
\begin{proof}
(i) $\implies$ (ii). Suppose $A\in O(n)$.  Then for every $\vec v,\vec w\in\mathbb R^n$,
$$A\vec v\cdot A\vec w=\vec v\cdot A^TA\vec w=\vec v\cdot I_n\vec w=\vec v\cdot\vec w$$
by the orthogonality of $A$ and the relations noted before this lemma was stated.

(ii) $\implies$ (i). For any $\vec v,\vec w\in\mathbb R^n$, $\vec v\cdot\vec w=A\vec v\cdot A\vec w=\vec v\cdot A^TA\vec w$.  Hence since the dot product is linear on the right, $\vec v\cdot(\vec w-A^TA\vec w)=0$; thus, $\vec v\cdot(I_n-A^TA)\vec w=0$.  Since this holds for \emph{any} $\vec v,\vec w$, we can take $\vec v=(I_n-A^TA)\vec w$ to conclude that $\|(I_n-A^TA)\vec w\|=0$, and hence $(I_n-A^TA)\vec w=0$.  With this holding for all $\vec w\in\mathbb R^n$, $I_n-A^TA=0$, hence $A^TA=I_n$ and $A\in O(n)$.

(ii) $\implies$ (iii) because $\|\vec v\|=\sqrt{\vec v\cdot\vec v}$ for any $\vec v\in\mathbb R^n$.

(ii) $\implies$ (iv) is clear.

(iii) $\implies$ (ii). It can be verified that $\vec v\cdot\vec w=\frac 12(\|\vec v+\vec w\|^2-\|\vec v\|^2-\|\vec w\|^2)$: use the fact that $\|\vec v\|^2=\vec v\cdot\vec v$, and the symmetry and bilinearity of the dot product, to express the right-hand side in terms of dot products.  Thus we also have $A\vec v\cdot A\vec w=\frac 12(\|A\vec v+A\vec w\|^2-\|A\vec v\|^2-\|A\vec w\|^2)=\frac 12(\|A(\vec v+\vec w)\|^2-\|A\vec v\|^2-\|A\vec w\|^2)$.  (ii) follows at once from these statements and the hypothesis (iii).

(iv) $\implies$ (ii). Let $\vec v,\vec w\in\mathbb R^n$.  Since $\{\vec e_1,\dots,\vec e_n\}$ is a basis of $\mathbb R^n$, we can write $\vec v=a_1\vec e_1+\dots+a_n\vec e_n$ and $\vec w=b_1\vec e_1+\dots+b_n\vec e_n$.  Thus, since $A$ is linear, $A\vec v=a_1(A\vec e_1)+\dots+a_n(A\vec e_n)$ and $A\vec w=b_1(A\vec e_1)+\dots+b_n(A\vec e_n)$.  With that, the hypothesis (iv) and the bilinearity of the dot product entail
$$A\vec v\cdot A\vec w=\sum_{j=1}^n\sum_{k=1}^n a_jb_k(A\vec e_j\cdot A\vec e_k)=a_1b_1+\dots+a_nb_n;$$
and similarly, $\vec v\cdot\vec w=a_1b_1+\dots+a_nb_n$.  Thus (ii) follows.
\end{proof}

\noindent $O(n)$ is a subgroup of the multiplicative group $GL_n(\mathbb R)$ of $n\times n$ nonsingular matrices.  This is easy to prove from either criterion (ii) or (iii) of Lemma 2.44.  It can also be proven directly from the definition of an orthogonal matrix:
\begin{itemize}
\item If $A$ and $B$ are orthogonal $n\times n$ matrices, then so is $AB$; this follows from the fact that $(AB)^T=B^TA^T$.

\item $I_n$ is orthogonal.

\item An $n\times n$ matrix $A$ is orthogonal if and only if $A$ is nonsingular and $A^{-1}=A^T$; in this case, $A^{-1}$ is also orthogonal (this follows from $(A^{-1})^T=(A^T)^{-1}$).
\end{itemize}
Thus, we call $O(n)$ the \textbf{orthogonal group of dimension $n$}.  Now we claim that every isometry is the composition of an orthogonal matrix and a translation; for that, we start with a lemma:\\

\noindent\textbf{Lemma 2.45.} \emph{If $S:\mathbb R^n\to\mathbb R^n$ is an isometry such that $S(\vec e_j)=\vec e_j$ for all $1\leqslant j\leqslant n$, and $S(\vec 0)=\vec 0$, then $S$ is the identity map on $\mathbb R^n$.}
\begin{proof}
For any $(a_1,\dots,a_n)\in\mathbb R^n$, let $(x_1,\dots,x_n)$ denote $S(a_1,\dots,a_n)$.  We show that $a_j=x_j$ for all $j=1,\dots,n$.  The statement in the lemma will follow.

Since $S$ is an isometry, we have, for each $j$,
$$\|(x_1,\dots,x_n)-\vec 0\|=\|S(a_1,\dots,a_n)-S(\vec 0)\|=\|(a_1,\dots,a_n)-\vec 0\|$$
$$\|(x_1,\dots,x_n)-\vec e_j\|=\|S(a_1,\dots,a_n)-S(\vec e_j)\|=\|(a_1,\dots,a_n)-\vec e_j\|$$
By squaring both sides of each equation, we get:
\begin{equation}\tag{1}
x_1^2+\dots+x_n^2=a_1^2+\dots+a_n^2
\end{equation}
\begin{equation}\tag{2}
x_1^2+\dots+(x_j-1)^2+\dots+x_n^2=a_1^2+\dots+(a_j-1)^2+\dots+a_n^2
\end{equation}
Subtracting equation (2) from equation (1), $2x_j-1=2a_j-1$, and therefore $x_j=a_j$.
\end{proof}
\noindent\textbf{Proposition 2.46.} (i) \emph{If $A\in O(n)$ and $\vec v\in\mathbb R^n$, then $T:\mathbb R^n\to\mathbb R^n$ defined by $T(\vec x)=A\vec x+\vec v$ is an isometry.}

(ii) \emph{Every isometry $T$ of $\mathbb R^n$ is uniquely of the form $\vec x\mapsto A\vec x+\vec v$, with $A\in O(n)$ and $\vec v\in\mathbb R^n$.}
\begin{proof}
(i) This follows from Lemma 2.44: for $\vec x,\vec y\in\mathbb R^n$,
$$\|T(\vec x)-T(\vec y)\|=\|(A\vec x+\vec v)-(A\vec y+\vec v)\|=\|A\vec x-A\vec y\|=\|A(\vec x-\vec y)\|=\|\vec x-\vec y\|.$$
(ii) Let $A:\mathbb R^n\to\mathbb R^n$ be the linear transformation, sending each standard basis vector $\vec e_j$ to the vector $T(\vec e_j)-T(\vec 0)$.  Then we have, for every $j$, %% You ask why A must be linear.  Any function from \{\vec e_1,\dots,\vec e_n\}\to\mathbb R^n extends to a unique linear map, and this is what I did here for \vec e_j\mapsto T(\vec e_j)-T(\vec 0).  I did not say \vec v\mapsto T(\vec v)-T(\vec 0).  But, I guess I'll reword this anyway.
$$\|A\vec e_j\|=\|T(\vec e_j)-T(\vec 0)\|=\|\vec e_j-\vec 0\|=1$$
(because $T$ is an isometry); thus, $A\vec e_j\cdot A\vec e_j=1$.  Furthermore, whenever $j\ne k$,
$$\|A\vec e_j-A\vec e_k\|=\|[T(\vec e_j)-T(\vec 0)]-[T(\vec e_k)-T(\vec 0)]\|=\|T(\vec e_j)-T(\vec e_k)\|$$
$$=\|\vec e_j-\vec e_k\|=\sqrt 2$$
because $(\vec e_j-\vec e_k)\cdot(\vec e_j-\vec e_k)=\vec e_j\cdot\vec e_j-2\vec e_j\cdot\vec e_k+\vec e_k\cdot\vec e_k=1-2\cdot 0+1=2$.  Since $\|A\vec e_j-A\vec e_k\|=\sqrt 2$, we conclude
$$2=\|A\vec e_j-A\vec e_k\|^2=(A\vec e_j-A\vec e_k)\cdot(A\vec e_j-A\vec e_k)=A\vec e_j\cdot A\vec e_j-2A\vec e_j\cdot A\vec e_k+A\vec e_k\cdot A\vec e_k$$
$$=2-2A\vec e_j\cdot A\vec e_k;$$
therefore $A\vec e_j\cdot A\vec e_k=0$.  Hence $A$ satisfies (iv) in Lemma 2.44, and therefore $A\in O(n)$.

Now let $\vec v=T(\vec 0)$; with that, $T(\vec x)=A\vec x+\vec v$.  This follows from Lemma 2.45 with $S$ defined via $S(\vec x)=A^{-1}(T(\vec x)-\vec v)$.

To show that $A$ and $\vec v$ are unique, suppose also that $T(\vec x)=A'\vec x+\vec v'$.  Then for all $\vec x\in\mathbb R^n$, $A\vec x+\vec v=A'\vec x+\vec v'$.  Taking $\vec x=\vec 0$ gives $\vec v=\vec v'$.  Hence $A\vec x=A'\vec x$ for all $\vec x$, which means $(A-A')\vec x=\vec 0$: hence $A-A'=0$ and $A=A'$.
\end{proof}

\noindent From Proposition 2.46, it is clear that every isometry of $\mathbb R^n$ is bijective, and its inverse is also an isometry.  Therefore the isometries form a group under function composition.  This group is denoted $\operatorname{Isom}(\mathbb R^n)$ and called the \textbf{isometry group of $\mathbb R^n$}.

It is also clear that isometries preserve $k$-planes, angles and $k$-spheres; for instance, isometries of $\mathbb R^2$ preserve lines, angles and circles.

The isometry given by $A\in O(n)$ and $\vec v\in\mathbb R^n$ via Proposition 2.46 is denoted $[A,\vec v]$.  Since $[A,\vec v]$ is defined by $\vec x\mapsto A\vec x+\vec v$, the following formulas are readily verified:
\begin{equation}\tag{*1}[A,\vec v]\circ[A',\vec v']=[AA',A\vec v'+\vec v];\end{equation}
\begin{center}
The identity isometry on $\mathbb R^n$ is $[I_n,\vec 0]$;
\end{center}
$$[A,\vec v]^{-1}=[A^{-1},-A^{-1}\vec v].$$
In particular, we have a homomorphism from $\operatorname{Isom}(\mathbb R^n)\to O(n)$ given by $[A,\vec v]\mapsto A$; it is well defined by Proposition 2.46(ii), and a homomorphism by (*1).

The next thing we shall observe is that if $A\in O(n)$, $\det A$ is either $1$ or $-1$: after all, $\det(A^T)=\det A$ [a well-known fact about determinants], and hence $A\in O(n)\implies(\det A)^2=\det(A^T)\det A=\det(A^TA)=\det I_n=1$.  If $\det A=1$ we say that $A$ is \textbf{orientation-preserving}; if $\det A=-1$ we say that $A$ is \textbf{orientation-reversing}.  We define $SO(n)=\{A\in O(n):\det A=1\}$; this is a subgroup of $O(n)$ of index $2$, which we call the \textbf{special orthogonal group of dimension $n$}.

Likewise, if $T=[A,\vec v]\in\operatorname{Isom}(\mathbb R^n)$, we say that $T$ is \textbf{orientation-preserving} if $\det A=1$ and \textbf{orientation-reversing} if $\det A=-1$.  Then the product of two orientation-preserving isometries is orientation-preserving, the product of two orientation-reversing isometries is orientation-preserving, and the product of an orientation-preserving and an orientation-reversing isometry is orientation-reversing.  The group of orientation-preserving isometries is denoted $\operatorname{Isom}^+(\mathbb R^n)$ and called the \textbf{orientation-preserving isometry group of $\mathbb R^n$}.

Since isometries preserve $k$-planes and $k$-spheres, we get many group actions (Section 1.6).  For instance, $\operatorname{Isom}(\mathbb R^n)$ acts on the set of all $k$-planes, by application of an isometry.  Also, $\operatorname{Isom}(\mathbb R^n)$ acts on the set of $k$-spheres of radius $r$, when $r$ is fixed.  We claim that these actions are actually transitive, thus accounting for the ``homogeneity'' of the space:\\

\noindent\textbf{Proposition 2.47.} \emph{$\operatorname{Isom}(\mathbb R^n)$ acts transitively on the following sets:} % You say "by application of an isometry" is redundant.  But using the Axiom of Choice or otherwise, one can certainly define group actions of Isom(\mathbb R^n) on \mathbb R^n,\mathcal P_k,\mathcal S_{k,r} which are not transitive!  In this retrospect, the new version of the statement is inaccurate.

(i) $\mathbb R^n$.

(ii) \emph{The set $\mathcal P_k$ of $k$-planes in $\mathbb R^n$, with $1\leqslant k\leqslant n$ fixed.}

(iii) \emph{The set $\mathcal S_{k,r}$ of $k$-spheres of radius $r$, with $k$ and $r>0$ fixed.}
\begin{proof}
(i) We show that the orbit $\mathcal O_{\mathbb R^n}(\vec 0)$ equals $\mathbb R^n$.  This follows because whenever $\vec v\in\mathbb R^n$, the translation $[I_n,\vec v]:\vec x\mapsto\vec x+\vec v$ maps $\vec 0$ to $\vec v$, and therefore $\vec v\in\mathcal O_{\mathbb R^n}(\vec 0)$.

(ii) If $E$ is the $k$-plane spanned by $\vec e_1,\dots,\vec e_k$, we show that $\mathcal O_{\mathcal P_k}(E)=\mathcal P_k$.  Let $P$ be a plane in $\mathcal P_k$.  Let $V$ be the plane through the origin parallel to $P$; this is a $k$-dimensional vector subspace of $\mathbb R^n$.  Let $\{\vec v_1,\dots,\vec v_k\}$ be a basis of $V$; extend this set to a basis $\{\vec v_1,\dots,\vec v_n\}$ of $\mathbb R^n$.  Then by recursively setting
$$\vec u_1=\frac{\vec v_1}{\|\vec v_1\|};$$
$$\vec u_k=\frac{\vec v_k-\sum_{j=1}^{k-1}(\vec v_k\cdot\vec u_j)\vec u_j}{\|\vec v_k-\sum_{j=1}^{k-1}(\vec v_k\cdot\vec u_j)\vec u_j\|}$$
we get an orthonormal basis $\{\vec u_1,\dots,\vec u_n\}$ of $\mathbb R^n$, and moreover $V$ is also the plane spanned by $\vec u_1,\dots,\vec u_k$.  [This is known as the \emph{Gram-Schmidt process}.]  Thus, if $A:\mathbb R^n\to\mathbb R^n$ is the linear map given by $\vec e_j\mapsto\vec u_j$, then $A$ is orthogonal (since the $\vec u$'s are orthonormal).  Finally, if $\vec v$ is any point in $P$, we get that $[A,\vec v]\in\operatorname{Isom}(\mathbb R^n)$ sends $E$ to $P$ as desired.

(iii) If $k=n-1$, then a $k$-sphere is a hypersphere; i.e., a $k$-sphere of radius $r$ is of the form $(x_1-c_1)^2+\dots+(x_n-c_n)^2=r^2$ where $(c_1,\dots,c_n)$ is the center.  Now let $W$ be the $k$-sphere of radius $r$ centered at the origin; it is straightforward to show that our $k$-sphere of radius $r$ is obtained by applying to $W$ the translation that carries $\vec 0$ to $(c_1,\dots,c_n)$.  Hence the isometries (the translations for that matter) are transitive on the $k$-spheres of radius $r$.

The case $k<n-1$ we leave as an exercise to the reader.
\end{proof}

\noindent\textbf{CATEGORIZING ISOMETRIES OF $\mathbb R^2$ AND $\mathbb R^3$}\\

\noindent We conclude this section by identifying different kinds of isometries of the plane and $3$-space, in terms of what our geometric point of view tells us.  First, note that if $A\in O(2)$ and $\det A=-1$, then $A^2=I_2$: after all, we must have $A=\begin{bmatrix}u&v\\v&-u\end{bmatrix}$ with $u^2+v^2=1$ (by (iv) of Lemma 2.44), and then $A^2=I_2$ is a straightforward calculation.

Now, what isometries of $\mathbb R^2$ are there?  For orientation-preserving isometries, there is obviously the identity (``doing nothing''), translation by a vector, and rotation around a point, as shown below.
\begin{center}\includegraphics[scale=.4]{TransRot.png}\end{center}
Simple casework on $[A,\vec v]$ with $A\in SO(2),\vec v\in\mathbb R^2$ shows that these are the only orientation-preserving isometries:
\begin{itemize}
\item If $A=I_2$ and $\vec v=\vec 0$, then $[A,\vec v]$ is the identity.

\item If $A=I_2$ and $\vec v\ne\vec 0$, then $[A,\vec v]$ translates along the vector $\vec v$.

\item If $A\ne I_2$, then we must have $A=\begin{bmatrix}u&-v\\v&u\end{bmatrix}$ with $u^2+v^2=1$.  We may write $u=\cos\theta,v=\sin\theta$ for some $0^\circ<\theta<360^\circ$.  Then $A=\begin{bmatrix}\cos\theta&-\sin\theta\\\sin\theta&\cos\theta\end{bmatrix}$, the matrix which rotates counterclockwise at an angle of $\theta$.

In this case, $I_2-A$ is nonsingular, because $\det(I_2-A)=\det\begin{bmatrix}1-u&v\\-v&1-u\end{bmatrix}=(1-u)^2+v^2=u^2-2u+1+v^2=2-2u$ (since $u^2+v^2=1$); moreover $u\ne 1$ because $A\ne I_2$, and therefore $2-2u\ne 0$.  It is then a straightforward calculation that $\vec p=(I_2-A)^{-1}\vec v$ is the unique vector fixed by $[A,\vec v]$ (because $A\vec x+\vec v=\vec x\iff(I_2-A)\vec x=\vec v$).

If $T$ is the translation $[I_2,\vec p]$, then $T^{-1}\circ[A,\vec v]\circ T=[A,\vec 0]$, which rotates counterclockwise around the origin through an angle of $\theta$.  From this it is clear that $[A,\vec v]$ rotates counterclockwise around $\vec p$ through an angle of $\theta$. %% I'm not going to insert a new proposition.  That would require me to change the numberings all over the book where the later propositions are referenced, and I guarantee if I do this, it won't be perfect.  (Besides, in what way did I assert that [A,\vec v] for \vec v\ne\vec 0 does not rotate around \vec 0 ?)
\end{itemize}
\noindent Hence, $\operatorname{Isom}^+(\mathbb R^2)$ consists only of the identity, translations and rotations.  It is worth thinking about the composition of any two of these things.  The composition of two translations is manifestly a translation.  The composition of a translation and a rotation is a rotation (though that's not so obvious without the results we just proved); and the composition of two rotations (around possibly different centers) could be a translation or a rotation.

What about orientation-reversing isometries of $\mathbb R^2$?  Well, an obvious example is a reflection around a line.  [Note in this case it sends an uppercase R to a Cyrillic Ya (seen by rotating this page), rather than an R.  This is because the isometry is orientation-reversing.]  However, there are other orientation-reversing isometries which are not reflections; they are \emph{glide reflections}, obtained by reflecting over a line then translating along a vector parallel to the line.  See below:
\begin{center}\includegraphics[scale=.4]{RefGlRef.png}\end{center}
Since a reflection fixes every point on the line of reflection, but a glide reflection does not fix any points, they are essentially distinct.

Every orientation-reversing isometry $[A,\vec v]$ is either a reflection or a glide reflection.  Recall that $A=\begin{bmatrix}u&v\\v&-u\end{bmatrix},u^2+v^2=1$, and that $A^2=I_2$.  We also note that $\operatorname{tr}A=0$, and hence the characteristic polynomial of $A$ is $x^2-(\operatorname{tr}A)x+\det A=x^2-1$.  Therefore, $A$ has $\pm 1$ as its eigenvalues.
\begin{itemize}
\item If $(I_2+A)\vec v=\vec 0$, then it is readily verified that $[A,\vec v]$ fixes $\frac 12\vec v$.  Also, since the eigenvalues of $A$ are $\pm 1$, there exists $\vec w\ne\vec 0$ such that $A\vec w=\vec w$.  If $\ell=\{\frac 12\vec v+t\vec w:t\in\mathbb R\}$, then $\ell$ is a line and $[A,\vec v]$ is reflection over $\ell$, as the reader can readily verify.  The reflection fixes each point of $\ell$.

\item Now suppose $(I_2+A)\vec v\ne\vec 0$.  In this case, let $\vec w_1=\frac 12(I_2-A)\vec v$ and $\vec w_2=\frac 12(I_2+A)\vec v$.  With that, $\vec v=\vec w_1+\vec w_2$, and hence $[A,\vec v]=[I_2,\vec w_2]\circ[A,\vec w_1]$.  Since $(I_2-A)\vec w_2=\frac 12(I_2-A^2)\vec v=\vec 0$, $A\vec w_2=\vec w_2$, which means $A$ reflects across the line spanned by $\vec w_2$.  Since $(I_2+A)\vec w_1=\vec 0$ as well, the previous paragraph entails that $[A,\vec w_1]$ is reflection over a line $\ell$ parallel to $\vec w_2$.  Since $[I_2,\vec w_2]$ is a translation along $\vec w_2$, which is parallel to $\ell$, the composition $[A,\vec v]$ is reflection over a line followed by a translation parallel to the line; in other words, $[A,\vec v]$ is a glide reflection.
\end{itemize}
\noindent We have just shown that every isometry of $\mathbb R^2$ is either (i) the identity, (ii) a translation, (iii) a rotation, (iv) a reflection, or (v) a glide reflection.  (iv) and (v) are orientation-reversing; the other three are orientation-preserving.  (ii) and (v) have no fixed points, (iii) has one fixed point, and (iv) has infinitely many fixed points (on a line).

Similarly, the isometries of $\mathbb R^3$ can be classified.  This requires sophisticated linear algebra, in terms of finding eigenvalues of $A$, so we merely state the results.  Here are all the kinds of isometries of $\mathbb R^3$:

(i) The identity $[I_3,\vec 0]$.  Orientation-preserving; fixes all points.

(ii) Translations: $[I_3,\vec v]$ for $\vec v\ne\vec 0$.  Orientation-preserving; no fixed points.

(iii) Rotations: $[A,\vec v]$ with $A\in SO(3),A\ne I_3,\vec v\in\operatorname{im}(I_3-A)$.  This rotates around a line which is its axis.  Orientation-preserving; fixes all points on the axis.

(iv) Screw translations: $[A,\vec v]$ with $A\in SO(3),A\ne I_3,\vec v\notin\operatorname{im}(I_3-A)$.  This rotates around a line and then translates along the same line.  Orientation-preserving; no fixed points.

(v) Reflections: $[A,\vec v]$ with $A\in O(3)-SO(3),A^2=I_3,A\ne -I_3,(I_3+A)\vec v=\vec 0$.  This reflects across a plane.  Orientation-reversing; fixes all points in the plane.

(vi) Glide reflections: $[A,\vec v]$ with $A\in O(3)-SO(3),A^2=I_3,(I_3+A)\vec v\ne\vec 0$.  This reflects across a plane then translates along a vector parallel to the plane.  Orientation-reversing; no fixed points.

(vii) Rotatory reflections: $[A,\vec v]$ with $A\in O(3)-SO(3)$, and either $A=-I_3$ or $A^2\ne I_3$.  This reflects across a plane, then rotates around a line perpendicular to the plane.  Orientation-reversing; fixes exactly one point.

\subsection*{Exercises 2.6. (Isometries of Euclidean $n$-space)}
\begin{enumerate}
\item (a) If $T\in\operatorname{Isom}(\mathbb R^n)$, show that the set $\{\vec x\in\mathbb R^n:T(\vec x)=\vec x\}$ is either empty, a $k$-plane for some $0\leqslant k<n$, or all of $\mathbb R^n$.

(b) If $S_1,S_2\in\operatorname{Isom}(\mathbb R^n)$, then $\{\vec x\in\mathbb R^n:S_1(\vec x)=S_2(\vec x)\}$ is either empty, a $k$-plane for some $0\leqslant k<n$, or all of $\mathbb R^n$.  [Apply part (a) to $T=S_1^{-1}\circ S_2$.]

\item (a) For each $A\in O(n)$, let $T_A:\mathbb R^n\to\mathbb R^n$ be the linear transformation induced by $A$; this is an isomorphism of the additive group $\mathbb R^n$, i.e., $T_A\in\operatorname{Aut}(\mathbb R^n)$.  Define $\varphi:O(n)\to\operatorname{Aut}(\mathbb R^n)$ via $\varphi(A)=T_A$.  Then $\varphi$ is a homomorphism.

(b) Show that $\operatorname{Isom}(\mathbb R^n)$ is isomorphic to the semidirect product $\mathbb R^n\rtimes_\varphi O(n)$.  [See Exercises 16-17 of Section 1.4.]

(c) Likewise, show that $\operatorname{Isom}^+(\mathbb R^n)\cong\mathbb R^n\rtimes_\psi SO(n)$, where $\psi=\varphi|_{SO(n)}:SO(n)\to\operatorname{Aut}(\mathbb R^n)$.

\item Let $A$ be an $n\times n$ matrix with complex entries.  We define the \textbf{Hermitian transpose} $A^*$ of $A$ as the complex conjugate of the transpose of $A$.  In other words, $(A^*)_{jk}=\overline{A_{kj}}$ for $1\leqslant j,k\leqslant n$.  We say that $A$ is \textbf{unitary} if $A^*A=I_n$.

(a) If $A$ has real entries, then $A$ is unitary if and only if it is orthogonal.

(b) The $n\times n$ unitary matrices form a subgroup of the multiplicative group $GL_n(\mathbb C)$ of $n\times n$ nonsingular matrices with complex entries.  This group is denoted $U(n)$ and called a \textbf{unitary group}.

(c) If $A$ is a unitary matrix, $|\det A|=1$.  [First show that $\det(A^*)=\overline{\det A}$ for any $n\times n$ matrix $A$.]

(d) $SU(n)=\{A\in U(n):\det A=1\}$ is a normal subgroup of $U(n)$, and $U(n)/SU(n)$ is isomorphic to the multiplicative group $\mathbb T$ of complex numbers with absolute value $1$.  $SU(n)$ is called a \textbf{special unitary group}.

(e) If $\vec v=(x_1,\dots,x_n),\vec w=(y_1,\dots,y_n)\in\mathbb C^n$, we define the \textbf{(Hermitian) inner product} $\left<\vec v,\vec w\right>$ to be $\sum_{j=1}^n\overline{x_j}y_j$.  Show that for $\vec u,\vec v,\vec w\in\mathbb C^n$ and $\alpha\in\mathbb C$:
$$\left<\vec u,\vec v+\vec w\right>=\left<\vec u,\vec v\right>+\left<\vec u,\vec w\right>;$$
$$\left<\vec u,\alpha\vec v\right>=\alpha\left<\vec u,\vec v\right>;$$
$$\left<\vec w,\vec v\right>=\overline{\left<\vec v,\vec w\right>};$$
$$\vec u\ne\vec 0\implies\left<\vec u,\vec u\right>>0.$$
Note that as a consequence of these conditions, we have that $\left<\vec v+\vec w,\vec u\right>=\left<\vec v,\vec u\right>+\left<\vec w,\vec u\right>$ and $\left<\alpha\vec v,\vec u\right>=\overline{\alpha}\left<\vec v,\vec u\right>$.  Thus the inner product is \emph{linear} on the right and \emph{antilinear} on the left.

(f) If $A$ is an $n\times n$ complex matrix and $\vec v,\vec w\in\mathbb C^n$, then $\left<A\vec v,\vec w\right>=\left<\vec v,A^*\vec w\right>$.  [First explain why $\left<\vec v,\vec w\right>=\vec v^*\vec w$ when they are considered as column vectors.]  Conclude that if $A$ is unitary, then $\left<A\vec v,A\vec w\right>=\left<\vec v,\vec w\right>$.

(g) Use part (f) to show that every eigenvalue of a unitary matrix $A$ has absolute value $1$.  [In particular, if $A$ is a (real) orthogonal matrix, every eigenvalue of $A$ has absolute value $1$.]

\item If $n\geqslant 3$ and $A_1\dots A_n$ is a regular $n$-gon, show that the subgroup of $\operatorname{Isom}(\mathbb R^2)$ consisting of isometries which fix the $n$-gon is isomorphic to $D_n$.

\item (a) Show that an orientation-preserving isometry of $\mathbb R^n$ (with $n$ arbitrary) is a composition of rotations and translations.  It need not itself be a rotation or a translation; e.g., screw translations of $\mathbb R^3$.

(b) Use part (a) to show that every isometry of $\mathbb R^n$ is a composition of reflections.

\item Every nonidentity element of $SO(3)$ is a rotation around a uniquely determined axis.

\item This exercise studies $SO(4)$, rotations in $4$-dimensional Euclidean space.

(a) Let $T\in SO(4)$.  Show that there exist orthogonally complementary $2$-dimensional subspaces $V,W\subset\mathbb R^4$, and real numbers $-180^\circ\leqslant\alpha,\beta\leqslant 180^\circ$, such that $V$ and $W$ are invariant under $T$, $T|_V$ is rotation by an angle of $\alpha$ and $T|_W$ is rotation by an angle of $\beta$.  [Note that $\alpha$ or $\beta$ could be zero, in which case $T$ is rotation around a plane axis.]

(b) The planes $V$ and $W$ are determined by this property if and only if $\alpha\ne\pm\beta$.

If $\alpha=\pm\beta$, there are infinitely many possible pairs of orthogonally complementary planes.  In this case, we say that $T$ is an \textbf{isoclinic} rotation.

Now suppose $\vec u_1,\vec u_2,\vec u_3,\vec u_4$ is a positive\footnote{This means that $\det\begin{bmatrix}\uparrow&\uparrow&\uparrow&\uparrow\\\vec u_1&\vec u_2&\vec u_3&\vec u_4\\\downarrow&\downarrow&\downarrow&\downarrow\end{bmatrix}$ is positive.} orthonormal basis of $\mathbb R^4$ such that $\vec u_1,\vec u_2\in V$ and $\vec u_3,\vec u_4\in W$; with respect to said basis an isoclinic rotation looks like one of these:
$$\begin{bmatrix}\cos\alpha&-\sin\alpha\\\sin\alpha&\cos\alpha\\&&\cos\alpha&-\sin\alpha\\&&\sin\alpha&\cos\alpha\end{bmatrix}~~~~~~\begin{bmatrix}\cos\alpha&-\sin\alpha\\\sin\alpha&\cos\alpha\\&&\cos\alpha&\sin\alpha\\&&-\sin\alpha&\cos\alpha\end{bmatrix}$$
In the former case, $T$ is said to be \textbf{left isoclinic}; in the latter case $T$ is \textbf{right isoclinic}.

(c) Show that whether an isoclinic rotation $\ne\pm I_4$ is left or right is well defined, i.e., independent of the particular invariant planes $V,W$ and positive orthonormal basis.  Then show that any left isoclinic rotations with a particular angle $\alpha$ are conjugate in $SO(4)$, and likewise for right-isoclinic rotations.

(d) Let $S^3_R=\left\{\begin{bmatrix}a&-b&-c&d\\b&a&-d&-c\\c&d&a&b\\-d&c&-b&a\end{bmatrix}:a,b,c,d\in\mathbb R,a^2+b^2+c^2+d^2=1\right\}$.  Then $S^3_R$ is a subgroup of $SO(4)$. % Fair point.  When I mentioned quaternions, I meant it as an "elective" statement which readers have the right to ignore.  But I'll remove it anyway.

(e) $SO(4)$ is generated by the following matrices for all $\alpha\in\mathbb R$:
$$\begin{bmatrix}\cos\alpha&-\sin\alpha\\\sin\alpha&\cos\alpha\\&&1\\&&&1\end{bmatrix},\begin{bmatrix}1\\&\cos\alpha&-\sin\alpha\\&\sin\alpha&\cos\alpha\\&&&1\end{bmatrix},\begin{bmatrix}1\\&1\\&&\cos\alpha&-\sin\alpha\\&&\sin\alpha&\cos\alpha\end{bmatrix}$$
[Given $A\in SO(4)$, consider the unit vector $A(\vec e_1)=(x,y,z,w)$.  There exists $\alpha$ such that $z\sin\alpha+w\cos\alpha=0$ (why?), now apply the above matrix on the right to $(x,y,z,w)$ and the last component becomes zero.  After that, the middle matrix can make the third component zero, and then the left matrix can make the vector equal to $\vec e_1$.  We thus get an element $A'$ of $SO(4)$ which fixes $\vec e_1$, by left-multiplying $A$ by the above matrices.  Now $A'(\vec e_2)$ is of the form $(0,y',z',w')$ by orthogonality; use only the two matrices on the right to transfer this vector to $\vec e_2$.  Similar arguments eventually land you at the identity element of $SO(4)$.]

(f) $S^3_R$ is a normal subgroup of $SO(4)$.  [Show that the normalizer $N(S^3_R)$ contains all of the matrices stated in part (e).]

(g) If $a,b,c,d\in\mathbb R$, the determinant of $\begin{bmatrix}a&-b&-c&d\\b&a&-d&-c\\c&d&a&b\\-d&c&-b&a\end{bmatrix}$ is equal to $(a^2+b^2+c^2+d^2)^2$.  (Note that we are \emph{not} assuming $a^2+b^2+c^2+d^2=1$ here.)  [Use the formula $\det(kA)=k^n\det A$ for $n\times n$ matrices $A$.]  Conclude that the characteristic polynomial of that matrix is $[(x-a)^2+b^2+c^2+d^2]^2$.

(h) Show that $S^3_R$ is the set of all right isoclinic rotations.  [Use part (f) and the second half of part (c) to show that $S^3_R$ contains all right isoclinic rotations.  Conversely, given an element of $S^3_R$, use part (g) to show that it is a right isoclinic rotation.]

(i) For $g\in O(4)-SO(4)$ [i.e., $g$ is an orientation-reversing isometry of $\mathbb R^4$], $S^3_L=\{gag^{-1}:a\in S^3_R\}$ is a normal subgroup of $SO(4)$, and is the set of all left isoclinic rotations (hence is independent of which $g$ is picked).

(j) $S^3_L\cap S^3_R=\{I_4,-I_4\}$.

(k) If $A\in S^3_L$ and $B\in S^3_R$, then $AB=BA$.  [Part (j) and the normality of the subgroups entail $A^{-1}B^{-1}AB=\pm I_4$.  Now, $A^{-1}B^{-1}AB=I_4$ if $A=B=I_4$, and the expression varies continuously in the entries of $A$ and $B$.]

\item\emph{(Frieze groups.)} \---- We let $S=\{(x,y)\in\mathbb R^2:|y|\leqslant 1\}$.  Observe that $\{T\in\operatorname{Isom}(\mathbb R^2):T(S)=S\}$ is a subgroup of $\operatorname{Isom}(\mathbb R^2)$, which we denote as $\operatorname{Isom}(S)$.

(a) $[A,\vec v]\in\operatorname{Isom}(S)$ if and only if $A$ is diagonal with $\pm 1$ as its entries and $\vec v$ is parallel to the $x$-axis.

We set $\operatorname{Isom}^+(S)=\operatorname{Isom}(S)\cap\operatorname{Isom}^+(\mathbb R^2)$, and $\operatorname{Tra}(S)$ the subgroup of $\operatorname{Isom}(S)$ consisting of the translations; i.e.,
$$\operatorname{Tra}(S)=\{[I_n,\vec v]:\vec v\text{ parallel to the }x\text{-axis}\}.$$
(b) $\operatorname{Tra}(S)\subset\operatorname{Isom}^+(S)\subset\operatorname{Isom}(S)$ are subgroups with $[\operatorname{Isom}(S):\operatorname{Isom}^+(S)]=2=[\operatorname{Isom}^+(S):\operatorname{Tra}(S)]$.  Furthermore, $\operatorname{Tra}(S)\cong\mathbb R$.

(c) Let $V=\left\{\begin{bmatrix}a&0\\0&b\end{bmatrix}:a,b=\pm 1\right\}$; this is an abelian group under multiplication.  There is a surjective homomorphism $\psi:\operatorname{Isom}(S)\to V$ given by $[A,\vec v]\mapsto A$, and the kernel of $\psi$ is $\operatorname{Tra}(S)$.

A \textbf{Frieze group} is a subgroup $G\subset\operatorname{Isom}(S)$ such that $G\cap\operatorname{Tra}(S)\cong\mathbb Z$.  Thus, $G$ has a nontrivial translation which generates all translations in $G$.  We will assume Frieze groups to be the same if, for some matrix $A=\begin{bmatrix}a&0\\0&\pm 1\end{bmatrix}$ and horizontal vector $\vec v=(b,0)$, the map $\mathbf x\mapsto A\vec x+\vec v$ conjugates one subgroup to the other.  (Thus, for example, the exact distance of the translations will not be important to us.)

Our ambition in this exercise is to show that there are exactly $7$ Frieze groups.

If $G$ is a Frieze group, let $\varphi=\psi|_G:G\to V$.  Then $\ker\varphi$ is a subgroup of $G$ of index $1$, $2$ or $4$, and $\ker\varphi\cong\mathbb Z$ (why?).

(d) If $\operatorname{im}\varphi=\{I_2\}$ (i.e., $\varphi$ is trivial), we have $G\cong\mathbb Z$; in other words, $G$ is cyclic, generated by a translation.

(e) Suppose $\operatorname{im}\varphi=\left\{I_2,\begin{bmatrix}1&0\\0&-1\end{bmatrix}\right\}$, which means that $G$ consists of translations and reflections/glide reflections over the $x$-axis.  Then either $G$ is generated by a translation and a reflection over the $x$-axis (in which case $G\cong\mathbb Z\times\mathbb Z/2\mathbb Z$), or else $G$ is cyclic and generated by a glide reflection, which moves half the distance of the shortest translation in $G$ (in which case $G\cong\mathbb Z$).  [If $\ker\varphi=\left<a\right>$ and $g\in G$ is an orientation-reversing isometry, then $g^2=a^n$ for some $n\in\mathbb Z$; depending on whether $n$ is even or odd, there exists an orientation-reversing isometry $g\in G$ such that either $g^2=1$ or $g^2=a$.]

(f) Suppose $\operatorname{im}\varphi=\left\{I_2,\begin{bmatrix}-1&0\\0&1\end{bmatrix}\right\}$, which means that $G$ consists of translations and reflections over lines parallel to the $y$-axis.  Show that $G$ is isomorphic to $D(\mathbb Z)$, the subgroup of $S(\mathbb Z)$ generated by the translation $x\mapsto x+1$ from $\mathbb Z\to\mathbb Z$ and the negation $x\mapsto -x$.  [$D(\mathbb Z)$ is called the \textbf{infinite dihedral group}.]  Note that the lines of reflection are spaced apart by half the distance of the shortest translation.

(g) Suppose $\operatorname{im}\varphi=\left\{I_2,\begin{bmatrix}-1&0\\0&-1\end{bmatrix}\right\}$, which means that $G$ consists of translations and $180^\circ$ rotations around points on the $x$-axis.  Show that $G\cong D(\mathbb Z)$ in this case as well.  [Thus this group is isomorphic to the one in part (f); yet, they are still different Frieze groups because this one has no orientation-reversing symmetry but the one in (f) does.]

(h) The final case to consider is when $\operatorname{im}\varphi=V$; i.e., $\varphi$ is surjective.  In this case, suppose $\ker\varphi=\left<a\right>$, and show that there exists $g\in\varphi^{-1}\left(\begin{bmatrix}1&0\\0&-1\end{bmatrix}\right)$ such that either $g^2=1$ or $g^2=a$.  Show that if $g^2=1$ then $G\cong D(\mathbb Z)\times\mathbb Z/2\mathbb Z$, and if $g^2=a$ then $G\cong D(\mathbb Z)$.  These are the remaining two Frieze groups.

Below are Frieze patterns whose isometry groups are the Frieze groups established in parts (d)-(h).
\begin{center}\includegraphics[scale=.4]{FriezePatterns.png}\end{center}
\item\emph{(Conic sections revisited.)} \---- A \textbf{quadratic curve} in $\mathbb R^2$ is given by an equation $Ax^2+Bxy+Cy^2+Dx+Ey+F=0$, where $A,B,C,D,E,F\in\mathbb R$ and $A,B,C$ are not all zero.

The number $B^2-4AC$ is called the \textbf{discriminant} of the curve.  The curve is:
\begin{itemize}
\item A \textbf{circle} if $A=C$ and $B=0$;

\item An \textbf{ellipse} if $B^2-4AC<0$;

\item A \textbf{parabola} if $B^2-4AC=0$;

\item A \textbf{hyperbola} if $B^2-4AC>0$.
\end{itemize}
Note that the curve can be given by many equations, but such equations are scalar multiples of one another (if $k\ne 0$, $kAx^2+kBxy+kCy^2+kDx+kEy+kF=0$ is an equation for the same curve).

Also, in this problem, a circle is considered an ellipse, since $B=0$ implies $B^2-4AC<0$.

(a) Isometries of $\mathbb R^2$ preserve circles, ellipses, parabolas and hyperbolas.  [It suffices to show that translations, rotations around the origin, and reflection over the $x$-axis preserve them.]

Show that the ellipse, parabola and hyperbola of Exercise 9 of Section 2.1 satisfy this exercise's definitions of their respective names.  Conclude that the notions of the conics coincide with Exercise 9 of Section 2.1.

(b) More generally, let $G$ be the subgroup of $S(\mathbb R^2)$ generated by $GL_2(\mathbb R)$ and the translations.  [Elements of $G$ are called \textbf{affine transformations} of $\mathbb R^2$.]  Then $G$ acts transitivitely on each of the following sets: (i) the ellipses; (ii) the parabolas; (iii) the hyperbolas.

Now consider the cone $z^2=x^2+y^2$ in $\mathbb R^3$.  (Technically, this is two antipodal cones with their points meeting at the origin.)  We claim that each of the quadratic curves can be obtained by intersecting this cone with a plane in $\mathbb R^3$ that does not go through the origin (hence the name ``conic sections'').  The intersection of such a plane with the cone is called a \textbf{cross section} of the cone.  It can be identified with a subset of $\mathbb R^2$, when one establishes an isometry from $\mathbb R^2$ to the plane (by taking a $3\times 2$ matrix with orthonormal columns whose image is parallel to the plane, then left-multiplying by a translation).

Thus, let $P$ be a plane that does not go through the origin.  By rotating around the $z$-axis, we may assume that the plane has equation $ax+bz=c$ ($a,b$ not both zero, $c\ne 0$).

(c) If $a=0$, the cross section is a circle.

(d) If $|a|<|b|$, the cross section is an ellipse.

(e) If $|a|=|b|$, the cross section is a parabola.

(f) If $|a|>|b|$, the cross section is a hyperbola.
\end{enumerate}

\subsection*{2.7. Finite Groups of Isometries}
\addcontentsline{toc}{section}{2.7. Finite Groups of Isometries}
In the remainder of the chapter, we shall focus on bounded figures with only finitely many isometries preserving them.  First, let us introduce the concept of isometries of various figures in $\mathbb R^n$.

In general, if $X\subset\mathbb R^n$ is a subset, we call $X$ a \textbf{figure} in $n$-space, and define $\operatorname{Isom}(X)=\{f\in\operatorname{Isom}(\mathbb R^n):f(X)=X\}$.  $\operatorname{Isom}(X)$ is a subgroup of $\operatorname{Isom}(\mathbb R^n)$, called the \textbf{isometry group} of $X$.  [See Exercise 8 of the previous section for an example.]

Now for some important notes:\\

\noindent(1) If $f\in\operatorname{Isom}(\mathbb R^n)$ maps $X$ into $X$, it is \emph{not} necessarily an isometry of $X$.  For example, suppose $X$ is the upper half plane $\{(x,y)\in\mathbb R^2:y>0\}$, and $f$ is the isometry $(x,y)\mapsto(x,y+1)$ of $\mathbb R^n$.  Then $f(X)\subset X$; however, $f(X)\ne X$ because, for example, $(0,1/2)$ is in $X$ but not in the image $f(X)$.

The example above shows that if $X$ is a figure, there may be distance-preserving maps on $X$ that are not bijective.  However, a distance-preserving map $g:X\to X$ must be \emph{injective}; for any $x,y\in X$, $g(x)=g(y)\iff\|g(x)-g(y)\|=0\iff\|x-y\|=0\iff x=y$.  Since the map need not be surjective, the distance-preserving maps need not form a group under function composition (due to lack of inverses).

A bijective distance-preserving map $X\to X$ (not assumed to extend to an isometry of $\mathbb R^n$) is called a \textbf{direct isometry} of $X$.  It is readily verified that a direct isometry has an inverse which is also a direct isometry, hence the direct isometries form a group under function composition.  The group of direct isometries of $X$ is denoted $\widetilde{\operatorname{Isom}}(X)$.\\

\noindent(2) A direct isometry $f:X\to X$ always extends to an isometry of $\mathbb R^n$, but that isometry need not be unique.  The proof that the extension exists requires a lot of details from analysis, so it will be omitted here.\footnote{For a proof, see Theorem 11.4 in \emph{Embeddings and Extensions in Analysis} by J.H.~Wells and L.R.~Williams.} %https://math.stackexchange.com/questions/1687436/every-partially-defined-isometry-can-be-extended-to-a-isometry

However, suppose $X$ is the $xy$-plane $\{(x,y,0):x,y\in\mathbb R\}$ in $\mathbb R^3$, and $f:X\to X$ is the identity map.  Then there are two isometries of $\mathbb R^3$ that extend $f$: the identity, and the map $(x,y,z)\mapsto(x,y,-z)$ which reflects over the $xy$-plane; in this case the extension is not unique.  [Note that uniqueness fails if and only if $X$ is contained in a hyperplane.]

We thus get a surjective homomorphism $f\mapsto f|_X$ from $\operatorname{Isom}(X)\to\widetilde{\operatorname{Isom}}(X)$.\\

\noindent If $X$ is a figure, we define the \textbf{orientation-preserving isometry group} of $X$, denoted $\operatorname{Isom}^+(X)$, to be $\operatorname{Isom}(X)\cap\operatorname{Isom}^+(\mathbb R^2)$.  This is the group of isometries which fix $X$ \emph{and} preserve orientation.

Note, however, that we cannot define a notion of orientation for direct isometries in $\widetilde{\operatorname{Isom}}(X)$.  Whenever a direct isometry of $X$ extends to more than one isometry of $\mathbb R^n$, some extensions preserve orientation and others reverse it (e.g., if $X$ is the $xy$-plane in $\mathbb R^3$, then the identity on $X$ extends to both the identity on $\mathbb R^3$ and the reflection $(x,y,z)\mapsto(x,y,-z)$ over the plane).  However, if $X$ is a $k$-plane, we can \textbf{orient} it, thus giving a notion of when a direct isometry is orientation-preserving.  We will not delve into details here.

We now start with a fundamental proposition about finite isometry groups; see Exercise 1 for a followup.\\

\noindent\textbf{Lemma 2.48.} \emph{If $T\in\operatorname{Isom}(\mathbb R^n)$, $\vec x_1,\dots,\vec x_n\in\mathbb R^n$ and $c_1,\dots,c_n\in\mathbb R$ with $c_1+\dots+c_n=1$, $T(c_1\vec x_1+\dots+c_n\vec x_n)=c_1T(\vec x_1)+\dots+c_nT(\vec x_n)$.  In other words, $T$ preserves affine combinations of vectors.}
\begin{proof}
Write $T=[A,\vec v]$ with $A\in O(n)$ and $\vec v\in\mathbb R^n$.  Then this is a straightforward computation:
$$T(c_1\vec x_1+\dots+c_n\vec x_n)=A(c_1\vec x_1+\dots+c_n\vec x_n)+\vec v=c_1A\vec x_1+\dots+c_nA\vec x_n+\vec v$$
$$=c_1A\vec x_1+\dots+c_nA\vec x_n+1\vec v=c_1A\vec x_1+\dots+c_nA\vec x_n+(c_1+\dots+c_n)\vec v$$
$$=c_1A\vec x_1+\dots+c_nA\vec x_n+c_1\vec v+\dots+c_n\vec v=c_1(A\vec x_1+\vec v)+\dots+c_n(A\vec x_n+\vec v)$$
$$=c_1T(\vec x_1)+\dots+c_nT(\vec x_n).$$
\end{proof}
\noindent\textbf{Proposition 2.49.} \emph{Let $G$ be a finite subgroup of $\operatorname{Isom}(\mathbb R^n)$.  Then there exists $\vec p\in\mathbb R^n$ such that $g(\vec p)=\vec p$ for all $g\in G$.}
\begin{proof}
Let $\vec p=\frac 1{|G|}\sum_{g\in G}g(\vec 0)$.  We claim that this does the trick.  After all, if $T\in\operatorname{Isom}(\mathbb R^n)$, then applying Lemma 2.48 with $n=|G|$ and $c_1=\dots=c_n=1/|G|$ gives
$$T(\vec p)=T\left(\frac 1{|G|}\sum_{g\in G}g(\vec 0)\right)=\frac 1{|G|}\sum_{g\in G}T(g(\vec 0)).$$
In particular, for $h\in G$, we have
$$h(\vec p)=\frac 1{|G|}\sum_{g\in G}h(g(\vec 0))=\frac 1{|G|}\sum_{g\in G}(h\circ g)(\vec 0).$$
Yet, as $g$ runs through all elements of $G$, since $G$ is a group, $h\circ g$ also runs through every element of $G$ exactly once, just in a different order.  Hence $\frac 1{|G|}\sum_{g\in G}(h\circ g)(\vec 0)$ is equal to $\frac 1{|G|}\sum_{g\in G}g(\vec 0)=\vec p$.  We conclude that $h(\vec p)=\vec p$ for all $h\in G$ as desired.
\end{proof}

\noindent Proposition 2.49 states that any finite group of isometries fixes some point entirely.  Such a group of isometries is called a \textbf{point group}.  If $G$ is a point group, then $G$ is conjugate to a subgroup of $O(n)$: namely, if $\vec p$ is fixed by $G$ and $\tau:\vec x\mapsto\vec x+\vec p$ is the translation, $\tau^{-1}\circ G\circ\tau$ is a subgroup of $\operatorname{Isom}(\mathbb R^n)$ which fixes $\vec 0$ entirely.  Hence, the conjugate is contained in $O(n)$ (the set of $[A,\vec v]$ with $\vec v=\vec 0$).

In particular, every finite group of isometries is conjugate to a subgroup of $O(n)$, which means that we may restrict ourselves to finite subgroups of $O(n)$.  Here are some examples:\\

(1) Fix an integer $n\geqslant 1$.  In $O(2)$, let $\theta=\begin{bmatrix}\cos(2\pi/n)&-\sin(2\pi/n)\\\sin(2\pi/n)&\cos(2\pi/n)\end{bmatrix}$; this rotates counterclockwise around the origin by $\frac{2\pi}n$, or an $n$-th of a full turn.  Moreover, $\theta^n=1$, as one can verify, and $\{1,\theta,\dots,\theta^{n-1}\}$ is a subgroup of $O(2)$; for that matter, of $SO(2)$.  It is the orientation-preserving symmetry group of a regular $n$-gon centered at the origin.\\

(2) Take $\theta\in O(2)$ as before, and let $\varphi=\begin{bmatrix}1&0\\0&-1\end{bmatrix}$.  Then $\varphi^2=1$ and $\varphi\theta\varphi=\theta^{-1}$.  Moreover, $\{1,\theta,\dots,\theta^{n-1},\varphi,\varphi\theta,\dots,\varphi\theta^{n-1}\}$ is a subgroup of $O(2)$ isomorphic to the dihedral group $D_n$; but this time, it is not contained in $SO(2)$.\\

(3) If $G$ is any finite subgroup of $O(2)$, then $G_1=\left\{\begin{bmatrix}1&0\\0&a\end{bmatrix}:a\in G\right\}$ is a finite subgroup of $O(3)$, where $\begin{bmatrix}1&0\\0&a\end{bmatrix}$ is viewed as a block matrix with $a$ as a $2\times 2$ block in the lower right corner.  Note that $G_1$ fixes every point on the line spanned by $\vec e_1$.\\

The remaining examples cover the five Platonic solids in Euclidean $3$-space.  
\begin{center}
\begin{tabular}{ccccc}
\includegraphics[scale=.1]{Tetrahedron.png}&
\includegraphics[scale=.1]{Octahedron.png}&
\includegraphics[scale=.1]{Icosahedron.png}&
\includegraphics[scale=.1]{Cube.png}&
\includegraphics[scale=.1]{Dodecahedron.png}\\
Tetrahedron&
Octahedron&
Icosahedron&
Cube&
Dodecahedron
\end{tabular}
\end{center}
A Platonic solid is obtained by picking one type of regular polygon, making copies of it, and hinging copies together by the edges, as shown above.  The polygons are called \textbf{faces}, and the edges (resp., vertices) of the polygon are called \textbf{edges} (resp., \textbf{vertices}) of the solid.  It is additionally required that every vertex of the solid have the same number of faces surrounding it.

Which Platonic solids exist?  We can use casework on the type of regular polygon used, and the number of faces surrounding each vertex. % Basic graph theory shows that the solid is uniquely determined by these parameters. (I don't understand what you meant by the revision)

Let us first consider Platonic solids whose faces are equilateral triangles.  If there are three equilateral triangles around a vertex, the tetrahedron (above) is obtained by gluing one more equilateral triangle as a base.  If there are four triangles around a vertex, you get the octahedron, by making another vertex with four triangles around it, then pasting them together.  If there are five triangles around a vertex, you get the icosahedron ($20$ faces)\----this time you need to make a copy of the vertex; then a ring of ten triangles, alternating between pointing up and down; then sandwich the ring between the caps.  If there are six triangles around a vertex, then the vertex lies flat in the plane\----continuing to add more triangles will merely get the triangular tiling from Section 2.3; it will never close up upon itself into a solid body.  The reader can see that it is impossible to construct a Platonic solid with 7 or more triangles to a vertex, because they will inevitably buckle with creases in different directions.\footnote{As a foretaste of Chapter 4, we remark that in hyperbolic geometry, any number $n\geqslant 7$ of equilateral triangles will be able to fit around a given vertex without buckling.  The same holds for any number $n\geqslant 5$ of squares, $n\geqslant 4$ of regular pentagons, etc.}

Hence, the only Platonic solids made from triangles are the tetrahedron, octahedron and icosahedron.

What about squares?  If there are three squares around a vertex, the cube (or hexahedron) is obtained by gluing on another triple of squares around another vertex.  If there are four squares around a vertex, then they lie flat in the plane, and you get a tiling from Section 2.3; and 5 or more squares will never work at all.  Thus, the only Platonic solid made from squares is the cube.

Now for pentagons.  If there are three pentagons around a vertex, the dodecahedron is obtained by gluing together four copies of that pentagon triple.  However, since a pentagon has an angle of $108^\circ>90^\circ$, trying to fit four pentagons to a vertex will not even work lying flat in the plane; it will buckle with creases in different directions.  Therefore, the only Platonic solid made from pentagons is the dodecahedron.

We may try to carry on with hexagons: if there are three hexagons to a vertex, they lie flat already.  And for $n\geqslant 7$, you can never fit three regular $n$-gons to a vertex.  This means that there are no more Platonic solids, and we have classified them all.

The following constructions in $\mathbb R^3$ yield the Platonic solids and their isometry groups.\\

(4) One can form a regular tetrahedron, $\mathbf{Tet}$, by taking for vertices the points
$$(1,1,1),(1,-1,-1),(-1,1,-1),(-1,-1,1)\in\mathbb R^3.$$
Any two of those points have a distance equal to $2\sqrt 2$ between them, and hence any three of those points will form an equilateral triangle with side length $2\sqrt 2$.  Since $\mathbf{Tet}$ is the convex hull of its vertices, an isometry $T\in\operatorname{Isom}(\mathbb R^3)$ will be in $\operatorname{Isom}(\mathbf{Tet})$ if and only if $T$ permutes the vertices.

What kinds of isometries $T$ do this?  The first important observation is that since the sum of the vertices is zero (verify), we have $T(\vec 0)=\vec 0$ by Lemma 2.48 (with $n=4$ and $c_1=\dots=c_n=1/4$).  Hence $T\in O(3)$, and we may think of $T$ as a matrix.

It is readily verified that the six points $\pm\vec e_1,\pm\vec e_2,\pm\vec e_3$ are precisely the midpoints of the six edges (unordered pairs of distinct vertices), hence are permuted by $T$.  Hence $T$ is a monomial matrix\footnote{This means $T$ has exactly one nonzero entry in each row and in each column.} with $\pm 1$ as its nonzero entries.  Moreover, since each vertex has its components multiplying to (positive) $1$, the nonzero entries of $T$ must multiply to $1$, in order for $T$ to permute the vertices.  The reader can then verify that under these conditions, we have isometries of the tetrahedron:
$$\begin{bmatrix}1&0&0\\0&-1&0\\0&0&-1\end{bmatrix},~~~~\begin{bmatrix}0&1&0\\1&0&0\\0&0&1\end{bmatrix},~~~~\begin{bmatrix}0&-1&0\\0&0&1\\-1&0&0\end{bmatrix},~~~~\text{etc.}$$
There are $24$ isometries of the tetrahedron: $6$ for the permutation matrix used to construct $T$, times $2^3=8$ for whether each nonzero entry is $1$ or $-1$, divided by $2$ due to the constraint that the nonzero entries must multiply to $1$.

Note that if $V$ is the set of vertices of $\textbf{Tet}$, we get an injective homomorphism $\varphi:\operatorname{Isom}(\textbf{Tet})\to S(V)$.  [It is injective because an affine map of the convex hull of $V$ is determined by its action on $V$.]  Since $|\operatorname{Isom}(\textbf{Tet})|=|S(V)|=24$, $\varphi$ is an isomorphism; hence, by Exercise 7(b) of Section 1.3, $\operatorname{Isom}(\textbf{Tet})\cong S_4$.  Thus we have concretely represented the isometry group of the tetrahedron.

An isometry $T\in\operatorname{Isom}(\textbf{Tet})$ is orientation-preserving if and only if its determinant is $1$.  Since the determinant of a monomial matrix is the sign of the permutation times the product of the nonzero entries, but the product of the nonzero entries of $T$ is necessarily $1$, we get that $\det T=1$ if and only if $T$ is an even permutation.  There are $3$ even permutations on the three dimensions, and hence $12$ orientation-preserving isometries of the tetrahedron.  In other words, $|\operatorname{Isom}^+(\mathbf{Tet})|=12$.  The isomorphism $\varphi$ corresponds $\operatorname{Isom}^+(\mathbf{Tet})$ with $A_4$, the only subgroup of $S_4$ of order $12$ (by Exercise 13(h) of Section 1.6).  Hence $\operatorname{Isom}^+(\mathbf{Tet})\cong A_4$.\\

(5) One can form a regular octahedron $\mathbf{Oct}$ by taking the six points
$$\pm\vec e_1,\pm\vec e_2,\pm\vec e_3\in\mathbb R^3$$
for the vertices.  A pair of vertices $\pm\vec e_j,\pm\vec e_k$ where $j\ne k$ and the two signs are independent of one another, is at a distance of $\sqrt 2$; there are $12$ such unordered pairs of vertices, and they give the edges.  (The vertices $\vec e_1$ and $-\vec e_1$ don't form an edge; they form a line segment of length $2$ going through the origin, a body diagonal.)  Finally, there are eight ways to assign either $+$ or $-$ to each of the three expressions $\pm\vec e_1,\pm\vec e_2,\pm\vec e_3$; each case gives a face of the octahedron upon taking the convex hull.

If $T\in\operatorname{Isom}(\mathbf{Oct})$, then since $T$ permutes the $\pm\vec e_j$, again $T(\vec 0)=\vec 0$ by Lemma 2.48, hence $T\in O(3)$.  Moreover $T$ must be a monomial matrix with $\pm 1$'s as its nonzero entries.  But this time, the nonzero entries need not multiply to $1$.  In fact, it is clear that \emph{any} monomial matrix with $\pm 1$'s as the nonzero entries is an isometry of $\mathbf{Oct}$.  There are $48$ such matrices; therefore $|\operatorname{Isom}(\mathbf{Oct})|=48$.

The reader can readily verify that $\operatorname{Isom}^+(\mathbf{Oct})$ has order $24$ [see Exercise 4(a)].  Exercise 6 shows that $\operatorname{Isom}^+(\mathbf{Oct})\cong S_4$, and $\operatorname{Isom}(\mathbf{Oct})\cong S_4\times\mathbb Z/2\mathbb Z$.\\

(6) One can form a regular cube $\mathbf{Cube}$ by taking the eight points $(\pm 1,\pm 1,\pm 1)$ as the vertices.  Two vertices are joined by an edge if and only if they differ in exactly one component (i.e., $(a,b,c)$ with $a,b,c\in\{-1,1\}$ is connected to $(-a,b,c)$, $(a,-b,c)$ and $(a,b,-c$)).  There are six possible ways to pick one of the three components of the vector and set it equal to $1$ or $-1$; in each case, you get a face of the cube.

It turns out that $\operatorname{Isom}(\mathbf{Cube})=\operatorname{Isom}(\mathbf{Oct})$ as subgroups of $\operatorname{Isom}(\mathbb R^3)$: both consist of monomial matrices with $\pm 1$'s as the nonzero entries.  This is because the cube and octahedron are \emph{dual polyhedra}; the faces of each one correspond bijectively to the vertices of the other, and the solids were arranged in these examples so that the octahedron's vertices are the centers of the cube's faces.  They can be rescaled so the cube's vertices are the centers of the octahedron's faces.  As an immediate consequence as well, $\operatorname{Isom}^+(\mathbf{Cube})=\operatorname{Isom}^+(\mathbf{Oct})$.

Since the cube and octahedron have the same symmetry, we may refer to either one interchangeably when dealing with the isometry group.  For definiteness, we shall normally refer to the octahedron; this is the standard convention, the isometry group being referred to as the ``octahedral symmetry group'' by mathematicians.\\

(7) Throughout this example and the next, we let $\phi=\frac{1+\sqrt 5}2=1.6180339\dots$.  This is the \textbf{golden ratio}, and it satisfies $\phi^2=\phi+1$, as one can readily verify.

There are $12$ points in $\mathbb R^3$ of the form
$$(\pm\phi,\pm 1,0),~~~~(0,\pm\phi,\pm 1),~~~~(\pm 1,0,\pm\phi)$$
where the signs are independent of one another (e.g., in $(\pm\phi,\pm 1,0)$, the first two coordinates need not have the same sign as each other).  The convex hull of the 12 points is a regular icosahedron $\mathbf{Icos}$.  There are $30$ edges (of length $2$), and $20$ (triangular) faces: the reader should take the time to work out which vertices they connect in terms of the above formulas.

Now let $T\in\operatorname{Isom}(\mathbf{Icos})$; again $T$ permutes the vertices, so as in the previous three examples we have $T\in O(3)$.

The midpoints of the edges are points of the form $(\pm\phi,0,0)$, $\frac 12(\pm\phi^2,\pm 1,\pm\phi)$, along with even permutations of them; $T$ must permute those points.  Since $T$ sends $(\phi,0,0)=\phi\vec e_1$ to one of those points, the left column of $T$ must look like one of these:
$$\begin{bmatrix}\pm 1\\0\\0\end{bmatrix},~~~~\begin{bmatrix}\pm\frac 12\phi\\\pm\frac 12\phi^{-1}\\\pm\frac 12\end{bmatrix},~~~~\text{or even permutations of those.}$$ % The even permutations could change the signs, but since \frac 12(\pm\phi^2,\pm 1,\pm\phi) already has complete choice of signs, just the permutations alone are enough...
The same manifestly holds for the other two columns of $T$.

If $T$ contains any $\pm 1$ entry, the other entries in the same row and column must be zero (by orthogonality).  Hence since the columns must be of the form written above, $T$ is a monomial matrix with $\pm 1$ as its nonzero entries.  In this case, \emph{$T$ must be an even permutation}, even though its nonzero entries need not multiply to $1$: indeed, if $T$ were an odd permutation, then $T$ would map $(\pm\phi,\pm 1,0)$ to an odd permutation of the vector, and such a vector is not a vertex of $\mathbf{Icos}$.  There are $24$ isometries $T$ of this form: $3$ for the even permutation times $2^3=8$ for the sign of each nonzero entry. % Here, T is a monomial matrix with \pm 1 as its nonzero entries, not necessarily a permutation matrix.  So it can change the signs.

Now suppose that $T$ does not contain $\pm 1$.  Then it must contain even permutations of $\begin{bmatrix}\pm\frac 12\phi\\\pm\frac 12\phi^{-1}\\\pm\frac 12\end{bmatrix}$ as its columns.  Yet since $T$ is orthogonal, the dot product of any two columns is zero.  The columns thus must have \emph{different} permutations of that vector, because if two columns have the same permutation with various signs on the entries, the following formulas should convince you that it is impossible for their dot product to be zero:
$$\left(\frac 12\phi\right)^2=\frac{\phi+1}4,~~~~\left(\frac 12\phi^{-1}\right)^2=\frac{2-\phi}4,~~~~\left(\frac 12\right)^2=\frac 14$$
Hence, each even permutation of the vector is involved in exactly one column; in other words, $T$ looks like this when the columns are permuted:
$$\begin{bmatrix}\pm\frac 12\phi&\pm\frac 12\phi^{-1}&\pm\frac 12\\\pm\frac 12\phi^{-1}&\pm\frac 12&\pm\frac 12\phi\\\pm\frac 12&\pm\frac 12\phi&\pm\frac 12\phi^{-1}\end{bmatrix}$$
Now we can use the fact that the dot product of two columns is zero to determine some of the signs from other ones.  From the formulas:
$$\left(\frac 12\phi\right)\left(\frac 12\phi^{-1}\right)=\frac 14,~~~~\left(\frac 12\phi^{-1}\right)\left(\frac 12\right)=\frac{\phi-1}4,~~~~\left(\frac 12\right)\left(\frac 12\phi\right)=\frac{\phi}4$$
we see that the dot product of the first two columns above is zero if and only if either the bottom two entries of the first two columns have a positive product, and each of the other two rows of the first two columns has a negative product; or else the bottom two entries have a negative product and each of the other two rows of the first two columns has a positive product.  Hence, once the first column is written out, there are only two possibilities for the signs in the second column.  The same holds for the third column.

It turns out that we will successfully get an isometry, \emph{if and only if the main diagonal of $T$ has distinct entries up to sign}.  The above matrix, for example, has
$$\begin{bmatrix}\frac 12\phi&-\frac 12\phi^{-1}&\frac 12\\\frac 12\phi^{-1}&-\frac 12&-\frac 12\phi\\\frac 12&\frac 12\phi&-\frac 12\phi^{-1}\end{bmatrix}$$
as a possible way to assign the signs.  We leave it to the reader to verify that this is really an isometry.

By multiplying by various monomial matrices in $\operatorname{Isom}(\mathbf{Icos})$, we see that we still have an isometry whenever we negate any column or row, or apply an \emph{even} permutation to the columns (odd permutations will not work).  There are $96$ isometries $T$ of this form: $3$ for the even permutation, times $2^3=8$ for the signs of the entries of the first column, times $2$ for the signs in the second column (according to the previous paragraph), times $2$ for the signs in the third column.

Overall, there are $24+96=120$ isometries of the regular icosahedron.  Since there are obviously some which reverse orientation, Exercise 4(a) shows that there are $60$ orientation-preserving isometries of the regular icosahedron.  Exercise 6 shows that $\operatorname{Isom}^+(\mathbf{Icos})\cong A_5$, and $\operatorname{Isom}(\mathbf{Icos})\cong A_5\times\mathbb Z/2\mathbb Z$.\\

(8) Consider the $20$ points in $\mathbb R^3$ of the form
$$(\pm 1,\pm 1,\pm 1),~~~~(\pm\phi^{-1},\pm\phi,0),~~~~(\pm\phi,0,\pm\phi^{-1}),~~~~(0,\pm\phi^{-1},\pm\phi)$$
where the signs are independent of one another.  Their convex hull is a regular dodecahedron $\mathbf{Dodec}$.  There are $30$ edges (of length $2\phi^{-1}$), and $12$ faces that are regular pentagons.  The reader should take the time to work out which vertices they connect.

As in Example (6) one can show that this dodecahedron is dual to the icosahedron: the center of a face of either of these solids is a scalar multiple of a vertex of the other solid.  Thus, we get $\operatorname{Isom}(\mathbf{Dodec})=\operatorname{Isom}(\mathbf{Icos})$ as subgroups of $\operatorname{Isom}(\mathbb R^3)$.  Therefore also $\operatorname{Isom}^+(\mathbf{Dodec})=\operatorname{Isom}^+(\mathbf{Icos})$.  Since the dodecahedron and icosahedron have the same symmetry, we may refer to either one interchangeably when dealing with the isometry group.  We shall usually refer to the icosahedron, as this isometry group is called the ``icosahedral symmetry group.''\\ % Far as I can tell, neither solid's vertices are inscribed at the face-centers of the other. Why?

\noindent Now that we have constructed sophisticated examples of finite groups of isometries of $\mathbb R^3$, let us conclude this chapter by showing that finite groups of isometries of $\mathbb R^2$ can never be that complicated.  In fact, we can easily classify them right now.

First recall from Section 2.6 that every element of $SO(2)$ is of the form $R_\theta=\begin{bmatrix}\cos\theta&-\sin\theta\\\sin\theta&\cos\theta\end{bmatrix}$ with $\theta\in\mathbb R$, and this matrix is rotation by an angle of $\theta$ around the origin: hence $SO(2)$ consists of only rotations.\\

\noindent\textbf{Theorem 2.50.} \textsc{(Classification of finite subgroups of $SO(2)$ and $O(2)$)}

(i) \emph{Every finite subgroup of $SO(2)$ is cyclic of order $n$, generated by a rotation of angle $2\pi/n$.}

(ii) \emph{Every finite subgroup of $O(2)$ is either cyclic of order $n$ as in \text{(i)}, or else it is isomorphic to $D_n$, generated by a rotation of angle $2\pi/n$ and a reflection.}\\

\noindent The theorem effectively states that Examples (1) and (2) above give all finite subgroups of $SO(2)$ and $O(2)$.
\begin{proof}
(i) Let $G\subset SO(2)$ be a finite subgroup.  We may assume $G\ne\{1\}$.  Write $G=\{1,R_{\alpha_1},\dots,R_{\alpha_k}\}$ where $0<\alpha_1<\alpha_2<\dots<\alpha_k<2\pi$.  [This is possible because $R_\theta=R_{\theta+2\pi}$ for any $\theta$, hence the subscript can be arranged to be between $0$ and $2\pi$; and then the subscripts can be sorted in ascending order.]  Let $\lambda=\alpha_1$.  Then $\lambda>0$ and $R_\lambda\in G$, but $R_\mu\notin G$ for any $0<\mu<\lambda$.

We now show that for $\gamma\in\mathbb R$,
\begin{equation}\tag{*}
R_\gamma\in G\iff\gamma/\lambda\in\mathbb Z.
\end{equation}
After all, if $\gamma/\lambda\in\mathbb Z$, then $\gamma=m\lambda$ for some $m\in\mathbb Z$; moreover, since $G$ is a subgroup, $R_\gamma=R_{m\lambda}=R_\lambda^m\in G$.  Conversely, suppose $R_\gamma\in G$.  Then let $m=\lfloor\gamma/\lambda\rfloor$; then $m$ is an integer with $m\leqslant\gamma/\lambda<m+1$.  With that, multiplying by $\lambda$ gives $m\lambda\leqslant\gamma<(m+1)\lambda$, then subtracting $m\lambda$ gives $0\leqslant\gamma-m\lambda<\lambda$.  Yet also $R_{\gamma-m\lambda}=R_\gamma R_\lambda^{-m}\in G$ since $G$ is a subgroup.  Since no $\mu$ strictly between $0$ and $\lambda$ satisfies $R_\mu\in G$, we must have $\gamma-m\lambda=0$.  Therefore $\gamma=m\lambda$ and $\gamma/\lambda=m\in\mathbb Z$.

From (*) it immediately follows that $2\pi/\lambda\in\mathbb Z$, because $R_{2\pi}=1\in G$.  Let $n=2\pi/\lambda$.  Then $\lambda=2\pi/n$ and therefore $R_\lambda$ is a rotation of order $n$.  Moreover, (*) entails at once that $G=\{R_{c\lambda}:c\in\mathbb Z\}=\left<R_\lambda\right>$.  Thus we also conclude $|G|=n$.

(ii) Let $G\subset O(2)$ be a finite subgroup.  If $G\subset SO(2)$, then $G$ is cyclic of order $n$ by (i).  So assume $G\not\subset SO(2)$.  Let $N=G\cap SO(2)$; this is a normal subgroup of $G$ of index $2$, because it is the kernel of the surjective homomorphism $\det:G\to\{-1,1\}$.

Since $N$ is a finite subgroup of $SO(2)$, we have $N=\left<R_{2\pi/n}\right>$ where $n=|N|$ by (i).  Now let $g\in G-N$; $g$ is an orientation-reversing operator in $O(2)-SO(2)$.  As we saw in Section 2.6, $g$ must be a reflection around a line through the origin.  One readily verifies $g^2=1$ and $gR_{2\pi/n}g=R_{-2\pi/n}$; one way to see the latter statement is that $gR_{2\pi/n}\in O(2)-SO(2)$, hence is also a reflection, so it squares to the identity.  Furthermore, every element of $G-N$ is of the form $gn$ with $n\in N$ (because $a\in G-N\implies g^{-1}a\in N$).  This implies that if $\lambda=2\pi/n$, then
$$G=\{1,R_\lambda,R_{2\lambda},\dots,R_{(n-1)\lambda},g,gR_\lambda,\dots,gR_{(n-1)\lambda}\}$$
and the reader can verify in this case that $G\cong D_n$, generated by a rotation of angle $\lambda$ and a reflection.
\end{proof}

\noindent\emph{Remark.} If $G_1=\left<R_\pi\right>$ and $G_2$ is cyclic generated by reflection over the $x$-axis, then $G_1\cong G_2~[\cong\mathbb Z/2\mathbb Z]$ as abstract groups.  However, they are essentially distinct as subgroups of $O(2)$, because $G_1\subset SO(2)$ but $G_2\not\subset SO(2)$.  In this context, we distinguish them by saying that $G_1$ is ``cyclic of order $2$'' and $G_2$ is ``dihedral of degree $1$.''  Different symmetry type, same group!

\subsection*{Exercises 2.7. (Finite Groups of Isometries)}
\begin{enumerate}
\item A \textbf{convex combination} of vectors $\vec v_1,\dots,\vec v_n$ is a linear combination $c_1\vec v_1+\dots+c_n\vec v_n$ where the $c_j$ are nonnegative real numbers and $c_1+\dots+c_n=1$.  If $X\subset\mathbb R^n$, the \textbf{convex hull} of $X$, denoted $\hat X$, is defined to be the set of all (finite) convex combinations of elements of $X$.

(a) Every isometry of $X$ is an isometry of $\hat X$.

(b) Show by example that an isometry of $\hat X$ need not be an isometry of $X$.

\item Let $G$ be a subgroup of $\operatorname{Isom}(\mathbb R^2)$.  If $G$ contains two rotations $\ne 1$ around distinct points, show that $G$ is infinite.

\item If $P_1$ and $P_2$ are planes which intersect in a line $\ell$, let $p$ be a point on $\ell$.  Construct lines through $p$ contained in $P_1$ and $P_2$ perpendicular to $\ell$.  The angle between these lines is called the \textbf{dihedral angle} between the planes $P_1$ and $P_2$.

(a) If $q$ is a point outside the planes, then the dihedral angle between $P_1$ and $P_2$ is supplementary to the angle between the lines through $q$ perpendicular to $P_1$ and $P_2$.

(b) Use part (a) to find the dihedral angles of the edges of the five Platonic solids.

(c) A \textbf{regular $4$-polytope} is obtained by taking a Platonic solid and hinging copies of it (called ``cells'') together in $\mathbb R^4$, so that faces meet faces, edges meet edges and vertices meet vertices, and every edge has the same number of cells \---- exactly how polygons were hinged together to get the Platonic solids.  For a regular $4$-polytope, explain why the number of cells around an edge must be less than $360^\circ$ divided by the dihedral angle of an edge.

(d) Use this to classify all possible regular $4$-polytopes.  [There are six of them, discovered by Ludwig Schl\"afli in the 19th century.  You do not need to prove that each one really exists.]

\item Let $X\subset\mathbb R^n$.

(a) If $\operatorname{Isom}(X)$ contains an orientation-reversing isometry, show that $\operatorname{Isom}^+(X)$ is a subgroup of $\operatorname{Isom}(X)$ of index $2$.

(b) Give an example of a figure with no orientation-reversing isometry.

\item\emph{(Simplices.)} \---- (a) The points
$$\left(0,0,1\right),\left(0,\frac{2\sqrt 2}3,-\frac 13\right),\left(\frac{\sqrt 6}3,-\frac{\sqrt 2}3,-\frac 13\right),\left(-\frac{\sqrt 6}3,-\frac{\sqrt 2}3,-\frac 13\right)\in\mathbb R^3$$
form a regular tetrahedron with its vertices on the unit sphere.

(b) The side length of this tetrahedron is $\frac{2\sqrt 6}3$.

(c) More generally, suppose for each positive integer $k$ that $\alpha_k=1/k$, and $\beta_k=\sqrt{1-\alpha_k^2}$.  Consider the following $n+1$ points in $\mathbb R^n$:
$$\begin{bmatrix}0\\\vdots\\0\\0\\0\\1\end{bmatrix},\begin{bmatrix}0\\\vdots\\0\\0\\\beta_n\\-\alpha_n\end{bmatrix},\begin{bmatrix}0\\\vdots\\0\\\beta_{n-1}\beta_n\\-\alpha_{n-1}\beta_n\\-\alpha_n\end{bmatrix},\begin{bmatrix}0\\\vdots\\\beta_{n-2}\beta_{n-1}\beta_n\\-\alpha_{n-2}\beta_{n-1}\beta_n\\-\alpha_{n-1}\beta_n\\-\alpha_n\end{bmatrix},\dots,\begin{bmatrix}\beta_2\dots\beta_n\\\vdots\\-\alpha_{n-2}\beta_{n-1}\beta_n\\-\alpha_{n-1}\beta_n\\-\alpha_n\end{bmatrix},\begin{bmatrix}-\beta_2\dots\beta_n\\\vdots\\-\alpha_{n-2}\beta_{n-1}\beta_n\\-\alpha_{n-1}\beta_n\\-\alpha_n\end{bmatrix}$$
Show that each point has a distance of $1$ from the origin, and that the distance between any two of these points is equal to $\sqrt{\frac{2n+2}n}$.  The convex hull of these points is a \textbf{regular simplex} centered at the origin, in $n$ dimensions.  In terms of the number of vertices, it is the smallest possible nondegenerate $n$-polytope.  [This is the line segment for $n=1$, the triangle for $n=2$, the tetrahedron for $n=3$ and the 5-cell for $n=4$.]

\item The aim of this exercise is to show that $\operatorname{Isom}^+(\mathbf{Oct})\cong S_4$, $\operatorname{Isom}(\mathbf{Oct})\cong S_4\times\mathbb Z/2\mathbb Z$, $\operatorname{Isom}^+(\mathbf{Icos})\cong A_5$, and $\operatorname{Isom}(\mathbf{Icos})\cong A_5\times\mathbb Z/2\mathbb Z$.

(a) If $\vec v$ is any vertex of the cube with vertices $(\pm 1,\pm 1,\pm 1)$, the line segment from $\vec v$ to $-\vec v$ is called a \textbf{body diagonal} of the cube.  The cube has four body diagonals, one for each pair of antipodal vertices.

Let $B$ be the set of body diagonals of the cube.  Show that $\operatorname{Isom}^+(\mathbf{Oct})$ acts on $B$ by application of an isometry.  [Recall that $\operatorname{Isom}^+(\mathbf{Oct})=\operatorname{Isom}^+(\mathbf{Cube})$.]

(b) The aforementioned group action induces a homomorphism $\varphi:\operatorname{Isom}^+(\mathbf{Oct})\to S(B)$.  Show that $\varphi$ is an isomorphism.  [The only orientation-preserving isometries that fix one particular body diagonal are rotations around that body diagonal.  This leads to a proof of injectivity.  As for surjectivity, what are the orders of both groups?]  Conclude that $\operatorname{Isom}^+(\mathbf{Oct})\cong S_4$.

(c) Now show that $\operatorname{Isom}(\mathbf{Oct})\cong S_4\times\mathbb Z/2\mathbb Z$.  [Using Exercise 13 of Section 1.4, show that $\operatorname{Isom}(\mathbf{Oct})$ is the internal direct product of the subgroups $\operatorname{Isom}^+(\mathbf{Oct})$ and $\{I_3,-I_3\}$.]

(d) $\operatorname{Isom}^+(\mathbf{Icos})$ acts on $\mathcal P(\mathbb R^3)$, the set of subsets of $\mathbb R^3$.  The stabilizer of $\mathbf{Tet}$ (as an element of $\mathcal P(\mathbb R^3)$) is the entire group $\operatorname{Isom}^+(\mathbf{Tet})$.  [Observe that it suffices to show that $\operatorname{Isom}^+(\mathbf{Tet})$ is a subgroup of $\operatorname{Isom}^+(\mathbf{Icos})$.]

(e) Let $X$ be the orbit of $\mathbf{Tet}$.  Then $|X|=5$ by the Orbit-Stabilizer Theorem (since $|\operatorname{Isom}^+(\mathbf{Icos})|=60$ and $|\operatorname{Isom}^+(\mathbf{Tet})|=12$).  The union of the five tetrahedra in $X$ looks like this:
\begin{center}
\includegraphics[scale=.35]{CompoundFiveTetrahedra.png}
\end{center}

(f) The action then induces a homomorphism $\varphi:\operatorname{Isom}^+(\mathbf{Icos})\to S(X)$.  Show that $\varphi$ is injective.  [The kernel of $\varphi$ is a normal subgroup of $\operatorname{Isom}^+(\mathbf{Icos})$ contained in $\operatorname{Isom}^+(\mathbf{Tet})$.]

(g) Use part (f) to show $\operatorname{Isom}^+(\mathbf{Icos})\cong A_5$.  [Use Exercise 13(h) of Section 1.6.]

(h) Now show that $\operatorname{Isom}(\mathbf{Icos})\cong A_5\times\mathbb Z/2\mathbb Z$.

\item Use the Orbit-Stabilizer Theorem to show that for any Platonic solid:

(a) if there are $k$ faces to a vertex, rotation by an angle $2\pi/k$ around the axis through that vertex is an isometry;

(b) the action of the orientation-preserving isometry group on the solid's vertices is transitive;

(c) the action of the orientation-preserving isometry group on the solid's faces is transitive;

(d) if a face is a regular $n$-gon, rotation by an angle $2\pi/n$ around the axis through the center of the face is an isometry;

(e) the action of the orientation-preserving isometry group on the solid's edges is transitive;

(f) a $180^\circ$ rotation around the axis through the midpoint of an edge is an isometry.

\item (a) Suppose there are $n$ available colors, where $n$ is a positive integer.  Each face of a cube is to be colored with one of these colors.  Two colorings are considered to be the same if one can be obtained from the other through rotating the cube (however, ``flipping'' the cube by an orientation-reversing transformation is not allowed; that can't be done with a cube in real life).  How many different ways are there to color the faces?  (The answer is a polynomial in $n$.)

[Let $X$ be the set of all colorings of the faces with the cube in standard position.  Then $\operatorname{Isom}^+(\mathbf{Cube})$ acts on $X$, and the different ways to color the faces (according to the problem) are the orbits of this action.  Now use Burnside's Counting Theorem (1.23).  To avoid having to do $24$ separate additions, partition the group into its conjugacy classes.]

(b) List the answers to part (a) for $n=1,2,\dots,7$.  Then (using number theory), show that for any $n\in\mathbb Z$ whatsoever, the expression obtained in part (a) is an integer.  (We know it is an integer by another method\----it comes from counting.)

(c) Now suppose there are $n$ available colors.  Each face of a regular icosahedron is to be colored with one of these colorings.  Two colorings are considered to be the same if (and only if) one can be obtained from the other through rotating the icosahedron.  How many different ways are there to color the faces?

\item\emph{(Wallpaper groups.)} \---- The aim of this exercise is to establish the $17$ wallpaper groups, established by Evgraf Fedorov and George P\'olya.

First let $T(\mathbb R^2)$ be the subgroup of $\operatorname{Isom}(\mathbb R^2)$ consisting of the translations.  Then $T(\mathbb R^2)\cong\mathbb R^2$ for obvious reasons (where the latter group is under addition).  Also, the map $\psi:\operatorname{Isom}(\mathbb R^2)\to O(2)$ sending $[A,\vec v]\mapsto A$ is a surjective homomorphism of groups with kernel $T(\mathbb R^2)$; and hence, $T(\mathbb R^2)$ is normal.

A \textbf{wallpaper group} is defined to be a subgroup $G$ of $\operatorname{Isom}(\mathbb R^2)$ such that $G\cap T(\mathbb R^2)\cong\mathbb Z^2$.  Such a group $G$ is the symmetry group of some plane design (i.e., a function $\mathbb R^2\to\mathcal C$ where $\mathcal C$ is the ``set of colors''), which repeats itself in a tiling pattern in two nonparallel directions.  We assume two wallpaper groups to be the same if conjugating one subgroup by an affine transformation (Exercise 9(b) of Section 2.6) yields the other.  Indeed, in this case, given any pattern whose symmetry group is the first group, one can simply apply the affine transformation to take it to the other. % Actually, that's not the completely accurate definition of a wallpaper group.  A subgroup of $\mathbb R^2$ isomorphic to $\mathbb Z^2$ could be generated by parallel vectors $\vec v,c\vec v$ with $c$ irrational.

Let $G$ be a wallpaper group and $\varphi=\psi|_G:G\to O(2)$.  We classify all possibilities for $G$.

(a) First suppose $G\subset T(\mathbb R^2)$.  Then $G\cong\mathbb Z\times\mathbb Z$ for obvious reasons; this is one wallpaper group.

Now we assume $G\not\subset T(\mathbb R^2)$, but also $G\subset\operatorname{Isom}^+(\mathbb R^2)$.  In this case, $G$ consists of only orientation-preserving isometries.  Let $H=\varphi(G)$; note that $H\subset SO(2)$.

(b) Let $L$ be the lattice $\{\vec v\in\mathbb R^2:[I_2,\vec v]\in G\}$.  Show that every $h\in H$ maps $L$ to itself.  [Take an element $g\in G$ such that $\varphi(g)=h$.  Conjugation by that element induces an automorphism of the normal subgroup $G\cap\operatorname{Tra}(\mathbb R^n)$.]

(c) Every $h\in H$ is conjugate in $GL_2(\mathbb R)$ to a matrix with integer coefficients (which necessarily has determinant $1$).  Conclude that $h$ has finite order, and its eigenvalues are either linear or quadratic over $\mathbb Q$.  Hence, the Euler totient of $|h|$ is either $1$ or $2$, so that $|h|\in\{1,2,3,4,6\}$.

(d) Use part (c) to conclude that $H$ is one of the subgroups
$$\left<R_\pi\right>,\left<R_{2\pi/3}\right>,\left<R_{\pi/2}\right>,\left<R_{\pi/3}\right>$$
of $SO(2)$.  [The assumption $G\not\subset T(\mathbb R^2)$ implies $H\ne\{1\}$.]

(e) If $H=\left<R_\lambda\right>$ with $\lambda\in\{\pi,2\pi/3,\pi/2,\pi/3\}$, show that $G$ is generated by its translations and a rotation by $\lambda$ around some point.  In each of these four cases, there is a unique wallpaper group.  [Note that if $|H|=4$, the lattice $L$ in part (b) must be a square lattice; if $|H|=3$ or $6$, it must be an equilateral triangle lattice; but if $|H|=2$, the lattice can be made up of any kind of parallelogram, thus giving more possibilities.]  Adding these to the wallpaper group in part (a), we have 5 wallpaper groups so far.

The remaining case, which is noticeably hardest, is where $G\not\subset\operatorname{Isom}^+(\mathbb R^2)$; i.e., $G$ contains orientation-reversing isometries.  Let $H=\varphi(G)$.  Then $H$ is a subgroup of $O(2)$ which is not contained in $SO(2)$; by Theorem 2.50, it is dihedral generated by a rotation of an angle $\lambda=2\pi/n$ and a reflection.  By conjugating $G$ via an isometry of $\mathbb R^2$, we may assume:

~~~~(i) $H$ contains a reflection over the $x$-axis;

~~~~(ii) $G$ contains a rotation of an angle $\lambda$ around the origin.

(f) Show that $n\in\{1,2,3,4,6\}$.  [Adapt the argument for $G\subset\operatorname{Isom}^+(\mathbb R^2)$.]

Define $L=\{\vec v\in\mathbb R^2:[I_2,\vec v]\in G\}$ as in part (b).  We say that a lattice point $\vec v\in L$ is \textbf{primitive} if $\vec v\ne\vec 0$ and the line segment from $\vec 0$ to $\vec v$ does not meet any other lattice point in $L$.  In particular, a lattice point $(a,b)\in\mathbb Z^2$ is primitive $\iff\gcd(a,b)=1$.

Again (b) applies to show that each element of $H$ maps $L$ to itself.

(g) Suppose $n=1$.  Then $H$ consists solely of the identity and reflection over the $x$-axis.  Show that either $G$ has a reflection, (in which case there are two wallpaper groups); or else $G$ has a glide reflection whose square is translation by a primitive point in $L$, (in which case, if $G$ has no reflection at all, there is only one wallpaper group).  This adds 3 more wallpaper groups, for a total of 8.

(h) Suppose $n=2$.  Then $H$ contains the identity, the $180^\circ$ rotation, and the horizontal and vertical flips.  Show that either $L$ is built out of rectangles with sides parallel to the $x$ and $y$ axes (in which case, there are 2 wallpaper groups, depending on whether there are reflections), or else $L$ is built out of rhombi with diagonals parallel to the $x$ and $y$ axes (in which case there are 2 wallpaper groups again).  This adds 4 more, for a total of 12.

(i) Suppose $n=3$.  Then $L$ is an equilateral-triangle lattice.  Then $G$ \emph{must} have a reflection, and there are 2 wallpaper groups in this case.  We have gathered 14 to this point.

(j) Suppose $n=4$.  Then $L$ is a square lattice.  In this case $G$ must have a reflection over a line making a $45^\circ$-angle with the $x$-axis.  Moreover there are 2 wallpaper groups in this case; thus we have a total of 16.

(k) Finally suppose $n=6$.  Then $L$ is an equilateral-triangle lattice.  Show that $G$ must have a reflection, and that there is only one wallpaper group.  This completes the classification at $17$ wallpaper groups, as desired.

(l) If $G$ is a wallpaper group, show that there exists a polygon $A_1\dots A_k$ in $\mathbb R^2$ such that the union of the polygons $g(A_1)\dots g(A_k),g\in G$ along with their interiors is $\mathbb R^2$, and any two of those polygons have disjoint interiors.  Then one can get a wallpattern for $G$ by merely drawing something fairly asymmetrical in that polygon, and then transforming it via every element of $G$.  [$A_1\dots A_k$ is called a \textbf{basic unit} for the group $G$.]
\end{enumerate}

\subsection*{2.8. Classification of Finite Subgroups of $SO(3)$ and $O(3)$}
\addcontentsline{toc}{section}{2.8. Classification of Finite Subgroups of $SO(3)$ and $O(3)$}
In the previous section, we have classified finite subgroups of $SO(2)$ and $O(2)$.  In this section, we shall tackle the isometries of the orthogonal group for $3$-space.  They are slightly more complicated, and we have gathered some in the previous section.  Once we know all possible finite subgroups of $O(3)$, we will effectively know every possible discrete pattern style on a finite figure in $3$-space.

Let us start with a few lemmas.  We let $S^2=\{\vec v\in\mathbb R^3:\|\vec v\|=1\}$ be the unit sphere centered at the origin.\\

\noindent\textbf{Lemma 2.51.} \emph{Every $g\ne 1$ in $SO(3)$ is rotation around a uniquely determined axis.}
\begin{proof}
We first show that the operator $g$ must have $1$ as an eigenvalue.

The characteristic polynomial of $g$ is a cubic polynomial with real coefficients.  Since any polynomial with real coefficients of odd degree has a real root, the characteristic polynomial of $g$ must have a real root; in other words, $g$ must have a real eigenvalue $\alpha$.  By Exercise 3(g) of Section 2.6, $\alpha=\pm 1$.

If the other two eigenvalues $\beta,\gamma$ of $g$ are real, then $g$ has $\pm 1$ for its eigenvalues counted with algebraic multiplicity; the product of these eigenvalues is $1$ (since $g\in SO(3)$, we have $\det g=1$), which means that among the three of them, there are an even number of $-1$'s.  Thus $g$ has an eigenvalue of $1$ in this case.  On the other hand, if $\beta$ and $\gamma$ are not real, they must be complex conjugates of each other; hence $\beta\gamma=|\beta|^2=1$ (by Exercise 3(g) of Section 2.6), which implies $\alpha=1$ (because $\det g=\alpha\beta\gamma=1$).  Therefore, $g$ has an eigenvalue of $1$ in this case as well.

Since $g$ must have an eigenvalue of $1$, there is a unit vector $\vec p\in S^2$ such that $g(\vec p)=\vec p$ (take an eigenvector for $1$ and multiply by the inverse of its magnitude).  Let $a$ be an element of $SO(3)$ such that $a(\vec e_3)=\vec p$; a basic argument using the Gram-Schmidt process shows that $a$ exists.  Now set $h=a^{-1}ga$.  Then $h\in SO(3)$ and $h(\vec e_3)=\vec e_3$.  Thus, the orthogonality of $h$ entails that $h(\vec e_1),h(\vec e_2)$ are perpendicular to $\vec e_3$, and hence $h$ looks like this:
$$h=\begin{bmatrix}x&y&0\\z&w&0\\0&0&1\end{bmatrix}$$
Since $\det h=1$ and its columns are orthonormal, we get that $\begin{bmatrix}x&y\\z&w\end{bmatrix}\in SO(2)$ by straightforward calculation.  Hence this $2\times 2$ submatrix must be of the form $R_\theta$ with $\theta\in\mathbb R$; in other words, $x=w=\cos\theta$ and $z=-y=\sin\theta$.  It is then clear that $h$ rotates by an angle of $\theta$ around the $z$-axis (parallel to $\vec e_3$).  Since $g=aha^{-1}$, $g$ rotates by an angle of $\theta$ around the axis parallel to $\vec p$, as desired.

To show that the axis is uniquely determined, we claim that $\pm\vec p$ are the \emph{only} unit vectors fixed by $g$; it will follow that any axis around which $g$ rotates must be parallel to the vectors $\pm\vec p$.  Well, suppose $\vec x\in S^2$ is fixed by $g$.  Then $h(a^{-1}(\vec x))=[a^{-1}ga](a^{-1}(\vec x))=a^{-1}gaa^{-1}(\vec x)=a^{-1}g(\vec x)=a^{-1}(\vec x)$; hence $\vec y=a^{-1}(\vec x)$ is fixed by $h$.  Moreover, $\vec y$ is in the kernel of the linear operator
$$h-I_3=\begin{bmatrix}\cos\theta-1&-\sin\theta&0\\\sin\theta&\cos\theta-1&0\\0&0&0\end{bmatrix},$$
whose upper-left $2\times 2$ submatrix is nonsingular (its determinant is $(\cos\theta-1)^2+(\sin\theta)^2=2-2\cos\theta\ne 0$, since $g\ne 1$ and hence $\cos\theta\ne 1$).  Consequently $\vec y$ must be of the form $(0,0,r)$ with $r\in\mathbb R$; since $\vec y\in S^2$ we conclude $\vec y=\pm\vec e_3$, and hence $\vec x=a(\vec y)=\pm\vec p$.
\end{proof}
\noindent\textbf{Lemma 2.52.} (i) \emph{Let $\vec p\in S^2$, and let $G=\{g\in SO(3):g(\vec p)=\vec p\}$.  Then there is an isomorphism $SO(2)\cong G$, sending every rotation of an angle $\theta$ to a rotation of the same angle $\theta$ around the axis parallel to $\vec p$.}

(ii) \emph{Let $\vec p\in S^2$, and let $G=\{g\in SO(3):g(\vec p)=\pm\vec p\}$.  Then there is an isomorphism $O(2)\cong G$, sending every rotation of an angle $\theta$ to a rotation of the same angle around the axis parallel to $\vec p$, and sending every reflection to an element of $G$ sending $\vec p\mapsto-\vec p$.}
\begin{proof}
In each case, by conjugating $G$ by an element of $SO(3)$ sending $\vec e_3\mapsto\vec p$, we may assume that $\vec p=\vec e_3$.

(i) Define $\varphi:SO(2)\to G$ via $\varphi(a)=\begin{bmatrix}a&0\\0&1\end{bmatrix}$, where the latter matrix is regarded as a block matrix.  In other words, $\varphi\left(\begin{bmatrix}x&y\\z&w\end{bmatrix}\right)=\begin{bmatrix}x&y&0\\z&w&0\\0&0&1\end{bmatrix}$.

$\varphi$ is well-defined, because a matrix of the form $\varphi(a)$ maps $\vec e_3\mapsto\vec e_3$.  It is clear that $\varphi$ is an injective homomorphism of groups.  Moreover, if $b\in G$ is any element, then $b(\vec e_3)=\vec e_3$, hence we must have $b=\begin{bmatrix}\cos\theta&-\sin\theta&0\\\sin\theta&\cos\theta&0\\0&0&1\end{bmatrix}$ as in the proof of Lemma 2.51.  Moreover, $b=\begin{bmatrix}R_\theta&0\\0&1\end{bmatrix}=\varphi(R_\theta)$, hence $\varphi$ is surjective, and an isomorphism.

The reader can readily verify that $\varphi$ sends rotation by an angle of $\theta$ to rotation by an angle of $\theta$.

(ii) Define $\psi:O(2)\to G$ via $\psi(a)=\begin{bmatrix}a&0\\0&\det a\end{bmatrix}$; i.e., we have $\psi\left(\begin{bmatrix}x&y\\z&w\end{bmatrix}\right)=\begin{bmatrix}x&y&0\\z&w&0\\0&0&xw-yz\end{bmatrix}$.  One can readily verify that every $\psi(a)$ is an element of $SO(3)$ mapping $\vec e_3$ to either $\vec e_3$ or $-\vec e_3$; hence $\psi$ is well-defined.  Also, $\psi$ is an injective homomorphism of groups.

To show that $\psi$ is surjective, adapt the proof of part (a); noting that $b\in G$ satisfies $b(\vec e_3)=-\vec e_3$ if and only if the upper-left $2\times 2$ submatrix is orientation-reversing.  Therefore $\psi$ is an isomorphism.  Again the reader can readily verify that $\psi$ sends rotation by an angle of $\theta$ to rotation by an angle of $\theta$, and that $\psi(a)$ sends $\vec e_3\mapsto-\vec e_3$ when $a\in O(2)$ is a reflection.
\end{proof}

\noindent According to the previous lemma, every subgroup of $O(2)$ can be identified with a subgroup of $SO(3)$ which fixes a certain axis [with possibly group elements swapping the endpoints].  We therefore get all the cyclic and dihedral groups as subgroups of $SO(3)$.  We also know three other finite subgroups: $\operatorname{Isom}^+(\mathbf{Tet})$, the orientation-preserving isometry group of the regular tetrahedron; $\operatorname{Isom}^+(\mathbf{Oct})$, that of the octahedron; and $\operatorname{Isom}^+(\mathbf{Icos})$, that of the icosahedron.  We claim that these are the only ones:\\

\noindent\textbf{Theorem 2.53.} \textsc{(Classification of finite subgroups of $SO(3)$)}

\emph{Every finite subgroup of $SO(3)$ is isomorphic to either a cyclic group, a dihedral group, $\operatorname{Isom}^+(\mathbf{Tet})$, $\operatorname{Isom}^+(\mathbf{Oct})$ or $\operatorname{Isom}^+(\mathbf{Icos})$.}
\begin{proof} % This proof is due to Prof. Claire Burrin (RU), I have memorized it from her
Let $G$ be a finite subgroup of $SO(3)$.  We may assume $G\ne\{1\}$ for obvious reasons.

We start by letting $P=\{\vec p\in S^2:g(\vec p)=\vec p\text{ for some }g\ne 1\text{ in }G\}$.  By Lemma 2.51, every $g\ne 1$ in $G$ is a rotation around a uniquely determined axis; moreover, $P$ is the set of unit vectors on these axes, which implies that $P$ is finite.

Moreover, $G$ acts on $P$; i.e., $g\cdot\vec p=g(\vec p)$.  Indeed, if $g\in G$ and $\vec p\in P$, then by definition of $P$, we have $h(\vec p)=\vec p$ for some $h\ne 1$ in $G$.  With that, $ghg^{-1}\ne 1$, and $ghg^{-1}(g(\vec p))=ghg^{-1}g(\vec p)=gh(\vec p)=g(\vec p)$.  Therefore $g(\vec p)\in P$.

We shall first establish an equation which will guide us swiftly through the rest of the proof.  To do this, we let
$$S=\{(g,\vec p)\in G\times P:g\ne 1,g\cdot\vec p=\vec p\},$$
and compute $|S|$ in two ways.  On the one hand, partitioning $S$ by the left component shows $|S|=\sum_{g\in G,g\ne 1}|\{\vec p\in P:g\cdot\vec p=\vec p\}|$.  Yet for $g\ne 1$, there are exactly two points fixed by $g$ (the endpoints of the rotation axis), so that $|\{\vec p\in P:g\cdot\vec p=\vec p\}|=2$.  Therefore $|S|=\sum_{g\in G,g\ne 1}2=2(|G|-1)$.

On the other hand, we may partition $S$ by the \emph{right} component, to get $|S|=\sum_{\vec p\in P}|\{g\in G:g\ne 1,g\cdot\vec p=\vec p\}|$.  Now, the set of $g\in G$ such that $g\cdot\vec p=\vec p$ is precisely the stabilizer $\operatorname{Stab}(\vec p)$.  Yet this subgroup always contains the identity, which we are deliberately removing from the set, thus decrementing its size by $1$.  We thus get $|\{g\in G:g\ne 1,g\cdot\vec p=\vec p\}|=|\operatorname{Stab}(\vec p)|-1$, and hence $|S|=\sum_{\vec p\in P}(|\operatorname{Stab}(\vec p)|-1)$.  Equating the two expressions for $|S|$,
$$2(|G|-1)=\sum_{\vec p\in P}(|\operatorname{Stab}(\vec p)|-1)$$
We may write $P=\bigsqcup_{j=1}^k\mathcal O_j$, where the $\mathcal O_j$ are the distinct orbits of the group action.  The summation above can then be interpreted as a sum over the orbits, where each summand sums over the elements of the orbit:
$$2(|G|-1)=\sum_{j=1}^k\sum_{\vec p\in\mathcal O_j}(|\operatorname{Stab}(\vec p)|-1)$$
By the Orbit-Stabilizer Theorem, we have
$$\sum_{\vec p\in\mathcal O_j}(|\operatorname{Stab}(\vec p)|-1)=\sum_{\vec p\in\mathcal O_j}\left(\frac{|G|}{|\mathcal O_j|}-1\right)=|\mathcal O_j|\left(\frac{|G|}{|\mathcal O_j|}-1\right)=|G|-|\mathcal O_j|$$
And hence
$$2(|G|-1)=\sum_{j=1}^k(|G|-|\mathcal O_j|)$$
For each $j$, let $\vec p_j$ be an element of $\mathcal O_j$.  Dividing both sides of the above equation by $|G|$, and using the Orbit-Stabilizer Theorem again, we arrive at
\begin{equation}\tag{1}
2\left(1-\frac 1{|G|}\right)=\sum_{j=1}^k\left(1-\frac 1{|\operatorname{Stab}(\vec p_j)|}\right)
\end{equation}
First observe that in (1), the left-hand side is $\geqslant 1$ (we are assuming $G\ne\{1\}$, hence $|G|\geqslant 2$, from which $1-\frac 1{|G|}\geqslant\frac 12$ follows), and every summand on the right-hand side is $<1$, which implies that $k>1$ (in other words, there must be more than one summand on the right-hand side).  Also, the left-hand side is $<2$, and every summand on the right-hand side is $\geqslant\frac 12$ (because by definition of $P$, $\vec p_j\in P$ implies $\operatorname{Stab}(\vec p_j)$ is a nontrivial subgroup, i.e., has order $\geqslant 2$).  This implies $k<4$ (because $k\geqslant 4$ would make the right-hand side $\geqslant 2$, a contradiction).  Therefore $k$ is either $2$ or $3$.\\

\noindent\textbf{Case 1}: $k=2$.  Subtracting both sides of equation (1) from $2$ entails
\begin{equation}\tag{2}
\frac 2{|G|}=\frac 1{|\operatorname{Stab}(\vec p_1)|}+\frac 1{|\operatorname{Stab}(\vec p_2)|}
\end{equation}
Multiplying by $|G|$ and using the Orbit-Stabilizer Theorem, $2=|\mathcal O_1|+|\mathcal O_2|$.  This implies that the orbits $\mathcal O_1$ and $\mathcal O_2$ are single-element sets, hence $\mathcal O_1=\{\vec p_1\}$ and $\mathcal O_2=\{\vec p_2\}$.  Moreover, since $\{\vec p_1\}$ is a single-element orbit, every $g\in G$ satisfies $g(\vec p_1)=\vec p_1$.  Thus $G$ is identified with a finite subgroup of $SO(2)$ via the isomorphism in Lemma 2.52(i).  According to Theorem 2.50(i), $G$ is cyclic of order $n$, generated by rotation of $2\pi/n$ around the axis parallel to $\vec p_1$.

Note in this case that $\vec p_2=-\vec p_1$.\\

\noindent\textbf{Case 2}: $k=3$.  Subtracting both sides of equation (1) from $3$ gives
\begin{equation}\tag{3}
1+\frac 2{|G|}=\frac 1{|\operatorname{Stab}(\vec p_1)|}+\frac 1{|\operatorname{Stab}(\vec p_2)|}+\frac 1{|\operatorname{Stab}(\vec p_3)|}
\end{equation}
Since the left-hand side is $>1$, we must have $|\operatorname{Stab}(\vec p_j)|=2$ for one of $j=1,2,3$ (for if every $|\operatorname{Stab}(\vec p_j)|\geqslant 3$, the right-hand side would be $\leqslant 1$, a contradiction).  By reordering and relabeling we may assume $|\operatorname{Stab}(\vec p_1)|=2$.  In this case, we subtract $1/2$ from equation (3) to get
\begin{equation}\tag{4}
\frac 12+\frac 2{|G|}=\frac 1{|\operatorname{Stab}(\vec p_2)|}+\frac 1{|\operatorname{Stab}(\vec p_3)|}
\end{equation}
Since the left-hand side is $>\frac 12$, we must have $|\operatorname{Stab}(\vec p_j)|<4$ for some $j=2,3$ (otherwise the right-hand side would be $\leqslant\frac 12$).  Without loss of generality we assume $|\operatorname{Stab}(\vec p_2)|\leqslant|\operatorname{Stab}(\vec p_3)|$.  In this case $|\operatorname{Stab}(\vec p_2)|<4$, hence is either $2$ or $3$.

If $|\operatorname{Stab}(\vec p_2)|=2$, then subtracting $1/2$ from both sides of (4) gives
\begin{equation}\tag{5}
\frac 2{|G|}=\frac 1{|\operatorname{Stab}(\vec p_3)|},
\end{equation}
and hence multiplying by $|G|$ and using the Orbit-Stabilizer Theorem gives $|\mathcal O_3|=2$.  This means that $\mathcal O_3=\{\vec p_3,\vec q\}$ for some $\vec q\in S^2$.  Since $\vec p_3\in P$, there exists $g\ne 1$ in $G$ such that $g(\vec p_3)=\vec p_3$ and (since $\mathcal O_3$ is an orbit), this implies $g(\vec q)=\vec q$.  Therefore $\vec q=-\vec p_3$ and $g$ rotates around the axis parallel to $\vec p_3$.  Hence, $\mathcal O_3=\{\vec p,-\vec p\}$.  This implies that every element of $G$ maps $\vec p\mapsto\pm\vec p$, and hence $G$ identifies with a finite subgroup of $O(2)$ via the isomorphism in Lemma 2.52(ii).  Moreover, there exists $g\in G$ such that $g(\vec p)=-\vec p$ (otherwise $\vec p,-\vec p$ would be in separate orbits), so the aforementioned subgroup of $O(2)$ contains orientation-reversing elements.  By Theorem 2.50(ii), this subgroup is generated by a rotation of angle $2\pi/n,n\in\mathbb Z$ and a reflection, and is moreover isomorphic to $D_n$; it follows that $G\cong D_n$ in this case and $G$ is dihedral.

The final case to consider (when $k=3$) is that $|\operatorname{Stab}(\vec p_2)|=3$.  With that, subtracting $1/3$ from both sides of (4) gives
\begin{equation}\tag{6}
\frac 16+\frac 2{|G|}=\frac 1{|\operatorname{Stab}(\vec p_3)|}.
\end{equation}
Since the left-hand side is $>\frac 16$, $|\operatorname{Stab}(\vec p_3)|<6$.  But also $|\operatorname{Stab}(\vec p_3)|\geqslant|\operatorname{Stab}(\vec p_2)|=3$, and hence $|\operatorname{Stab}(\vec p_3)|$ is either $3$, $4$ or $5$.  From (6) it follows that if $|\operatorname{Stab}(\vec p_3)|=3$ then $|G|=12$; if $|\operatorname{Stab}(\vec p_3)|=4$ then $|G|=24$; and if $|\operatorname{Stab}(\vec p_3)|=5$ then $|G|=60$.  We summarize the three cases in the following chart:
\begin{center}
\begin{tabular}{cc|c|c||c}
& $|\operatorname{Stab}(\vec p_1)|$ & $|\operatorname{Stab}(\vec p_2)|$ & $|\operatorname{Stab}(\vec p_3)|$ & $|G|$ \\\hline
(i) & $2$ & $3$ & $3$ & $12$ \\
(ii) & $2$ & $3$ & $4$ & $24$ \\
(iii) & $2$ & $3$ & $5$ & $60$
\end{tabular}
\end{center}
Observe in each case that $\operatorname{Stab}(\vec p_3)$ is identified with a subgroup of $SO(2)$ via the isomorphism in Lemma 2.52(i), with $\vec p=\vec p_3$.  Hence by Theorem 2.50(i), $\operatorname{Stab}(\vec p_3)$ is cyclic generated by a rotation around $\vec p_3$'s axis, of an angle $2\pi/n$, where $n=|\operatorname{Stab}(\vec p_3)|$.

We start by tackling case (i): By the Orbit-Stabilizer Theorem, $|\mathcal O_3|=12/3=4$.  Suppose $\mathcal O_3=\{\vec v_1,\vec v_2,\vec v_3,\vec v_4\}$ with $\vec v_1=\vec p_3$.  Since $|\operatorname{Stab}(\vec p_3)|=3$, the preceding paragraph implies $\operatorname{Stab}(\vec p_3)$ is cyclic generated by a rotation $R\in G$ of angle $2\pi/3$ (or $120^\circ$).  As $R^3=1$, and $R$ fixes $\vec p_3=\vec v_1$, $R$ must permute the vectors $\vec v_2,\vec v_3,\vec v_4$ in a $3$-cycle ($R$ cannot fix them all because it only fixes two points on the sphere by Lemma 2.51).  Therefore $\vec v_2,\vec v_3,\vec v_4$ form an equilateral triangle whose planar interior is perpendicular to $\vec p_3$ (why?).  Yet the same argument can be repeated with \emph{any} element of $\mathcal O_3$ in place of $\vec p_3$: that element's stabilizer will be cyclic of order $3$, hence generated by a $120^\circ$-rotation, which implies that the other three elements of $\mathcal O_3$ form an equilateral triangle.  Since any three points of $\mathcal O_3$ form an equilateral triangle, the four points form a regular tetrahedron via convex hull.  Moreover, every element of $G$ permutes the four points, and is therefore an isometry of the tetrahedron.  We thus get an injective homomorphism $G\to\operatorname{Isom}^+(\mathbf{Tet})$; this is an isomorphism because both groups have order $12$.  Hence $G\cong\operatorname{Isom}^+(\mathbf{Tet})$ in this case.

In case (ii), $|\mathcal O_3|=6$.  Moreover, $\operatorname{Stab}(\vec p_3)$ is cyclic generated by a $90^\circ$ rotation $R\in G$.  This rotation fixes $\vec p_3$ and $-\vec p_3$ and orbits every other element of the sphere in a $4$-cycle (there are no $2$-cycles in this rotation).  Thus, $R$ must fix one point $\vec v_2$ of $\mathcal O_3$ other than $\vec p_3$ and permute the remaining four elements $\vec v_3,\dots,\vec v_6$ in a $4$-cycle (because it cannot fix more than two points), from which we conclude that $\vec v_2=-\vec p_3$ and $\vec v_3,\dots,\vec v_6$ form a square whose planar interior is perpendicular to $\vec p_3$.  Thus,
$\mathcal O_3$ consists of $\vec p_3$, $-\vec p_3$ and four points forming a square perpendicular to $\vec p_3$.
However, the above argument works with \emph{any} element of $\mathcal O_3$ in place of $\vec p_3$; it particularly follows that the antipode of any point in $\mathcal O_3$ is also in $\mathcal O_3$.  This, along with the above sentence, implies that $\mathcal O_3$ forms a regular octahedron via convex hull, and $G$ consists only of isometries of this octahedron.  Since $|G|=|\operatorname{Isom}^+(\mathbf{Oct})|=24$, we conclude $G\cong\operatorname{Isom}^+(\mathbf{Oct})$.

We leave it to the reader to show that case (iii) implies $|\mathcal O_3|=12$, and that $\mathcal O_3$ forms a regular icosahedron via convex hull, leading to $G\cong\operatorname{Isom}^+(\mathbf{Icos})$.
\end{proof}

\noindent It is worth remarking that by Exercise 6 of Section 2.7, Proposition 2.53 shows that every finite subgroup of $SO(3)$ is isomorphic to either $\mathbb Z/n\mathbb Z$ for some $n$, $D_n$ for some $n$, $A_4$, $S_4$ or $A_5$.

Once finite subgroups of $SO(3)$ are classified, it is easy to classify finite subgroups of $O(3)$.  However, much casework needs to be considered; thus this classification deserves a new proposition.\\
% AUTHOR'S NOTES: Suppose G is a finite subgroup of O(3), not contained in SO(3).  Then let N = G\cap SO(3).
% N is either cyclic, dihedral, Isom+(Tet), Isom+(Oct) or Isom+(Icos) by the previous theorem.
% Let P = {(g,p)\in N\times P:g\ne 1,g\cdot p=p} as before.  Now all of G (including OR isometries) acts on P by application of an isometry.
% If N is cyclic, either G is dihedral, or G\cong Z_n\times Z_2 (if the reflection goes across the plane with the n-gon), or G\cong Z_{2n} (cyclic generated by a rotatory reflection, rotating half the angle of the shortest rotation).  In particular if N = {1}, then G is cyclic of order 2, generated by either a reflection or the central inversion.
% If N=D_1, then N is cyclic generated by a 180-degree rotation, reducing to previous case
% If N=D_2, then N is the Klein four-group.  There are 2 possibilities for G in this case, Z_2\times Z_2\times Z_2 and "strange-looking" D_4
% If N=D_n with n >= 3, then there are 2 possibilities for G in this case, depending on whether the mirror planes containing the tall axis contain the 2n equatorial points in P
% If N=Isom+(Oct), by consideration of each of the three orbits unioning to P, each orbit has the same "rotatory order" at every point and it's different for each orbit, so they must still be orbits for all of G.  Thus G must consist of isometries of the octahedron and therefore be Isom(Oct).
% If N=Isom+(Icos), G = Isom(Icos) by the same token.
% If N=Isom+(Tet), this time two orbits have the same rotatory order so the situation is different.  In fact, P consists of the vertices of a cube along with the centers of its faces centrally projected onto S^2.  Consideration of the two distinct rotatory axes shows that G is contained in Isom(Cube).  Hence there are two possibilities for G: Isom(Tet)\cong S_4, and the group generated by Isom+(Tet) and orientation-reversing isometries that send Tet to its dual image, isomorphic to A_4\times Z_2.
% *** And here's a fun little exercise: for each finite subgroup of O(3), come up with a sphere design for which it is the isometry group.

\noindent\textbf{Theorem 2.54.} \textsc{(Classification of finite subgroups of $O(3)$)}

\emph{Every finite subgroup of $O(3)$ is isomorphic to one of the following:}

(i) \emph{The cyclic group of order $n$, generated by a rotation of angle $2\pi/n$;}

(ii) \emph{The dihedral group of order $2n$, generated by a rotation of angle $2\pi/n$ around an axis and a reflection over a plane containing said axis;}

(iii) \emph{The abelian group $\mathbb Z/n\mathbb Z\times\mathbb Z/2\mathbb Z$, generated by a rotation of angle $2\pi/n$ around an axis and a reflection over the plane perpendicular to said axis;}

(iv) \emph{The cyclic group of order $2n$, generated by a rotatory reflection of angle $\pi/n$;}

(v) \emph{The dihedral group of order $2n$, generated by a rotation of angle $2\pi/n$ around an axis and a $180$-degree rotation around a perpendicular axis;}

(vi) \emph{The group $D_n\times\mathbb Z/2\mathbb Z$, generated by a rotation of angle $2\pi/n$ around an axis, reflection over the plane perpendicular to said axis, and a $180$-degree rotation around an axis perpendicular to that axis;}

(vii) \emph{The dihedral group of order $4n$, generated by a rotatory reflection of angle $\pi/n$ around an axis and a reflection over a plane containing said axis;}

(viii) \emph{$\operatorname{Isom}^+(\mathbf{Tet})$;}

(ix) \emph{$\operatorname{Isom}(\mathbf{Tet})$;}

(x) \emph{$\operatorname{Isom}^+(\mathbf{Oct})$;}

(xi) \emph{$\operatorname{Isom}(\mathbf{Oct})$;}

(xii) \emph{$\operatorname{Isom}^+(\mathbf{Icos})$;}

(xiii) \emph{$\operatorname{Isom}(\mathbf{Icos})$;}

(xiv) \emph{The group $A_4\times\mathbb Z/2\mathbb Z$, generated by $\operatorname{Isom}^+(\mathbf{Tet})$ and the central inversion $-I_3$.}\\

\noindent We remark that of the five Platonic solids, the tetrahedron is the only one for which $-I_3$ is not an isometry.  This is why (xiv) is separate from (viii)-(ix), and no other Platonic solid has this separate case.

\begin{proof}
Let $G$ be a finite subgroup of $O(3)$.  If $G\subset SO(3)$, then $G$ is isomorphic to either (i), (v), (viii), (x) or (xii) by Theorem 2.53.  Thus, we assume $G\not\subset SO(3)$.  With that, $N=G\cap SO(3)$ is a normal subgroup of $G$ of index $2$ (because $\det:G\to\{-1,1\}$ is a surjective homomorphism with kernel $N$).  Since $N$ is a finite subgroup of $SO(3)$, we have, by Theorem 2.53, that $N$ is either cyclic, dihedral, $\operatorname{Isom}^+(\mathbf{Tet})$, $\operatorname{Isom}^+(\mathbf{Oct})$ or $\operatorname{Isom}^+(\mathbf{Icos})$.

Also, if $P=\{\vec p\in S^2:g(\vec p)=\vec p\text{ for some }g\ne 1\text{ in }N\}$ as in Theorem 2.53, then all of $G$ (not just $N$) acts on $P$ by application of an isometry.  The reader can readily verify this by imitating the corresponding part of the proof Theorem 2.53.

We shall use casework on $N$.\\

\noindent\textbf{Case 1}: \emph{$N$ is cyclic of order $n$, generated by a rotation of angle $2\pi/n$.}

If $n=1$, then $N$ is trivial, and hence (since $[G:N]=2$), $G$ is cyclic of order $2$, generated by an orientation-reversing isometry $a$.  Since $|a|=2$, $a$ must be either a reflection or the central inversion $-I_3$ (since a rotatory reflection by $\theta$ squares to a rotation by $2\theta$, and a rotatory reflection by $180^\circ$ is the central inversion).  If $a$ is a reflection then $G$ is isomorphic to (iii) (with $n=1$), and if $a$ is the central inversion then $G$ is isomorphic to (iv).  We henceforth assume $n>1$.

In this case, $P$ consists of two antipodal points $\vec p,-\vec p$ (the endpoints of the axis of the generating rotation in $N$).  Let $a\in G-N$.  By the remarks preceding the casework, the isometry $a$ must permute $P$.  If $a$ fixes both $\vec p,-\vec p$, $a$ must be a reflection across a plane containing $\vec p$'s axis (rotatory reflections only fix the center of the sphere).  By composing $a$ with various elements of $N$, it is clear that every element of $G-N$ is a reflection across a plane containing $\vec p$'s axis, and $G$ is isomorphic to (ii).

On the other hand, what if $a$ swaps $\vec p,-\vec p$ (i.e., $a(\vec p)=-\vec p$)?  In this case, (since $a$ is orthogonal), $a$ must fix the plane $\Pi$ through the origin perpendicular to $\vec p$, though not necessarily pointwise.  In this case, the reader can see that $a$ is either a reflection or a rotatory reflection across $\Pi$. Let $R$ be the rotation of angle $2\pi/n$ that generates $N$; then $a^2\in N$ and hence $a^2=R^k$ for some $0\leqslant k<n$.  In this case, $a$ is a reflection / rotatory reflection that rotates at either an angle of $k\pi/n$ or $\pi+k\pi/n$; either way the angle of rotation for $a$ is of the form $c\pi/n,c\in\mathbb Z$.  If $c=2\ell$ is even, then $aR^{-\ell}$ is a reflection across $\Pi$, and $G$ is isomorphic to (iii).  If $c=2m+1$ is odd, then $aR^{-m}$ is a rotatory reflection which rotates at an angle of $\pi/n$, and we are in the situation of (iv).\\

\noindent\textbf{Case 2}: \emph{$N$ is dihedral of order $2n$, generated by a rotation of angle $2\pi/n$ around an axis and a $180$-degree rotation around a perpendicular axis.}

If $n=1$, then $N$ is cyclic generated by a $180^\circ$ rotation, thus this case reduces to Case 1 ($N$ cyclic of order $2$).

If $n=2$, then $N$ is generated by two perpendicular $180$-degree rotations, hence is conjugate to the subgroup consisting of:
$$\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix},~~~~\begin{bmatrix}1&0&0\\0&-1&0\\0&0&-1\end{bmatrix},~~~~\begin{bmatrix}-1&0&0\\0&1&0\\0&0&-1\end{bmatrix},~~~~\begin{bmatrix}-1&0&0\\0&-1&0\\0&0&1\end{bmatrix}$$
(why?); without loss of generality, we may assume $N$ is that subgroup.  This means that $P$ consists of the six points $\{\pm\vec e_1,\pm\vec e_2,\pm\vec e_3\}$.  Since earlier we noted that $G$ acts on $P$ by application of an isometry, every element of $G$ must therefore be a monomial matrix with $\pm 1$'s as the nonzero entries.  Now consider the four elements of $G-N$: taking one of them and multiplying it by the elements of $N$ gives the rest of them.  Since every element of $N$ is a diagonal matrix (i.e., a monomial matrix for the identity permutation), we conclude that every element of $G-N$ must be a monomial matrix \emph{for the same permutation}.  This permutation must square to the identity (since elements of $G-N$ square to elements of $N$), hence either be the identity or a transposition.  In either case, there are exactly four ways to assign $\pm 1$ to the nonzero entries that make the matrix orientation-reversing, as required of $G-N$; and these four matrices must be the elements of $G-N$.

If $G-N$ consists of monomial matrices for the identity permutation, then $G$ consists of all diagonal matrices with $\pm 1$'s as the nonzero entries.  With that it is clear that $G\cong\mathbb Z/2\mathbb Z\times\mathbb Z/2\mathbb Z\times\mathbb Z/2\mathbb Z$.  This satisfies (vi) with $n=2$, because $D_2\cong\mathbb Z/2\mathbb Z\times\mathbb Z/2\mathbb Z$.

If $G-N$ consists of monomial matrices for a transposition, then we may assume (by conjugating by $G$ by a permutation matrix), that the transposition swaps the first two components.  In this case the matrices are
$$\begin{bmatrix}0&1&0\\1&0&0\\0&0&1\end{bmatrix},~~~~\begin{bmatrix}0&1&0\\-1&0&0\\0&0&-1\end{bmatrix},~~~~\begin{bmatrix}0&-1&0\\1&0&0\\0&0&-1\end{bmatrix},~~~~\begin{bmatrix}0&-1&0\\-1&0&0\\0&0&1\end{bmatrix}$$
If $r=\begin{bmatrix}0&1&0\\-1&0&0\\0&0&-1\end{bmatrix}$ and $d=\begin{bmatrix}1&0&0\\0&-1&0\\0&0&-1\end{bmatrix}$, the reader can verify that $r,d\in G$, $|r|=4$, $|d|=2$ and $dr=r^{-1}d$.  From this it follows that $G\cong D_4$ and we have (vii) with $n=2$.

This concludes the case $n=2$; now we assume $n\geqslant 3$.  With that, $P$ consists of two antipodes $\pm\vec p$ (the endpoints of the rotation by $2\pi/n$), and $2n$ equally spaced points on the plane $\Pi$ through the origin perpendicular to $\vec p$.  If we define the ``rotatory order'' of a point in $P$ to be the number of rotations around that point that will map $P$ to itself, it is clear that elements of $G$ preserve points in $P$ of a particular rotatory order, and that $\pm\vec p$ have rotatory order $2n$ and the other $2n$ points have rotatory order $2$.  Therefore, an element $a\in G-N$ must permute $\pm\vec p$ again.

If $a(\vec p)=\vec p$, then $a$ is a reflection across a plane containing $\vec p$.  Since this reflection preserves the $2n$ points in $\Pi$, the mirror plane must either go through two of these points, or through the midpoints of edges of the regular $2n$-gon they span (one or the other, not both, because $2n$ is even).  If the mirror plane goes through equatorial points $\pm\vec q\in P$, composing the reflection with the $180$-degree rotation around $\pm\vec q$ results in a reflection across $\Pi$, thus $G$ contains a reflection across $\Pi$ and is isomorphic to (vi).  If the mirror plane instead goes through the midpoints of edges of the regular $2n$-gon, let $\pm\vec q$ be antipodal vertices of the $2n$-gon that meet the edges the mirror plane goes through.  The reflection is then the reflection whose mirror plane goes through $\pm\vec q$, composed with a $\pi/n$-angle rotation.  Consequently, when the reflection is composed with the $180$-degree rotation around $\pm\vec q$, we have in our hands a rotatory reflection across $\Pi$ with an angle of $\pi/n$.  This lands us in (vii) as one can readily verify.

If $a(\vec p)=-\vec p$, then (unlike Case 1), we may reassign $a$ to $ah$, where $h$ is a $180$-degree rotation in $N$ which swaps $\pm\vec p$, thus boiling this case to the one settled in the previous paragraph.\\

\noindent\textbf{Case 3}: \emph{$N\cong\operatorname{Isom}^+(\mathbf{Oct})$.}

In this case, $P$ consists of $6$ points $(\pm\vec e_1,\pm\vec e_2,\pm\vec e_3)$ with a rotatory order of $4$, $12$ points with a rotatory order of $2$, and $8$ points with a rotatory order of $3$.  (The reader should take the time to verify this.)  Hence, since elements of $G$ preserve points in $P$ of a particular rotatory order, every element of $G$ must permute $\pm\vec e_1,\pm\vec e_2,\pm\vec e_3$, and therefore $G\subset\operatorname{Isom}(\mathbf{Oct})$.  Yet $G$ has order $2|N|=48$, making the aforementioned inclusion an equality.  Therefore $G=\operatorname{Isom}(\mathbf{Oct})$ and we have (xi).\\

\noindent\textbf{Case 4}: \emph{$N\cong\operatorname{Isom}^+(\mathbf{Icos})$.}

In this case, $P$ consists of $12$ points with a rotatory order of $5$; $30$ points with a rotatory order of $2$; and $20$ points with a rotatory order of $3$.  As in the previous case, $G\subset\operatorname{Isom}(\mathbf{Icos})$, and therefore $G=\operatorname{Isom}(\mathbf{Icos})$ because their orders are equal; this is (xiii).\\

\noindent\textbf{Case 5}: \emph{$N\cong\operatorname{Isom}^+(\mathbf{Tet})$.}

In this case, we \emph{cannot} simply adapt the previous two cases to conclude $G=\operatorname{Isom}(\mathbf{Tet})$.  After all, $P$ consists of \emph{eight} points with a rotatory order of $3$ (the vertices and the centers of the faces of the tetrahedron), and $6$ points with a rotatory order of $2$.  Thus an element of $G$ need not permute the four vertices of the tetrahedron.

Now $N$ consists of all the monomial matrices for even permutations, with $\pm 1$'s as nonzero entries, and with determinant $1$.  Every $180^\circ$ rotation in $\operatorname{Isom}^+(\mathbf{Tet})$ must be a diagonal matrix (being a monomial matrix for an even permutation which squares to the identity); it follows that the six points of $P$ with a rotatory order of $2$ are $\pm\vec e_1,\pm\vec e_2,\pm\vec e_3$.  Hence, every element of $G$, permuting those six points, is a monomial matrix with $\pm 1$'s as nonzero entries.  The elements of $G-N$ have determinant $-1$ by definition; they could be either even or odd permutations, but since $N$ has monomial matrices for even permutations, we get that \emph{$G-N$ has monomial matrices for permutations of the same sign}, i.e., all even or all odd.

If $G-N$ consists of monomial matrices for odd permutations, there are exactly $12$ of them with determinant $-1$, and moreover they must be the elements of $G-N$.  In this case $G$ consists of all monomial matrices with $\pm 1$'s as its nonzero entries, such that the nonzero entries multiply to $1$ [as the permutation is odd if and only if the determinant is $-1$].  Hence, $G=\operatorname{Isom}(\mathbf{Tet})$ [look back at Example (4) in Section 2.7] which is (ix).

But what if $G-N$ consists of monomial matrices for even permutations?  Then again, exactly $12$ have determinant $-1$, and those twelve must be the elements of $G-N$.  With that, $G$ consists of all monomial matrices for even permutations with $\pm 1$'s as its nonzero entries.  In particular $-I_3\in G$, and it is clear that $N=\operatorname{Isom}^+(\mathbf{Tet})$ and $\{I_3,-I_3\}$ are trivially-intersecting subgroups which commute with each other elementwise; hence $G$ is the internal direct product of $\operatorname{Isom}^+(\mathbf{Tet})$ and $\{I_3,-I_3\}$, therefore $G\cong A_4\times\mathbb Z/2\mathbb Z$.  This is (xiv).
\end{proof}

Some of the subgroups listed above have clever names; e.g., (vi) is called \textbf{prismatic symmetry} and (vii) is called \textbf{antiprismatic symmetry}.  The orientation-preserving isometry groups (viii), (x), (xii) are called \textbf{chiral symmetry groups}\footnote{``Chirality'' refers to the lack of orientation-reversing symmetry.  Though the Platonic solids have orientation-reversing symmetry, the notion of chirality counts that symmetry out.}, and (xiv) is called \textbf{pyritohedral symmetry}.  Note that (i)-(vii) are the only ones which fix an axis entirely; they are called the \textbf{axial groups}.  These families also fix an equatorial strip bounded by planes perpendicular to the axis (such as $\{(x,y,z)\in S^2:|z|<\frac 12\}$ if the axis is the $z$-axis); they correspond to the Frieze groups [Exercise 8 of Section 2.6], and are ``circular'' versions of them.

A fun little exercise is to take any of the finite subgroups above, and come up with a design on a sphere for which the subgroup is its isometry group.  For example, (vi) is the symmetry group of the design of $n$ equally spaced circles centered on the equator.  (iii) is the symmetry group of the design of $n$ equally spaced arrows centered on the equator, pointing clockwise from the North Pole's point of view \---- note the inability to reflect vertically.  [In addition, if certain small values of $n$ are excluded, (vi) is the isometry group of the regular $n$-gonal prism, and (vii) is the isometry group of the regular $n$-gonal antiprism.] % No, I meant the first picture.  The second one doesn't have the reflection over the xy-plane as an isometry

This theorem leads to a classification of Archimedean (and Catalan) solids, which will be carried out in Chapter 5.

\subsection*{Exercises 2.8. (Classification of Finite Subgroups of $SO(3)$ and $O(3)$)}
\begin{enumerate}
\item Give an example of a design on the sphere for which the isometry group is the group (xiv) stated in Theorem 2.54.

\item Look back at Exercise 9(l) of Section 2.7.  Give a reasonable definition for a ``basic unit'' of a finite subgroup of $O(3)$; then show that every finite subgroup of $O(3)$ has one.

\item The aim of this exercise is to classify finite subgroups of $SO(4)$.  Recall from Exercise 7 of Section 2.6 that $SO(4)$ has two normal subgroups, $S^3_L$ and $S^3_R$.

(a) Show that $S^3_L\cong S^3_R\cong SU(2)$.  [$SU(n)$ is defined in Exercise 3 of Section 2.6.  Show that a typical element of $SU(2)$ is of the form $\begin{bmatrix}a+bi&-c-di\\c-di&a-bi\end{bmatrix}$, with $a,b,c,d\in\mathbb R$, $a^2+b^2+c^2+d^2=1$.]

(b) Every element of $SO(4)$ is a product of a left isoclinic rotation and a right isoclinic rotation.  [Think of the element as a double-rotation, which rotates two orthogonally complementary planes by angles $\alpha,\beta$ (which are allowed to be zero).  Consider isoclinic rotations via $\frac{\alpha+\beta}2$ and $\frac{\alpha-\beta}2$.]

(c) Now show that there is a surjective homomorphism $\varphi:SU(2)\times SU(2)\to SO(4)$, whose kernel is cyclic generated by $(-I_2,-I_2)$.  [There is a function $S^3_L\times S^3_R\to SO(4)$ sending $(a,b)\mapsto ab$.  This is surjective by part (b).  Use Exercise 7(k) of Section 2.6 to show that it is a homomorphism.  Then use Exercise 7(j) of Section 2.6 to find its kernel.  Finally, use part (a) of this exercise.]

By the First Isomorphism Theorem (1.17), it follows that $SO(4)\cong\frac{SU(2)\times SU(2)}{\left<(-I_2,-I_2)\right>}$.  Hence, by Exercise 4(d) of Section 1.5, there is a one-to-one correspondence between subgroups of $SO(4)$ and subgroups of $SU(2)\times SU(2)$ containing $(-I_2,-I_2)$.  Clearly this correspondence matches finite subgroups with one another.  Thus the problem boils down to finding finite subgroups of $SU(2)\times SU(2)$ containing $(-I_2,-I_2)$.

(d) Let $S$ be the subset of $SU(2)$ consisting of matrices with trace zero.  [By part (a), $S=\left\{\begin{bmatrix}bi&-c-di\\c-di&-bi\end{bmatrix}:b,c,d\in\mathbb R,b^2+c^2+d^2=1\right\}$.]  Though $S$ is not a subgroup, it is closed under conjugation, due to the linear algebra fact that similar matrices have the same trace.  Thus for each $a\in SU(2)$, we have the permutation $g\mapsto aga^{-1}$ of $S$.  Use this to establish a surjective group homomorphism $SU(2)\to SO(3)$ whose kernel is $\left<-I_2\right>$.  [Observe that $S$ is identified with a sphere in $\mathbb R^3$.]

(e) Use Theorem 2.53 and part (d) to classify all finite subgroups of $SU(2)$.

(f) Now classify all finite subgroups of $SU(2)\times SU(2)$.  [Let $\pi_1:(a,b)\mapsto a$ and $\pi_2:(a,b)\mapsto b$ be the projection maps from $SU(2)\times SU(2)\to SU(2)$.  Then if $G$ is a finite subgroup of $SU(2)\times SU(2)$, $\pi_1(G),\pi_2(G)$ are finite subgroups of $SU(2)$.  Use part (e) and casework on $\pi_1(G),\pi_2(G)$.]  By listing only those which contain $(-I_2,-I_2)$, the finite subgroups of $SO(4)$ will be in your hands.
\end{enumerate}







































\chapter{Projective Geometry}

Projective geometry studies structures of things as they are directly viewed by the eye.  It takes place in a space with more ``outsider'' points than the Euclidean space, and has several remarkable properties that are not possessed in Euclidean geometry.  We shall introduce this space in the first two sections of this chapter.  In projective geometry, we lose the Euclidean notion of distance between two points, and the notion of angle.  In Section 3.3, we shall learn about a property of points, analogous to the distance, that is invariant in the projective viewpoint.  Later, Section 3.7 will prove some famous results.  This material will reappear in Section 4.8.

\subsection*{3.1. Perspective Projections}
\addcontentsline{toc}{section}{3.1. Perspective Projections}
Imagine a standard sheet of paper.  Rectangular, dimensions 8.5 x 11 inches, a useful thing to write on.  But what if the sheet of paper were flat on the tabletop and you were peering at it with your eyes directly facing the wall?  Would it look like a rectangle?  Of course not.  In fact, the top edge of the sheet of paper would appear to be shorter than the bottom edge.  Exactly how is this explained?  Why does the top edge \emph{look} shorter when it is really the same length?

The answer is, of course, because it is farther away from your eyes.  Each eye, when thought of as a point, registers an object when the light reflected from the object travels in a straight line towards the eye.  Thus the size of an object (or part of an object) as perceived by the eye depends on the structure of the cone taken from the eye to the shape of the object.  With the eye thought of as a point, this cone is thinner and has a smaller tip angle when an object is farther away, explaining its small appearance.  [Technically, you have two eyes, and they see things through different cones at different angles, but our brains combine the two images.]

The general concept of an object looking different when you look at it from a different direction is called \emph{perspective}.  Thus when you look at the paper in the sense explained in the first paragraph, you are seeing it from a different perspective than you normally would when writing on it.  There are other examples of the perspective concept.  Suppose you take a carrot and hold it vertically, and then someone flies above you, saying ``nice-looking circle-shaped green vegetable!''  The carrot is, obviously not such a vegetable, but that is what it \emph{looks} like from the point of view directly above.  The flier looked \emph{down} at the carrot, hence saw the green stem, without seeing the orange part which was blocked from their sight.  There's also the example where you're standing next to a sign with the number 68 on it, and someone walks by you upside-down on their hands, and later tells their friend, ``He's at the 89 sign.''

With this idea in mind, we proceed to define a perspective projection.  We will then seek mathematical formulas which describe it, to get a starting glimpse of projective geometry.\\

\noindent\textbf{Definition.} \emph{Let $P_1$ and $P_2$ be hyperplanes in $\mathbb R^n$, and let $p\in\mathbb R^n$ be a point which is not in either hyperplane.  The \textbf{perspective projection} from $p$ sends a point $x\in P_1$, to the intersection of line $\overset{\longleftrightarrow}{p~x}$ with $P_2$.}\\

\noindent For instance, in $\mathbb R^3$, if $P_1$ is the $xy$-plane and $P_2$ is the $xz$-plane, then a perspective projection takes every point in the $xy$-plane, connects it straight to the eye point, and sees where all the connections meet the $xz$-plane, as shown below.  Thus the $xz$-plane captures what the eye directly sees from its position.
\begin{center}\includegraphics[scale=.5]{PerspProjection.png}\end{center}
Of course, the perspective projection is not generally a well-defined function $P_1\to P_2$.  Indeed, it is not (yet) defined at any $x\in P_1$ with the property $\overset{\longleftrightarrow}{p~x}\parallel P_2$.  Also, there are points in $P_2$ which are not (yet) hit by any point in $P_1$; if $y\in P_2$ and $\overset{\longleftrightarrow}{p~y}\parallel P_1$, then no point in $P_1$ maps to $y$ (why?).

It really shouldn't be a surprise that a perspective projection is not defined everywhere.  Indeed, if you take a picture of an outdoor scene, you see the grass cut off right below the sky, even though it doesn't technically stop at an edge like that.  The fact of the matter is, when you are looking directly at the edge, your eyes make an angle with the grass that avoids ever hitting it.  The grass continues, but out of your direct vision.  Our instinct tells us that we should add ``infinitely far'' points to the plane at which parallel things can meet, with the aim of making a perspective projection bijective.  But how do we approach this?

To get a starting idea, we shall return to our example where $P_1$ is the $xy$-plane and $P_2$ is the $xz$-plane.  Let $p$ be any point which is not on either plane; e.g., $(0,2,3)$.  Now let $(x,y,0)\in P_1$.  Suppose that the perspective projection from $p$ maps $(x,y,0)\mapsto(x',0,z')\in P_2$.  Then the following three points are collinear:
$$(0,2,3),~~~~(x,y,0),~~~~(x',0,z')$$
There are many ways to interpret this fact; one way is to observe that if $\vec a,\vec b,\vec c\in\mathbb R^3$ are vectors, then $\vec a,\vec b,\vec c$ are collinear $\iff\vec a-\vec b$ and $\vec a-\vec c$ lie in the same line $\iff(\vec a-\vec b)\times(\vec a-\vec c)=\vec 0$, where $\times$ denotes the cross product [see Exercise 4 of Section 2.5; that last equivalence of statements follows from part (h)].  In particular, if $\vec a=(0,2,3)$, $\vec b=(x,y,0)$ and $\vec c=(x',0,z')$, then
$$\vec 0=(\vec a-\vec b)\times(\vec a-\vec c)=(-x,2-y,3)\times(-x',2,3-z')$$
$$=((2-y)(3-z')-6,-3x'+x(3-z'),-2x+x'(2-y))$$
As the above vector is zero, we can compute $x'$ and $z'$ from $x$ and $y$.  First, $-2x+x'(2-y)=0$, so that $x'(2-y)=2x$ and $x'=\frac{2x}{2-y}$.  [We assume $2-y\ne 0$, since $y=2$ would imply that the line $\overset{\longleftrightarrow}{p~x}$ is parallel to the second plane.]  Also, $(2-y)(3-z')=6$, which means that $z'=3-\frac 6{2-y}=\frac{-3y}{2-y}$.  Hence, through the perspective projection
$$(x,y,0)\mapsto\left(\frac{2x}{2-y},0,\frac{-3y}{2-y}\right)$$
Thinking about it as just a function from $\mathbb R^2$ to itself, it is given by
\begin{equation}\tag{*}(x,y)\mapsto(x',y')=\left(\frac{2x}{2-y},\frac{-3y}{2-y}\right)\end{equation}
It is only defined (in Euclidean space) when $y\ne 2$, and the range only consists of points $(x',y')$ with $y'\ne 3$.  And as we expected, it is not a linear or affine transformation; there are denominators with variables.  However, observe that both expressions have the \emph{same} denominator, namely $2-y$.  This is no coincidence; formulating a perspective projection will always give you fractions with matching denominators.  In fact, there is a clever way we can \emph{see} that map as a linear map.

Think of $\mathbb R^3$ as the union of the lines through the origin.  Let $q=(x,y,z)$ be a point in $\mathbb R^3$; assume $q$ is not the origin (because the origin is on all of the lines).  Now think of all points of the form $(\lambda x,\lambda y,\lambda z),\lambda\ne 0$, and declare them to be ``essentially similar'' to $q$.  It is clear that this is an equivalence relation on nonzero vectors; moreover, every point outside the $xy$-plane is essentially similar to a unique point of the form $(x,y,1)$ (why?), thus embedding $\mathbb R^2$ in this set of equivalence classes.  There are also equivalence classes in the $xy$-plane, which are not part of the embedding.

If we apply (*) to the point $(x,y,1)$, while thinking about the embedding of $\mathbb R^2$, we get $(2x/(2-y),-3y/(2-y),1)$.  Yet this output is essentially similar to $(2x,-3y,2-y)$ \---- this is our trick.  Moreover, we have a linear map on $\mathbb R^3$ sending $(x,y,1)\mapsto(2x,-3y,2-y)$; namely, take $(x,y,z)\mapsto(2x,-3y,2z-y)$.  It is given by the matrix $\begin{bmatrix}2&0&0\\0&-3&0\\0&-1&2\end{bmatrix}$.  Thus, although our perspective projection is not linear, we have viewed it as a linear map by switching over to a set of equivalence classes in $3$-space.  The equivalence classes in the $xy$-plane \---- called ``points at infinity'' \---- are output when $y=2$, and when they are input they return a result with $z'=3$. % My bad, forgot the notation from earlier...

We will eventually see that this kind of thing can be done for all perspective projections.  We now introduce the notion of a projective plane.\\

\noindent\textbf{Definition.} \emph{On $\mathbb R^3-\{\vec 0\}$, let $\sim$ be the equivalence relation on two vectors $(x,y,z),(x',y',z')$ stating that there exists $\lambda\ne 0$ in $\mathbb R$ such that $x'=\lambda x,y'=\lambda y,z'=\lambda z$.  The equivalence class for $(x,y,z)$ is denoted $[x:y:z]$.  The set of equivalence classes is denoted $P^2(\mathbb R)$ and is called the \textbf{projective plane}.  We assume $\mathbb R^2$ to be identified with its image in the embedding $(x,y)\mapsto[x:y:1]$; points outside this image (i.e., points of the form $[x:y:0]$) are called \textbf{points at infinity}.}\\

\noindent Note that this $\mathbb R^2$ is an affine plane, not naturally a vector space (why?).

The projective plane is just like the Euclidean plane, but with extra points added that are infinitely far away.  These extra points form a line.  Moreover, any perspective projection is a well-defined bijection of this plane, even though it is not bijective for the Euclidean plane; e.g., the one above given by $(x,y)\mapsto\left(\frac{2x}{2-y},\frac{-3y}{2-y}\right)$ can be rewritten as $[x:y:z]\mapsto[2x:-3y:2z-y]$; it is easy to see that this is well-defined and bijective.

We now introduce the concept of lines in the projective plane.\\

\noindent\textbf{Definition.} \emph{A \textbf{line} in $P^2(\mathbb R)$ is defined to be the image under the quotient map\footnote{By this, of course, we mean the map $(x,y,z)\mapsto[x:y:z]$.} $\mathbb R^3-\{\vec 0\}\to P^2(\mathbb R)$ of a plane in $\mathbb R^3$ through the origin.}\\

\noindent Observe that if $P$ is a plane in $\mathbb R^3$ \emph{through the origin}, then $(x,y,z)\in P\implies(\lambda x,\lambda y,\lambda z)\in P$ (why?), which means that $P$ is a union of equivalence classes given in the definition of $P^2(\mathbb R)$ (along with the origin).  Moreover, the line is the set of these equivalence classes.  We easily conclude that $P$ can be recovered from the line as a subset of $P^2(\mathbb R)$, simply by taking the union of the equivalence classes.

Corresponding to the $xy$-plane is the set of all points at infinity, which we call the \textbf{line at infinity}.  Every other line is obtained by taking a Euclidean line in $\mathbb R^2$ and adjoining one infinity point.  After all, suppose the line $\ell\subset P^2(\mathbb R)$ is given by a plane through the origin $Ax+By+Cz=0$ which is not the $xy$-plane.  Then $A$ and $B$ are not both zero.  Moreover, $[B:-A:0]$ is the unique point at infinity on $\ell$ (recall that $Ax+By=0\implies B/(-A)=x/y\implies[B:-A:0]=[x:y:0]$).  A Euclidean-plane point $[x:y:1]$ is in $\ell$ if and only if $Ax+By=-C$; this is the equation of a line in the Euclidean plane.

Conversely, every Euclidean line extends to a unique projective line, by addition of a single infinity point.  Let $Ax+By=C$ be a Euclidean line ($A,B$ not both zero).  Then $Ax+By-Cz=0$ is a plane through the origin which defines a projective line.  The only infinity point on this line is $[B:-A:0]$.  This plane is unique (for every such plane must contain all points $(x,y,z)$ such that $z\ne 0$ and $Ax+By=Cz$: check!).

The interesting thing about points and lines in the projective plane is this:\\

\noindent\textbf{Proposition 3.1.} \emph{In the projective plane $P^2(\mathbb R)$:}

(i) \emph{Any two points determine a line;}

(ii) \emph{Any two lines intersect in a unique point.}
\begin{proof}
By identifying points in $P^2(\mathbb R)$ with lines through the origin in $\mathbb R^3$ as defined above, and identifying lines in $P^2(\mathbb R)$ with planes through the origin in $\mathbb R^3$, we translate both these statements to basic facts in linear algebra.  After all, lines (resp., planes) through the origin are $1$-dimensional (resp., $2$-dimensional) subspaces of the vector space $\mathbb R^3$.

(i) Given two distinct lines $\ell_1,\ell_2$ through the origin in $\mathbb R^3$, suppose $\ell_1$ is spanned by the nonzero vector $\vec v$ and $\ell_2$ by the nonzero vector $\vec w$.  Since the lines are distinct, neither $\vec v$ nor $\vec w$ is a multiple of the other.  Hence they are linearly independent; their span is a plane through the origin, which is the unique plane through the origin containing both lines.

(ii) If $P_1$ and $P_2$ are distinct planes through the origin, then by Proposition 2.43(i), $P_1\cap P_2$ is either the empty set or a line.  But $P_1\cap P_2$ contains the origin, hence is not empty.  Therefore $P_1\cap P_2$ is a line, and this line certainly contains the origin.  This is the unique line contained in both planes, hence corresponds to the unique point given by the statement.
\end{proof}

\noindent\emph{Remarks}: (ii) is not true in general for the Euclidean plane, in which there are parallel lines, and such lines do not intersect at all.  However, in $P^2(\mathbb R)$, lines that appear parallel actually intersect at a point at infinity.  For example, the lines extending the Euclidean (vertical) lines given by $x=0$ and $x=1$ intersect at the point $[0:1:0]$: after all, $[x:y:z]$ is in the first line if and only if $x=0$, and $[x:y:z]$ is in the second line if and only if $x=z$.  Thus, the first line is $\{[0:y:z]:y,z\in\mathbb R\}$ and the second line is $\{[x,y,x]:x\in\mathbb R\}$, and so their intersection is $\{[0,x,0]:x\in\mathbb R\}$< which consists of $[0:1:0]$ alone.  Contrariwise, lines that intersect in the Euclidean plane do not intersect at infinity, as they meet the line at infinity at different points.

In part (i), if the two points are both at infinity, the line determined by them is the line at infinity.

The special case of (ii) where one of the lines is the line at infinity, shows that every line other than the line at infinity has exactly one infinity point.\\

\noindent In the projective plane, distances and angles do not make any sense (among other reasons, what is the distance from the origin to an infinity point?).  Section 3.3 will cover an invariant that does make sense, except that it uses four points as input, rather than just two.

The most commonly studied concept in the projective plane involves collinearity of points and concurrence of lines.  We define three or more points to be \textbf{collinear} if there exists a line passing through all of them.  We define three or more lines to be \textbf{concurrent} if they all intersect in a common point.

Higher-dimensional projective spaces can be defined similarly, and the perspective projections generalize to the concept of a projective transformation; this will be covered in the next section.

\subsection*{Exercises 3.1. (Perspective Projections)} % Explain the concept of a perspective projection, then introduce the projective plane.
% Show basic things about points and lines: any two points determine a line, and any two lines intersect in a point.
\begin{enumerate}
\item In $\mathbb R^3$, let $P_1$ be the $xy$-plane and $P_2$ the $xz$-plane.

(a) Find a formula for the perspective projection $P_1\to P_2$ from the point $p=(5,-7,2)$.

(b) Write it as a well-defined bijection from $P^2(\mathbb R)$ to itself, as done above.

\item (a) A perspective projection preserves lines and conic sections.  [See Exercise 9 of Section 2.2 and Exercise 9 of Section 2.6.]

(b) Show by example that a perspective projection need not send a circle to a circle.

\item Establish a natural bijection between $P^2(\mathbb R)$ and the set of unordered pairs of antipodal points in the sphere $S^2$.  We will revisit this in Chapter 5.

\item In $P^2(\mathbb R)$, find an equation for:

(a) The line determined by $(2,3)$ and $(4,7)$;

(b) The line determined by $(1,-1)$ and the infinity point $[3:5:0]$.

\item In each of the following cases, you are given two lines in the Euclidean plane $\mathbb R^2$.  Extend them to lines in the projective plane, then find the intersection point of the lines.

(a) $2x+3y=8$, $5x-y=3$

(b) $x+y=7$, $x=0$

(c) $3x+4y=11$, $3x+4y=15$

\item Let $\ell_1$ and $\ell_2$ be lines in $P^2(\mathbb R)$, and let $p$ be a point that is not on either line.  Then perspective projection from $p$ is a well-defined bijection $\ell_1\to\ell_2$.  [$p$ could be an infinity point, or either line could be the line at infinity.]
\end{enumerate}

\subsection*{3.2. Projective $n$-Space.  Projective Transformations}
\addcontentsline{toc}{section}{3.2. Projective $n$-Space.  Projective Transformations}
The definition of projective $n$-space $P^n(\mathbb R)$ mirrors that of $P^2(\mathbb R)$, starting from the set of lines through the origin in $\mathbb R^{n+1}$.  This space shares many properties with Euclidean $n$-space, as we will later see.  It is formally defined as follows:\\

\noindent\textbf{Definition.} \emph{On $\mathbb R^{n+1}-\{\vec 0\}$, let $\sim$ be the equivalence relation defined by $\vec v\sim\vec w\iff\vec v=\lambda\vec w$ for some $\lambda\ne 0$ in $\mathbb R$.  The equivalence class for $(x_1,\dots,x_{n+1})$ is denoted $[x_1:\dots:x_{n+1}]$.  The set of equivalence classes is denoted $P^n(\mathbb R)$ and is called \textbf{projective $n$-space}.}

\emph{We will identify $\mathbb R^n$ with its image in the embedding $(x_1,\dots,x_n)\mapsto[x_1:\dots:x_n:1]$.  Points outside this image (i.e., points of the form $[x_1:\dots:x_n:0]$) are called \textbf{points at infinity}.}

\emph{If $0\leqslant k\leqslant n$, then a \textbf{(projective) $k$-plane} in $P^n(\mathbb R)$ is defined to be the image in $P^n(\mathbb R)$ of some $(k+1)$-plane in $\mathbb R^{n+1}$ through the origin (i.e., a $(k+1)$-dimensional subspace of the vector space $\mathbb R^{n+1}$).  $0$-planes are called \textbf{points}, $1$-planes are called \textbf{lines}, $2$-planes are called \textbf{planes}, and $(n-1)$-planes in $P^n(\mathbb R)$ are called \textbf{hyperplanes}.}\\

\noindent With this definition, the reader can readily make the following observations (using linear algebra):
\begin{itemize}
\item If a $(k+1)$-dimensional subspace $V$ of $\mathbb R^{n+1}$ yields a $k$-plane $\Pi$ in projective $n$-space, then $V$ can be recovered from $\Pi$, because it is the set of points in $\mathbb R^{n+1}$ with images in $\Pi$, along with $\vec 0$.  There is a bijective correspondence between $(k+1)$-dimensional subspaces of $\mathbb R^{n+1}$ and $k$-planes in $P^n(\mathbb R)$.

\item Every $k$-plane in $P^n(\mathbb R)$ comes in a bijection with $P^k(\mathbb R)$ given by a projective transformation.

\item The set of all points at infinity is a hyperplane (which we henceforth call the \textbf{hyperplane at infinity}).  It corresponds to the linear subspace of $\mathbb R^{n+1}$ given by $x_{n+1}=0$; and this subspace has dimension $n$.  In particular, $P^1(\mathbb R)$ has a unique point at infinity (represented by $[1:0]$), and thus we have $P^1(\mathbb R)=\mathbb R\sqcup\{\infty\}$.

\item A $k$-plane ($k\geqslant 1$) in $P^n(\mathbb R)$ is either contained in the hyperplane at infinity, or it is the union of a Euclidean $k$-plane in $\mathbb R^n$ with a $(k-1)$-plane in the hyperplane at infinity.  Moreover, every Euclidean $k$-plane extends to a unique projective $k$-plane.
\end{itemize}
\noindent Thus, we have a notion of projective $n$-space and $k$-planes in the space.  However, the closest reasonable thing to $k$-spheres are ``quadric $k$-surfaces''; these are conics when $n=2,k=1$, and will be studied in Section 3.5.

We now introduce the notion of a projective transformation.  We recall from Section 3.1 that a perspective projection between $2$-dimensional planes can be obtained by taking a nonsingular linear transformation of $\mathbb R^3$ and considering its action on the lines through the origin.  The concept generalizes to higher dimensions, as we will now see.\\

\noindent\textbf{Definition.} \emph{A \textbf{projective transformation} from $P^m(\mathbb R)$ to $P^n(\mathbb R)$ is a function $T:P^m(\mathbb R)\to P^n(\mathbb R)$ of the form $[\vec v]\mapsto[\varphi(\vec v)]$, where $\varphi:\mathbb R^{m+1}\to\mathbb R^{n+1}$ is an injective linear map of vector spaces, and $[\vec v]$ is the image of $\vec v\in\mathbb R^{m+1}-\{\vec 0\}$ in $P^m(\mathbb R)$.}\\

\noindent Observe that since $\varphi(k\vec v)=k\varphi(\vec v)$ for $k\in\mathbb R$, the above function $T$ is well defined for any linear $\varphi$.

Also, note that the injectivity of $\varphi$ is necessary for $T$ to be defined, because $[\vec v]$ is only defined when $\vec v\ne\vec 0$.  In particular, it follows that $m\leqslant n$ and that the image of $T$ is a (projective) $m$-plane in $P^n(\mathbb R)$.  In particular, \emph{a projective transformation from $P^n(\mathbb R)$ to itself is always bijective and invertible}.

It is clear that projective transformations preserve $k$-planes.  However, projective transformations do \emph{not} generally preserve parallel lines the same way linear maps do \---- indeed, there is not really such a thing as ``parallel lines'' at all.  What a projective transformation does intuitively is take a figure on a sheet of paper, then rotate the paper away so that the eye's perspective meets it at a different angle.  For example, if $\varphi$ is the linear operator $\begin{bmatrix}1&0&0\\0&0&1\\0&1&0\end{bmatrix}$ of $\mathbb R^3$, then the corresponding projective transformation $T$ on $P^2(\mathbb R)$ sends $[x:y:z]\mapsto[x:z:y]$; in particular, it maps a Euclidean-plane point $(x,y),y\ne 0$ to $\left(\frac xy,\frac 1y\right)$.  Here is what $T$ does to a square with vertices $(\pm 1,1),(\pm 1,3)$ with a circle in it:
\begin{multicols}{2}
\includegraphics[scale=.5]{SquareCircle.png}

\emph{The original figure.}\\
\includegraphics[scale=.16666667]{SquareCircleTrans.png}

\noindent\emph{The figure after the transformation $T$.}
\end{multicols}
\noindent Since a projective transformation $T$ is defined based on an injective linear map $\varphi:\mathbb R^{m+1}\to\mathbb R^{n+1}$, it is natural to ask whether $\varphi$ is completely determined by $T$.  The answer to this question is \emph{no}: let $k\ne 0$ be an element of $\mathbb R$, and let $\psi=k\varphi$, which is also an injective linear map.  Then for any $\vec v\in\mathbb R^{m+1}$, $\psi(\vec v)=k\varphi(\vec v)$, from which it follows that $[\psi(\vec v)]=[\varphi(\vec v)]$ in $P^n(\mathbb R)$.  This means that $\psi$ induces the \emph{same} projective transformation $P^m(\mathbb R)\to P^n(\mathbb R)$ that $\varphi$ does.  Obviously $\psi\ne\varphi$ if $k\ne 1$.

Thus, a projective transformation $T$ actually comes from an infinite family ($\lambda\varphi,0\ne\lambda\in\mathbb R$) of linear transformations.  Exercise 1 shows that these are the only linear maps which induce $T$.

Since all projective transformations on $P^n(\mathbb R)$ are invertible, they form a group $G$ under function composition; and we now have enough information to identify this group.  There is a map $\theta:GL_{n+1}(\mathbb R)\to G$ sending each nonsingular $\varphi:\mathbb R^{n+1}\to\mathbb R^{n+1}$ to the transformation $[\vec v]\mapsto[\varphi(\vec v)]$.  It is clear that $\theta$ is a homomorphism, and $\theta$ is surjective because every projective transformation comes from a nonsingular linear map by definition.  By Exercise 1, a projective transformation $\varphi$ is in $\ker\theta$ if and only if $\varphi=\lambda I_{n+1}$ for some $\lambda\ne 0$ in $\mathbb R$.  Supposing $\mathbb R^*=\ker\theta=\{\lambda I_{n+1}:\lambda\ne 0\text{ in }\mathbb R\}$, we get $GL_{n+1}(\mathbb R)/\mathbb R^*\cong G$ by Theorem 1.17.

We thus make the following definition: noting that $\mathbb R^*$ is a normal subgroup of $GL_{n+1}(\mathbb R)$ (central for that matter), the quotient group $GL_n(\mathbb R)/\mathbb R^*$ is denoted $PGL_n(\mathbb R)$ and called the \textbf{projective general linear group}.  [This was introduced in Exercise 8 of Section 1.5, but now we have a better view of the intuition behind it.]

The tricky thing is that $PGL_n(\mathbb R)$ is \emph{not} the group of projective transformations on projective $n$-space.  Instead, it is the group of projective transformations on projective $(n-1)$-space.  This is because the linear space has dimension one more than the projective space defined off of it.  The notation can get confusing for this reason.  The reader is advised to understand $PGL_n(\mathbb R)$ as a quotient of $GL_n(\mathbb R)$, so as to not make flawed arguments.

An important example of a projective transformation is one on the projective line $P^1(\mathbb R)$.  Such a transformation is given by a nonsingular linear operator on $\mathbb R^2$.  Suppose $\begin{bmatrix}a&b\\c&d\end{bmatrix}$ is the matrix for this operator (with $ad-bc\ne 0$).  Then the transformation $T$ on $P^1(\mathbb R)$ sends $[x:y]\mapsto[ax+by:cx+dy]$; hence, when $P^1(\mathbb R)$ is viewed as $\mathbb R\sqcup\{\infty\}$ with $\infty=[1:0]$, we have the following:
\begin{itemize}
\item For any $x\in\mathbb R$ such that $cx+d\ne 0$, $T(x)=\frac{ax+b}{cx+d}$.  [In other words, $T$ is mostly given by a quotient of linear polynomials.]

\item If $c=0$, then $T(\infty)=\infty$.

\item If $c\ne 0$, then $T(-d/c)=\infty$ and $T(\infty)=a/c$.  [Observe that when only real numbers are involved, $x\mapsto\frac{ax+b}{cx+d}$ has $-d/c$ outside its domain and has $a/c$ outside its range.]
\end{itemize}
Such a transformation $T$ is called a \textbf{homography} of the projective line.  The homographies form a group isomorphic to $PGL_2(\mathbb R)$, and they will spark great significance.\\ % Only called a "M\"obius transform" over the field \mathbb C

\noindent\textbf{RELATION TO PERSPECTIVE PROJECTIONS}\\

\noindent More generally, if $P'$ is a $k'$-plane in $P^{n'}(\mathbb R)$, and $P$ is a $k$-plane in $P^n(\mathbb R)$, we define a \textbf{projective transformation} $P'\to P$ to be the function $[\vec v]\mapsto[\varphi(\vec v)]$ given by an injective linear map $\varphi$ of the corresponding subspaces of $\mathbb R^{n'+1},\mathbb R^{n+1}$.  [For these to exist, we must have $k'\leqslant k$, and if $k'=k$ then every projective transformation is bijective.]

Our aim in the rest of this section is to show that a perspective projection from one hyperplane to another is a projective transformation.  Let $H_1$ and $H_2$ be distinct hyperplanes in $P^n(\mathbb R)$, and $p$ a point which is in neither hyperplane; we will use linear algebra to identify the perspective projection $H_1\to H_2$ from $p$.

$H_1,H_2$ correspond to vector subspaces $V_1,V_2$ of $\mathbb R^{n+1}$.  By definition $\dim(V_1)=\dim(V_2)=n$; also, since $V_1$ and $V_2$ are distinct, $\dim(V_1\cap V_2)=n-1$ by elementary linear algebra.  Also, the point $p$ is given by a line $\ell\subset\mathbb R^{n+1}$ through the origin (i.e., a one-dimensional subspace).  Suppose $\vec p$ is a nonzero vector which spans $\ell$.  Then $\vec p\notin V_1,V_2$ (why?).

Let $\{\vec u_1,\dots,\vec u_{n-1}\}$ be a basis of $V_1\cap V_2$, and let $\vec w_1\in V_1-V_2$ and $\vec w_2\in V_2-V_1$ be vectors.  Then the reader can readily verify:
\begin{itemize}
\item For each $j=1,2$, $\{\vec u_1,\dots,\vec u_{n-1},\vec w_j\}$ is a basis of $V_j$;

\item $\{\vec u_1,\dots,\vec u_{n-1},\vec w_1,\vec w_2\}$ is a basis of $\mathbb R^{n+1}$.
\end{itemize}
We will use these bases to construct a nonsingular linear map from $V_1$ to $V_2$.  First, we may write $\vec p$ as a linear combination of the basis of $\mathbb R^{n+1}$:
$$\vec p=c_1\vec u_1+\dots+c_{n-1}\vec u_{n-1}+d_1\vec w_1+d_2\vec w_2$$
Since $\vec p$ is not in $V_1$ or $V_2$, we have that $d_1,d_2$ are nonzero.  Now, we define a linear map $\varphi:V_1\to V_2$ using the following action on the basis:
$$\varphi(\vec u_j)=\vec u_j\text{ for }j=1,\dots,n-1$$
$$\varphi(\vec w_1)=-\frac 1{d_1}(c_1\vec u_1+\dots+c_{n-1}\vec u_{n-1}+d_2\vec w_2)$$ % What do you mean, c_1,\dot,sc_{n-1},d_2 being free variables?  They are constants, and I defined a linear map \varphi by giving the action on the basis.  Every set function from a basis extends to a unique linear map.
There are many ways to see that $\varphi$ is nonsingular; one way is to write out the matrix for $\varphi$ with respect to the bases $\{\vec u_1,\dots,\vec u_{n-1},\vec w_j\}$, and observe that its determinant is $-\frac{d_2}{d_1}\ne 0$.  Our main claim is:\\

\noindent\textbf{Lemma 3.2.} \emph{For every vector $\vec v\ne\vec 0$ in $V_1$, the vector $\varphi(\vec v)$ lies in the plane spanned by $\vec p$ and $\vec v$.\footnote{$\vec p$ and $\vec v$ are necessarily linearly independent, since $\vec v\ne\vec 0$ in $V_1$ and $\vec p\notin V_1$.  Hence, they span a plane.}}
\begin{proof}
We use our basis of $V_1$ to write $\vec v=a_1\vec u_1+\dots+a_{n-1}\vec u_{n-1}+b_1\vec w_1$.  With that, we have
$$\varphi(\vec v)=a_1\vec u_1+\dots+a_{n-1}\vec u_{n-1}-\frac{b_1}{d_1}(c_1\vec u_1+\dots+c_{n-1}\vec u_{n-1}+d_2\vec w_2)$$
$$=\left(a_1-\frac{b_1c_1}{d_1}\right)\vec u_1+\dots+\left(a_{n-1}-\frac{b_1c_{n-1}}{d_1}\right)\vec u_{n-1}-\frac{b_1d_2}{d_1}\vec w_2$$
by definition of $\varphi$.  Direct calculation then shows that $\varphi(\vec v)=\vec v-\frac{b_1}{d_1}\vec p$.  Hence, $\varphi(\vec v)$ is in the plane spanned by $\vec v$ and $\vec p$ as desired.
\end{proof}
\noindent As one more preliminary, we note that if $\ell$ is a line in $P^n(\mathbb R)$ such that $\ell\not\subset H_2$, $\ell\cap H_2$ consists of a single point, as can be seen easily through linear algebra.\\

\noindent\textbf{Proposition 3.3.} \emph{Let $T$ be the projective transformation $H_1\to H_2$ determined by $\varphi$.  Then $T$ is the perspective projection $H_1\to H_2$ from $p$.  Therefore, every perspective projection of hyperplanes is a projective transformation.}
\begin{proof}
Translating Lemma 3.2 from a linear-algebra setting to a projective setting, we get that for every $x\in H_1$, $T(x)$ is on the line $\ell$ determined by the points $p$ and $x$, which implies that it is the unique intersection point of $\ell$ and $H_2$.  In other words, $T$ takes a point $x\in H_1$ and returns the intersection of line $\overset{\longleftrightarrow}{px}$ with $H_2$, so that $T$ is the perspective projection $H_1\to H_2$ from $p$ as desired.
\end{proof}
\noindent In particular, in a perspective projection from the $xy$-plane to the $xz$-plane in $P^3(\mathbb R)$, one gets quotients of linear combinations with matching denominators, as stated in the previous section.  The reader should take the time to see this.

Note that if $T:H_1\to H_2$ is a perspective projection, then $T(x)=x$ for every $x\in H_1\cap H_2$.  The reasons are obvious.  Conversely, Exercise 10 shows that every projective transformation $T:H_1\to H_2$ such that $T(x)=x$ for all $x\in H_1\cap H_2$ is a perspective projection.

\subsection*{Exercises 3.2. (Projective $n$-Space.  Projective Transformations)} % Introduce higher-dimensional projective spaces, and prove
% certain things with the aid of linear algebra.  Also go over projective transformations, and show that every perspective projection between hyperplanes is one.
% POTENTIAL EXERCISES: If \ell_1 and \ell_2 are lines in P^2(\mathbb R), a projective transformation \ell_1\to\ell_2 is a perspective projection iff it fixes the point \ell_1\cap\ell_2
\begin{enumerate}
\item If $\varphi_1,\varphi_2$ are injective linear maps $\mathbb R^{m+1}\to\mathbb R^{n+1}$, let $T_1,T_2$ be the projective transformations $P^m(\mathbb R)\to P^n(\mathbb R)$ determined by them respectively.  Show that $T_1=T_2$ if and only if $\varphi_1=\lambda\varphi_2$ for some $\lambda\ne 0$ in $\mathbb R$.

\item If $T:P^m(\mathbb R)\to P^n(\mathbb R)$ and $U:P^n(\mathbb R)\to P^p(\mathbb R)$ are projective transformations, show that $U\circ T:P^m(\mathbb R)\to P^p(\mathbb R)$ is a projective transformation.

\item (a) Let $\ell_1$ and $\ell_2$ be projective lines in $P^3(\mathbb R)$.  Show that $\ell_1\cap\ell_2\ne\varnothing$ if and only if $\ell_1$ and $\ell_2$ lie in a common plane.  [If $\ell_1$ and $\ell_2$ do not satisfy these conditions, they are said to be \textbf{skew} lines.]

(b) Explain why part (a) may be false if $\ell_1$ and $\ell_2$ are Euclidean lines in $\mathbb R^3$.

\item For a fixed $k$, the group of projective transformations on $P^n(\mathbb R)$ acts transitively on the $k$-planes.

\item (a) Show that three points $p_1,p_2,p_3\in P^2(\mathbb R)$ are collinear if and only if there exists a projective transformation on $P^2(\mathbb R)$ sending all of them to infinity points.  [Any line can be transformed to the line at infinity.]

(b) Show that three lines $\ell_1,\ell_2,\ell_3\subset P^2(\mathbb R)$ are concurrent if and only if there exists a projective transformation sending all of them to lines which are parallel in the Euclidean plane.

(c) If $\ell_1$ is the line at infinity, then $\ell_1,\ell_2,\ell_3$ are concurrent if and only if $\ell_2$ and $\ell_3$ are parallel in the Euclidean plane.

\item If $n$ is even, show that every projective transformation $P^n(\mathbb R)\to P^n(\mathbb R)$ has a fixed point.  [Observe that the fixed points correspond to the eigenvectors of the linear operator.]  Show by example that this may not hold if $n$ is odd.

\item Let $a,b,c$ be distinct points in $P^1(\mathbb R)=\mathbb R\sqcup\{\infty\}$.  Show that there exists a unique homography $T:P^1(\mathbb R)\to P^1(\mathbb R)$ such that $T(a)=\infty$, $T(b)=0$ and $T(c)=1$.  We say that the homographies are \textbf{uniquely triply transitive} on the projective line.

\item Let $a,b,c$ be distinct points in $P^1(\mathbb R)$, and let $a',b',c'$ be distinct points of $P^1(\mathbb R)$ as well.  [We are not requiring that $a\ne a',a\ne b'$, etc.; only that $a,b,c$ are distinct from each other, as are $a',b',c'$.]  Then there is a unique homography $T:P^1(\mathbb R)\to P^1(\mathbb R)$ such that $T(a)=a'$, $T(b)=b'$ and $T(c)=c'$.  [Use Exercise 7.]

\item Show that the homographies $x\mapsto x+a$ ($a\in\mathbb R$), $x\mapsto bx$ ($b\ne 0$) and $x\mapsto\frac 1x$ generate the group $PGL_2(\mathbb R)$ of all homographies on $P^1(\mathbb R)$.

\item (a) Let $\ell_1$ and $\ell_2$ be projective lines in $P^2(\mathbb R)$.  If $\varphi:\ell_1\to\ell_2$ is a projective transformation, show that $\varphi$ is a perspective projection from some point if and only if $\varphi(\ell_1\cap\ell_2)=\ell_1\cap\ell_2$.  [If $\varphi(\ell_1\cap\ell_2)=\ell_1\cap\ell_2$, then let $q_1,q_2\in\ell_1$ be points which are distinct from $\ell_1\cap\ell_2$ and from each other.  Then let $q_1'=\varphi(q_1),q_2'=\varphi(q_2)$ and let $p$ be the intersection of the lines $\overset{\longleftrightarrow}{q_1~q_1'}$ and $\overset{\longleftrightarrow}{q_2~q_2'}$.  Then $\varphi$ and the perspective projection from $p$ are projective transformations which agree on the points $\ell_1\cap\ell_2$, $q_1$ and $q_2$; use Exercise 8.]

(b) More generally, if $H_1,H_2$ are hyperplanes in $P^n(\mathbb R)$, a projective transformation $\varphi:H_1\to H_2$ is a perspective projection if and only if $\varphi(x)=x$ for all $x\in H_1\cap H_2$.

\item In $P^3(\mathbb R)$, show that there is a projective transformation $T$ from the $xy$-plane to the $xz$-plane sending a Euclidean-space point $(x,y,0)\mapsto(x,0,y)$.  By Exercise 10, $T$ is a perspective projection; what point does it project from?

\item A projective transformation $T:P^n(\mathbb R)\to P^n(\mathbb R)$ is said to be \textbf{affine} if $T$ fixes the hyperplane $H_\infty$ at infinity (though not necessarily pointwise).

(a) There is a bijection between affine transformations of $P^n(\mathbb R)$ and invertible affine transformations of $\mathbb R^n$ (i.e., compositions of elements of $GL_n(\mathbb R)$ with translations).  This bijection restricts an affine transformation of $P^n(\mathbb R)$ to $\mathbb R^n$.

(b) An affine transformation $T$ fixes $H_\infty$ pointwise if and only if the restriction of $T$ to $\mathbb R^n$ is the composition of a scaling and a translation.  [The scaling could be by a negative scalar.]

(c) If $T:P^n(\mathbb R)\to P^n(\mathbb R)$ is a projective transformation (not necessarily affine), then there exists an $(n-2)$-plane $P\subset H_\infty$ such that $T(P)\subset H_\infty$.  Conclude that there exists a hyperplane $H\ne H_\infty$ such that $T|_H$ is an affine transformation from $H$ to some hyperplane, either the same one or a different one.  [Consider $H_\infty\cap T^{-1}(H_\infty)$.]
\end{enumerate}

\subsection*{3.3. The Cross Ratio: A Projective Invariant}
\addcontentsline{toc}{section}{3.3. The Cross Ratio: A Projective Invariant}
In Euclidean $n$-space, one can take a pair of points, and measure how far apart they are, through the notion of the distance.  The distance between two points is invariant under isometries (by definition), and therefore part of Euclidean $n$-space's intrinsic geometry.\footnote{``Intrinsic geometry'' intuitively refers to anything that is invariant under whatever group is operating.  In Euclidean geometry, that group is the isometries.}

However, in projective $n$-space, what happens if we try to define the distance between two points?  We run into trouble.  We cannot define the distance between two points to be the Euclidean distance, because one of the points could be at infinity.  More generally, the projective transformations act \emph{transitively} on pairs of distinct points in $P^n(\mathbb R)$.  [In fact, Exercise 1 below shows that they act transitively on triples of distinct points.]  Thus if $\rho:P^n(\mathbb R)\times P^n(\mathbb R)\to X$ were \emph{any} kind of distance function invariant under the projective transformations, $\rho$ would be constant on all the pairs $(x,y),x\ne y$, and so this distance would be useless.\footnote{In Section 5.6, however, we will see projective $n$-space in a different light.  There will be a distance metric, and we will define isometries to be maps which preserve distance.  Moreover, the isometries will be projective transformations (not all of them).}

So perhaps we would find a more convenient measure which inputs a more than two points on a single line?  From Exercise 1 one sees that three points is still not enough; any invariant would be mostly constant.  However, four is the perfect amount for this scenario, as we will now see.

It is best to start with the projective line, and generalize to higher dimensions later.  Let $a,b,c,d$ be distinct points of $P^1(\mathbb R)$.  By Exercise 7 of the previous section, there is a unique homography $T:P^1(\mathbb R)\to P^1(\mathbb R)$ such that $T(a)=\infty$, $T(b)=0$ and $T(c)=1$.  With that, the point $T(d)$ will then be a reasonable ``measure'' of the quadruple $(a,b,c,d)$.  If $a,b,c,d\ne\infty$, then $T$ is given by $T(x)=\frac{c-a}{c-b}\left(\frac{x-b}{x-a}\right)$ [see the hint of Exercise 7].  Hence, $T(d)=\frac{c-a}{c-b}\left(\frac{d-b}{d-a}\right)=\frac{(c-a)(d-b)}{(d-a)(c-b)}$.  Note that the denominator is nonzero because $a,b,c,d$ are distinct.

Moreover, we observe from the previous paragraph that the map $(a,b,c,d)\mapsto\frac{(c-a)(d-b)}{(d-a)(c-b)}$ is invariant under homographies.  Indeed, if $(A(a),A(b),A(c),A(d))=(a',b',c',d')$ and $T:P^1(\mathbb R)\to P^1(\mathbb R)$ is the homography sending $a'\mapsto\infty,b'\mapsto 0,c'\mapsto 1$, then $T(d')=\frac{(c'-a')(d'-b')}{(d'-a')(c'-b')}$; yet also $T\circ A$ is a homography sending $a\mapsto\infty,b\mapsto 0,c\mapsto 1$ which implies $T(d')=T(A(d))=\frac{(c-a)(d-b)}{(d-a)(c-b)}$.  This leads to the following definition.\\

\noindent\textbf{Definition.} \emph{Let $[a:a_0],[b:b_0],[c:c_0],[d:d_0]\in P^1(\mathbb R)$.  Assume that given any three of those points, they are not all equal.  Then we define the \textbf{cross ratio} of the points via}
$$[\![[a:a_0],[b:b_0],[c:c_0],[d:d_0]]\!]=[(a_0c-c_0a)(b_0d-d_0b):(a_0d-d_0a)(b_0c-c_0b)].$$

\noindent We have given a complicated definition using the equivalence classes of vectors in $\mathbb R^2$, so that we could be conscientious about how the cross ratio is defined when any of the inputs (or the output) are $\infty$.  We will see that the definition is easier to understand when using our chosen embedding of $\mathbb R$ into $P^1(\mathbb R)$.

It is easy to verify that the above expression $[(a_0c-c_0a)(b_0d-d_0b):(a_0d-d_0a)(b_0c-c_0b)]$ is independent of the particular way the inputs from $P^1(\mathbb R)$ are represented, hence is well-defined.  Also, we recall that a point $[x:y]$ is not defined when $x$ and $y$ are both zero, so we need to make sure the expressions on the two sides are not both zero.  The reader can readily verify that this holds if and only if, given any three of the input points, they are not all equal.  This is exactly the condition assumed in the definition.\footnote{Thus, $[\![a,a,b,b]\!]$ is a well-defined cross ratio if $a\ne b$, but $[\![a,a,a,b]\!]$ is not defined because it has three equal inputs.} % a_0c-c_0a if and only if [a:a_0]=[c:c_0].  After all, the vectors (a,a_0,0),(c,c_0,0)\in\mathbb R^3 are both nonzero, hence their cross product (which is (0,0,a_0c-c_0a)) is zero \iff they are linearly dependent \iff they are scalar multiples.  Similarly for b_0d-d_0b, etc.  This acn be used to verify that the cross ratio is the undefined [0:0] if and only if there are three equal inputs.

To get a better understanding of the cross ratio, we consider the case where $a_0=b_0=c_0=d_0=1$.  In this case, the points are identified with the real numbers $a,b,c,d$ (recall that $x\in\mathbb R$ is identified with $[x:1]$).  Moreover, the definition entails $[\![a,b,c,d]\!]=[(c-a)(d-b):(d-a)(c-b)]$.  If $(d-a)(c-b)=0$, this is $\infty$; otherwise, it is the real number $\frac{(c-a)(d-b)}{(d-a)(c-b)}$.  Thus we have\\

\noindent\textbf{Proposition 3.4.} \emph{If $a,b,c,d\in\mathbb R$ and $(d-a)(c-b)\ne 0$, then $[\![a,b,c,d]\!]=\frac{(c-a)(d-b)}{(d-a)(c-b)}$.}\\

\noindent and we can compute cross ratios of real numbers without referring back to the original definition of the projective line.  Also, if exactly one of the points $a,b,c,d$ is equal to $\infty$, the cross ratio is obtained by taking $\frac{(c-a)(d-b)}{(d-a)(c-b)}$ and deleting the factor in both the numerator and the denominator with this variable.  (E.g., $[\![a,b,\infty,d]\!]=\frac{d-b}{d-a}$.)  If more than one of them is $\infty$, Proposition 3.5(i)-(iii) gives the cross ratio.

Now for a few basic properties of cross ratios on the projective line.  Throughout this book, whenever we refer to $[\![a,b,c,d]\!]$ we will assume that it's well-defined (i.e., the inputs don't consist of three equal points).\\

\noindent\textbf{Proposition 3.5.} \emph{If $a,b,c,d\in P^1(\mathbb R)$, then}

(i) \emph{$[\![a,b,c,d]\!]=\infty$ if and only if either $a=d$ or $b=c$;}

(ii) \emph{$[\![a,b,c,d]\!]=0$ if and only if either $a=c$ or $b=d$;}

(iii) \emph{$[\![a,b,c,d]\!]=1$ if and only if either $a=b$ or $c=d$;}

(iv) \textsc{(Invariance under homographies)} \emph{If $T:P^1(\mathbb R)\to P^1(\mathbb R)$ is a homography, then $[\![T(a),T(b),T(c),T(d)]\!]=[\![a,b,c,d]\!]$.}
\begin{proof}
(i), (ii) and (iii) can be easily proved using the definition of the cross ratio (note that $[x:y]=\infty\iff y=0$).  We leave the verifications to the reader.

As for (iv), we let
$$X=\{(a,b,c,d):a,b,c,d\in P^1(\mathbb R),\text{no three are all equal}\}.$$
The group $PGL_2(\mathbb R)$ of homographies acts on $X$ via $T\cdot(a,b,c,d)=(T(a),T(b),T(c),T(d))$.  Moreover, we may define a function $f:X\to P^1(\mathbb R)$ given by $f(a,b,c,d)=[\![a,b,c,d]\!]$.

Now (iv) states that $f$ is a function on orbits from the set $X$ on which the group acts.  Let $S$ be the subset of $PGL_2(\mathbb R)$ consisting of the homographies $x\mapsto x+r$ ($r\in\mathbb R$), $x\mapsto sx$ ($s\ne 0$) and $x\mapsto\frac 1x$.  By Exercise 9 of the previous section, $S$ is a generating set.  Thus, by Proposition 1.22, it suffices to show that $f(T\cdot x)=f(x)$ for all $T\in S,x\in X$.

First suppose $T$ is of the form $x\mapsto x+r$.  Then for points $a,b,c,d$, if $a,b,c,d\in\mathbb R$ and $a\ne d,b\ne c$,
$$[\![T(a),T(b),T(c),T(d)]\!]=\frac{[(c+r)-(a+r)][(d+r)-(b+r)]}{[(d+r)-(a+r)][(c+r)-(b+r)]}$$
$$=\frac{(c-a)(d-b)}{(d-a)(c-b)}=[\![a,b,c,d]\!]$$
The reader can readily verify the cases where any inputs or the output is $\infty$.  Thus, $f(T\cdot x)=f(x)$ in this case.

Likewise, if $T$ is of the form $x\mapsto sx$ with $s\ne 0$, then
$$[\![T(a),T(b),T(c),T(d)]\!]=\frac{(sc-sa)(sd-sb)}{(sd-sa)(sc-sb)}=\frac{s^2(c-a)(d-b)}{s^2(d-a)(c-b)}$$
$$=\frac{(c-a)(d-b)}{(d-a)(c-b)}=[\![a,b,c,d]\!]$$
The case where $T(x)=\frac 1x$ is the least trivial.  However, experience with complex fractions helps us sort it out:
$$[\![T(a),T(b),T(c),T(d)]\!]=\frac{\left(\frac 1c-\frac 1a\right)\left(\frac 1d-\frac 1b\right)}{\left(\frac 1d-\frac 1a\right)\left(\frac 1c-\frac 1b\right)}$$
$$=\frac{abcd\left(\frac 1c-\frac 1a\right)\left(\frac 1d-\frac 1b\right)}{abcd\left(\frac 1d-\frac 1a\right)\left(\frac 1c-\frac 1b\right)}=\frac{ac\left(\frac 1c-\frac 1a\right)\cdot bd\left(\frac 1d-\frac 1b\right)}{ad\left(\frac 1d-\frac 1a\right)\cdot bc\left(\frac 1c-\frac 1b\right)}=\frac{(a-c)(b-d)}{(a-d)(b-c)}$$
$$=\frac{(c-a)(d-b)}{(d-a)(c-b)}=[\![a,b,c,d]\!].$$
\end{proof}

\noindent Note that, using Proposition 3.4 and the statements right after it, one can see that every $d\in P^1(\mathbb R)$ satisfies $[\![\infty,0,1,d]\!]=d$.  Furthermore, if $a,b,c\in P^1(\mathbb R)$ are distinct points, and $T$ is the homography sending $a\mapsto\infty,b\mapsto 0,c\mapsto 1$, then $T(d)=[\![T(a),T(b),T(c),T(d)]\!]=[\![a,b,c,d]\!]$ by Proposition 3.5(iv).  Moreover, $T$ is precisely the map $d\mapsto[\![a,b,c,d]\!]$, which proves
\begin{center}
\textbf{If $a,b,c\in P^1(\mathbb R)$ are distinct points, then the map $d\mapsto[\![a,b,c,d]\!]$ is precisely the homography of $P^1(\mathbb R)$ sending $a\mapsto\infty,b\mapsto 0,c\mapsto 1$.  In particular, it is bijective.}
\end{center}
\noindent Thus we have the \emph{cancellation law for cross ratios}: if $[\![a,b,c,d]\!]=[\![a,b,c,d']\!]$ then $d=d'$.  Note, however, that this is false if $a,b,c$ are not all distinct (why?).

We have thus far defined cross ratios on the projective line.  They can also be defined in higher dimensions, but only on points that are on a common line.  We now suppose $a,b,c,d\in P^n(\mathbb R)$ are points such that:
\begin{itemize}
\item $a,b,c,d$ lie on a common line $\ell$;

\item Given any three of $a,b,c,d$, they are not all equal.
\end{itemize}
Note that $\ell$ is unique (because $a,b,c,d$ are not all equal, they consist of at least two distinct points, so that the line is determined by Proposition 3.1(i)).  We define the \textbf{cross ratio} $[\![a,b,c,d]\!]$ as follows: let $T:\ell\to P^1(\mathbb R)$ be a projective transformation (it is clear that it exists), and set $[\![a,b,c,d]\!]=[\![T(a),T(b),T(c),T(d)]\!]$.

Several things need justification.  First, there are many homographies $T:\ell\to P^1(\mathbb R)$, so we need to show that the cross ratio is well defined and independent of the particular one used.  Well, suppose $T':\ell\to P^1(\mathbb R)$ is also a projective transformation.  Set $U=T'\circ T^{-1}:P^1(\mathbb R)\to P^1(\mathbb R)$; this is a projective transformation of the projective line, a.k.a.~a homography.  Moreover, we have $T'=U\circ T$, so that
$$[\![T'(a),T'(b),T'(c),T'(d)]\!]=[\![U(T(a)),U(T(b)),U(T(c)),U(T(d))]\!]$$
$$\overset{(*)}=[\![T(a),T(b),T(c),T(d)]\!]$$
where $(*)$ uses Proposition 3.5(iv).  This proves that the cross ratio is well defined.

Moreover, we leave it to the reader to prove the following facts using the previously established results about cross ratios on $P^1(\mathbb R)$.\\

\noindent\textbf{Proposition 3.6.} \emph{Let $a,b,c,d\in P^m(\mathbb R)$ such that $[\![a,b,c,d]\!]$ is defined.}

(i) \emph{$[\![a,b,c,d]\!]=\infty$ if and only if either $a=d$ or $b=c$;}

(ii) \emph{$[\![a,b,c,d]\!]=0$ if and only if either $a=c$ or $b=d$;}

(iii) \emph{$[\![a,b,c,d]\!]=1$ if and only if either $a=b$ or $c=d$;}

(iv) \emph{If $T:P^m(\mathbb R)\to P^n(\mathbb R)$ is a projective transformation, then $[\![T(a),T(b),T(c),T(d)]\!]=[\![a,b,c,d]\!]$.  More generally this holds if $T$ is a projective transformation between $k$-planes (various $k$) with $a,b,c,d$ in the domain.}\\

\noindent Thus, we know that the cross ratio is invariant under projective transformations.  It is our desired projective-geometry analogue of the Euclidean distance between points.  Note that $[\![\infty,0,1,d]\!]=d$; this is analogous to the fact that on the Euclidean line $\mathbb R$, the distance from $d$ to $0$ is $|d|$.  The cross ratio does notably have some nice properties.

For example, in the projective-plane construction below, $[\![a,b,c,d]\!]=[\![a',b',c',d']\!]$:
\begin{center}\includegraphics[scale=.4]{PerspCrossRatio.png}\end{center}
The reason is that the perspective projection from $p$ is a projective transformation of the lines by Proposition 3.3; hence the statement follows from Proposition 3.6(iv).

Cross ratios have been used to prove certain famous results, such as Pappus' Hexagon Theorem (to be covered in Section 3.7).  The concept will reappear in Chapters 4 and 5, but using complex numbers instead of real numbers.

\subsection*{Exercises 3.3. (The Cross Ratio: A Projective Invariant)}
\begin{enumerate}
\item Let $\ell$ and $\ell'$ be lines in $P^n(\mathbb R)$.  Suppose $a,b,c$ are distinct points on $\ell$, and $a',b',c'$ are distinct points on $\ell'$.  Show that there exists a projective transformation $T\in PGL_{n+1}(\mathbb R)$ such that $T(a)=a',T(b)=b',T(c)=c'$.

\item Let $x_1,x_2,x_3,x_4\in P^1(\mathbb R)$ and $\sigma\in S_4$.  If $\lambda=[\![x_1,x_2,x_3,x_4]\!]$ and $\mu=[\![x_{\sigma(1)},x_{\sigma(2)},x_{\sigma(3)},x_{\sigma(4)}]\!]$, then:

(a) $\mu$ is not necessarily equal to $\lambda$.  In other words, permuting the inputs in a cross ratio may change the result.

(b) For a fixed $\sigma\in S_4$, the relation $\lambda\mapsto\mu$ is a homography which depends only on $\sigma$.  [Since the permutations $\sigma$ satisfying these statements are manifestly closed under composition, you may assume (Exercise 13(c) of Section 1.6) that $\sigma$ is a transposition.  There are six transpositions in $S_4$; use casework.]  Moreover, $\mu$ depends on only $\lambda$ and $\sigma$, and not on the particular points $x_j$.

(c) For each $\sigma\in S_4$, the homography given in part (b) is one of the following:
$$\lambda\mapsto\lambda~~~~~~~~\lambda\mapsto 1-\lambda~~~~~~~~\lambda\mapsto\frac 1{\lambda}$$
$$\lambda\mapsto\frac 1{1-\lambda}~~~~~~~~\lambda\mapsto\frac{\lambda-1}{\lambda}~~~~~~~~\lambda\mapsto\frac{\lambda}{\lambda-1}$$
[Use Proposition 3.5(i)-(iii).]

(d) Use parts (b) and (c) to get a group homomorphism $S_4\to PGL_2(\mathbb R)$ with kernel $V=\{(1),(12)(34),(13)(24),(23)(14)\}$.  [$V$ is known as the \textbf{canonical Klein 4-group} of $S_4$.]

\item Let $A\in PGL_2(\mathbb R)$ be a homography of $P^1(\mathbb R)$ which is not the identity but fixes at least two points of $P^1(\mathbb R)$. % Part (a) is false if I just say "fixes at least one point" because x\mapsto x+1 is a homography which fixes only \infty.  [Note: the corresponding 2\times 2 matrix is a Jordan block, hence is not diagonalizable even over \mathbb C]

(a) Show that $A$ fixes exactly two points.  We will refer to these points as $x_1$ and $x_2$.

(b) Let $x\ne x_1,x_2$ be a point of $P^1(\mathbb R)$.  Show that the following statements are equivalent:

~~~~(i) $A^2=\overline{I_2}$, where $\overline{I_2}$ is the identity element of $PGL_2(\mathbb R)$; % "Why the bar on I_2?"  I_2 is the identity matrix, and \overline{I_2} is its congruence class modulo \mathbb R^*.  Ever since the chapter on universal algebra, I used \overline for equivalence classes in a quotient set

~~~~(ii) $A^2(x)=x$;

~~~~(iii) $[\![x_1,x_2,x,A(x)]\!]=-1$.

[(i) $\iff$ (ii): According to part (a), no homography other than the identity can have more than two fixed points.  (ii) $\iff$ (iii): Use the invariance of cross ratios under homographies, Exercise 2, and the cancellation law for cross ratios.]

In particular, since statement (i) doesn't involve the particular point $x$, it follows that (ii) and (iii) depend on only $A$ and the fixed points $x_1,x_2$, and not $x$.

\item If $a,b,c,d\in P^1(\mathbb R)$ and $[\![a,b,c,d]\!]=-1$, then $d$ is said to be the \textbf{harmonic conjugate} of the ordered triple $(a,b,c)$.  [The word ``the'' is justified by the fact that $a,b,c$ must be distinct (why?), and hence $x\mapsto[\![a,b,c,x]\!]$ is a homography.]

(a) If $a\ne b$ in $P^1(\mathbb R)$, define $T:P^1(\mathbb R)\to P^1(\mathbb R)$ as follows: $T(a)=a,T(b)=b$ and for $c\ne a,b$, $T(c)$ is the harmonic conjugate of $(a,b,c)$.  Then $T$ is the unique homography such that $T(a)=a$, $T(b)=b$, $T^2=\overline{I_2}$ and $T\ne\overline{I_2}$.  [Use Exercise 3.]

(b) If $c$ is a nonzero real number, the harmonic conjugate of $(0,\infty,c)$ is $-c$.  More generally, the harmonic conjugate of $(a,\infty,c)$ is $2a-c$.  The harmonic conjugate of $(a,b,\infty)$ is $\frac{a+b}2$.  The harmonic conjugate of $(a,b,c)$ (when $c\ne a,b,\frac{a+b}2$) is $\frac{(a+b)c-2ab}{2c-(a+b)}$.

(c) Let $a,b,c\in P^2(\mathbb R)$ be three collinear points; say they lie in the line $\ell$.  Let $h$ be a point outside $\ell$, and let $\ell'$ be a line through $c$.  Set $m=\ell'\cap\overset{\longleftrightarrow}{h~a}$ and $n=\ell'\cap\overset{\longleftrightarrow}{h~b}$.  Let $k=\overset{\longleftrightarrow}{a~n}\cap\overset{\longleftrightarrow}{b~m}$ and let $d=\overset{\longleftrightarrow}{h~k}\cap\ell$.  Then $d$ is the harmonic conjugate of $(a,b,c)$.  [By applying a projective transformation, one may assume $a=(-1,0),b=(1,0),c=[1:0:0]$ and $h=[0:1:0]$.]

\item Let $a,b,c,d,a',b',c',d'\in P^n(\mathbb R)$ be points.  Suppose that all the necessary conditions are satisfied so that $[\![a,b,c,d]\!]$ and $[\![a',b',c',d']\!]$ are defined.  Then there exists a projective transformation $T:P^n(\mathbb R)\to P^n(\mathbb R)$ sending $a\mapsto a',b\mapsto b',c\mapsto c',d\mapsto d'$ if and only if $[\![a,b,c,d]\!]=[\![a',b',c',d']\!]$.

\item Consider the following diagram in $P^2(\mathbb R)$.  It involves two points and two lines [but the lines are only partially drawn in the figure].
\begin{center}\includegraphics[scale=.4]{PassDiagram.png}\end{center}
Draw the diagram resulting from passing this through the projective transformation given by $\begin{bmatrix}0&2&0\\0&0&1\\1&0&0\end{bmatrix}\in PGL_3(\mathbb R)$.

\item Suppose that the following is constructed in the Euclidean plane $\mathbb R^2$, and then the plane is embedded into $P^2(\mathbb R)$ the usual way.
\begin{center}\includegraphics[scale=.4]{CRAngles.png}\end{center}
Show that $[\![A,B,C,D]\!]=\frac{(\sin m\angle APC)(\sin m\angle BPD)}{(\sin m\angle APD)(\sin m\angle BPC)}$.  [Multiply the numerator and denominator of that fraction by $\frac 14(PA)(PB)(PC)(PD)$, then use Exercise 2.3(a) to write them in terms of areas of triangles.]
\end{enumerate}

\subsection*{3.4. The Fundamental Theorem of Projective Geometry}
\addcontentsline{toc}{section}{3.4. The Fundamental Theorem of Projective Geometry}
Projective geometry may be less familiar than Euclidean geometry.  In this chapter we shall cover a fundamental theorem which can help to simplify the study of projective geometry further.

We recall (Section 3.2, Exercise 7) that if $a,b,c\in P^1(\mathbb R)$ are distinct points, and $a',b',c'\in P^1(\mathbb R)$ are distinct points as well, then there is a unique homography $T$ such that $T(a)=a'$, $T(b)=b'$ and $T(c)=c'$.  There are similar kinds of subsets of $P^n(\mathbb R)$ that satisfy the analogous property.  They are called \emph{projective frames}, and they will be covered in detail in this section.

A projective frame is a finite list of points which is analogous to a basis of a vector space.  Just like a basis spans, a projective frame is supposed to span the space (in other words, not be contained in a single hyperplane).  It is also not supposed to have too many collinear points, as that would be similar to linear dependence of a set of vectors.  With enough conditions, projective frames are determined up to projective transformations, and can be used to define a coordinate system which describes every point of the projective space.  The full definition is given as follows:\\

\noindent\textbf{Definition.} \emph{A \textbf{projective frame} in $P^n(\mathbb R)$ is an ordered list $(p_1,p_2,\dots,p_{n+2})$ of points of $P^n(\mathbb R)$, such that if $\vec v_1,\dots,\vec v_{n+2}$ are nonzero vectors in $\mathbb R^{n+1}$ with $p_j=[\vec v_j]$, then for every $1\leqslant j\leqslant n+2$, $\vec v_1,\dots,\vec v_{j-1},\vec v_{j+1},\dots,\vec v_{n+2}$ are linearly independent (and therefore form a basis of $\mathbb R^{n+1}$).  In other words, any $n+1$ of the vectors are linearly indenepdent.}\\

\noindent If $n=1$, a projective frame is precisely an ordered list of three distinct points.  After all, two vectors in $\mathbb R^2$ are linearly independent if and only if neither is a scalar multiple of the other, i.e., they induce different points in $P^1(\mathbb R)$.

If $n=2$, things start to get more interesting.  A projective frame cannot contain three collinear points (even though it always does for $n=1$).  If $(p_1,p_2,p_3,p_4)$ were a projective frame and, for example, $p_1,p_2,p_3$ were collinear, then $\vec v_1,\vec v_2,\vec v_3$ would span a plane and therefore not be a basis of $\mathbb R^3$; this contradicts the definition.  In fact, a projective frame in $P^2(\mathbb R)$ is \emph{equivalent} to a list of four points, no three of which are collinear.  [Note that such points must be distinct \---- why?]  An example is the list of vertices of a quadrilateral in the Euclidean plane, as it is embedded in $P^2(\mathbb R)$.

For more general $n$, one can think of a projective frame as a list of $n+2$ points such that no $n+1$ of them lie in a single hyperplane.  It is clear that this is equivalent to the definition given above.

We shall first establish a way to transform the projective frame concept to a linear-algebra setting.\\

\noindent\textbf{Proposition 3.7.} (i) \emph{Let $p_1,\dots,p_{n+2}\in P^n(\mathbb R)$.  Then $(p_1,\dots,p_{n+2})$ is a projective frame if and only if there exist $\vec u_1,\dots,\vec u_{n+2}\in\mathbb R^{n+1}$ with $p_j=[\vec u_j]$, such that the $\vec u$'s span $\mathbb R^{n+1}$ and $\vec u_1+\dots+\vec u_{n+2}=\vec 0$.} % Wrong font?  I always head these statements without italics.  Maybe I should remove the italics from the reference in the next line

\emph{Moreover, if $\vec u_1,\dots,\vec u_{n+2}\in\mathbb R^{n+1}$ are vectors satisfying the condition in part }(i)\emph{:}

(ii) \emph{For $c_1,\dots,c_{n+2}\in\mathbb R$, $c_1\vec u_1+\dots+c_{n+2}\vec u_{n+2}=\vec 0$ if and only if $c_1=\dots=c_{n+2}$.}

(iii) \emph{Every $\vec v\in\mathbb R^{n+1}$ is of the form $a_1\vec u_1+\dots+a_{n+2}\vec u_{n+2}$ with $a_1,\dots,a_{n+2}\in\mathbb R$, $a_1+\dots+a_{n+2}=0$.}

(iv) \emph{If $a_1+\dots+a_{n+2}=0$ and the $a_j$ are not all zero, the projective point $[a_1\vec u_1+\dots+a_{n+2}\vec u_{n+2}]$ is determined by the projective points $[\vec u_j]\in P^n(\mathbb R)$ and the coefficients $a_j\in\mathbb R$ (subject to the $\vec u$'s satisfying the condition in part (i)).}\\

\noindent It is worth remarking that (iv) states that we can take linear combinations of points in a projective frame (via $a_1[\vec u_1]+\dots+a_{n+2}[\vec u_{n+2}]=[a_1\vec u_1+\dots+a_{n+2}\vec u_{n+2}]$), where the coefficients are not all zero and their sum is zero.  Notice also that the coefficients are only unique up to scalar multiplication by a nonzero real number.  Such coefficients are called \textbf{barycentric coordinates}.

\begin{proof}
(i) Suppose $(p_1,\dots,p_{n+2})$ is a projective frame.  Let $\vec v_1,\dots,\vec v_{n+2}\in\mathbb R^{n+1}$ be nonzero vectors such that $p_j=[\vec v_j]$.  Then since there are $n+2$ $\vec v$'s in an $(n+1)$-dimensional space, they are linearly dependent, thus we have a relation of the form
$$c_1\vec v_1+c_2\vec v_2+\dots+c_{n+2}\vec v_{n+2}=0$$
with $c_1,\dots,c_{n+2}\in\mathbb R$, not all zero.  Observe that none of the $c_j$ can be zero: for if $c_j=0$, then $\vec v_1,\dots,\vec v_{j-1},\vec v_{j+1},\dots,\vec v_{n+2}$ would be linearly dependent, contradicting the definition of a projective frame.  Now take $\vec u_j=c_j\vec v_j$; then $p_j=[\vec v_j]=[\vec u_j]$, these vectors sum to zero, and they span $\mathbb R^{n+1}$ because any $n+1$ of the $\vec v$'s does.  Thus they satisfy the conditions in the statement.

Conversely, suppose $\vec u_1,\dots,\vec u_{n+2}\in\mathbb R^{n+1}$ are vectors which satisfy the conditions.  First, if $\vec u_j=\vec 0$, then the remaining $n+1$ $\vec u$'s must form a basis of $\mathbb R^{n+1}$ (because they span), but they must also sum to zero, which is a contradiction.  Therefore, every $\vec u_j\ne\vec 0$.  Now take any $j$; we show that $\{\vec u_1,\dots,\vec u_{j-1},\vec u_{j+1},\dots,\vec u_{n+2}\}$ spans $\mathbb R^{n+1}$; it will follow that it is a basis, and hence $(p_1,\dots,p_{n+2})$ is a projective frame as required.

Let $\vec w\in\mathbb R^{n+1}$ be arbitrary.  Since the $\vec u$'s span $\mathbb R^{n+1}$, we have a relation of the form
$$\vec w=c_1\vec u_1+\dots+c_{n+2}\vec u_{n+2}.$$
Yet, $c_j\vec u_1+\dots+c_j\vec u_{n+2}=c_j(\vec u_1+\dots+\vec u_{n+2})=\vec 0$.  Subtracting this off from the above equation gives
$$\vec w=(c_1-c_j)\vec u_1+\dots+(c_{n+2}-c_j)\vec u_{n+2}.$$
Since the $\vec u_j$ coefficient is $c_j-c_j=0$, the above is a linear combination of $\vec u_1,\dots,\vec u_{j-1},\vec u_{j+1},\dots,\vec u_{n+2}$.  Hence those $n+1$ vectors span $\mathbb R^{n+1}$ as required.

(ii) If $c_1=\dots=c_n=c$ say, then $c_1\vec u_1+\dots+c_{n+2}\vec u_{n+2}=c(\vec u_1+\dots+\vec u_{n+2})=\vec 0$.

Conversely, suppose $c_1\vec u_1+\dots+c_{n+2}\vec u_{n+2}=\vec 0$ with $c_1,\dots,c_{n+1}\in\mathbb R$.  Subtracting $c_1(\vec u_1+\dots+\vec u_{n+2})=\vec 0$ from this equation, we have $(c_2-c_1)\vec u_2+\dots+(c_{n+2}-c_1)\vec u_{n+2}=\vec 0$.  Yet the proof of part (i) shows that the $n+1$ vectors $\vec u_2,\dots,\vec u_{n+2}$ form a basis of $\mathbb R^{n+1}$.  Therefore $c_2-c_1=\dots=c_{n+2}-c_1=0$, and so $c_1=c_2=\dots=c_{n+2}$.

(iii) Every $\vec v\in\mathbb R^{n+1}$ is of the form $c_1\vec u_1+\dots+c_{n+2}\vec u_{n+2}$ because the $\vec u$'s span the space.  Now, set $c=\frac{c_1+\dots+c_{n+2}}{n+2}$, and let $a_j=c_j-c$ for each $j$.  Then
$$\vec v=\vec v-\vec 0=(c_1\vec u_1+\dots+c_{n+2}\vec u_{n+2})-c(\vec u_1+\dots+\vec u_{n+2})=a_1\vec u_1+\dots+a_{n+2}\vec u_{n+2},$$
and $a_1+\dots+a_{n+2}=c_1+\dots+c_{n+2}-(n+2)c=0$.

(iv) Suppose $\vec u'_1,\dots,\vec u'_{n+2}\in\mathbb R^{n+1}$ also satisfy the condition in part (i), and $[\vec u'_j]=[\vec u_j]$ for every $j$.  Then this means that there is a nonzero real number $c_j$ with $\vec u'_j=c_j\vec u_j$.  Since
$$\vec 0=\vec u'_1+\dots+\vec u'_{n+2}=c_1\vec u_1+\dots+c_{n+2}\vec u_{n+2},$$
we have, by part (ii), $c_1=\dots=c_{n+2}=$ say $c$.  Then $a_1\vec u'_1+\dots+a_{n+2}\vec u_{n+2}=c(a_1\vec u_1+\dots+a_{n+2}\vec u_{n+2})$, and so $[a_1\vec u'_1+\dots+a_{n+2}\vec u'_{n+2}]=[a_1\vec u_1+\dots+a_{n+2}\vec u_{n+2}]$ as desired.
\end{proof}

\noindent We remark that (ii) can alternatively be proved using the rank-nullity theorem.  The linear map $\varphi:\vec e_{n+2}\mapsto\vec u_{n+2}$ from $\mathbb R^{n+2}\to\mathbb R^{n+1}$ has rank $n+1$ (because the $\vec u$'s span, and so $\varphi$ is surjective); therefore, since $\operatorname{rank}+\operatorname{nullity}=\dim(\mathbb R^{n+2})=n+2$, $\varphi$ has nullity $1$.  This means $\ker\varphi$ is a one-dimensional subspace.  Since $(1,1,\dots,1)$ is a nonzero vector in $\ker\varphi$, we conclude $\ker\varphi=\operatorname{span}(1,1,\dots,1)$, which is identical to the statement of (ii).

If $(p_1,\dots,p_{n+2})$ is a projective frame and $T:P^n(\mathbb R)\to P^n(\mathbb R)$ is a projective transformation, it is clear that $(T(p_1),\dots,T(p_{n+2}))$ is also a projective frame.  The results of Proposition 3.7 enable us to easily generalize the ``unique homography'' statement to projective spaces of various dimensions:\\

\noindent\textbf{Theorem 3.8.} \textsc{(The Fundamental Theorem of Projective Geometry)} \emph{If $(p_1,\dots,p_{n+2})$ and $(q_1,\dots,q_{n+2})$ are projective frames, then there is a unique projective transformation $T:P^n(\mathbb R)\to P^n(\mathbb R)$ such that $T(p_j)=q_j$ for all $j$.}
\begin{proof}
By Proposition 3.7(i), there exist $\vec u_1,\dots,\vec u_{n+2}\in\mathbb R^{n+1}$ such that $p_j=[\vec u_j]$, the $\vec u$'s span $\mathbb R^{n+1}$ and $\vec u_1+\dots+\vec u_{n+2}=\vec 0$.  Similarly, there exist vectors $\vec v_1,\dots,\vec v_{n+2}$ which represent the $q_j$, span $\mathbb R^{n+1}$ and sum to zero.

Let $\psi_1:\mathbb R^{n+2}\to\mathbb R^{n+1}$ be the linear map sending $\vec e_j\mapsto\vec u_j$ for each $j=1,2,\dots,n+2$.  $\psi_1$ is surjective due to the $\vec u$'s spanning $\mathbb R^{n+1}$.  By Proposition 3.7(ii) (or the rank-nullity theorem), $(c_1,\dots,c_{n+2})\in\ker\psi_1\iff c_1=c_2=\dots=c_{n+2}$; in other words, $\ker\psi_1=\operatorname{span}(1,1,\dots,1)$.  Moreover, by Theorem 1.17, $\psi_1$ induces an isomorphism $\overline{\psi_1}$ from the quotient space $\mathbb R^{n+2}/\operatorname{span}(1,1,\dots,1)$ to $\mathbb R^{n+1}$.  We have $\overline{\psi_1}(\overline{\vec e_j})=\vec u_j$ for each $j$.

Repeating the above argument with the $\vec v$'s in place of the $\vec u$'s gives an isomorphism $\overline{\psi_2}:\mathbb R^{n+2}/\operatorname{span}(1,1,\dots,1)\to\mathbb R^{n+1}$ satisfying $\overline{\psi_2}(\overline{\vec e_j})=\vec v_j$ for each $j$.  Let $\varphi=\psi_2\circ\psi_1^{-1}$; this is a vector space isomorphism $\mathbb R^{n+1}\to\mathbb R^{n+1}$, hence induces a projective transformation $T$ on $P^n(\mathbb R)$.  By construction, $\varphi(\vec u_j)=\vec v_j$ for all $j$, hence $T(p_j)=q_j$.

What remains is to show that $T$ is unique.  Suppose $T':P^n(\mathbb R)\to P^n(\mathbb R)$ is also a projective transformation such that $T'(p_j)=q_j$ for all $j$.  Represent $T'$ by a linear map $\varphi':\mathbb R^{n+1}\to\mathbb R^{n+1}$.  Since $T([\vec u_j])=[\vec v_j]$, there exists $c_j\ne 0$ in $\mathbb R$ such that $\varphi'(\vec u_j)=c_j\vec v_j$.  Now,
$$\vec 0=\varphi'(\vec 0)=\varphi'(\vec u_1+\dots+\vec u_{n+2})=c_1\vec v_1+\dots+c_{n+2}\vec v_{n+2},$$
from which $c_1=\dots=c_{n+2}$ follows from Proposition 3.7(ii).  Letting $c=c_1(=\dots=c_{n+2})$, we have $\varphi'(\vec u_j)=c\vec v_j$ for all $j$.  Since the $\vec u$'s span, we conclude that $\varphi'=c\varphi$ (because the linear maps agree on all of the $\vec u$'s).  Therefore, $\varphi',\varphi$ induce the same projective transformation and $T'=T$, proving uniqueness.
\end{proof}
\noindent The special case where $n=1$ (so that a projective frame is any ordered triple of distinct points), gives Exercise 7 of Section 3.2.

The reason why the theorem is fundamental is this: whenever we have a construction made out of projective intrinsics, and we observe a projective frame from it, we may (by applying a projective transformation) assume this projective frame to be whatever will make things easy for us.  It will help for the projective frame to have several points at infinity, since lines through an infinity point are parallel to the Euclidean eye.  With any event, we will be using this to our advantage in Section 3.7.

\subsection*{Exercises 3.4. (The Fundamental Theorem of Projective Geometry)} % https://en.wikipedia.org/wiki/Homography#Fundamental_theorem_of_projective_geometry
\begin{enumerate}
\item If $(p_1,\dots,p_{n+2})$ is a projective frame, and $\sigma\in S_{n+2}$, then $(p_{\sigma(1)},\dots,p_{\sigma(n+2)})$ is also a projective frame.  Use this to construct an injective homomorphism from $S_{n+2}$ to the group $PGL_{n+1}(\mathbb R)$ of projective transformations of $P^n(\mathbb R)$.

\item\emph{(Pappus' Hexagon Theorem)} \---- In $P^2(\mathbb R)$, suppose $\ell$ is a line with points $A,B,C$, and $\ell'$ is a line with points $A',B',C'$.  If $D=\overset{\longleftrightarrow}{AB'}\cap\overset{\longleftrightarrow}{A'B}$, $E=\overset{\longleftrightarrow}{AC'}\cap\overset{\longleftrightarrow}{A'C}$ and $F=\overset{\longleftrightarrow}{BC'}\cap\overset{\longleftrightarrow}{B'C}$, show that the points $D,E,F$ are collinear.  [We may assume $A,A',D,\ell\cap\ell'$ form a projective frame \---- if, contrariwise, any three of them are collinear, then the problem is easy.  By Theorem 3.8, we may further assume that $A=(-1,1),A'=(-1,-1),D=(0,0)$ and $\ell\cap\ell'=[1:0:0]$.]

This is a sample use of Proposition 3.8; we will delve more into results like these in Section 3.7.

\item Every projective frame of $P^n(\mathbb R)$ must have at least two points away from infinity.  Show this in two ways:

(a) Directly from the definition.  [Think of the hyperplane at infinity.]

(b) Using Theorem 3.8.  [Exercise 12(b) classifies the projective transformations which fix every point at infinity.  Use this to show that a projective frame with at most one point away from infinity cannot exist.]

\item Tell whether each of the following quadruples is a projective frame in $P^2(\mathbb R)$.  For the ones that are, represent them with nonzero vectors in $\mathbb R^3$ which span and add to zero.

(a) $((0,0),(0,1),(1,0),(1,1))$

(b) $((0,0),(0,1),(1,0),(0,2))$

(c) $((0,0),(0,1),(1,0),(5,17))$

(d) $((0,0),(0,1),(2,0),[3:1:0])$

(e) $((0,0),(13,1),(0,0),(3,7))$

(f) $((0,0),(0,1),[1:0:0],[0:1:0])$

(g) $((0,0),(0,1),[1:0:0],[1:1:0])$

\item In each case, explicitly describe the projective transformation of $P^2(\mathbb R)$ which sends the first projective frame to the second one.

(a) $((0,0),(0,1),(1,0),(1,1)) \mapsto ((0,0),(0,1),(1,1),(1,0))$

(b) $((0,0),(0,1),(1,0),(1,1)) \mapsto ((0,0),(0,1),(2,0),(3,3))$

(c) $((0,0),(0,1),(1,0),(1,1)) \mapsto ((0,0),(0,1),[1:0:0],[1:1:0])$

(d) $((0,0),(0,1),(2,0),(3,3)) \mapsto ((-5,-5),(-5,5),(5,-5),(5,5))$

\item Explain why $((0,0),[1:0:0],[1:1:0],[0:1:0])$ is \emph{not} a projective frame of $P^2(\mathbb R)$.

\item Let $(p_1,\dots,p_{n+2})$ be a projective frame of $P^n(\mathbb R)$.  For $k<n$, show that no $k+2$ of the points $p_j$ lie in a common $k$-plane.  [Represent the points using nonzero vectors in $\mathbb R^{n+1}$ and use linear algebra.]  Note that this statement is obviously false if $k=n$.

\item Let $\ell_1$ and $\ell_2$ be distinct lines in $P^2(\mathbb R)$, $p$ a point on neither line, and fix $r\in\mathbb R$, $r\ne 0,1$.  Let $S$ be the set of points $q\in P^2(\mathbb R)$ such that if $\ell=\overset{\longleftrightarrow}{p~q}$, then $[\![p,q,\ell\cap\ell_1,\ell\cap\ell_2]\!]=r$.  Show that $S$ is a line which is concurrent with $\ell_1,\ell_2$, but missing the point of concurrence.  [Assume $\ell_1\cap\ell_2=[1:0:0]$.]
\end{enumerate}

\subsection*{3.5. Conics}
\addcontentsline{toc}{section}{3.5. Conics}
For the remaining sections of this chapter, we will stick to the projective plane $P^2(\mathbb R)$.

In the Euclidean plane, we recall that a circle is the set of points with a given distance $r$ from a given point $O$.  They are the basic curves that are not lines, and many things have been proven about them.

In the projective plane, however, there isn't a notion of distance between two points.  Instead, there is the cross ratio between four points, but the points have to be collinear, so we can't get clever ``loci'' out of them directly.  Our best attempt for a one-dimensional locus is obtained by letting two of the three points move along lines, but by Exercise 8 of the previous section, that will get us nothing but another line.

We may, however, still consider the circle $x^2+y^2=1$ when it is embedded into $P^2(\mathbb R)$.  What is a projectively-intrinsic description of this thing?  To answer this question, we transfer the setting to $\mathbb R^3$.  The reader can readily verify that the circle becomes the surface given by $x^2+y^2=z^2$, which is a cone.\footnote{Remember, if $z\ne 0$, then $[x:y:z]=[x/z:y/z:1]=(x/z,y/z)$ in $P^2(\mathbb R)$.}

If $\vec x=(x,y,z)$ and $A=\begin{bmatrix}1&0&0\\0&1&0\\0&0&-1\end{bmatrix}$, the cone can alternatively be given by the equation $\vec x\cdot A\vec x=0$.  This leads to the following concept: if $A$ is an $n\times n$ real matrix, then the map $\vec x\mapsto\vec x\cdot A\vec x$ from $\mathbb R^n\to\mathbb R$ is called a \textbf{quadratic form}.  Note that this is a homogeneous degree-$2$ polynomial in the components of $\vec x$.  $A$ may be assumed to be a \emph{symmetric} matrix: it is readily verified that $\vec x\cdot A\vec x=\vec x\cdot A^T\vec x=\vec x\cdot\left(\frac{A+A^T}2\right)\vec x$, and $\frac{A+A^T}2$ is symmetric.

Whenever $A$ is a symmetric $n\times n$ matrix, the equation $\vec x\cdot A\vec x=0$ gives either the origin alone, or else a union of lines in $\mathbb R^n$ through the origin (why?).  In the latter case, the lines spell out a quadric hypersurface in $P^{n-1}(\mathbb R)$.

In this section we shall stick to the case where $n=3$.  In this case we have conic sections in the projective plane.  More succinctly, a \textbf{conic section} is a nonempty subset of $P^2(\mathbb R)$ given by the equation $\vec x\cdot A\vec x=0$, where $A$ is a nonsingular symmetric $3\times 3$ real matrix.

Perhaps the most surprising thing about conics is that projective transformations act transitively on them.  Circles, ellipses, parabolas and hyperbolas are all the same in $P^2(\mathbb R)$.\\

\noindent\textbf{Proposition 3.9.} \emph{Let $\omega$ be the unit circle.  If $\omega_1$ is an arbitrary conic in $P^2(\mathbb R)$, then there exists a projective transformation $T$ such that $T(\omega)=\omega_1$.  Therefore, the projective transformations act transitively on the conics.} % Yes, it'll use linear forms, which will be covered in the first few Exercises.
\begin{proof}
Let $\omega_1$ be given by $\vec x\cdot A\vec x=0$.  Since $A$ is nonsingular, all its eigenvalues are nonzero, and real by Exercise 1(c), hence its signature (Exercise 3) is an odd integer.  Depending on the signature, $A$ is congruent to one of the following matrices:
$$\begin{bmatrix}1\\&1\\&&1\end{bmatrix},~~~~\begin{bmatrix}1\\&1\\&&-1\end{bmatrix},~~~~\begin{bmatrix}1\\&-1\\&&-1\end{bmatrix},~~~~\begin{bmatrix}-1\\&-1\\&&-1\end{bmatrix}$$
Represent such a matrix as $P^TAP$ with $P\in GL_3(\mathbb R)$.  If $P^TAP=I_3$ (the first matrix), then $A$ is positive definite, which means $\vec x\ne\vec 0\implies\vec x\cdot A\vec x>0$, and $\omega_1$ does not contain any points of $P^2(\mathbb R)$.  By our definition, then, $\omega_1$ is not a conic.  Likewise, $P^TAP$ cannot be the fourth matrix $-I_3$ either.

If $P^TAP=D=\begin{bmatrix}1\\&1\\&&-1\end{bmatrix}$, then for any $\vec x=(x,y,z)\ne\vec 0$,
$$(P\vec x)\cdot A(P\vec x)=0\iff P\vec x\cdot AP\vec x=0\iff \vec x\cdot P^TAP\vec x=0$$
$$\iff \vec x\cdot D\vec x=0\iff x^2+y^2=z^2$$
This means that if $[\vec x]\in P^2(\mathbb R)$, then $[P\vec x]\in\omega_1$ if and only if $[\vec x]$ is on the unit circle $\omega$.  If $T$ is the projective transformation given by $P$, it follows that $T(\omega)=\omega_1$ as desired.

If $P^TAP=\begin{bmatrix}1\\&-1\\&&-1\end{bmatrix}$, then taking $Q$ to be the orthogonal matrix $\begin{bmatrix}&&1\\1\\&1\end{bmatrix}$, we have $(PQ)^TA(PQ)=Q^T(P^TAP)Q=Q^T\begin{bmatrix}1\\&-1\\&&-1\end{bmatrix}Q=\begin{bmatrix}-1\\&-1\\&&1\end{bmatrix}=-D$.  Thus, adapting the previous argument shows $[PQ\vec x]\in\omega_1$ if and only if $[\vec x]\in\omega$; hence this time, let $T$ be the projective transformation given by $PQ$.
\end{proof}

\noindent We have just studied conics from a linear-algebra point of view.  From this point onwards, we shall think of them in projective setting.

First observe that if $\ell$ is a line and $\omega$ is a conic, then $\ell\cap\omega$ can consist of zero, one or two points, but no more.  This is easy to prove if $\omega$ is the unit circle; and then Proposition 3.9 entails it for arbitrary conics.  If $|\ell\cap\omega|=1$ then $\ell$ is said to be \textbf{tangent} to $\omega$; and if $|\ell\cap\omega|=2$ then $\ell$ is said to be \textbf{secant} to $\omega$.

Let $\ell_\infty$ be the line at infinity.  If $\ell_\infty\cap\omega=\varnothing$, $\omega$ is called an \textbf{ellipse}; if $\ell_\infty$ is tangent to $\omega$, $\omega$ is called a \textbf{parabola}; and if $\ell_\infty$ is secant to $\omega$ then $\omega$ is called a \textbf{hyperbola}.  This is not a projectively intrinsic property (because the line at infinity is not); however, it is clear from the definitions that these properties are preserved by affine transformations (Exercise 12 of Section 3.2).

We can, in fact, relate this to Exercise 9 of Section 2.6.  A quadratic curve in $\mathbb R^2$ is given by $\mathfrak ax^2+\mathfrak bxy+\mathfrak cy^2+\mathfrak dx+\mathfrak ey+\mathfrak f=0$, where $\mathfrak a,\mathfrak b,\mathfrak c,\mathfrak d,\mathfrak e,\mathfrak f\in\mathbb R$ and $\mathfrak a,\mathfrak b,\mathfrak c$ are not all zero.  If we view it in the projective plane, it contains all $[x:y:z]\in\mathbb R^3$ with $z\ne 0$ satisfying:
\begin{equation}\tag{*}\mathfrak ax^2+\mathfrak bxy+\mathfrak cy^2+\mathfrak dxz+\mathfrak eyz+\mathfrak fz^2=0\end{equation}
because when $z\ne 0$, $[x:y:z]=(x/z,y/z)$, and dividing by $z^2$ throughout shows that this point is in the Euclidean plane curve.  It is thus natural for us to extend this equation (*) to points of the form $[x:y:0]$.  Since (*) is a homogeneous polynomial, it indeed gives a well-defined subset of $P^2(\mathbb R)$, which is obtained by (possibly) adjoining infinity points to the Euclidean plane curve.

Note that if $A$ is the symmetric matrix $\begin{bmatrix}\mathfrak a&\frac 12\mathfrak b&\frac 12\mathfrak d\\\frac 12\mathfrak b&\mathfrak c&\frac 12\mathfrak e\\\frac 12\mathfrak d&\frac 12\mathfrak e&\mathfrak f\end{bmatrix}$, then the above equation is tantamount to $\vec x\cdot A\vec x=0$ for $\vec x=(x,y,z)$.  However, $A$ may be singular, and in this case the quadratic curve is really a union of two lines.  We will assume that $A$ is nonsingular, and hence we have a conic in $P^2(\mathbb R)$ in the original sense that we defined.

It is clear that an infinity point $[x:y:0]$ is on the conic if and only if $\mathfrak ax^2+\mathfrak bxy+\mathfrak cy^2=0$.  This is a homogeneous quadratic polynomial in two variables, which can be sorted by completing the square:
$$\mathfrak ax^2+\mathfrak bxy+\mathfrak cy^2=0$$
$$\mathfrak a^2x^2+\mathfrak a\mathfrak bxy+\mathfrak a\mathfrak cy^2=0$$
$$\mathfrak a^2x^2+\mathfrak a\mathfrak bxy=-\mathfrak a\mathfrak cy^2$$
$$\mathfrak a^2x^2+\mathfrak a\mathfrak bxy+\frac{\mathfrak b^2}4y^2=\left(\frac{\mathfrak b^2}4-\mathfrak a\mathfrak c\right)y^2$$
$$\left(\mathfrak ax+\frac{\mathfrak b}2y\right)^2=\left(\frac{\mathfrak b^2-4\mathfrak a\mathfrak c}4\right)y^2$$
Thus we consider $\mathfrak b^2-4\mathfrak a\mathfrak c$ the discriminant of the conic.

If $\mathfrak b^2-4\mathfrak a\mathfrak c<0$, the equation has no real solution, because the right-hand side must either be negative (contradicting that the left-hand side is a square), or zero (in which case $x=y=0$ follows, which fails to hold water because $[0:0:0]$ is not defined in $P^2(\mathbb R)$).  Thus in this case, there are no infinity points on the conic, so the conic is an ellipse.  This is identical to what has been stated in Exercise 9 of Section 2.6.

If $\mathfrak b^2-4\mathfrak a\mathfrak c=0$, then we must have $\mathfrak ax+\frac{\mathfrak b}2y=0$.  If $\mathfrak a$ and $\mathfrak b$ are both zero, then $0=0x^2+0xy+\mathfrak cy^2=\mathfrak cy^2$ from the original equation.  Hence $y=0$ (since $\mathfrak c\ne 0$; by the given conditions $\mathfrak a,\mathfrak b,\mathfrak c$ cannot all be zero).  In this case $[1:0:0]$ is the unique infinity point on the conic.  However, if $\mathfrak a,\mathfrak b$ are not both zero, then the equation $\mathfrak ax+\frac{\mathfrak b}2y=0$ (in the $xy$-plane) gives a line through the origin, which, when transferred from $\mathbb R^3$ to $P^2(\mathbb R)$, is the unique infinity point on the conic.  So if $\mathfrak b^2-4\mathfrak a\mathfrak c=0$, there is exactly one infinity point on the conic, making it a parabola.

If $\mathfrak b^2-4\mathfrak a\mathfrak c>0$, then there are two infinity points on the conic, as we see: If $\mathfrak a=0$, then the cross section of the conic in the $xy$-plane in $\mathbb R^3$ has equation $\mathfrak bxy+\mathfrak cy^2=0$, or $y(\mathfrak bx+\mathfrak cy)=0$.  Also, $\mathfrak b\ne 0$ (because $\mathfrak b^2=\mathfrak b^2-4\mathfrak a\mathfrak c>0$).  Thus, the infinity points are the distinct lines given by $y=0$ and $x=-\frac{\mathfrak c}{\mathfrak b}y$.

Now suppose $\mathfrak a\ne 0$.  Then we must have $y\ne 0$ (if $y=0$ then $\mathfrak ax^2=0$, hence $x=0$ since $\mathfrak a\ne 0$, but this is a contradiction because $[0:0:0]$ does not exist).  Letting $u=\frac xy$ and dividing the last equation by $y^2$ throughout gives $\left(\mathfrak au+\frac{\mathfrak b}2\right)^2=\frac{\mathfrak b^2-4\mathfrak a\mathfrak c}4$.  Since the right side is strictly positive, we get that $\mathfrak au+\frac{\mathfrak b}2$ is one of two distinct square roots $\frac{\pm\sqrt{\mathfrak b^2-4\mathfrak a\mathfrak c}}2$, so that $u=\frac{-\mathfrak b\pm\sqrt{\mathfrak b^2-4\mathfrak a\mathfrak c}}{2\mathfrak a}$ (which hopefully looks familiar).  In each case $x=uy$ gives an infinity point on the conic, and these are the only two.  Hence the conic is a hyperbola.

Here are a few pictures of some conics.
\begin{center}
\begin{tabular}{ccc}
\includegraphics[scale=.5]{ProjEllipse.png} &
\includegraphics[scale=.3]{ProjParabola.png} &
\includegraphics[scale=.25]{ProjHyperbola.png} \\
Ellipse, given by &
Parabola, given by &
Hyperbola, given by \\
$\frac{x^2}9+\frac{y^2}4=1$ & $x^2=4y$ & $\frac{y^2}4-\frac{x^2}9=1$
\end{tabular}
\end{center}
On the above parabola, the infinity point is the same infinity point that the $y$-axis possesses.  On the above hyperbola, the infinity points are the infinity points on the projective lines given by $y=\pm\frac 23x$, the asymptotes of the hyperbola.  It is clear from these examples how a general conic looks in the projective plane.\\

\noindent\textbf{INTERIOR POINTS OF A CONIC}\\

\noindent In the Euclidean plane, the interior of the circle $x^2+y^2=1$ is given by the inequality $x^2+y^2<1$.  It is the open disk centered at the origin, and is preserved by any isometry of $\mathbb R^2$.  Projective conics have a similar notion, which is surprisingly intrinsic:\\

\noindent\textbf{Definition.} \emph{Let $\omega$ be a conic, and $p\in P^2(\mathbb R)-\omega$.  If every line through $p$ intersects $\omega$, $p$ is said to be an \textbf{interior point} of the conic $\omega$.  If at least one line through $p$ is disjoint from $\omega$, $p$ is said to be an \textbf{exterior point} of $\omega$.}\\

\noindent Note that points of $\omega$ are not considered to be either interior or exterior.  From this definition, it is clear that if $T$ is a projective transformation, then $T$ sends interior (resp., exterior) points of $\omega$ to interior (resp., exterior) points of $T(\omega)$.

Now if $\omega$ is the unit circle, then the reader can easily verify that a point $p\notin\omega$ is interior if and only if $p=(x,y)$ with $x^2+y^2<1$, and $p$ is exterior if and only if either $p\in\ell_\infty$ or $p=(x,y)$ with $x^2+y^2>1$.  Thus the interior of the circle is exactly what we see it as in the Euclidean plane.  By the previous paragraph, every projective transformation which fixes the unit circle \emph{must} preserve these interior points, hence also fix the closed disk.  [This is worth contrasting with the unit circle of the Riemann sphere, which will be introduced in Chapter 4.]

Here are the pictures of the previously shown conics, with the interiors shaded.
\begin{center}
\begin{tabular}{ccc}
\includegraphics[scale=.5]{ProjEllipse_int.png} &
\includegraphics[scale=.3]{ProjParabola_int.png} &
\includegraphics[scale=.25]{ProjHyperbola_int.png} \\
Interior of ellipse, &
Interior of parabola, &
Interior of hyperbola, \\
given by $\frac{x^2}9+\frac{y^2}4<1$ & given by $x^2<4y$ & given by $\frac{y^2}4-\frac{x^2}9>1$
\end{tabular}
\end{center}

The unit circle in the projective plane $P^2(\mathbb R)$, along with its interior, will play a major role in Section 4.8.

\subsection*{Exercises 3.5. (Conics)}
\begin{enumerate}
\item\emph{(Spectral Theorem.)} \---- (a) If $A$ is an $n\times n$ matrix with real entries, and there is an orthogonal matrix $S$ such that $S^{-1}AS$ is diagonal (in this case we say that $A$ is \textbf{orthogonally diagonalizable}), show that $A$ is symmetric.

The aim of the rest of this exercise is prove the converse.  We assume $A$ is a symmetric matrix.  We wish to prove that it is orthogonally diagonalizable.

(b) If $\vec v_1$ and $\vec v_2$ are eigenvectors of $A$ with distinct eigenvalues $\lambda_1\ne\lambda_2$, show that $\vec v_1\cdot\vec v_2=0$.  [Recall that $A\vec v_1\cdot\vec v_2=\vec v_1\cdot A^T\vec v_2$.  Use the bilinearity of the dot product and the fact that $A$ is symmetric.]

(c) Show that all complex eigenvalues of $A$ are real.  [Suppose $p+iq$ (with $p,q\in\mathbb R,q\ne 0$) is an eigenvalue.  Express its eigenvector as $\vec a+i\vec b$, where $\vec a,\vec b\in\mathbb R^n$.  Then $p-iq$ is an eigenvalue with eigenvector $\vec a-i\vec b$ (why?).  Adapt the proof of part (b) to show that the bilinear map $((z_1,\dots,z_n),(w_1,\dots,w_n))\mapsto z_1w_1+\dots+z_nw_n$ from $\mathbb C^n\times\mathbb C^n\to\mathbb C$ sends $(\vec a+i\vec b,\vec a-i\vec b)$ to zero.\footnote{This bilinear map is \emph{not} an inner product; an inner product of complex vector spaces is merely sesquilinear, like $z_1\overline{w_1}+\dots+z_n\overline{w_n}$.  Some complex conjugation (the $\overline{w_j}$) must appear in the formula for a complex inner product.}  Yet the bilinear map also sends it to $\|\vec a\|^2+\|\vec b\|^2$; why?]

(d) Now show that $A$ is orthogonally diagonalizable.  [$A$ has an eigenvalue $\lambda$, which must be real by part (c).  Let $\vec v$ be an eigenvector of $\lambda$; by dividing it by its magnitude, assume $\|\vec v\|=1$.  Use the Gram-Schmidt process to extend $\vec v$ to an orthonormal basis.  Putting these vectors together as columns, we get an orthogonal matrix $S$ with $\vec v$ at the left column.  Then $S^{-1}AS$ has $(\lambda,0,\dots,0)$ as its left column, hence also as its top row, because it is symmetric.  Thus, $S^{-1}AS=\begin{bmatrix}\lambda&0\\0&B\end{bmatrix}$ where $B$ is an $(n-1)\times(n-1)$ matrix.  Now use induction.]

\item\emph{(Definiteness of Matrices.)} \---- Let $A$ be a symmetric $n\times n$ matrix with real entries.  By Exercise 1, it is orthogonally diagonalizable.  Thus, it has some orthonormal eigenbasis $\vec u_1,\dots,\vec u_n$; let $\lambda_1,\dots,\lambda_n\in\mathbb R$ be the respective eigenvalues.

(a) If $\vec x=a_1\vec u_1+\dots+a_n\vec u_n$, show that $\vec x\cdot A\vec x=\lambda_1a_1^2+\dots+\lambda_na_n^2$.

Now show that:

(b) The $\lambda_j$ are all positive $\iff$ $\vec x\cdot A\vec x>0$ for all $\vec x\ne\vec 0$.  [In this case $A$ is \textbf{positive definite}.]

(c) The $\lambda_j$ are all nonnegative $\iff$ $\vec x\cdot A\vec x\geqslant 0$ for all $\vec x$.  [In this case $A$ is \textbf{positive semidefinite}.]

(d) The $\lambda_j$ are all negative $\iff$ $\vec x\cdot A\vec x<0$ for all $\vec x\ne\vec 0$.  [In this case $A$ is \textbf{negative definite}.]

(e) The $\lambda_j$ are all nonpositive $\iff$ $\vec x\cdot A\vec x\leqslant 0$ for all $\vec x$.  [In this case $A$ is \textbf{negative semidefinite}.]

(f) There exist both positive and negative $\lambda_j$ $\iff$ $\vec x\cdot A\vec x$ is positive for some $\vec x$ and negative for some $\vec x$.  [In this case $A$ is \textbf{indefinite}.]

(g) Show that $\vec x\cdot A\vec x=0$ gives a quadric hypersurface in $P^{n-1}(\mathbb R)$ (in other words, it is nonempty) if and only if $A$ is neither positive definite nor negative definite.

\item If $A$ and $B$ are symmetric $n\times n$ matrices with real entries, then $A$ is said to be \textbf{congruent} to $B$ if there exists a nonsingular matrix $P$ with real entries, such that $B=P^TAP$.  [Note that $A$ and $B$ need not be similar matrices, as $P^T$ is generally different from $P^{-1}$.]  It is clear that this is an equivalence relation.

(a) Congruent matrices have the same rank.

(b) The \textbf{signature} of a symmetric matrix is defined to be the number of positive eigenvalues minus the number of negative ones, counted with multiplicity.  [Any eigenvalues of zero are ignored in the case that the matrix is singular.]  For example, if $A=\begin{bmatrix}2\\&3\\&&-4\end{bmatrix}$, then $A$ has two positive and one negative eigenvalue, so its signature is $2-1=1$.  The identity matrix $I_n$ has signature $n$.  Note that if there are more negative eigenvalues than positive ones, the signature is negative.

If $A$ and $B$ are symmetric $n\times n$ matrices with real entries, show that $A$ and $B$ are congruent if and only if they have the same rank and signature.  [Use the Spectral Theorem, and think about what diagonal matrices would be congruent to.]  This is \textbf{Sylvester's Law of Inertia}.

(c) If $A$ and $B$ are congruent matrices, then there is an invertible linear map $\varphi:\mathbb R^n\to\mathbb R^n$, which sends the set $\vec x\cdot A\vec x=0$ exactly to the set $\vec x\cdot B\vec x=0$.

\item A conic in $P^2(\mathbb R)$ is a hyperbola if and only if some of its interior points are at infinity.

\item Let $\omega$ be a conic, and $p\in P^2(\mathbb R)$.

(a) If $p$ is an interior point, there is no line through $p$ tangent to $\omega$.

(b) If $p\in\omega$, there is exactly one line through $p$ tangent to $\omega$.

(c) If $p$ is an exterior point, there are exactly two lines through $p$ tangent to $\omega$.

\item If $\omega_1$ and $\omega_2$ are distinct conics, show that $|\omega_1\cap\omega_2|\leqslant 4$.  [By Proposition 3.9, we may assume $\omega_1$ is the unit circle.]  This is a special case of Bezout's Theorem.

\item Let $\omega$ be a conic, and $G$ be the group of projective transformations that fix $\omega$.  Show that $G$ acts transitively on each of the following sets:

(a) The set of points on $\omega$;

(b) The set of interior points;

(c) The set of exterior points.

\item\emph{(Sylvester's Criterion.)} \---- Let $A=\begin{bmatrix}a_{11}&\dots&a_{1n}\\\vdots&\ddots&\vdots\\a_{1n}&\dots&a_{nn}\end{bmatrix}$ be an $n\times n$ symmetric matrix with real entries.  Show that the following are equivalent:

~~~~(i) $A$ is positive definite;

~~~~(ii) $A=B^2$ for some nonsingular symmetric matrix $B$ with real entries;

~~~~(iii) $A=B^TB$ for some nonsingular matrix $B$ with real entries;

~~~~(iv) For each $1\leqslant k\leqslant n$, the upper-left hand submatrix $\begin{bmatrix}a_{11}&\dots&a_{1k}\\\vdots&\ddots&\vdots\\a_{1k}&\dots&a_{kk}\end{bmatrix}$ has a positive determinant.

[(i) $\implies$ (ii): Pick an orthonormal eigenbasis of $A$, and use it to define the linear operator $B$.  (ii) $\implies$ (iii) because if $B$ is symmetric, $B^2=B^TB$.  (iii) $\implies$ (i): verify that $\vec x\cdot A\vec x=\|B\vec x\|^2$.  (iii) $\implies$ (iv): the $k\times k$ upper-left hand submatrix of $A$ is $P^TP$, where $P$ is the $n\times k$ matrix consisting of the first $k$ columns of $B$.  This is positive definite since $\vec x\cdot P^TP\vec x=\|P\vec x\|^2$; now look back at Exercise 2.  (iv) $\implies$ (iii): Note that the entries of $A$ are supposed to be the dot products of $B$'s columns.  Now use induction on $n$ to show that there exists an \emph{upper triangular} matrix $B$ such that $A=B^TB$.] % I thought (ii) $\implies$ (iii) would be obvious, but I'll add it.

Sylvester's Criterion states the equivalence (i) $\iff$ (iv).

\item\emph{(Cross Ratios on a Conic.)} \---- Let $\omega$ be a conic in $P^2(\mathbb R)$.

(a) Let $p\in\omega$ be a point, and $\ell$ any line of $P^2(\mathbb R)$ not containing $p$.  We define \textbf{stereographic projection} $\varphi:\omega\to\ell$ as follows.  If $q\ne p$ is a point in $\omega$, $\varphi(q)=\overset{\longleftrightarrow}{p~q}\cap\ell$.  Finally, set $\varphi(p)=\ell'\cap\ell$ where $\ell'$ is the line through $p$ tangent to $\omega$.  Show that this is a bijection from $\omega$ to $\ell$.  [It is, in fact, a continuous one, but the details of that will not be important to us.]

Now let $a,b,c,d\in\omega$ be points.  Assume that given any three of them, they are not all equal.

We define the \textbf{conic cross ratio} $[\![a,b,c,d]\!]_\omega$ as follows: Let $p\in\omega$ be a point, and $\ell$ any line of $P^2(\mathbb R)$ not containing $p$.  Then if $\varphi:\omega\to\ell$ is the stereographic projection of part (a), we set $[\![a,b,c,d]\!]_\omega=[\![\varphi(a),\varphi(b),\varphi(c),\varphi(d)]\!]$.

(b) Show that this cross ratio depends on only the conic $\omega$ and the points $a,b,c,d$, and does not depend on the point $p$ or the line $\ell$.  [Verify that the stereographic projections from the same point to different lines differ by the perspective projection between the lines from the point.  By Proposition 3.3, this shows independence of the line $\ell$.  To show independence of $p$, assume $\omega$ is the unit circle.  Show that a (Euclidean) rotation of the circle, when conjugated by the stereographic projection, becomes a projective transformation of the line.  Use this to show that a rotation of $a,b,c,d$ leaves the cross ratio unchanged.  Yet changing $p$ can be achieved by rotating $a,b,c,d$ and then applying a projective transformation of $P^2(\mathbb R)$ to everything: why?]

(c) The cross ratios on the conic satisfy Proposition 3.6(i)-(iii).  Also, if $T:P^2(\mathbb R)\to P^2(\mathbb R)$ is a projective transformation, $\omega$ is a conic and $a,b,c,d\in\omega$, then $[\![T(a),T(b),T(c),T(d)]\!]_{T(\omega)}=[\![a,b,c,d]\!]_\omega$.

(d) Let $\omega_1$ be another conic also containing $a,b,c,d$.  Are $[\![a,b,c,d]\!]_\omega$ and $[\![a,b,c,d]\!]_{\omega_1}$ necessarily equal?  Prove or give a counterexample.
\end{enumerate}

\subsection*{3.6. Duality}
\addcontentsline{toc}{section}{3.6. Duality}
After reading the statements of Proposition 3.1, you may have observed a similarity between them.  Part (i) associates each pair of points to a line; part (ii) associates each pair of lines to a point.  There is a clever correlation between these: one can correspond each point with a line, such that if two points correspond to two lines, then the line through the points corresponds to the intersection point of the two lines.  This will be the main theme of this section.

The concept is referred to as \textbf{duality} in the projective plane.  If $\mathcal L^2$ is the set of lines in $P^2(\mathbb R)$ [including the line at infinity], then a \textbf{duality correspondence} is defined to be a bijection $\sigma:P^2(\mathbb R)\to\mathcal L^2$, such that whenever $p,q\in P^2(\mathbb R)$, we have $p\in\sigma(q)$ if and only if $q\in\sigma(p)$.  In other words, it makes each point correspond to a line in such a way, that a point $p$ is on a line $\ell$ if and only if the line corresponding to $p$ contains the point corresponding to $\ell$.

The reader can verify that a duality correspondence matches the two things in each row of the following chart.  Here, it is assumed that $p_0$ is the point corresponding to the line at infinity.
\begin{center}
\begin{tabular}{c|c}
A point&A line\\\hline
A point on a line&A line containing a point\\\hline
Line determined by two points&Intersection of two lines\\\hline
Collinear points&Concurrent lines\\\hline
$p_0$&Line at infinity\\\hline
Points at infinity&Lines through $p_0$
\end{tabular}
\end{center}
It is easy to show that duality correspondences exist; here is one of them.  Let us use the standard dot product of $\mathbb R^3$.  Whenever $V$ is a vector subspace of $\mathbb R^3$, the set $V^\perp=\{\vec x\in\mathbb R^3:\vec x\cdot\vec v=0\text{ for all }\vec v\in V\}$, called the \textbf{orthogonal complement} of $V$, is a vector subspace such that $\mathbb R^3=V\oplus V^\perp$.  Moreover, $\dim(V)+\dim(V^\perp)=3$.  Thus if $\dim(V)=1$ then $\dim(V^\perp)=2$, so that $V\mapsto V^\perp$ is a correspondence from one-dimensional subspaces to two-dimensional subspaces; i.e., from points to lines in $P^2(\mathbb R)$.  Yet also, since $(V^\perp)^\perp=V$ (verify!), we have the correspondence $V\mapsto V^\perp$ from two-dimensional to one-dimensional subspaces (i.e., lines to points), and it is a two-sided inverse of the first one.

Thus the orthogonal complement in $\mathbb R^3$ gives a bijection between points and lines in $P^2(\mathbb R)$.  Since $V\subset W$ implies $ V^\perp\supset W^\perp$ (because if $\vec x\in W^\perp$, $\vec x\cdot\vec v=0$ for all $\vec v\in W$, hence in particular if $\vec v\in V$, and therefore $\vec x\in V^\perp$), this bijection indeed satisfies the law that a point is on a line $\iff$ the corresponding line is on the corresponding point.  Thus we have a duality correspondence.  [Note that $p_0=(0,0)$ in this case, as the line spanned by $(0,0,1)$ has the $xy$-plane as its orthogonal complement; i.e., the line at infinity.]  Here is an example of a diagram and its dual, color-coded.  The lines have only been partially drawn, to make the concept of what we are going over clear to the reader.

\begin{center}
\includegraphics[scale=.4]{DiagAndDual.png}

\emph{Red point (on left) is $(-\frac 32,-1)$.  Red line (on right) is $3x+2y=2$.}

\emph{Green line (on left) is $4x-3y=-3$.  Green point (on right) is $(\frac 43,-1)$.}

\emph{Blue point (on left) is $(0,1)$.  Blue line (on right) is $y=-1$.}

\emph{Purple line (on left) is $3x+2y=2$.  Purple point (on right) is $(-\frac 32,-1)$.}

\emph{Orange point (on left) is $(1,-\frac 12)$.  Orange line (on right) is $2x-y=-2$.}
\end{center}

A duality correspondence can turn any intrinsic-geometry statement about the projective plane, into an ``opposite'' statement where the lines take the role of the points and the points take the role of the lines.  Simply make the replacements in the above chart, along with any other correspondences deduced from the main property that a point is on a line $\iff$ the corresponding line contains the corresponding point.  Thus we have:\\

\noindent\textbf{Principle of Duality.} \emph{If any statement in the projective plane is true, so is the \textbf{dual} statement obtained by exchanging ``points'' with ``lines'', ``point on line'' with ``line containing point'', ``line determined by two points'' with ``intersection of two lines'', and ``collinear points'' with ``concurrent lines'' throughout.}\\

\noindent The dual statement must keep all logical quantifiers exactly how they are.  For instance, if the original statement involves three fixed points that are not collinear, then the dual statement involves three fixed lines that are not concurrent.  If the original statement states ``for every point, there exists another point such that the line determined by them...'' then the dual statement states, ``for every line, there exists another line such that their intersection point...''

Section 3.7 will make many uses of this.  For example, the dual statement of Pascal's Theorem is Brianchon's Theorem, and the dual statement of Desargues' Theorem is its own converse.

Duality can be extended to higher-dimensional projective spaces; via the orthogonal complement in $\mathbb R^{n+1}$, one gets a correspondence between $k$-planes and $(n-1-k)$-planes in $P^n(\mathbb R)$ for each $k$ [in particular, between points and hyperplanes], which overall reverses inclusion (just like the inclusion of a point in a line in the case $n=2$).  However, we will not delve into this.\\

\noindent\textbf{COMPATIBILITY WITH CONICS}\\

\noindent It is natural to ask how the conics from the previous section relate to the duality concept.  Indeed, let us use our orthogonal-complement duality correspondence: if $\omega$ is an arbitrary conic, what can be said about the lines corresponding to the points on the conic?  It turns out that there is another conic $\omega_1$ for which these are precisely the lines tangent to $\omega_1$ [Exercise 7].  $\omega_1$ is generally a different conic from $\omega$, but it would be especially interesting if they were the same.

Thus, we say that a duality correspondence is \textbf{compatible with $\omega$} if it makes all points on $\omega$ correspond to lines tangent to $\omega$.  For instance, if $\omega$ is the unit circle $x^2+y^2=1$, then the orthogonal-complement duality correspondence is compatible with $\omega$, because it makes a typical unit-circle point $(\cos\theta,\sin\theta)$ correspond with the line $(\cos\theta)x+(\sin\theta)y=-1$, as the reader can verify by transforming to the linear-algebra setting.  This line shares the unique point $(-\cos\theta,-\sin\theta)$ with $\omega$, hence is tangent.

Note, by the way, that though this line is tangent to the circle, it is not tangent \emph{through the corresponding point}.  In fact, in the orthogonal-complement correspondence, it is impossible for any conic that the line be tangent through the corresponding point, because no line in $P^2(\mathbb R)$ contains the corresponding point.  However, there are other duality correspondences that go around this difficulty, as we will see later.  We say that a duality correspondence is \textbf{strongly compatible with $\omega$} if it makes every point on $\omega$ correspond with the tangent line to $\omega$ through that particular point.

The existence of duality correspondences which are compatible with $\omega$ can readily be proved.  In fact, we have that\\

\noindent\textbf{Proposition 3.9.} \emph{If $\omega$ is a conic, then there is a unique duality correspondence which is strongly compatible with $\omega$.}\\

\noindent It is worth remarking that, though the strongly compatible correspondence is unique, there are many compatible correspondences which are not strong (such as the orthogonal complement in the case that $\omega$ is the unit circle).  This will not be important to us, however, so we will not establish it.
\begin{proof}
Let $\vec x\cdot A\vec x=0$ be an equation for the conic, with $A$ symmetric.  For each subspace $V$, we let $V^*=(AV)^\perp=\{\vec x\in\mathbb R^3:\vec x\cdot A\vec v=0\text{ for all }\vec v\in V\}$.  Since $A$ is nonsingular and symmetric, it is easy to see that $(V^*)^*=V$ and $\dim(V)+\dim(V^*)=3$.  However, $\mathbb R^3$ need not be the direct sum of $V$ and $V^*$, as we will see soon.

Given these conditions, it is clear that $V\mapsto V^*$, from one-dimensional to two-dimensional subspaces, gives a duality correspondence in $P^2(\mathbb R)$.  Thus, we must show that it is strongly compatible with $\omega$.  Suppose $V=\operatorname{span}(\vec x)$ is point on the conic; i.e., $\vec x$ is a nonzero vector with $\vec x\cdot A\vec x=0$.  Then $\vec x\in V^*$ by definition of $V^*$.\footnote{Incidentally, this proves that $\vec 0\ne\vec x\in V\cap V^*$, and hence $\mathbb R^3$ is not the direct sum of $V$ and $V^*$.}  Moreover, to show tangency, we must show that if $\vec y$ is both in the conic and on the line $V^*$ (which equates to $\vec y\cdot A\vec y=\vec y\cdot A\vec x=0$), then $\vec y$ represents the same point as $\vec x$.

Well, since $\vec x\cdot A\vec x=\vec y\cdot A\vec x=0$, we have $(\vec x+\vec y)\cdot A\vec x=0$ from bilinearity.  But also, since $\vec y\cdot A\vec y=0$, we obtain $\vec y\cdot A(\vec x+\vec y)=0$ (via bilinearity on the right).  Therefore $(\vec x+\vec y)\cdot A\vec y=0$ by symmetry of $A$, and adding the left-hand side to $(\vec x+\vec y)\cdot A\vec x$ entails $(\vec x+\vec y)\cdot A(\vec x+\vec y)=0$.  This means that $\vec x+\vec y$ is on the conic $\omega$.  Since $\vec x,\vec y,\vec x+\vec y$ are linearly dependent for obvious reasons, (assuming they are all nonzero), they represent collinear points in $P^2(\mathbb R)$, all of which are on $\omega$.  Since a conic and a line can't intersect in more than two points, at least two of $\vec x,\vec y,\vec x+\vec y$ must represent the same point.  Raising that assumption for any two of them entails that $\vec x,\vec y$ are scalar multiples of one another, hence represent the same point in $P^2(\mathbb R)$ as desired.

Therefore, $V\mapsto V^*$ is a duality correspondence which is strongly compatible with $\omega$.

As for uniqueness, let $\sigma,\tau:P^2(\mathbb R)\to\mathcal L^2$ be strongly compatible duality correspondences; we show $\sigma=\tau$.  First, every $\sigma(p)=\tau(p)$ is determined for $p\in\omega$, because both must be the tangent line to $\omega$ through $p$.  If $p$ is an exterior point, then by Exercise 5(c) of the previous section, there are two lines $\ell_1,\ell_2$ through $p$ tangent to $\omega$.  If they meet $\omega$ at $q_1,q_2$ respectively, then $\sigma(q_j)=\ell_j$ for $j=1,2$.  Just as $p$ is on both $\ell_1$ and $\ell_2$, the lines $\sigma(p),\tau(p)$ must pass through the corresponding points $q_1,q_2$; as two points determine a line, this means that $\sigma(p)=\tau(p)$ for exterior points $p$.

Finally, if $p$ is an interior point, then let $\ell$ be a line through $p$.  $\ell$ must meet $\omega$ at exactly two points; and when the tangent lines to these points are constructed, their intersection is the point corresponding to $\ell$ (by the previous paragraph).  Whatever point this is must be in $\sigma(p)$ because $p\in\ell$.  Since this argument works for \emph{any} line $\ell$ through $p$, one can take any such line, construct the corresponding point, and reason that it must be in $\sigma(p)$.  These points must be in $\tau(p)$ too, and clearly the resulting points are not all the same, hence $\sigma(p)=\tau(p)$.
\end{proof}

\noindent According to the proof above, if $\ell$ is a secant line of $\omega$, then the strongly compatible duality correspondence must make $\ell$ correspond to the point obtained by taking the intersection points of $\ell$ with $\omega$, forming the tangent lines to $\omega$ at these points, then taking their intersection:
\begin{center}
\includegraphics[scale=.3]{PolePoint.png}

\emph{Construction of the point corresponding to the blue line.}
\end{center}
This point is called the \textbf{pole point} of $\ell$ via $\omega$.  The concept of pole points is usually dealt with for circles in Euclidean geometry.  It's just that diameters do not have pole points in the Euclidean plane, because the tangent lines described in the previous paragraph are parallel.  However, one can take the pole point of a diameter in the projective plane, and such a point is at infinity.  [See Exercise 3.]

Since duality correspondences can be arranged to be compatible with conics, we can generalize the principle of duality for situations involving one conic in the plane:\\

\noindent\textbf{Principle of Duality (Conic-Compatible Version).} \emph{If any statement in the projective plane which uses exactly one conic $\omega$ is true, so is the \textbf{dual} statement obtained by exchanging ``points'' with ``lines'', ``point on line'' with ``line containing point'', ``line determined by two points'' with ``intersection of two lines'', ``collinear points'' with ``concurrent lines'', ``points on $\omega$'' with ``lines tangent to $\omega$'', ``exterior points of $\omega$'' with ``lines secant to $\omega$'', and ``interior points of $\omega$'' with ``lines disjoint from $\omega$'' throughout.}\\

\noindent As we will see later, this principle gives the equivalence between Pascal's Theorem and Brianchon's Theorem.

\subsection*{Exercises 3.6. (Duality)} % Explain the concept first.  Then establish a genuine bijection between lines and points (via orthogonal complement in \mathbb R^3).
% Given a fixed conic, there is another bijection which corresponds points on the conic to lines tangent to it.  Also mention higher dimensions.
\begin{enumerate}
\item (a) If $(a,b)\ne(0,0)$ in $P^2(\mathbb R)$, then $(a,b)$ corresponds to the line $ax+by=-1$ in the orthogonal-complement duality correspondence.

(b) Let $p$ and $\ell$ be a point and a line in $P^2(\mathbb R)$.  Then $p$ and $\ell$ correspond in the orthogonal-complement duality correspondence if and only if, either $p=(0,0)$ and $\ell$ is the line at infinity, or $p$ is at infinity and $\ell$ is the line through the origin perpendicular (in the Euclidean sense) to lines containing $p$, or else $\ell$ is the line perpendicular to the vector $\vec p$ passing through $-\frac 1{\|\vec p\|^2}\vec p$.

\item Draw the dual of the following diagram via the orthogonal-complement duality correspondence.  [One of the points is at infinity.]
\begin{center}
\includegraphics[scale=.4]{DualExercise.png}
\end{center}
\item Let $\omega$ be the unit circle $x^2+y^2=1$.

(a) A chord in $\omega$ is a diameter if and only if its pole point is at infinity.

(b) Let $a,b$ be fixed real numbers with $a^2+b^2>1$.  Show that $ax+by=1$ is a chord of the circle and that its pole point is $(a,b)$.  [Use the strongly compatible duality correspondence.]

(c) If $0<c<1$ is a fixed real number, then part (b) implies that the pole point of $x=c$ is $(\frac 1c,0)$.  Give an alternate geometric approach to this fact.

\item If $\omega$ is a hyperbola, then the line $\ell_\infty$ at infinity is secant to $\omega$.  What is the pole point?

\item If $\omega$ is a parabola and $\ell$ is a secant line, give necessary and sufficient conditions on $\ell$ for the pole point to be an infinity point.

\item (a) Let $A$ be an arbitrary nonsingular symmetric $3\times 3$ matrix with real entries.  For vector subspaces $V$ of $\mathbb R^3$, set $V^*=(AV)^\perp=\{\vec x\in\mathbb R^3:\vec x\cdot A\vec v=0\text{ for all }\vec v\in V\}$.  Then $V\mapsto V^*$ is a duality correspondence.

(b) Is every duality correspondence in $P^2(\mathbb R)$ necessarily of this form?

\item (a) Let $\omega$ be the conic given by $\vec x\cdot A\vec x=0$ ($A$ nonsingular and symmetric), and let $\omega_1$ be the conic given by $\vec x\cdot A^{-1}\vec x=0$.  Show that in the orthogonal-complement duality correspondence, points on $\omega$ correspond exactly to lines tangent to $\omega_1$.

(b) More generally, let $A$ be a nonsingular symmetric $3\times 3$ matrix with real entries, and let $\omega$ be a conic.  Under the duality correspondence induced by $A$ given in Exercise 6(a), show that there is a conic $\omega_1$ such that points on $\omega$ correspond exactly to lines tangent to $\omega_1$.
\end{enumerate}

\subsection*{3.7. Some Famous Results}
\addcontentsline{toc}{section}{3.7. Some Famous Results}
In the previous sections of this chapter, we have thoroughly introduced the basic concept of projective geometry.  In this section, we will state and prove some famous results that dated back from the first few centuries of the common era, along with the 17th century.  These results have originally been thought of in the Euclidean plane; projective geometry was only later considered popular by mathematicians in the 19th century,\footnote{Conrad, A. ``Projective Geometry: The early years.'' From \emph{sites.math.rutgers.edu} \---- Retrieved from http://sites.math.rutgers.edu/$\sim$cherlin/History/Papers2000/conrad.html} and then it was established that it was an easier setting to prove the results.\\

\noindent\textbf{Theorem 3.10.} \textsc{(Desargues' Theorem)} \emph{Let $\triangle ABC$ and $\triangle A'B'C'$ be triangles in $P^2(\mathbb R)$.  Then the lines $\overset{\longleftrightarrow}{AA'},\overset{\longleftrightarrow}{BB'},\overset{\longleftrightarrow}{CC'}$ are concurrent if and only if the points $\overset{\longleftrightarrow}{AC}\cap\overset{\longleftrightarrow}{A'C'}$, $\overset{\longleftrightarrow}{CB}\cap\overset{\longleftrightarrow}{C'B'}$ and $\overset{\longleftrightarrow}{BA}\cap\overset{\longleftrightarrow}{B'A'}$ are collinear.}\\

\noindent The triangles are said to be \textbf{in perspective centrally} if the lines $\overset{\longleftrightarrow}{AA'}$, etc.~named in the theorem are concurrent; and they are said to be \textbf{in perspective axially} if the points named in the theorem are collinear.  Thus Desargues' theorem states that two triangles in $P^2(\mathbb R)$ are in perspective centrally if and only if they are in perspective axially.  Here is a picture of such triangles.
\begin{center}
\includegraphics[scale=.4]{Desargues.png}
\end{center}
\begin{proof}
Suppose first that $\overset{\longleftrightarrow}{AA'},\overset{\longleftrightarrow}{BB'},\overset{\longleftrightarrow}{CC'}$ are concurrent, say $P=\overset{\longleftrightarrow}{AA'}\cap\overset{\longleftrightarrow}{BB'}\cap\overset{\longleftrightarrow}{CC'}$.  By applying a projective transformation, we may assume $P=[1:0:0]$, $\overset{\longleftrightarrow}{AC}\cap\overset{\longleftrightarrow}{A'C'}=[0:1:0]$ and $B=(0,0)$ [these points are not collinear].  Then $\overset{\longleftrightarrow}{AC}$ and $\overset{\longleftrightarrow}{A'C'}$ are vertical, and corresponding vertices of the triangles share the same horizontal line (because the line through them meets $P$), so we can conveniently write
$$A=(x_1,y_1),B=(0,0),C=(x_1,y_2)$$
$$A'=(x'_1,y_1),B'=(x'_2,0),C'=(x'_1,y_2)$$
(unless any of these points are infinity points; the reader is encouraged to work out these cases).  With that, algebraic manipulation shows that:
$$\overset{\longleftrightarrow}{AB}\cap\overset{\longleftrightarrow}{A'B'}=\left(\frac{x_1x'_2}{x'_2-x'_1+x_1},\frac{y_1x'_2}{x'_2-x'_1+x_1}\right)$$
$$\overset{\longleftrightarrow}{BC}\cap\overset{\longleftrightarrow}{B'C'}=\left(\frac{x_1x'_2}{x'_2-x'_1+x_1},\frac{y_2x'_2}{x'_2-x'_1+x_1}\right)$$
(unless the denominators are zero).  Since they have the same $x$-coordinate, the line going through them is vertical, hence contains $[0:1:0]=\overset{\longleftrightarrow}{AC}\cap\overset{\longleftrightarrow}{A'C'}$.  This proves that the three points stated are collinear, as desired.

The converse follows from the Principle of Duality, as one can see that the central perspectivity and the axial perspectivity are exactly dual statements.
\end{proof}

\noindent There is another famous result concerning the collinearity of three points, due to Pappus from Alexandria.  Incidentally, the German mathematician Gerhard Hessenberg showed that this theorem implies Desargues' Theorem (3.10), and Exercise 1 will outline such a proof.\\

\noindent\textbf{Theorem 3.11.} \textsc{(Pappus' Hexagon Theorem)} \emph{If $A,B,C$ are points on a line $\ell$ and $A',B',C'$ are points on another line $\ell'$, then the points $\overset{\longleftrightarrow}{AC'}\cap\overset{\longleftrightarrow}{A'C}$, $\overset{\longleftrightarrow}{CB'}\cap\overset{\longleftrightarrow}{C'B}$ and $\overset{\longleftrightarrow}{BA'}\cap\overset{\longleftrightarrow}{B'A}$ are collinear.}
\begin{proof}
\includegraphics[scale=.5]{Pappus.png}

The theorem is trivial unless $(A,C,A',C')$ is a projective frame.  Thus, by Theorem 3.8, we may assume $A=(1,0),A'=(0,1),C=[1:0:0],C'=[0:1:0]$.  This means that $\ell$ is the horizontal line $y=0$, and $\ell'$ is the vertical line $x=0$.  Moreover, we may write $B=(b,0)$ and $B'=(0,b')$ with $b,b'\in\mathbb R$.

Since $B=(b,0)$ and $C'=[0:1:0]$, the lines through $C'$ are precisely the vertical lines and the line at infinity.  Moreover, $\overset{\longleftrightarrow}{BC'}$ is the vertical line $x=b$.  Similarly, $\overset{\longleftrightarrow}{B'C}$ is the horizontal line $y=b'$.  Furthermore, $\overset{\longleftrightarrow}{BC'}\cap\overset{\longleftrightarrow}{B'C}=(b,b')$.  Similar reasoning shows that $\overset{\longleftrightarrow}{AC'}\cap\overset{\longleftrightarrow}{A'C}=(1,1)$.

As for $\overset{\longleftrightarrow}{AB'}\cap\overset{\longleftrightarrow}{A'B}$, we use the fact that $A,B,A',B'$ are on the coordinate axes, to easily derive equations for the lines.  Since $A=(1,0)$ and $B'=(0,b')$, we see that $\overset{\longleftrightarrow}{AB'}$ is given by the equation $x+\frac y{b'}=1$, or what is the same thing, $b'x+y=b'$.  By the same token, $\overset{\longleftrightarrow}{A'B}$ is given by $x+by=b$.  Solving the system of equations, we arrive at
$$\overset{\longleftrightarrow}{AB'}\cap\overset{\longleftrightarrow}{A'B}=\left(\frac{bb'-b}{bb'-1},\frac{bb'-b'}{bb'-1}\right)$$
(unless $bb'-1=0$; the reader is encouraged to work this case out).

Thus we wish to show that the points $\left(\frac{bb'-b}{bb'-1},\frac{bb'-b'}{bb'-1}\right),(1,1),(b,b')$ are collinear.  Well, this follows from the fact that they all lie on the line $(b'-1)x-(b-1)y=b'-b$; each point can be directly substituted in the equation.  (If $b'-1=b-1=0$, then $B=A$ and $B'=A'$; what does the theorem statement become in this case?)
\end{proof}
\noindent\textbf{Corollary 3.12.} \emph{Let $\ell_1,\ell_2,\ell_3$ be lines passing through a point $P$, and let $\ell'_1,\ell'_2,\ell'_3$ be lines passing through another point $Q$.  Then, if $A_{ij}=\ell_i\cap\ell'_j$ for each $i,j=1,2,3$, then the lines $\overset{\longleftrightarrow}{A_{12}A_{21}},\overset{\longleftrightarrow}{A_{23}A_{32}},\overset{\longleftrightarrow}{A_{31}A_{13}}$ are concurrent.}
\begin{proof}
Theorem 3.11 and the Principle of Duality.
\end{proof}

\noindent It turns out that Pappus' theorem is a special case of a theorem involving a conic:\\

\noindent\textbf{Theorem 3.13.} \textsc{(Pascal's Theorem)} \emph{If $A,B,C,A',B',C'$ are points on a conic $\omega$, then the points $\overset{\longleftrightarrow}{AC'}\cap\overset{\longleftrightarrow}{A'C}$, $\overset{\longleftrightarrow}{CB'}\cap\overset{\longleftrightarrow}{C'B}$ and $\overset{\longleftrightarrow}{BA'}\cap\overset{\longleftrightarrow}{B'A}$ are collinear.}\\

\noindent If $\omega$ is taken to be a degenerate conic (a union of two lines), then this theorem is Pappus' hexagon theorem.  However, in the previous sections, we have considered all conics to be nondegenerate, and so to be conscientious, we keep the proofs of the theorems separate.
\begin{proof}
\includegraphics[scale=.5]{PascalExpl.png}

We shall use the concept of cross ratios on a conic, introduced in Exercise 9 of Section 3.5.  Points have been labeled on the above diagram to make the proof comprehensible.

Observe first that $[\![A,B,C,B']\!]_\omega=[\![A,X,F,B']\!]$, because the stereographic projection from $\omega$ to $\overset{\longleftrightarrow}{AB'}$ with respect to $A'$ sends $A,B,C,B'$ to $A,X,F,B'$ respectively (see Exercise 9 of Section 3.5).

The same reasoning \---- this time stereographically projecting to $\overset{\longleftrightarrow}{B'C}$ with respect to $C'$ \---- shows $[\![A,B,C,B']\!]_\omega=[\![G,Z,C,B']\!]$.  Therefore, $[\![A,X,F,B']\!]=[\![G,Z,C,B']\!]$.

Now let $\psi:\overset{\longleftrightarrow}{AB'}\to\overset{\longleftrightarrow}{B'C}$ be the perspective projection from $Y$.  It is clear that $\psi(A)=G$, $\psi(F)=C$ and $\psi(B')=B'$.  Therefore, since $\psi$ is a projective transformation, we have
$$[\![G,\psi(X),C,B']\!]=[\![\psi(A),\psi(X),\psi(F),\psi(B')]\!]=[\![A,X,F,B']\!]=[\![G,Z,C,B']\!],$$
from which the cancellation law for cross ratios implies $\psi(X)=Z$.  By definition, this means that $Z$ is the point $\overset{\longleftrightarrow}{YX}\cap\overset{\longleftrightarrow}{B'C}$; therefore $Z\in\overset{\longleftrightarrow}{YX}$, and the points $X,Y,Z$ are collinear as desired.
\end{proof}

\noindent By using the conic-compatible version of Principle of Duality, we instantly get\\

\noindent\textbf{Theorem 3.14.} \textsc{(Brianchon's Theorem)} \emph{If $\ell_1,\ell_2,\ell_3,\ell'_1,\ell'_2,\ell'_3$ are lines tangent to a conic $\omega$, then let $A_{ij}=\ell_i\cap\ell'_j$ for each $i,j=1,2,3$.  Then the lines $\overset{\longleftrightarrow}{A_{12}A_{21}},\overset{\longleftrightarrow}{A_{23}A_{32}},\overset{\longleftrightarrow}{A_{31}A_{13}}$ are concurrent.}\\

\noindent Here is an illustration of Brianchon's Theorem, with the lines $\ell_1,\ell'_2,\ell_3,\ell'_1,\ell_2,\ell'_3$ in clockwise order:
\begin{center}
\includegraphics[scale=.5]{Brianchon.png}
\end{center}

\subsection*{Exercises 3.7. (Some Famous Results)} % Desargues' theorem, Pappus' hexagon, Pascal's theorem, Brianchon's theorem
% Try to find proofs that use cross ratios instead of conveniencing a projective frame.  (We want to make use of Section 3.3.)
% POTENTIAL EXERCISE: HOW DESARGUES' FOLLOWS FROM PAPPUS' THEOREM
\begin{enumerate}
\item The goal of this exercise is to show how Pappus' hexagon theorem can be used to prove Desargues' theorem. %http://math.ucdenver.edu/~tvis/Teaching/4220spring09/Notes/Desarguescompleted.pdf

Suppose Pappus' hexagon theorem holds.  Now let $\triangle ABC$ and $\triangle A'B'C'$ be triangles which are in perspective centrally; i.e., the lines $\overset{\longleftrightarrow}{AA'},\overset{\longleftrightarrow}{BB'},\overset{\longleftrightarrow}{CC'}$ are concurrent.  Let $V=\overset{\longleftrightarrow}{AA'}\cap\overset{\longleftrightarrow}{BB'}\cap\overset{\longleftrightarrow}{CC'}$.

(a) Set $S=\overset{\longleftrightarrow}{B'C'}\cap\overset{\longleftrightarrow}{AC}$, $L=\overset{\longleftrightarrow}{BC}\cap\overset{\longleftrightarrow}{B'C'}$, $T=\overset{\longleftrightarrow}{B'A}\cap\overset{\longleftrightarrow}{CC'}$, and $U=\overset{\longleftrightarrow}{BA}\cap\overset{\longleftrightarrow}{VS}$.  Show that the points $U,T,L$ are collinear.  [Apply Pappus' theorem with $V,B,B'$ on one line, and $A,S,C$ on the other.]

(b) Set $P=\overset{\longleftrightarrow}{B'A'}\cap\overset{\longleftrightarrow}{VS}$, and $M=\overset{\longleftrightarrow}{CA}\cap\overset{\longleftrightarrow}{C'A'}$.  Show that $P,M,T$ are collinear.  [Apply Pappus' theorem with $V,A,A'$ on one line, and $B',C',S$ on the other.]

(c) If $N=\overset{\longleftrightarrow}{AB}\cap\overset{\longleftrightarrow}{A'B'}$.  Show that $L,M,N$ are collinear.  [Apply Pappus' theorem with $U,S,P$ on one line, and $B',T,A$ on the other.]  Conclude that the triangles are in perspective axially, proving one direction of Desargues' theorem.  The Principle of Duality will then imply the converse.

\item Give geometric interpretations of Pascal's and Brianchon's theorems, where the conic is a parabola.
\end{enumerate}







































\chapter{Hyperbolic Geometry}

For many centuries in the realm of the mathematicians, people tried to prove the parallel postulate (Axiom 2.5) from the other axioms, but all attempts failed.  Later, Carl Friedrich Gauss and Ferdinand Karl Schweikart discovered non-Euclidean types of geometry,\footnote{``Non-Euclidean geometry.'' \emph{Wikipedia}, Wikimedia Foundation, 28 October 2003, 10:55, en.wikipedia.org/wiki/Non-Euclidean\_geometry.} where most of the axioms in Section 2.1 \emph{except the parallel postulate} hold.  In this chapter, hyperbolic geometry will be covered.  The term was named by Felix Klein.  The chapter starts with basic geometry of the hyperbolic plane, then presents the mathematics behind polygonal tilings in the hyperbolic plane.  Then it introduces higher-dimensional hyperbolic spaces, and covers many different models by which the spaces can be represented, comparing them to one another.

One of the uses of the hyperbolic plane was due to M.C.~Escher.  He has made many pieces of hyperbolic-tiling art.  How to make one of them will be in the last exercise of Section 4.5.  There is also a downloadable computer game \emph{HyperRogue} developed by Zeno Rogue.\footnote{``HyperRogue.'' Retrieved from: https://zenorogue.itch.io/hyperrogue}  This game takes place in a truncated order-7 triangular tiling of the hyperbolic plane, where the player explores a two-dimensional world in many ways that are not feasible in a Euclidean-plane setting.  All in all, the hyperbolic plane has properties worth learning about.

\subsection*{4.1. Stereographic Projection.  Central Inversion}
\addcontentsline{toc}{section}{4.1. Stereographic Projection.  Central Inversion}
The aim of this section will be to introduce the fundamental concept of stereographic projection.  It is similar to a perspective projection, but it maps from a sphere to (mostly) a plane.  It will serve an important role in both this chapter and the next.

Consider the unit sphere $x^2+y^2+z^2=1$.  We let $N=(0,0,1)$ be the north pole, and we let $\Pi$ be a plane below the sphere, parallel to the $xy$-plane.  [$\Pi$ does not actually need to be below the sphere; we are assuming it is just this once, so that the diagram is easy to understand.]  For each point $p\ne N$ on the sphere, we can take the line passing through $N$ and $p$ and see where it intersects the plane $\Pi$:
\begin{center}\includegraphics[scale=.25]{StereoProjection.png}\end{center}
[We know the line is not parallel to $\Pi$; if it were, it would be tangent to the sphere.]  This maps each point on the sphere other than $N$ to a point on the plane.

Conversely, connecting $N$ to any point in the plane gives another point on the sphere, as the reader can readily see.  Thus, we appear to have a bijection between the points on the sphere and those on the plane.  There is, however, one problem: the north pole $N$ has no place to go, because there are numerous lines passing through just $N$, some of which are parallel to the plane and the rest of which meet all the points of the plane.

We go around this caveat by adjoining a symbolic point $\infty$ to the plane $\Pi$.  We let $\overline{\Pi}=\Pi\sqcup\{\infty\}$.  If we extend our previously described projection to the entire sphere by sending $N$ to $\infty$, we finally have a bijection from the sphere to $\overline{\Pi}$.  Moreover, it is a topological homeomorphism when $\overline{\Pi}$ is given the topology of the Alexandroff one-point compactification.  It is clear that we can regard $\overline{\Pi}$ as an abstract ``extended plane,'' $\overline{\Pi}=\mathbb R^2\sqcup\{\infty\}$, by merely thinking of $\Pi$ as a plane through the origin, taking an orthonormal basis, and using it to establish an isometry with $\mathbb R^2$.  If we regard $\Pi$ as the complex plane $\mathbb C$ (which will be done in most of the next sections), $\overline{\Pi}$ is referred to as the \textbf{Riemann sphere} or \textbf{extended complex plane}.\\

\noindent\emph{Warning}: Do not confuse $\mathbb R^2\sqcup\{\infty\}$ with the projective plane $P^2(\mathbb R)$.  $P^2(\mathbb R)$ has many points at infinity (which form a line), whereas $\mathbb R^2\sqcup\{\infty\}$ has only \emph{one} such point.  They are different concepts; however, the next section shows a way to think of $\mathbb R^2\sqcup\{\infty\}$ in projective terms.\\

\noindent The mapping just described has a special name:\\

\noindent\textbf{Definition.} \emph{Let $S$ be a sphere in $\mathbb R^3$, let $N$ be a point on $S$, and let $\Pi$ be a plane perpendicular to the line through $N$ and the center, such that $N\notin\Pi$.  The \textbf{stereographic projection} from $S$ to $\overline{\Pi}=\Pi\sqcup\{\infty\}$ is the function $\varphi:S\to\overline{\Pi}$, such that $\varphi(N)=\infty$, and for each $p\ne N$ in $S$, $\varphi(p)$ is the intersection of the line through $N$ and $p$ with the plane $\Pi$.}\\

\noindent The plane $\Pi$ must be perpendicular to the radius in order for the stereographic projection to be well-defined.  (Otherwise, there would be points $p\ne N$ in $S$, such that the line through $N$ and $p$ is parallel to $\Pi$, so that there would be no intersection.)  If $N\in\Pi$, (in this case $\Pi$ is tangent to the sphere at $N$), then $\varphi(p)$ would be $N$ for \emph{all} $p\in S$ (other than $N$), so $\varphi$ would not really be a bona fide projection.

For the remainder of this section, we will take $S^2$ to be the unit sphere $x^2+y^2+z^2=1$, $N$ to be $(0,0,1)$ and $\Pi$ to be the $xy$-plane, $z=0$.  %This may seem rather misleading, because earlier we said $\Pi$ needed to be \emph{below} the sphere \---- and in this case, the $xy$-plane passes through it.  However, there is no actual need for $\Pi$ to be below the sphere; we only declared it to be at the beginning, so that the concept would be easy to illustrate clearly.
Note that $\Pi$ passes through the sphere in this case, so that certain points $p\in S^2$ will map to plane points lying in \emph{between} $N$ and $p$.

We shall find a general formula for the stereographic projection from a point $p\ne N$ of $S^2$, as follows.  Suppose $p=(x,y,z)$.  Then since $\varphi(p)$ is in the $xy$-plane, it is of the form $(x',y',0)$.  Moreover, the following three points are collinear:
$$N=(0,0,1),~~~~p=(x,y,z),~~~~\varphi(p)=(x',y',0)$$
This means that the vector cross product $(\varphi(p)-N)\times(p-N)=(x',y',-1)\times(x,y,z-1)$ is the zero vector.\footnote{The cross product of two vectors in $\mathbb R^3$ is zero if and only if the vectors lie on a line.}  The reader can then work out the expressions and conclude that $x'=\frac x{1-z}$ and $y'=\frac y{1-z}$.  Therefore we have
\begin{center}
\textbf{If $\varphi:S^2\to\overline{\Pi}$ is the stereographic projection from the unit sphere to the extended $xy$-plane, and $N\ne p=(x,y,z)$ is a point of $S^2$, then:}
\begin{equation}\tag{F1}\varphi(p)=\left(\frac x{1-z},\frac y{1-z},0\right)\end{equation}
\end{center}
Once we have a formula for this projection, we would like a formula for its inverse, from $\overline{\Pi}$ back to $S^2$.  Suppose $(x,y,0)$ is a point of the $xy$-plane, and we wish to find the the corresponding point $(x_0,y_0,z_0)\in S^2$.  In this case, we have two equations:
$$x_0^2+y_0^2+z_0^2=1\text{ (because the point is on the sphere);}$$
$$N=(0,0,1),~~~~(x_0,y_0,z_0),~~~~(x,y,0)\text{ are collinear.}$$
The second equation can be rephrased by saying $(x_0,y_0,z_0-1)=\lambda(x,y,-1)$ for some $\lambda\in\mathbb R$.  After all, those vectors are obtained by connected pairs of the above points, hence the points are collinear if and only if one of the vectors is a scalar multiple of another.  [We note that $(x,y,-1)$ cannot be the zero vector, since $-1\ne 0$.]  We can compute the value of $\lambda$ by taking the magnitudes of the vectors $(x_0,y_0,z_0-1)$ and $(x,y,-1)$:
$$\|(x_0,y_0,z_0-1)\|^2=x_0^2+y_0^2+(z_0-1)^2=(x_0^2+y_0^2+z_0^2)-2z_0+1=2-2z_0=2(1-z_0)$$
$$\|(x,y,-1)\|^2=x^2+y^2+(-1)^2=x^2+y^2+1\implies\|\lambda(x,y,-1)\|^2=\lambda^2(x^2+y^2+1)$$
Therefore, since $(x_0,y_0,z_0-1)=\lambda(x,y,-1)$ and their magnitudes are equal, we get $\lambda^2(x^2+y^2+1)=2(1-z_0)$.

Yet, looking in the $z$-component of the equation $(x_0,y_0,z_0-1)=\lambda(x,y,-1)$, we immediately get $\lambda=1-z_0$.  Hence we have an equation
$$(1-z_0)^2(x^2+y^2+1)=2(1-z_0)$$
Since $(x_0,y_0,z_0)\ne N$, $z_0\ne 1$, and we may divide by $1-z_0$:
$$(1-z_0)(x^2+y^2+1)=2$$
We instantly get $1-z_0=\frac 2{x^2+y^2+1}$ as a result, so that $z_0=\frac{x^2+y^2-1}{x^2+y^2+1}$.  Furthermore, since $\lambda=1-z_0$, looking at the first two components of $(x_0,y_0,z_0-1)=\lambda(x,y,-1)$ entails $x_0=x(1-z_0)$ and $y_0=y(1-z_0)$.  Therefore,
\begin{equation}\tag{F2}(x_0,y_0,z_0)=\left(\frac{2x}{x^2+y^2+1},\frac{2y}{x^2+y^2+1},\frac{x^2+y^2-1}{x^2+y^2+1}\right)\end{equation}
is the point of $S^2$ that maps to $(x,y,0)\in\Pi$ via the stereographic projection $\varphi$.

The reader is left to verify the following basic facts about formulas (F1) and (F2):
\begin{itemize}
\item The point of (F2) is really on the sphere $S^2$.

\item The two formulas give inverse mappings both ways.  [When going from the sphere to the plane and then back to the sphere, one must be aware that the original point was on the sphere.]
\end{itemize}
The formulas will make it easier to study the stereographic projection.\\

\noindent\textbf{THE CENTRAL INVERSION}\\

\noindent In the remainder of this section, we shall introduce the concept of the central inversion.  [This does not refer to the negation of the identity linear operator $-I$; it is a different kind of ``central inversion.'']

The central inversion (a.k.a., circle inversion) is a transformation of $\overline{\mathbb R^2}=\mathbb R^2\sqcup\{\infty\}$.  As we will see, it fixes every point on the unit circle $x^2+y^2=1$, and it exchanges points inside the circle with points outside.  It is conformal (Exercise 4 below) but orientation-reversing.  It also maps circles and lines to circles and lines.

Here is how we intuitively obtain it.  Start with the reflection $R$ of the unit sphere $S^2$ over the $xy$-plane; this is given by $R(x,y,z)=(x,y,-z)$, and it is clear that this is an involution sending $S^2$ to itself.  The question remains: if we apply stereographic projection to both the starting and ending points of this transformation, how do the results relate?  This can be answered by \emph{conjugating} $R$ by the stereographic projection.  Indeed, if $R(p)=p'$, then $\varphi\circ R\circ\varphi^{-1}$ is a transformation of $\overline{\mathbb R^2}$ sending $\varphi(p)\mapsto\varphi(p')$.

Thus, we may take any point $(x,y)\in\overline{\mathbb R^2}$ and ask ourselves what its image is under $\varphi\circ R\circ\varphi^{-1}$.

First, by (F2) above, $\varphi^{-1}(x,y)=\left(\frac{2x}{x^2+y^2+1},\frac{2y}{x^2+y^2+1},\frac{x^2+y^2-1}{x^2+y^2+1}\right)$.  Applying $R$ to this vector merely negates the last component, so that gives us $\left(\frac{2x}{x^2+y^2+1},\frac{2y}{x^2+y^2+1},\frac{-x^2-y^2+1}{x^2+y^2+1}\right)$.
Finally, we must apply $\varphi$ to that vector.  We recall (F1) that if $N\ne(x',y',z')\in S^2$ then $\varphi(x',y',z')=\left(\frac{x'}{1-z'},\frac{y'}{1-z'}\right)$.  Taking $x'=\frac{2x}{x^2+y^2+1},y'=\frac{2y}{x^2+y^2+1},z'=\frac{-x^2-y^2+1}{x^2+y^2+1}$, it can be observed that
$$1-z'=\frac{2x^2+2y^2}{x^2+y^2+1}=\frac{2(x^2+y^2)}{x^2+y^2+1}$$
from which we have $\frac{x'}{1-z'}=\frac x{x^2+y^2}$ and $\frac{y'}{1-z'}=\frac y{x^2+y^2}$.  Thus, $\varphi\circ R\circ\varphi^{-1}$ sends $(x,y)$ [$x,y$ not both zero] to the point $\left(\frac x{x^2+y^2},\frac y{x^2+y^2}\right)$.  Moreover, it sends $(0,0)\mapsto\infty$ and $\infty\mapsto(0,0)$, since the stereographic projection makes $\infty\leftrightarrow(0,0,1)$ and $(0,0)\leftrightarrow(0,0,-1)$ correspond.\\

\noindent\textbf{Definition.} \emph{On $\overline{\mathbb R^2}=\mathbb R^2\sqcup\{\infty\}$, the \textbf{central inversion} (or \textbf{circle inversion}) is the map $\psi$ given by $\psi(0,0)=\infty,\psi(\infty)=(0,0)$ and $\psi(x,y)=\left(\frac x{x^2+y^2},\frac y{x^2+y^2}\right)$ for $x,y$ not both zero.}\\

\noindent The reader can readily see that $\psi$ is an involution, and that $\psi(p)=p$ for all points $p$ on the unit circle.  Also, if $p$ is regarded as a nonzero vector then $\psi(\vec p)=\frac{\vec p}{\|\vec p\|^2}$; moreover, $\|\psi(\vec p)\|=\frac 1{\|\vec p\|}$, so that $\vec p$ is inside the circle if and only if $\psi(\vec p)$ is outside.  Useful properties of the central inversion will now be stated and shown.\\

\noindent\textbf{Proposition 4.1.} (i) \emph{The central inversion of a line through the origin is the line itself.}

(ii) \emph{The central inversion of a line not through the origin, is a circle whose arc passes through the origin, and vice versa.}

(iii) \emph{The central inversion of a circle whose arc does not pass through the origin is another such circle.}

\begin{proof}
The proofs use basic direct substitution into the formula.  The idea is that if any subset of $\overline{\mathbb R^2}$ is given by an equation $f(p)=0$, its central inversion is given by $f(\psi(p))=0$.  We will prove the first half of (ii) as an example to illustrate this principle; the rest will be left to the reader.

Let $\ell$ be a line not through the origin.  Then $\ell$ is given by an equation $ax+by=c$, where $a,b$ are not both zero, and $c\ne 0$.  [If $c$ were zero, the line would go through the origin.]  Then by the principle described in the paragraph above, $\ell$'s central inversion is given by
$$a\frac x{x^2+y^2}+b\frac y{x^2+y^2}=c;$$
basic algebra then entails $x^2+y^2-\frac acx-\frac bcy=0$, which is the equation of a circle [see Exercise 9 of Section 2.6].  The equation can be rewritten as:
$$\left(x-\frac a{2c}\right)^2+\left(y-\frac b{2c}\right)^2=\frac{a^2+b^2}{4c^2}$$
and so the central inversion of $\ell$ is the circle centered at $\left(\frac a{2c},\frac b{2c}\right)$ with radius $\frac{\sqrt{a^2+b^2}}{2|c|}$.  Direct substitution into the equation shows that the origin $(0,0)$ is on the circle.
\end{proof}

\noindent The following diagram illustrates the central inversion.  It takes a Euclidean triangle $\triangle ABC$, and passes it to the green figure, where $A$ gets mapped to $A'$, $B$ to $B'$ and $C$ to $C'$ (which is $C$ itself because $C$ is on the unit circle):
\begin{center}\includegraphics[scale=.35]{CentInversion.png}\end{center}

\subsection*{Exercises 4.1. (Stereographic Projection.  Central Inversion)} % Introduce stereographic projection (first noting how important it'll be for this and the next chapter)
% from S^2\to\mathbb R^2\sqcup\{\infty\}.  Find formulas.  Then note that conjugating the equatorial reflection of the sphere yields central inversion, so go over that.
% POTENTIAL EXERCISE: stereographic projection is conformal
\begin{enumerate}
\item Show that stereographic projection sends every circle on $S^2$ that does not have $N$ on its arc to a circle.  [A circle in $S^2$ can be realized as the intersection of $S^2$ with a plane in $\mathbb R^3$; now use (F2) to equate which points of the $xy$-plane have corresponding points on this plane.]

\item What is the stereographic projection of a circle that has $N$ on its arc?

\item Use the previous two exercises to give an alternate proof that the central inversion preserves circles and lines.  [Remember that the central inversion is the conjugation of the sphere's equatorial reflection $(x,y,z)\mapsto(x,y,-z)$ by stereographic projection.]

\item\emph{(Conformality of stereographic projection.)} \---- Let $U\subset\mathbb R^2$ be an open set\footnote{This means that for each $x\in U$, there exists $\varepsilon>0$ such that $\{y\in\mathbb R^2:\|y-x\|<\varepsilon\}\subset U$.} and $f:U\to\mathbb R^3$ a differentiable map.  $f$ is said to be \textbf{conformal} if it preserves angles between curves.

(a) Show that the following statements are equivalent:

~~~~(i) $f$ is conformal.

~~~~(ii) For each point $p\in U$, and nonzero vectors $\vec v_1,\vec v_2$ of $\mathbb R^2$, the angle between $\vec v_1$ and $\vec v_2$ equals the angle between $df_p(\vec v_1)$ and $df_p(\vec v_2)$.

~~~~(iii) For each point $p\in U$, there is a constant $\lambda>0$ (which depends only on $p$), such that $df_p(\vec v_1)\cdot df_p(\vec v_2)=\lambda^2\vec v_1\cdot\vec v_2$ for all $\vec v_1,\vec v_2\in\mathbb R^2$.

~~~~(iv) For each point $p\in U$, there is a constant $\lambda>0$ (which depends only on $p$), such that $\|df_p(\vec v)\|=\lambda\|\vec v\|$ for all $\vec v\in\mathbb R^2$.

~~~~(v) For each point $p\in U$, $df_p$ has orthogonal columns with the same magnitude. % 3\times 2, because U is 2-dimensional and \mathbb R^3 is 3-dimensional.  If df_p were 2\times 3, then the only way it could have orthogonal columns with the same magnitude is for it to be zero, as you can't have an orthonormal set with more vectors than the dimension of the space

~~~~(vi) For each point $p\in U$, $(df_p)^T(df_p)$ is a scalar multiple of the identity matrix.

[(i) $\iff$ (ii) because the Chain Rule implies that the differential sends a tangent vector to a curve in $U$ to a tangent vector of the output curve.  (ii) $\iff$ (iii): Recall that the angle between $\vec v_1$ and $\vec v_2$ is $\cos^{-1}\frac{\vec v_1\cdot\vec v_2}{\|\vec v_1\|\|\vec v_2\|}$.  (iii) $\iff$ (iv): Use the fact that $\vec v\cdot\vec w=\frac 12(\|\vec v+\vec w\|^2-\|\vec v\|^2-\|\vec w\|^2)$.  (iii) $\implies$ (v): Take the $\vec v_j$ to be the standard basis vectors.  (v) $\iff$ (vi) because for any matrix $A$, the entries of $A^TA$ are the dot products of $A$'s columns with each other.  (vi) $\implies$ (iii): $(df_p)^T(df_p)=\lambda^2I_2$ for some $\lambda$.]

(b) Show that the inverse of the stereographic projection, given by (F2), is conformal.  [Find a formula for its differential, then use part (a).]

(c) Show that the composition of conformal maps is conformal.

(d) Use this to show that the central inversion is conformal.

\item The central inversion of a point $p\ne(0,0)$ is the point whose distance from the origin is the multiplicative inverse of the distance from $p$ to the origin, and is on the same ray from the origin as $p$.

\item If $\overline{\mathbb R^2}$ is regarded as $\mathbb C\sqcup\{\infty\}$, then the central inversion of $z\ne 0$ in $\mathbb C$ is $1/\overline z$, where $\overline z$ is the complex conjugate of $z$.

\item Here is a geometric interpretation of the central inversion. %http://mathworld.wolfram.com/Inversion.html

(a) Suppose $P$ is a point inside the unit circle, but is not equal to the center $O$ of the circle.  Let $\ell$ be the line through $P$ perpendicular to $\overset{\longleftrightarrow}{OP}$.  Suppose $Q$ is an intersection point of the unit circle with $\ell$.  Let $\ell'$ be the line through $Q$ perpendicular to $\overset{\longleftrightarrow}{OQ}$, and let $P'=\overset{\longleftrightarrow}{OP}\cap\ell'$.  Show that $P'$ is the central inversion of $P$. [Use Exercise 5 and similar triangles (covered in Section 2.1).]

(b) On the other hand, if $P$ is outside the circle, let $\ell$ be a line through $P$ tangent to the circle.  Suppose it meets the circle at a point $Q$, and let the perpendicular from point $Q$ to line $\overset{\longleftrightarrow}{OP}$ meet the line at $P'$.  Then $P'$ is the central inversion of $P$.
\end{enumerate}

\subsection*{4.2. Generalized Circles and M\"obius Transforms}
\addcontentsline{toc}{section}{4.2. Generalized Circles and M\"obius Transforms}
The fundamental setup for hyperbolic geometry will start from the concept of the M\"obius transform of $\mathbb R^2\sqcup\{\infty\}$.  We will first define the notion of a generalized circle, so we can later use it to study M\"obius transforms in depth.

We shall start with a fundamental lemma.  It is assumed that every line in $\mathbb R^2\sqcup\{\infty\}$ contains $\infty$ along with the Euclidean points of the line.\\

\noindent\textbf{Lemma 4.2.} \emph{Stereographic projection sends circles not containing $N$ to circles, and circles containing $N$ to lines.  Conversely, every line or circle in the plane corresponds to a circle in the sphere.}\\

\noindent Incidentally, this lemma was stated in Exercises 1-2 in the previous section.
\begin{proof}
A circle in $S^2$ can be realized as the intersection with $S^2$ of a plane, say $Ax+By+Cz=D$, where $A,B,C$ are not all zero.  We wish to find an equation for points in the $xy$-plane whose corresponding points on $S^2$ are part of this plane; for, these points are then the image of the circle through the stereographic projection.  Note that the circle contains $N$ $\iff$ the plane contains $N=(0,0,1)\iff C=D$.

By equation (F2) from the previous section, this equation is
$$A\frac{2x}{x^2+y^2+1}+B\frac{2y}{x^2+y^2+1}+C\frac{x^2+y^2-1}{x^2+y^2+1}=D$$
which equates to:
$$(C-D)(x^2+y^2)+2Ax+2By-C-D=0.$$
If the original circle on $S^2$ does not contain $N$, which means $C\ne D$, then the equation is quadratic in $x,y$, where the $xy$ coefficient is zero, and the $x^2$ and $y^2$ coefficients are nonzero and equal.  Hence this is the equation of a circle, by Exercise 9 of Section 2.6.  If the original circle on $S^2$ contains $N$, which means $C=D$, the equation becomes $2Ax+2By=C+D$, which is a line ($A,B$ can't both be zero, because that would make the plane $Ax+By+Cz=D$ tangent to $S^2$, so it wouldn't intersect in a circle).  Of course, the image also contains $\infty$ in this case.

Conversely, if $\omega$ is a line or a circle in the $xy$-plane, let $a,b,c\in\omega$ be three distinct points.  Then observe that \emph{$\omega$ is determined by $a,b,c$}: if $\omega$ is a line then $a,b,c$ are collinear, and hence, any two of these points determines $\omega$ (unless one of them is $\infty$), and if $a,b,c$ are not collinear, $\omega$ is a circle, and is determined by them.\footnote{The perpendicular bisectors of $\overline{a~b}$ and $\overline{a~c}$ intersect in a point which must be the center of $\omega$.  Since both the center and one point on the arc are determined, the circle is.}  Let $a',b',c'$ be the corresponding points $S^2$: then these points cannot be collinear (since the sphere intersects any line in at most two points), which means that they determine a circle $\omega_1$.  Thus $\omega_1$ must be contained in $S^2$ (because they meet in at least three points $a',b',c'$); and by the preceding part of the argument, the stereographic projection sends $\omega_1$ to either a line or a circle containing $a,b,c$.  This figure must be $\omega$.
\end{proof}

\noindent In view of Lemma 4.2, the images under stereographic projection of the circles are the circles and the unions of the lines with $\infty$.  These subsets of $\mathbb R^2\sqcup\{\infty\}$ have a special name.\\

\noindent\textbf{Definition.} \emph{In $\overline{\mathbb R}=\mathbb R^2\sqcup\{\infty\}$, a \textbf{generalized circle} is defined to be either a circle, or a set of the form $\ell\cup\{\infty\}$ where $\ell\subset\mathbb R^2$ is a line.}\\

\noindent Several remarks are in order.  First, by Proposition 4.1, central inversion sends generalized circles to generalized circles.  Clearly, isometries and scalings of the plane also do; the M\"obius transform will be a more general kind of transformation which does this.  We shall define these transformations, by first changing the setting.  Instead of thinking of $\mathbb R^2$ as a two-dimensional plane, we think of it as the complex plane $\mathbb C$.  Thus the extended plane we are dealing with is the Riemann sphere $\overline{\mathbb C}=\mathbb C\sqcup\{\infty\}$.

A clever way to look at $\overline{\mathbb C}$ is by adapting the definition of projective $n$-space.  We recall that projective $n$-space is obtained by taking $\mathbb R^{n+1}-\{\vec 0\}$, and then quotienting out by an equivalence relation which identifies $\vec v,\vec w$ if $\vec v=\lambda\vec w$ for some $\lambda\ne 0$ in $\mathbb R$.  We can easily do the same thing over the complex numbers $\mathbb C$: we define an equivalence relation on $\mathbb C^{n+1}-\{\vec 0\}$ by identifying vectors $\vec v,\vec w$ if there exists $\lambda\ne 0$ in $\mathbb C$ such that $\vec v=\lambda\vec w$, then take the quotient by the relation to get $P^n(\mathbb C)$.\footnote{$P^n(\mathbb C)$ is referred to as \textbf{complex projective space}.  The notion was introduced by Karl Georg Christian von Staudt in 1860.}

Just as in the case for projective space over the real numbers, we may define a projective transformation $P^m(\mathbb C)\to P^n(\mathbb C)$ by taking an injective linear transformation $\mathbb C^{m+1}\to\mathbb C^{n+1}$ and viewing it on equivalence classes of vectors.  In particular, the group of projective transformations of $P^n(\mathbb C)$ is $PGL_{n+1}(\mathbb C)$, obtained by taking the group of nonsingular $(n+1)\times(n+1)$ matrices over $\mathbb C$, modulo the normal subgroup $\{\lambda I_{n+1}:\lambda\in\mathbb C_{\ne 0}\}$.

We will only be interested in the case where $n=1$: in this case, $P^1(\mathbb C)$ is the Riemann sphere $\mathbb C\sqcup\{\infty\}$, for essentially the same reason $P^1(\mathbb R)=\mathbb R\sqcup\{\infty\}$.  In this case, a projective transformation $\varphi$ from $\mathbb C\sqcup\{\infty\}$ to itself takes on the form
$$\varphi(z)=\frac{az+b}{cz+d}$$
with $a,b,c,d\in\mathbb C$ constant, $ad-bc\ne 0$.  Furthermore, if $c=0$ then $\varphi(\infty)=\infty$, and if $c\ne 0$, then $\varphi(-d/c)=\infty$ and $\varphi(\infty)=a/c$.  Such a map is called a \textbf{M\"obius transformation}.

We shall also define an \textbf{anti-M\"obius transform} to be the composition of a M\"obius transform with complex conjugation.  It can be interpreted with the complex conjugation on either side; more generally, an anti-M\"obius transform is of the form $z\mapsto\frac{a\overline z+b}{c\overline z+d}$ with $a,b,c,d\in\mathbb C,ad-bc\ne 0$.  For instance, central inversion is an anti-M\"obius transform by Exercise 6 of the previous section.  Our main result of the chapter is:\\

\noindent\textbf{Proposition 4.3.} \emph{M\"obius transforms and anti-M\"obius transforms are conformal, and preserve generalized circles.  Conversely, every bijection of $\overline{\mathbb C}$ which is conformal and preserves generalized circles is either a M\"obius or an anti-M\"obius transform.}
\begin{proof}
The conformality of M\"obius transforms follows from a more general fact that holomorphic functions are conformal on the complex plane.  Indeed, let $U\subset\mathbb C$ be an open set, and let $f:U\to\mathbb C$ be holomorphic.  Then the limit $f'(a)=\lim_{h\to 0}\frac{f(a+h)-f(a)}h$ is well-defined, and independent of what path in $\mathbb C$ is taken by $h$: if this derivative is $u+vi$, then equating the limits as $h\to 0$ from the positive real axis, and $h\to 0$ from the imaginary axis where the imaginary part is positive, we get $df_a=\begin{bmatrix}u&-v\\v&u\end{bmatrix}$.  This means that at each point $a\in U$, the differential $df_a$, when $f$ is regarded as a map from $U\subset\mathbb R^2$ to $\mathbb R^2$, is of the form $\begin{bmatrix}u&-v\\v&u\end{bmatrix},u,v\in\mathbb C$.  Yet $\begin{bmatrix}u&-v\\v&u\end{bmatrix}$ has orthogonal columns with the same magnitude; it follows from Exercise 4(a) of the previous section that $f$ is conformal.

The anti-M\"obius transforms are not analytic, but they are complex conjugates of analytic functions; thus they preserve angles, but reverse their orientation.

For a proof of conformality which does not use analysis, note that, in view of Exercise 1(b) below, it suffices to show that plane scaling, isometries of $\mathbb R^2$, and the central inversion are conformal [because conformal maps are closed under composition].  This is clear for plane scaling and isometries; for the central inversion, it is Exercise 4(d) of the previous section.

As for the proof that M\"obius/anti-M\"obius transforms preserve generalized circles, again we only need to show that plane scaling, isometries of $\mathbb R^2$, and the central inversion preserve generalized circles.  For plane scaling and isometries, this is clear; for the central inversion, it follows from Proposition 4.1.

As for the converse statement of this proposition, we will let $G$ be the group of bijections of $\overline{\mathbb C}$ which are conformal and preserve generalized circles; and $H$ the group of M\"obius and anti-M\"obius transformations.  By the first statement in this proposition, which we just proved, $H$ is a subgroup of $G$.  We will use Proposition 1.9 to show straightforwardly that $H=G$.

Let $f\in G$.  If $f(\infty)=a\ne\infty$, we let $T_a$ be the translation $z\mapsto z+a$ and $\psi$ the central inversion.  Then $T_a,\psi\in H$, and $\psi\circ T_a^{-1}\circ f$ sends $\infty\mapsto\infty$:
$$\infty\overset{f}\longrightarrow a\overset{T_a^{-1}}\longrightarrow 0\overset{\psi}\longrightarrow\infty$$
By Proposition 1.9, $f\in H$ if and only if $\psi\circ T_a^{-1}\circ f\in H$.  Thus, we need only prove $\psi\circ T_a^{-1}\circ f\in H$; in other words, in showing $f\in G\implies f\in H$, we may assume $f(\infty)=\infty$.

Furthermore, if $f(0)=b$, then again by Proposition 1.9, $f\in H$ if and only if $T_b^{-1}\circ f\in H$.  Yet, $T_b^{-1}\circ f(0)=T_b^{-1}(b)=0$, so we may likewise assume $f(0)=0$.  Finally, by multiplying $f$ by a nonzero complex number, we may assume $f(1)=1$.  Note that this last operation scales and rotates $f$; if the complex number is $re^{i\theta}$ in polar form, it rotates $f$ by an angle of $\theta$ and scales it by $r$.

Hence, in showing $f\in G\implies f\in H$, we may assume $f(0)=0$, $f(1)=1$ and $f(\infty)=\infty$.  In this case, $f$ preserves lines (because lines are precisely the generalized circles containing $\infty$), and so $f$ must fix the line determined by $0,1$; i.e., $f$ must fix the $x$-axis.  If $\ell$ is a horizontal line other than the $x$-axis, then $\ell$ does not meet the $x$-axis at any points $\ne\infty$, hence $f(\ell)$ doesn't meet the $x$-axis away from $\infty$ either, because $f$ is bijective.  This means that $f(\ell)$ is also parallel to the $x$-axis and hence a horizontal line, and $f$ sends all horizontal lines to horizontal lines.  Since $f$ is conformal, it follows that $f$ maps all vertical lines (lines parallel to the $y$-axis) to vertical lines as well.

Therefore, $f$ can be written in the form $(x,y)\mapsto(g(x),h(y))$ from $\mathbb R^2\to\mathbb R^2$.  [Since $f$ sends horizontal lines to horizontal lines, $f(x_1,y)$ and $f(x_2,y)$ are on the same horizontal line, and hence $h$ does not depend on the left argument $x$; similarly for $g$.]  Its differential is then of the form
$$df=\begin{bmatrix}\frac{\partial g}{\partial x}&0\\0&\frac{\partial h}{\partial y}\end{bmatrix}$$
which means $\frac{\partial g}{\partial x}=\pm\frac{\partial h}{\partial y}$ [for all $x,y$] by Exercise 4(a) of the previous section.  Since $\frac{\partial g}{\partial x}$ is a function in $x$ alone, and $\frac{\partial h}{\partial y}$ is a function in $y$ alone, our conclusion is that both of them are actually constant.  If $a=\frac{\partial g}{\partial x}\in\mathbb R$, then $g(x)=ax$ and $h(y)=\pm ay$ for all $x,y$.  Since $f(1)=1$ as complex numbers, $f(1,0)=(1,0)$, hence $g(1)=1$ and $a=1$.  Therefore, $f$ is either the identity or complex conjugation, both of which are in $H$.
\end{proof}

\noindent As we will see in this chapter and the next, M\"obius transforms are essentially the isometries of the hyperbolic and spherical planes.  Section 4.6 will cover a higher-dimensional analogue of the M\"obius transform, which is not so trivial (because there aren't genuine number systems of arbitrary dimensions over $\mathbb R$ which generalize $\mathbb C$), and will go over higher-dimensional spaces. % I admit "spherical plane" is weird.  But does "hyperbolic plane" really make any more sense?

\subsection*{Exercises 4.2. (Generalized Circles and M\"obius Transforms)} % Prove that stereographic projection preserves circles, except the ones that go through the north pole
% which land on lines.  Then define a generalized circle.  Carry everything over to the Riemann sphere P^1(\mathbb C)=\mathbb C\sqcup\{\infty\}, and define the concept of a
% M\"obius transform, showing that they are conformal, and preserve generalized circles.
\begin{enumerate}
\item (a) The group of all M\"obius transforms of $\overline{\mathbb C}$ is generated by the transforms $z\mapsto z+b$, $z\mapsto az$ ($a\ne 0$) and $z\mapsto\frac 1z$.

(b) Every M\"obius or anti-M\"obius transform of $\overline{\mathbb C}$ is an composition of a scaling ($z\mapsto rz,r\in\mathbb R_{>0}$), an isometry of $\mathbb R^2$, and maybe or maybe not the central inversion.

\item The group of M\"obius transforms acts transitively on the set of generalized circles.

\item If $a,b,c\in\overline{\mathbb C}$ are distinct points, show that there is a unique generalized circle passing through them.  Conclude that any two generalized circles intersect in at most two points.

\item If $[a:a_0],[b:b_0],[c:c_0],[d:d_0]\in P^1(\mathbb C)$, define their \textbf{cross ratio} as follows:
$$[\![[a:a_0],[b:b_0],[c:c_0],[d:d_0]]\!]=[(a_0c-c_0a)(b_0d-d_0b):(a_0d-d_0a)(b_0c-c_0b)].$$
Verify that this is well-defined (if and only if the inputs don't consist of three equal points), and Propositions 3.4 and 3.5 hold just as in the case of cross ratios on the real projective line.

Show that $[\![a,b,c,d]\!]\in\mathbb R\cup\{\infty\}$ if and only if $a,b,c,d$ lie on a common generalized circle.  Use this to show that if $a,b,c$ are distinct points, then the generalized circle they determine is $\{x\in\overline{\mathbb C}:[\![a,b,c,x]\!]\in\mathbb R\cup\{\infty\}\}$.  [Since cross ratios are invariant under M\"obius transforms (Proposition 3.5(iv) can be adapted), this provides an alternate proof that M\"obius transforms preserve generalized circles.]

\item If $T$ is an anti-M\"obius transform, show that $[\![T(a),T(b),T(c),T(d)]\!]=\overline{[\![a,b,c,d]\!]}$, i.e., the complex conjugate of $[\![a,b,c,d]\!]$.  [Note that this provides an alternate proof that generalized circles are preserved by $T$.]

\item Let $\omega$ be a generalized circle.  Show that the following are equivalent:

~~~~(i) The central inversion sends $\omega$ to itself;

~~~~(ii) $\omega$ meets the unit circle orthogonally;

~~~~(iii) Either $\omega$ is a line through the origin, or else it is a circle with a radius $r$ and a center $a\in\mathbb C$ such that $|a|^2-r^2=1$.

[(i) $\iff$ (ii) by Exercise 4(d) of the previous section.  (ii) $\iff$ (iii): Explain why (ii) is equivalent to saying that, if you take a radius of the unit circle which lands at an intersection point of the unit circle and $\omega$, then it is tangent to $\omega$.]

\item Let $\omega$ be a generalized circle.  Show that the following are equivalent:

~~~~(i) The central inversion sends $\omega$ to its own negation;

~~~~(ii) The corresponding circle on the sphere (via stereographic projection) is a great circle;

~~~~(iii) The corresponding circle on the sphere contains at least one point and its antipode;

~~~~(iv) The corresponding circle on the sphere is closed under taking antipodes;

~~~~(v) Either $\omega$ is a line through the origin, or else it is a circle with a radius $r$ and a center $a\in\mathbb C$ such that $r^2-|a|^2=1$.

[Show that the negation of the central inversion, when conjugated by stereographic projection, is the map of the sphere sending every point to its antipode.]

\item Suppose $u,a,b,c,v\in\overline{\mathbb C}$ are points with $u\ne v,u,v\notin\{a,b,c\}$.  (We are not requiring $a,b,c$ to be distinct from each other.) %[In other words, there is a segment of the generalized circle with endpoints $u,v$, which contains $a,b,c$, such that $a$ is between $u,b$ and $c$ is between $b,v$.  See diagram.]
Show that $[\![b,a,u,v]\!][\![c,b,u,v]\!]=[\![c,a,u,v]\!]$.  [By applying a M\"obius transform, one may assume $u=0,a=1,v=\infty$.]

\item (a) The group of M\"obius transforms which fix the real line $\mathbb R\cup\{\infty\}$ is isomorphic to $PGL_2(\mathbb R)$.  In other words, a M\"obius transform fixes $\mathbb R\cup\{\infty\}$ if and only if, when considered as an element of $PGL_2(\mathbb C)$, it can be represented with a real matrix.

(b) Use this to show that the group of M\"obius/anti-M\"obius transforms which fix the upper half-plane $\{x+yi\in\mathbb C:y>0\}$ is isomorphic to $PGL_2(\mathbb R)$.  [A M\"obius transformation which fixes the real line, but sends the upper half-plane to the lower-half, can be composed with complex conjugation.]

(c) Conclude that the group of M\"obius/anti-M\"obius transforms which fix the interior of the unit disk is isomorphic to $PGL_2(\mathbb R)$.  [Conjugate the group in part (b) using a M\"obius transform that sends the real line to the unit circle.]

\item (a) Let $a,b,c\in\overline{\mathbb C}$ be distinct points.  Let $a',b',c'\in\overline{\mathbb C}$ be another triple of distinct points.  Show that there is a unique M\"obius transform sending $a\mapsto a',b\mapsto b',c\mapsto c'$. [Adapt Exercise 8 of Section 3.2.]  Conclude that a M\"obius transform which is not the identity fixes at most two points.

(b) Show by example that part (a) may be false for anti-M\"obius transforms.

\item Suppose $a,b,c,d,a',b',c',d'\in\overline{\mathbb C}$.  Assuming the cross ratios are defined, there exists a M\"obius transform sending $a\mapsto a',b\mapsto b',c\mapsto c',d\mapsto d'$ if and only if $[\![a,b,c,d]\!]=[\![a',b',c',d']\!]$.
\end{enumerate}

\subsection*{4.3. The Hyperbolic Plane: Poincar\'e Disk and Half-Plane Models}
\addcontentsline{toc}{section}{4.3. The Hyperbolic Plane: Poincar\'e Disk and Half-Plane Models}
The Poincar\'e disk model is the most common model in which the hyperbolic plane is pictured.  After all, it is of finite structure, its lines are easily depictable, and its circles are also circles to the Euclidean eye.  It was originally proposed by Eugenio Beltrami, yet it is named after Henri Poincar\'e because his discoveries in this matter are better known. % To quote the Wikipedia page on the Poincar\'e disk model: "Along with the Klein model and the Poincaré half-space model, it was proposed by Eugenio Beltrami who used these models to show that hyperbolic geometry was equiconsistent with Euclidean geometry. It is named after Henri Poincaré, because his rediscovery of this representation fourteen years later became better known than the original work of Beltrami."  And isn't it bad to cite Wikipedia?

We shall start by considering the unit circle $x^2+y^2=1$, which is a generalized circle, and showing how to make its interior into a clever kind of plane.  Let $H^2(\mathbb R)$ be the open unit disk $\{(x,y)\in\mathbb R^2:x^2+y^2<1\}$.  Moreover, let $\overline{H^2}(\mathbb R)$ be the closed unit disk, $\{(x,y)\in\mathbb R^2:x^2+y^2\leqslant 1\}$.  Finally, let $H^2_\infty(\mathbb R)=\overline{H^2}(\mathbb R)-H^2(\mathbb R)$, which is the unit circle.

$H^2(\mathbb R)$ is referred to as the \textbf{hyperbolic plane} (rather, the \textbf{Poincar\'e disk model} of the hyperbolic plane).  We will call $H^2_\infty(\mathbb R)$ the \textbf{rim} / \textbf{boundary at infinity}, and its elements are to be referred to as \textbf{ideal points}.  These points are significantly different from the points in $H^2(\mathbb R)$, even from the point of view of intrinsic geometry (the constructions invariant under isometries), as we will see later.  We will call $\overline{H^2}(\mathbb R)$ the \textbf{extended hyperbolic plane}.

Clasiccal hyperbolic geometry deals only with $H^2(\mathbb R)$, just as classical Euclidean geometry deals with the affine plane without the ideal points that give us $P^2(\mathbb R)$.

In $\overline{H^2}(\mathbb R)$, a \textbf{line} (or \textbf{geodesic}) is defined to be a set of the form $\omega\cap\overline{H^2}(\mathbb R)$, where $\omega$ is a generalized circle orthogonal to the unit circle.  In other words, lines are generalized circles which are perpendicular to the rim.  It is clear that $\omega$ can be recovered from the line.
\begin{center}\includegraphics[scale=.25]{PoincareDisk.png}\end{center}

We could build another model as follows: instead of the unit circle, take the $x$-axis, $y=0$.  Then define $H^2(\mathbb R)=\{(x,y)\in\mathbb R^2:y>0\}$, $H^2_\infty(\mathbb R)$ as the $x$-axis (in particular, it contains $\infty$), and $\overline{H^2}(\mathbb R)=H^2(\mathbb R)\cup H^2_\infty(\mathbb R)$.  In this model, a line refers to a set of the form $\omega\cap\overline{H^2}(\mathbb R)$ where $\omega$ is a generalized circle orthogonal to the $x$-axis; thus the lines are vertical lines, and semicircle arcs centered on the $x$-axis.  This is called the \textbf{Poincar\'e (upper) half-plane model} of the hyperbolic plane.  This model is often used in the study of modular forms and elliptic curves.
\begin{center}\includegraphics[scale=.3]{PoincareHalfPlane.png}\end{center}
For the sake of definiteness, $H^2(\mathbb R),\overline{H^2}(\mathbb R),H^2_\infty(\mathbb R)$ will henceforth refer specifically to the Poincar\'e disk model.

Note that a line has exactly two ideal points.  Thus in hyperbolic geometry, lines appear to have endpoints just like line segments; but the fact of the matter is that they continue indefinitely, and have no endpoints in $H^2(\mathbb R)$, even though in $\overline{H^2}(\mathbb R)$, they may be considered to touch the rim.  A portion of a line between two points is called a \textbf{ray} if exactly one of the points is ideal, and a \textbf{line segment} if neither point is ideal.  It is clear that a line segment/ray can be uniquely extended to a line.

If $a,b$ are points on a line, we let $p,q$ be the ideal points of the line, with $p$ on $a$'s side:
\begin{center}\includegraphics[scale=.3]{HDistance.png}\end{center}
We define the \textbf{distance} between $a$ and $b$, denoted as $ab$ or $\rho(a,b)$, to be $\ln[\![b,a,p,q]\!]$, the natural logarithm of the cross ratio.  This is a real number by Exercise 4 of the previous section.  This can be shown to be positive (Exercise 3).  Later we will see that this distance depends on only $a,b$, because the two points determine a line.  By Exercise 8 of the previous section, and well-known properties of logarithms, we have the segment addition postulate: if $a,b,c$ are on the line with $b$ between $a$ and $c$, then $\rho(a,c)=\rho(a,b)+\rho(b,c)$. % You say I need to prove the triangle inequality?  I don't remember using it anywhere.  Plus, the triangle inequality is equivalent to saying that, if \overline{AB} is drawn and circles are drawn centered at A and B, the circles only intersect if their interiors overlap and neither circle goes around the other.  [Because if C is an intersection point of the circles, you can conclude about \triangle ABC.]  This is easy to prove for hyperbolic geometry once you know that circles are Euclidean circles in these models.

If $\ell_1$ and $\ell_2$ are lines, they intersect in at most one point (since two points determine a line).  However, such a point may or may not be an ideal point.  We say that lines are \textbf{divergent-parallel} / \textbf{ultraparallel} if they do not intersect at all; \textbf{convergent-parallel} / \textbf{limiting parallel} if they intersect at an ideal point; and \textbf{intersecting} if they intersect at a regular point of $H^2(\mathbb R)$:
\begin{center}\includegraphics[scale=.2]{HLinePairs.png}\end{center}
If two lines intersect, we define the \textbf{angle measure} between them to be the Euclidean angle measure between them (namely, the angle between the Euclidean tangent lines to the curves at their intersection point).\footnote{A model of a kind of geometry where angle measures coincide with those of the Euclidean angles is called a \textbf{conformal model}.}  If the intersection point is ideal, the angle is always zero (since both lines meet the rim perpendicularly), whereas if the intersection point is regular, the angle is only zero when the lines coincide (Exercise 5(a)).  The angle addition postulate is clear, but we will later see that if we construct two parallel lines and a transversal, we do \emph{not} have congruence of corresponding angles like we did in the Euclidean plane. % A transversal through two parallel lines does not generally have the corresponding angles congruent.  I suppose I should have specified that.

It is now natural to try to show that two points determine a line, etc.  Surprisingly, the best way to carry on right now is to cover what the isometries are.\\

\noindent\textbf{ISOMETRIES}\\

\noindent In the Poincar\'e disk (resp., half-plane) model, isometries are taken precisely to be the M\"obius and anti-M\"obius transforms of $\overline{\mathbb C}$ which fix the unit disk (resp., the upper half of the plane).  Since such transforms are conformal, it is clear that they preserve generalized circles orthogonal to the rim; i.e., they preserve hyperbolic lines.  They also preserve distances, because M\"obius transforms preserve cross ratios, and anti-M\"obius transforms conjugate cross ratios (thus preserving the real cross ratios) by Exercise 5 of the previous section.  Finally, they preserve angle measures, since they are conformal by Proposition 4.3.

The M\"obius transforms are called \textbf{orientation-preserving isometries}, while the anti-M\"obius transforms are called \textbf{orientation-reversing isometries}.

We note that if $a,b,c\in H^2_\infty(\mathbb R)$ are distinct points, as are $a',b',c'\in H^2_\infty(\mathbb R)$, there is a unique isometry sending $a\mapsto a',b\mapsto b',c\mapsto c'$.    Indeed, by Exercise 10(a) of the previous section, there is a unique M\"obius transform $T$ of $\overline{\mathbb C}$ for which $T(a)=a',T(b)=b',T(c)=c'$.  In this case, $T$ must fix the unit circle $H^2_\infty(\mathbb R)$ [because, by Exercise 3 of the previous section, $T$ sends the unique generalized circle through $a,b,c$ to that through $a',b',c'$].  \emph{However}, $T$ may send the interior of the circle to the exterior, and thus it is not technically a symmetry of the plane, since the plane is only the \emph{interior} of the circle.  In this case, however, we compose $T$ with the central inversion $\psi:z\mapsto 1/\overline z$, and we get the desired isometry sending $a\mapsto a',b\mapsto b',c\mapsto c'$, which is incidentally orientation-reversing.

We can do the same for the half-plane model: this time, take $a,b,c,a',b',c'$ on the (extended) $x$-axis.  In this case, $T$ may send the upper half of the plane to the lower half of the plane, but then you can compose it with complex conjugation to get an orientation-reversing isometry.

We shall now obtain a general formula for isometries of the hyperbolic plane.  Let us stick to the half-plane model to start; suppose $T$ is an \emph{orientation-preserving} isometry of the half-plane model.  Then one can write $T(z)=\frac{az+b}{cz+d},ad-bc\ne 0$.  With that, $c,d$ can't both be zero.  Thus by multiplying $a,b,c,d$ by a nonzero constant complex number (which results in the same isometry), we may assume either $c=1$, or $c=0$ and $d=1$.

If $c=1$, then $a=T(\infty)\in\mathbb R$ (because $T$ preserves the half-plane's rim $\mathbb R\cup\{\infty\}$), and similarly, $-d=T^{-1}(\infty)\in\mathbb R$.  Therefore $a,d$ are both real numbers.  Since $T(0)=\frac bd$ and is in $\mathbb R\cup\{\infty\}$, we conclude that $b\in\mathbb R$ as well; thus, $a,b,c,d$ are all real numbers.  If $c=0$ and $d=1$, then $T(z)=az+b$; in this case it is clear that $a,b\in\mathbb R$.

Thus $T$ can be written as $T(z)=\frac{az+b}{cz+d}$ with $a,b,c,d\in\mathbb R,ad-bc\ne 0$.  [Obviously, not \emph{every} rational-function expression of $T$ has real coefficients, though.  We are just saying that there exists such a rational-function expression.]  Moreover, since $T$ fixes the upper half of the plane, it sends $i$ (a point in the upper half) to a point in the upper half.  Computing,
$$T(i)=\frac{ai+b}{ci+d}=\frac{b+ai}{d+ci}=\frac{(b+ai)(d-ci)}{c^2+d^2}=\frac{bd+ac}{c^2+d^2}+i\frac{ad-bc}{c^2+d^2}$$
Since this point is in the upper half of the plane, its imaginary part is positive; therefore $ad-bc>0$.  The reader can readily verify that, conversely, every $z\mapsto\frac{az+b}{cz+d}$ with $a,b,c,d\in\mathbb R,ad-bc>0$ is an orientation-preserving isometry, thus we have:
\begin{center}
\textbf{The orientation-preserving isometries of the Poincar\'e half-plane model are the rational functions of the form $\frac{az+b}{cz+d}$, where $a,b,c,d\in\mathbb R$ and $ad-bc>0$.}
\end{center}
Clearly, $\begin{bmatrix}a&b\\c&d\end{bmatrix}\mapsto\frac{az+b}{cz+d}$ is a surjective homomorphism from $GL^+_2(\mathbb R)$ to the orientation-preserving isometry group $\operatorname{Isom}^+(H^2(\mathbb R))$ of the half-plane model, whose kernel is $\mathbb R^*=\{aI_2:a\ne 0\text{ in }\mathbb R\}$.  By Theorem 1.17, we get that the orientation-preserving isometry group of the half-plane model is isomorphic to $PGL^+_2(\mathbb R)$.  By Exercise 8(c) of Section 1.5, it is hence also isomorphic to $PSL_2(\mathbb R)$.

The following facts can be likewise shown, and the arguments are left to the reader:
\begin{itemize}
\item The orientation-reversing isometries of the Poincar\'e half-plane model are the maps of the form $z\mapsto\frac{a\overline z+b}{c\overline z+d}$, where $a,b,c,d\in\mathbb R$ and $ad-bc<0$.

\item The orientation-preserving isometries of the Poincar\'e disk model are the rational functions of the form $\frac{az+b}{\overline bz+\overline a}$, where $a,b\in\mathbb C$ and $|a|^2>|b|^2$. [\emph{Note}: Instead of proving this from scratch, one could conjugate the half-plane model's isometries via the transforms in Exercise 6.]

\item The orientation-reversing isometries of the Poincar\'e disk model are the functions $z\mapsto\frac{a\overline z+b}{\overline b\overline z+\overline a}$, where $a,b\in\mathbb C$ and $|a|^2>|b|^2$.
\end{itemize}
\noindent Exercises 1 and 2 show that the isometries of the hyperbolic plane act transitively on the regular points, and that the stabilizer of a regular point consists of rotations and reflections over the point.  Thus, \emph{locally} speaking, this plane behaves identically to the Euclidean plane.\\

\noindent\textbf{BASIC GEOMETRIC RESULTS}\\

\noindent At this point, we shall look back at Section 2.1's results on Euclidean plane geometry.  We will show that certain statements, but not others, also hold for the hyperbolic plane.  However, we will not be deducing theorems from axioms, because the setting we are dealing with is concrete.  We shall often use the isometries (and their transitivity \---- Exercise 1) to our advantage, as they will enable us to arrange for points/lines/etc.~to be situated conveniently.  We are also free to switch between the disk and half-plane models (Exercise 6).\\

\noindent\textbf{Proposition 4.4.} \emph{In the hyperbolic plane, two points determine a line (regardless of whether the points are ideal).  If at most one is ideal, they also determine a ray.  If both points are regular, they determine a line segment and two rays.}
\begin{proof}
Let $p_1,p_2\in\overline{H^2}(\mathbb R)$ be distinct points.

If both points are ideal, we may assume we are in the half-plane model, and that $p_2=\infty$.  Being an ideal point different from $p_2$, $p_1$ is some real number $r$.  Since the only lines containing $\infty$ are the vertical lines, it follows that the unique line passing through $p_1,p_2$ is the vertical line $x=r$.

Now let us suppose that $p_1$ is \emph{not} an ideal point.  We may assume that we are in the Poincar\'e disk model, and (by applying an isometry), that $p_1=0$.  Then $p_2$ is some nonzero complex number with absolute value $\leqslant 1$; moreover, the Euclidean line through $p_1,p_2$ is a diameter of the unit circle, hence is a hyperbolic line.  To show that this line is unique, we let $\ell$ be a line through $p_1,p_2$, then we show that it must be a Euclidean line (which uniquely determines it).  If $\ell$ is not a Euclidean line, then it is (by Exercise 6 of the previous section), a circle with a radius $r$ and a center $a\in\mathbb C$ such that $|a|^2-r^2=1$.  Moreover, this circle is given by an equation $|z-a|=r$.  Since $0=p_1\in\ell$, we get $|0-a|=r$, hence $|a|=r$ and so $|a|^2-r^2=0\ne 1$, a contradiction.

This proves that there is a unique line through $p_1$ and $p_2$.  If $p_2$ is an ideal point, the Euclidean line segment from $p_1$ to $p_2$ is a ray (it has only one \emph{regular} endpoint), and is the unique ray determined by them.  If $p_2$ is not an ideal point, the verification of the line segment and the two rays is left to the reader.
\end{proof}

\noindent We continue to run through our list of Euclidean axioms. % I said it explicitly before Proposition 4.4, but I guess that's insufficient

Proposition 2.2's analogue holds as well; a hyperbolic line segment has well-defined endpoints.  Rays and lines have endpoints too, except some of them are ideal points.  We leave the verifications to the reader.  Axiom 2.4 can be carried over as well; the addition postulate has been covered when distances were defined at the beginning of the section, and Axiom 2.4(i) can easily be proven for the hyperbolic plane by assuming $A$ is the center of the Poincar\'e disk and using Exercise 4.  Incidentally, points of a given distance are closer to the Euclidean eye when they are closer to the rim, as arbitrary distances can be measured from any regular points, and nothing will go outside the bounds.

However, the parallel postulate (Axiom 2.5) does \emph{not} hold in the hyperbolic plane; instead, we have\\

\noindent\textbf{Proposition 4.5.} \emph{Given a line $\ell$ and a point $a\notin\ell$, there are infinitely many lines through $a$ parallel to $\ell$.  Exactly two of them are convergent-parallel.}
\begin{center}\includegraphics[scale=.3]{HypParallels.png}\end{center}
\begin{proof}
The above diagram, where $\ell$ is the horizontal Euclidean line, should give an intuitive picture why the proposition is true.  Here is a formal proof.

We may assume that we are in the half-plane model, and that $\ell$ is the $y$-axis, $x=0$.  Then $a$ is some point $u+vi$, such that $u\ne 0$ (because $a\notin\ell$).  We may further assume $u>0$.

Fix any $0\leqslant r\leqslant u$, and consider the line $\ell'$ through $r$ (as an ideal point) and $a$.  We claim it is parallel to $\ell$.  If $r=u$, then $\ell'$ is the vertical line $x=u$, hence is convergent-parallel to $\ell$, meeting at $\infty$.  If $r<u$, then $\ell'$ is a semicircle arc, where $r$ is one of the endpoints of the diameter.  Clearly, $r$ is the left endpoint, because the semicircle contains the point $a$ with a larger real coordinate.  This implies in particular that the real coordinate of any point on the semicircle is $\geqslant r\geqslant 0$, hence $\ell'$ is parallel to $\ell$ (and convergent-parallel if and only if $r=0$).  Thus, $\ell'$ is parallel to $\ell$; it does not intersect at any \emph{regular} point.  Clearly these lines are all distinct for the different values of $x$, hence there are infinitely many lines through $a$ parallel to $\ell$.

The reader can readily verify that if $p$ and $q$ are the ideal points of $\ell$, then the lines $\overset{\longleftrightarrow}{p~a}$ and $\overset{\longleftrightarrow}{q~a}$ are the only two lines through $a$ that are convergent-parallel to $\ell$.
\end{proof}
\noindent Next up is Axiom 2.6, concerning angles.  Since angle measures in Poincar\'e disk/half-plane models are just the Euclidean angle measures, the statements of Axiom 2.6 are easy to prove, using either one of the following two strategies: (1) use the disk model, and apply an isometry to assume the angles' endpoint $A$ is the center, so that all lines through $A$ are Euclidean lines; or (2) use Exercise 5(a) to prove the hyperbolic analogue of Axiom 2.6(i).

The concept of complementary, supplementary and right angles are carried over from Euclidean geometry, as are the concepts of linear pairs and vertical angles.  It is clear, just as in the Euclidean case, that angles in a linear pair are supplementary, and vertical angles are congruent.  We also have the perpendicular postulate:\\

\noindent\textbf{Proposition 4.6.} \textsc{(Perpendicular postulate)} \emph{Given a line $\ell$ and a point $a$ which is not equal to either of ideal points of $\ell$, there exists a unique line through $a$ perpendicular to $\ell$.}\\

\noindent Note that the proposition works if $a$ is an ideal point, just not if $a$ is one of the ideal points of $\ell$.
\begin{proof}
We may assume we are in the half-plane model and $\ell$ is the $y$-axis.  Note that the lines perpendicular to $\ell$ are precisely the semicircle arcs centered at $0$ (why?).  Under the hypothesis, $a$ is some complex number $u+vi\ne 0$ (since $0$ and $\infty$ are the ideal points of $\ell$, $a$ is not equal to either of them).  It is then clear that the unique line through $a$ perpendicular to $\ell$ is the semicircle arc of radius $\sqrt{u^2+v^2}$ centered at $0$.
\end{proof}

\noindent Since the parallel postulate fails, we do not have the nice properties of parallel lines and their transversals that Euclidean geometry possesses (Axiom 2.9 and Proposition 2.10).  However, given two divergent-parallel lines, there is one particular point which holds all the ``nice'' transversals, as we will eventually see.  The first main result is\\

\noindent\textbf{Theorem 4.7.} \textsc{(Ultraparallel Theorem)} \emph{If $\ell_1$ and $\ell_2$ are divergent-parallel lines, there is a unique line $\ell$ simultaneously perpendicular to both $\ell_1$ and $\ell_2$.}\\

\noindent Note that the line would not be unique in Euclidean geometry: given two parallel lines, there are infinitely many lines perpendicular to one of them, and all those lines are perpendicular to the other as well.
\begin{proof}
Again we may assume we are in the half-plane, and that $\ell_1$ is the $y$-axis.  Since $\ell_1$ and $\ell_2$ are divergent-parallel, $\ell_2$ is a semicircle arc whose endpoints are both (strictly) positive or both negative.  We may assume the endpoints are both positive.
\begin{center}\includegraphics[scale=.5]{UltraparallelTheorem.png}\end{center}
In this case, if $a$ and $r$ are the center and radius of $\ell_2$ respectively, then $a>r>0$ and $\ell_2$ is given by an equation $|z-a|=r$.  If $\ell$ is a line perpendicular to $\ell_1$ which intersects $\ell_2$, then $\ell$ is a semicircle arc centered at $0$; let $u$ be its radius.  Then since a radius of a circle is perpendicular to the circle's arc, we get that $\ell\perp\ell_2\iff$ the radii of $\ell,\ell_2$ going to their intersection $\ell\cap\ell_2$ are perpendicular $\iff$ the triangle with vertices $0,\ell\cap\ell_2,a$ has a right angle at $\ell\cap\ell_2\iff$ the squares of the Euclidean radii of $\ell,\ell_2$ add to $a^2$ (by the Pythagorean Theorem) $\iff u^2+r^2=a^2\iff u=\sqrt{a^2-r^2}$.

Furthermore, the unique line $\ell$ perpendicular to both lines is the semicircle arc centered at $0$ with radius $\sqrt{a^2-r^2}$.
\end{proof}

\noindent $\ell$ is referred to as the \textbf{common perpendicular} to $\ell_1,\ell_2$.  Note that if $\ell_1,\ell_2$ intersect at any point, even if it is an ideal point, then no such line $\ell$ exists; this would contradict the uniqueness part of Proposition 4.6, because if $p=\ell_1\cap\ell_2$ then $\ell_1$ and $\ell_2$ would both be lines through $p$ perpendicular to $\ell$.  Thus Theorem 4.7 is false if $\ell_1,\ell_2$ are convergent-parallel.

If $\ell$ is the common perpendicular to $\ell_1,\ell_2$, the midpoint of the line segment from $\ell\cap\ell_1$ to $\ell\cap\ell_2$ will be referred to as the \textbf{attention point} of $\ell_1$ and $\ell_2$.  Later we will see that a transversal has congruent corresponding angles if and only if it passes through the attention point.\\%for lack of better terminology

\noindent The best way to do that is to cover triangle theorems first.  The definition of a triangle is identical to that for the Euclidean plane; it uses three noncollinear points, and the \emph{hyperbolic} line segments connecting them.  Note that the vertices of a triangle may be ideal points; clearly a vertex is an ideal point if and only if the angle measure is zero.  A triangle is called \textbf{regular} if it has no ideal vertices; an \textbf{omega triangle} if it has exactly one ideal vertex; and an \textbf{ideal triangle} if all its vertices are ideal vertices.

We recall (Proposition 2.11) that the angles of a Euclidean triangle always add to $\pi$, or $180^\circ$.  Contrariwise, the sum of the angles of a hyperbolic triangle is always \emph{less} than $180^\circ$:\\

\noindent\textbf{Proposition 4.8.} \emph{The measures of the angles of a hyperbolic triangle add to $<180^\circ$.}
\begin{proof}
This proposition is trivial for ideal triangles, so we may assume that the triangle is not an ideal triangle.  We let $a,b,c$ be the vertices of the triangle.  Assume $a$ is a regular point, and (by applying an isometry) that it is the center of the Poincar\'e disk:
\begin{center}\includegraphics[scale=.25]{HypTriangle.png}\end{center}
The \emph{Euclidean triangle} $\triangle abc$ shares the same line segments $\overline{ab},\overline{ac}$.  However, the Euclidean line segment from $b$ to $c$ stretches outside the triangle, and the hyperbolic triangle's side curves inwards.  To see why this is so, note that the hyperbolic $\overline{bc}$ must be from a circle with a center $u$ and radius $r$ such that $|u|^2-r^2=1$, and that such a circle has $0$ as an exterior point, and hence, inside the unit disk, this circle necessarily curves toward the origin.

Due to this, it is easy to see that the angles at $b$ and $c$ are smaller than the corresponding angles of the Euclidean triangle.  Yet the vertex $a$ has the exact same angle.  Thus the sum of the three angles is smaller than the sum of the three angles of the Euclidean triangle, which is $180^\circ$.
\end{proof}

\noindent The concept of congruence of triangles is taken exactly from the Euclidean case: $\triangle abc\cong\triangle a'b'c'$ if $\overline{ab}\cong\overline{a'b'},\angle bac\cong\angle b'a'c'$, etc.  It so happens that in the hyperbolic plane, triangles with congruent angles are always congruent.  Thus, there is no irredundant notion of ``similar'' triangles in hyperbolic geometry.\\

\noindent\textbf{Proposition 4.9.} (i) \textsc{(Side-side-side / SSS)} \emph{If $\overline{ab}\cong\overline{a'b'},\overline{bc}\cong\overline{b'c'},\overline{ca}\cong\overline{c'a'}$, and all vertices of the triangles are regular, then $\triangle abc\cong\triangle a'b'c'$.}

(ii) \textsc{(Side-angle-side / SAS)} \emph{If $\overline{ab}\cong\overline{a'b'}$, $\overline{ac}\cong\overline{a'c'}$, $\angle a\cong\angle a'$ and $a,a'$ are regular, then $\triangle abc\cong\triangle a'b'c'$.}

(iii) \textsc{(Hypotenuse-leg / HL / RHS)} \emph{If $\angle b,\angle b'$ are right angles, $\overline{ab}\cong\overline{a'b'}$ and $\overline{ac}\cong\overline{a'c'}$, then $\triangle abc\cong\triangle a'b'c'$.}

(iv) \textsc{(Angle-side-angle / ASA)} \emph{If $\angle a\cong\angle a'$, $\overline{ab}\cong\overline{a'b'}$, $\angle b\cong\angle b'$, and both $a,b$ are regular, then $\triangle abc\cong\triangle a'b'c'$.}

(v) \textsc{(Angle-angle-side / AAS)} \emph{If $\angle a\cong\angle a'$, $\angle b\cong\angle b'$, $\overline{bc}\cong\overline{b'c'}$, and both $a,b$ are regular, then $\triangle abc\cong\triangle a'b'c'$.}

(vi) \textsc{(Angle-angle-angle / AAA)} \emph{If $\angle a\cong\angle a'$, $\angle b\cong\angle b'$, and $\angle c\cong\angle c'$, then $\triangle abc\cong\triangle a'b'c'$.}\\

\noindent Several remarks are in order.  First, side-side-angle is once again invalid; the argument of Exercise 5 of Section 2.1 applies.  Also, in the Euclidean case, (iv) automatically implied (v), due to two angle measures of a triangle determining the measure of the third angle.  This is not true for hyperbolic triangles (the angle measures can have any sum $<180^\circ$), and hence, (iv) and (v) must be given separately.
\begin{proof}
Each of these can be proved by applying convenient isometries to the triangles, which cause certain corresponding parts to coincide.  We will prove (ii) here to illustrate the idea.

By using the Poincar\'e disk, and applying an isometry to $\triangle abc$, we may assume $a=0$, $b$ is a positive real number, and $c$ has a positive imaginary part.  We may do the same thing to $\triangle a'b'c'$, assuming $a'=0$, $b'$ is a positive real number and $c'$ has a positive imaginary part.  [Of course, there generally isn't an isometry which does this to both triangles at once, but we do not need such an isometry.]

First, we are given that $\overline{ab}\cong\overline{a'b'}$; comparing their segment lengths and using Problem 4, $\ln\frac{1+b}{1-b}=\ln\frac{1+b'}{1-b'}$, and therefore $b=b'$.  Since $\angle bac\cong\angle b'a'c'=\angle bac'$, we conclude that $c,c'$ lie on the same ray from the origin $a$.  Finally, from $\overline{ac}\cong\overline{a'c'}$ we conclude $c=c'$.  Therefore, we certainly have $\triangle abc\cong\triangle a'b'c'$, because $\triangle abc$ \emph{is} the triangle $\triangle a'b'c'$ [after the application of the isometries].
\end{proof}

\noindent\textbf{Proposition 4.10.} \textsc{(Isosceles Triangle Theorem)} \emph{If $\triangle abc$ is a triangle, then $\overline{ab}\cong\overline{ac}$ if and only if $\angle b\cong\angle c$.}
\begin{proof}
Copy the proof of Proposition 2.15, using Proposition 4.9 in place of Axiom 2.13.
\end{proof}

\noindent Hyperbolic triangles will be studied in more detail in Section 4.5.

With the aid of triangle congruence theorems, we may finally prove\\

\noindent\textbf{Proposition 4.11.} \emph{Let $\ell_1$ and $\ell_2$ be two divergent-parallel lines, $\ell_3$ a transversal, and $p$ the attention point of $\ell_1,\ell_2$.  Then the following are equivalent:}

(i) \emph{$p\in\ell_3$.}

(ii) \emph{Corresponding angles are congruent.}

(iii) \emph{Corresponding exterior angles are supplementary.}

(iv) \emph{Alternate exterior angles are congruent.}

(v) \emph{Corresponding interior angles are supplementary.}

(vi) \emph{Alternate interior angles are congruent.}
\begin{proof}
(i) $\implies$ (vi). Let $\ell$ be the common perpendicular to $\ell_1$ and $\ell_2$; let $a_1=\ell\cap\ell_1,a_2=\ell\cap\ell_2$.  Then $p$ is the midpoint of $\overline{a_1a_2}$ by definition.  Moreover, let $b_1=\ell_3\cap\ell_1,b_2=\ell_3\cap\ell_2$.  Then $\angle a_1pb_1\cong\angle a_2pb_2$ because they are vertical angles; $\angle pa_1b_1\cong\angle pa_2b_2$, because $\ell$ is perpendicular to both $\ell_j$, so those are both right angles; and $\overline{a_1p}\cong\overline{a_2p}$ since $p$ is the midpoint of $\overline{a_1a_2}$.  By ASA congruence (Proposition 4.9(iv)), $\triangle a_1b_1p\cong\triangle a_2b_2p$.  Hence $\angle a_1b_1p\cong\angle a_2b_2p$, which proves (vi).

(vi) $\implies$ (i). Let $b_1=\ell_3\cap\ell_1,b_2=\ell_3\cap\ell_2$, and let $m$ be the midpoint of $\overline{b_1b_2}$.  By Proposition 4.6, there is a unique line $\ell$ through $m$ which is perpendicular to $\ell_1$.  (We do not yet know if $\ell$ is perpendicular to $\ell_2$.)  Set $a_1=\ell\cap\ell_1,a_2=\ell\cap\ell_2$.  Then $\angle a_1mb_1\cong\angle a_2mb_2$ (vertical angles), $\overline{mb_1}\cong\overline{mb_2}$ (virtue of a midpoint), and $\angle a_1b_1m\cong\angle a_2b_2m$, by the hypothesis (vi).  Therefore by ASA, $\triangle a_1b_1m\cong\triangle a_2b_2m$.  Hence $\overline{a_1m}\cong\overline{a_2m}$, so $m$ is the midpoint of $\overline{a_1a_2}$.  Yet also, $\angle ma_2b_2\cong\angle ma_1b_1$, which is a right angle, and hence $\ell$ is perpendicular to $\ell_2$.  Thus $\ell$ is the common perpendicular of $\ell_1,\ell_2$ and $m$ is its midpoint; in other words, $m=p$ and so $p\in\ell_3$.

As in Proposition 2.10, all of (ii) - (vi) are equivalent because angles in linear pairs are supplementary.
\end{proof}

\noindent Circles shall now be introduced: as in the Euclidean case, if $r$ is a positive real number and $o\in H^2(\mathbb R)$ is a regular point, the set of points $a$ whose distance to $o$ equals $r$ is called a circle.  It is clear that $o$ needs to be regular; if $o$ were an ideal point, its distances to other points would be infinite.  Nevertheless, there is a notion of a ``circle centered at an ideal point''; it will be covered in the next section.

For circles in the hyperbolic plane, the length of a diameter is exactly twice the radius.  Two different circles have at most two intersection points.  [All of this follows exactly as in the Euclidean case.]  Moreover, an interesting result about the Poincar\'e disk and half-plane models is this:\\

\noindent\textbf{Proposition 4.12.} \emph{In the Poincar\'e disk and half-plane models of the hyperbolic plane, the circles are precisely the Euclidean circles consisting entirely of regular points.  Moreover, Axiom 2.19 is satisfied in the hyperbolic plane.}\\

\noindent Each circle is a Euclidean circle, but unless it is centered at the origin in the Poincar\'e disk model, the center of the circle is a different point from the center of the Euclidean circle; see Exercise 12.
\begin{proof}
In the Poincar\'e disk model, it is clear that circles centered at $0$ are Euclidean circles.  However, any regular point can be transferred to $0$ via an isometry (Exercise 1(a)), and the isometries, being (anti)-M\"obius transforms, preserve generalized circles.  Thus, every circle is a generalized circle of $\overline{\mathbb C}$.  Since the disk and half-plane models are related via a M\"obius transform (Exercise 6), this is also true for circles in the half-plane model.  Since $\infty$ is not a regular point of either model, we conclude that every circle is a Euclidean circle (since it does not contain $\infty$, it is not a Euclidean line).

This proves that every circle is a Euclidean circle consisting of regular points.  Conversely, if $\omega$ is a Euclidean circle consisting of regular points, let $a,b,c\in\omega$ be three distinct points, and take perpendicular bisectors of the line segments connecting them, as in Exercise 17 of Section 2.1.  If $p$ is the intersection point of two of these perpendicular bisectors, then $p$ is equidistant from the points $a,b,c$; by applying an isometry, we may assume we are in the Poincar\'e disk model and $p=0$.  By Exercise 4, $a,b,c$ are then equidistant from the origin in the Euclidean sense, which implies that $\omega$ \---- the generalized circle determined by them \---- is a Euclidean circle centered at the origin.  As such, it is also a circle in the hyperbolic plane centered at the origin, and thus every Euclidean circle consisting of regular points is a hyperbolic circle.

Axiom 2.19 for the hyperbolic plane can readily be proven, by assuming $O$ is the origin of the Poincar\'e disk, and $O'$ is on the positive $x$-axis.  The argument is left to the reader.  Note Exercise 4 and the segment addition postulate.
\end{proof}

\noindent Observe that Proposition 2.20 cannot be applied to hyperbolic geometry because angles of triangles do not add to $180^\circ$ (they add to less than that).  Proposition 2.20 does not exactly hold in hyperbolic geometry; it does have an analogue, but we will not need it.\\ % https://www.maths.gla.ac.uk/wws/cabripages/hyperbolic/hybrid2.html

\noindent\textbf{CATEGORIZING ISOMETRIES OF $H^2(\mathbb R)$}\\

\noindent We shall conclude this section by naming different kinds of isometries of the hyperbolic plane, just like we did for the Euclidean plane in Section 2.6.  In the long run, these isometries are similar to the Euclidean ones.  We recall that every isometry of the Euclidean plane $\mathbb R^2$ is either the identity, a translation, a rotation, a reflection, or a glide reflection.  All of these isometries exist for the hyperbolic plane too.

A \textbf{translation} along a line $\ell$ is an orientation-preserving isometry which fixes $\ell$ and moves all the points in $\ell$ in the same direction along the line.  For example, if $\ell$ is the $y$-axis in the half-plane model, a translation is a Euclidean scaling map, $z\mapsto rz$ with $r>0$.  [Certainly these are not Euclidean isometries, but being M\"obius transforms which fix the rim, they are hyperbolic isometries.]  A \textbf{rotation} is an orientation-preserving isometry which fixes a regular point; for example, rotations around $0$ in the Poincar\'e disk model are Euclidean rotations (Exercise 2).  A \textbf{reflection} is an orientation-reversing isometry which fixes a line pointwise, and a \textbf{glide reflection} is a composition of a reflection over a line and a translation along that line.  The reader is encouraged to work these out for the $y$-axis in the half-plane model.  However, there is one kind of isometry which differs from the rest and doesn't exist in Euclidean geometry: it is called a \textbf{horolation}, and it intuitively rotates around an ideal point.  Just as we didn't define circles centered at ideal points, horolations have a different state of mind.

If $p$ is a regular point, the rotations around $p$ form a group isomorphic to the circle group $SO(2)$ (or $\mathbb R/\mathbb Z$), because one can use an isometry to assume $p=0$ in the disk, then apply Exercise 2.  If $p$ is an ideal point, however, there are many orientation-preserving isometries which fix $p$.  Examples are translations along a line containing $p$; such an isometry fixes one other ideal point (namely the other end of the line).  Horolations, contrariwise, fix the ideal point $p$ and \emph{no other points of the plane, whether ideal or not}; they form a group isomorphic to $\mathbb R$.

An example of a horolation can be given in the half-plane model: $z\mapsto z+r$, with $r\in\mathbb R$ fixed.\footnote{Do not get fooled: this is \emph{not} a translation, because horizontal lines are \emph{not} hyperbolic lines in this model.}  The only fixed point of this map is $\infty$, which is an ideal point.  Hence it ``rotates around $\infty$.''  An example of a horolation of the Poincar\'e disk model is
$$z\mapsto\frac{(2+ir)z-ir}{irz+(2-ir)}$$
because, as the reader can verify, the only fixed point of this in $\overline{H^2}(\mathbb R)$ is $1$.  It turns out that the trajectory of a regular point under the above map in the disk model (for various $r$) is a Euclidean circle tangent to the unit circle at $1$, as we will see in the next section.

Horolations are the only ``new'' kinds of isometries of the hyperbolic plane, as we will now show.\\

\noindent We start by letting $T$ be an orientation-preserving isometry of the half-plane (although it doesn't matter which model we use).  Then $T$ can be expressed as $T(z)=\frac{az+b}{cz+d}$ with $a,b,c,d\in\mathbb R,ad-bc>0$ (see the discussion under ``\textsc{Isometries}'').  What we can advantageously do is conjugate $A=\begin{bmatrix}a&b\\c&d\end{bmatrix}$ by an invertible real-valued matrix, to get a conjugate of $T$ in the group of isometries.  [If we conjugate by a matrix with negative determinant, the M\"obius transform corresponding to what we conjugated by would send the upper half of the plane to the lower half, but then we can compose it with complex conjugation, which commutes with the other transforms involved because they use real coefficients: in this case, we would be conjugating $T$ by an orientation-reversing isometry.] % I suppose I should have clarified what I meant.  If we conjugate by a matrix with the negative determinant, the *conjugate* will still fix the upper half, but the matrix we conjugated *by* sends the upper to the lower.

Thus, it is tempting to put the matrix $A$ in Jordan normal form.  However, we can only conjugate $A$ by \emph{real}-valued matrices, hence this cannot be done if $A$ has nonreal eigenvalues.  In this case, $A$'s eigenvalues are complex conjugates $u\pm vi$, and then $A$ is conjugate (by a real matrix) to $\begin{bmatrix}u&-v\\v&u\end{bmatrix}$.  Thus, depending on whether the eigenvalues are real, and whether their geometric multiplicities coincide with their algebraic multiplicities, $A$ can be conjugated to a real matrix of one of these forms:
$$\text{(i)} \begin{bmatrix}u&0\\0&v\end{bmatrix},u,v\ne 0;~~~~\text{(ii)} \begin{bmatrix}u&1\\0&u\end{bmatrix},u\ne 0;~~~~\text{(iii)} \begin{bmatrix}u&-v\\v&u\end{bmatrix},v\ne 0.$$
In case (i), $T$ gets conjugated to the map $U:z\mapsto\frac{uz}v=\frac uvz$.  The map $U$ multiplies elements of the complex plane by a fixed real scalar $\frac uv$, which must be positive (otherwise $U$ would send the upper half of the plane to the lower half).  In other words, $U$ is a dilation of the complex plane centered at the origin.  As a hyperbolic isometry, (unless $U$ is the identity), the only line which gets fixed by $U$ is the $y$-axis, and in fact, $U$ is a translation of the plane along the $y$-axis.  This means that $T$ is a translation (which could be along any line; it's just that whatever isometry we conjugated by must send this line to the $y$-axis).

In case (ii), $T$ gets conjugated to the map $U:z\mapsto\frac{uz+1}u=z+\frac 1u$.  This is a Euclidean horizontal translation, and its only fixed point is the ideal point $\infty$, which means that it is a horolation around the point $\infty$.  Consequently, $T$ is a horolation (which again could be around any ideal point).

In case (iii), $T$ gets conjugated to the map $U:z\mapsto\frac{uz-v}{vz+u}$.  Basic algebra shows that the only points in the extended complex plane fixed by $U$ are $\pm i$, as follows.  First, since $v\ne 0$, $U$ does not fix $\infty$.  If $z\in\mathbb C$, then
$$U(z)=z\iff\frac{uz-v}{vz+u}=z\iff uz-v=z(vz+u)=vz^2+uz$$
$$\iff vz^2+v=0\iff v(z^2+1)=0\iff z^2+1=0\iff z=\pm i$$
Since $-i$ is not in the upper half-plane, but $i$ is, we get that $i$ is the only fixed point of $U$, and is a regular point; therefore, $U$ rotates around the point $i$, and in this case $T$ is a rotation.

Hence the only orientation-preserving isometries of the hyperbolic plane are the identity, translations, horolations and rotations.

Now we let $T$ be an orientation-\emph{reversing} isometry.  This time, one can express it as $T(z)=\frac{a\overline z+b}{c\overline z+d}$ where $a,b,c,d\in\mathbb R,ad-bc<0$.  Again let $A=\begin{bmatrix}a&b\\c&d\end{bmatrix}$.  When we conjugate $A$ by an invertible real-valued matrix, we get a matrix which gives rise to a conjugate of $T$ in the isometry group.  [Commuting of the complex conjugation map is needed to verify this, but this map always commutes with the rational functions with real coefficients, i.e., if $a,b,c,d\in\mathbb  R$ then $\overline{\left(\frac{az+b}{cz+d}\right)}=\frac{a\overline z+b}{c\overline z+d}$.]  This time, since $\det A=ad-bc<0$, $A$ must have two distinct real eigenvalues, one positive and one negative (because, for instance, if its eigenvalues were nonreal, they would be complex conjugates, hence their product \---- the determinant of $A$ \---- would be positive).

Write the conjugate of $A$ as $\begin{bmatrix}u&0\\0&v\end{bmatrix}$ where $u>0,v<0$ are real numbers.  Then $T$ is conjugated to the map $U:z\mapsto\frac uv\overline z$.  If $\frac uv=-1$, then $U(z)=-\overline z$, which means $U$ is the Euclidean reflection over the imaginary axis (the $y$-axis).  Since the $y$-axis is a hyperbolic line, $U$ (hence also $T$) is a reflection over a hyperbolic line in this case.  We leave it to the reader to verify that if $\frac uv\ne -1$, $T$ is a glide reflection.  This means that, just like in the Euclidean case, every orientation-reversing isometry of the plane is either a reflection or a glide reflection.

\subsection*{Exercises 4.3. (The Hyperbolic Plane: Poincar\'e Disk and Half-Plane Models)} % Outline:
% Introduce points and lines in the two Poincar\'e models.  Now, define isometries, and show that they preserve lines, and use the notion to define
% circles, and Section 2.1's results (or analogues of the results).
\begin{enumerate}
\item Isometries of the hyperbolic plane act transitively on each of the following sets:

(a) The set of regular points, $H^2(\mathbb R)$.

(b) The set of ideal points $H^2_\infty(\mathbb R)$.

(c) The set of lines.

\item Let $T$ be an isometry of the Poincar\'e disk which fixes the origin, i.e., $T(0)=0$.  Show that $T$ is a Euclidean rotation or reflection around the origin.

\item (a) If $y_2>y_1>0$ are real numbers, then the distance between $iy_1,iy_2\in\overline{\mathbb C}$ in the Poincar\'e half-plane model is $\ln\frac{y_2}{y_1}$.  [Compute it as $\ln[\![iy_2,iy_1,0,\infty]\!]$.] % My fault

(b) Conclude that the distance between any two distinct non-ideal points in the hyperbolic plane is positive.  [Use a M\"obius transform.]

\item Let $z$ be a complex number with $|z|<1$.  Show the distance from $z$ (as a point in the Poincar\'e disk model) to $0$ is equal to $\ln\frac{1+|z|}{1-|z|}$.

\item (a) If $p\in H^2(\mathbb R)$, and $\vec v$ is a nonzero vector at $p$, show that there is a unique (hyperbolic) line through $p$ tangent to $\vec v$.

(b) Explain why this is false if $p$ is an ideal point.

\item Let $T:z\mapsto\frac{z-i}{z+i}$ and $U:w\mapsto i\frac{1+w}{1-w}$.  Then $T$ and $U$ are M\"obius transforms, with $T$ sending the upper half-plane model of the hyperbolic plane to the Poincar\'e disk model, and $U$ being its inverse.  This provides a way to convert between the two models given in this section.

\item (a) Show that every isometry of $H^2(\mathbb R)$ is the composition of finitely many reflections.  [Use the categorization of isometries.]

(b) If $0\ne a\in H^2(\mathbb R)$, then the unique reflection of $H^2(\mathbb R)$ sending $0$ to $a$ is given by $\psi(z)=\frac{a(\overline z-\overline a)}{\overline a(a\overline z-1)}$.  What line does it reflect across?

\item The isometry group of the upper half-plane model is generated by $z\mapsto z+r$ for $r\in\mathbb R$; $z\mapsto sz$ for $s\in\mathbb R_{>0}$; and the isometries $z\mapsto -\overline z$ and $z\mapsto 1/\overline z$.

\item If $y_1,y_2>0$, consider the regular points $z_1=x_1+y_1i,z_2=x_2+y_2i$ of the upper half-plane model of the hyperbolic plane.  The aim of this exercise is to show that their distance is
$$\rho(z_1,z_2)=\cosh^{-1}\left(1+\frac{(x_1-x_2)^2+(y_1-y_2)^2}{2y_1y_2}\right).$$
Here $\cosh^{-1}r=\ln(r+\sqrt{r^2-1})$, a partial inverse (for real numbers $\geqslant 1$) of the hyperbolic cosine $\cosh x=\frac{e^x+e^{-x}}2$.

(a) Show that the formula is satisfied if $x_1=x_2=0$, i.e., the points are on the $y$-axis.  [Use Exercise 3(a).]

(b) Show that $\rho$ is invariant under isometries: i.e., if $T$ is an isometry of the upper half-plane, then $\rho(T(z_1),T(z_2))=\rho(z_1,z_2)$.  [If the group of isometries acts on the set of ordered pairs of regular points, this is equivalent to saying $\rho$ is a function on orbits of said set.  By Proposition 1.22 and Exercise 8, it thus suffices to show the statement whenever $T$ is one of the isometries given in Exercise 8.]

(c) Conclude that the formula gives the distance for any two points.  [Use Exercise 1(c) and Proposition 1.20.]

\item Let $z_1,z_2$ be points in the Poincar\'e disk model of the hyperbolic plane (viewed as complex numbers).  Show that the distance between the points is
$$\rho(z_1,z_2)=\ln\frac{|\overline{z_1}z_2-1|+|z_1-z_2|}{|\overline{z_1}z_2-1|-|z_1-z_2|}.$$
[Show that $z\mapsto\frac{z_1-z}{\overline{z_1}z-1}$ is an isometry of the Poincar\'e disk sending $z_1\mapsto 0$; then use Exercise 4.]

\item Let $a,b,c,d$ be points in sequence around the rim $H^2_\infty(\mathbb R)$ of the hyperbolic plane.  Then $\overline{ab}$ and $\overline{cd}$ are divergent parallel lines, and their attention point is $\overline{ac}\cap\overline{bd}$.

\item In the Poincar\'e disk model of the hyperbolic plane, consider the circle with center $\frac 13\in H^2(\mathbb R)$ and radius $1$.  Find its Euclidean center and radius.  [Use symmetry considerations to show that the real axis is a diameter of both the hyperbolic circle and the Euclidean circle.]

\item Show that all nontrivial horolations of the hyperbolic plane are conjugate.

\item (a) If $T$ is a nontrivial translation of the hyperbolic plane, then the line along which it translates is unique.  [Consider ideal points which are fixed by $T$.]

(b) Explain why part (a) is false if $T$ is a translation of the Euclidean plane.

\item Let $\ell$ be the line in the upper half-plane model given by $|z|=1$.  (This is a semicircle arc of unit radius.)  Show that the reflection over $\ell$ coincides with the central inversion on this half-plane.

\item\emph{(Cross ratio formula for angle.)} \---- Let $\ell_1,\ell_2$ be intersecting lines in either the Poincar\'e disk or half-plane model (i.e., they intersect at a regular point).  Let $a,c$ be the ideal points of $\ell_1$ and $b,d$ the ideal points of $\ell_2$.  If $\theta$ is the angle between the lines, as seen in the sector between $a,b$, then $[\![a,b,c,d]\!]=\frac 2{1+\cos\theta}$.  [Use an isometry to assume that the lines are in the disk model and $\ell_1\cap\ell_2=0$.]

\item (a) Prove the inequalities in Exercise 11 of Section 2.1 for hyperbolic triangles.

(b) Show that for each point on a circle, there is a unique tangent line, which is perpendicular to the radius at that point.

\item\emph{(Angle bisectors.)} \---- If $\angle bac$ is an angle in the hyperbolic plane, define its \textbf{angle bisector} by extending the rays $\overset{\longrightarrow}{a~b}$ and $\overset{\longrightarrow}{a~c}$ to meet at ideal points $p,q$, then taking the line through $a$ perpendicular to $\overset{\longleftrightarrow}{p~q}$. 

If $a$ is a regular point, this bisects the angle in the usual sense.  However, if $a$ is an ideal point, \emph{every} ray from $a$ technically bisects the angle (because angles at $a$ are zero); yet the one whose construction is shown in this exercise is the most ``canonical.''
\end{enumerate}

\subsection*{4.4. Hypercycles and Horocycles}
\addcontentsline{toc}{section}{4.4. Hypercycles and Horocycles}
Hypercycles and horocycles are two special types of curves in the hyperbolic plane.  A hypercycle is a curve with a constant perpendicular distance to a line.  A horocycle is similar to a circle but it is centered at an ideal point.  Later we will see that both of these are, once again, generalized circles in the Poincar\'e disk and half-plane models.

To understand the concept of a hypercycle, we start by letting $\ell$ be a line in the \emph{Euclidean} plane.  Let $p$ be a point outside $\ell$, and consider all points that this point maps to via translations along $\ell$.  By basic algebra, these points form the line $\ell'$ through $p$ parallel to $\ell$.  Moreover, every point on $\ell'$ has the same distance to $\ell$ (Exercise 3 of Section 2.1), and every line perpendicular to $\ell$ is also perpendicular to $\ell'$ (by Proposition 2.10).

In the hyperbolic plane, however, no \emph{line} satisfies the above statements.  After all, suppose the trajectory of $p$ via the translations along $\ell$ formed a line $\ell'$.  Then if $\ell_1$ is the perpendicular to $\ell$ at any point, then (by symmetry considerations), $\ell_1$ meets $\ell'$ in a linear pair of two congruent angles, hence they are perpendicular.  This means that every perpendicular line to $\ell$ is also perpendicular to $\ell'$.  But this is impossible if $\ell'$ is a line: if $\ell$ and $\ell'$ intersect in either a regular or ideal point, we cannot have any line perpendicular to both of them, because that would entail a triangle with two right angles, contradicting Proposition 4.8; and if $\ell$ and $\ell'$ are divergent-parallel, only \emph{one} line is perpendicular to both of the lines by Theorem 4.7.

Thus, the points met by $p$ via the translations along $\ell$ do not form a line.  We then ask the question of what set $\ell'$ they \emph{do} form.  We may answer it easily by assuming that we are in the half-plane model, and that $\ell$ is the $y$-axis.  In this case, $p$ takes on the form $x+yi$ with $x\ne 0,y>0$ (because $p\notin\ell$).  The translations along $\ell$ (the $y$-axis) are the Euclidean dilations $z\mapsto rz,r>0$.  From there, we conclude that $\ell'=\{r(x+yi):r>0\}$, which is a slanted Euclidean line meeting the origin.
\begin{center}\includegraphics[scale=.16]{Hypercycle3.png}\end{center}
The following properties can be easily shown for the half-plane model with $\ell$ the $y$-axis.  Hence by isometry considerations, they follow in the general case (in either of the two models).
\begin{itemize}
\item $\ell'$ is a generalized circle, meeting the rim (the set of ideal points) at the same points where $\ell$ meets it.  $\ell'$ is not a geodesic unless $\ell'=\ell$. % At first I thought you were telling me "\ell' is not a geodesic unless \ell'=\ell", and I was like "I said it was a generalized circle, I didn't say it was a geodesic?"  Then I realized you were suggesting that I write this sentence

\item Every line perpendicular to $\ell$ is also perpendicular to $\ell'$, and vice versa.

\item Every point on $\ell'$ has the same (perpendicular) distance to $\ell$.

\item The reflection over any line perpendicular to $\ell$ (or $\ell'$) sends $\ell'$ to itself.

\item The translations along $\ell$ are precisely the orientation-preserving isometries that fix $\ell'$.  [Note that they are not the only orientation-preserving isometries which fix the line $\ell$; for that, there are also $180$-degree rotations around points on $\ell$.]
\end{itemize}
Examples of this in the Poincar\'e disk model are shown below.
\begin{center}\includegraphics[scale=.25]{Hypercycle1.png}\includegraphics[scale=.25]{Hypercycle2.png}\end{center}
The curve $\ell'$ in the hyperbolic plane is called a \textbf{hypercycle} (or \textbf{equidistant curve}).  The line $\ell$ is called its \textbf{axis}, \textbf{center} or \textbf{base line}.  The line segments from points on $\ell$ to points on $\ell'$ which are perpendicular to them are called \textbf{radii}, and the common length of these line segments is called the \textbf{radius} of the hypercycle.

As for horocycles, we recall that a circle centered at a regular point $p$ is the set of all points with a fixed given distance from $p$.  We wish to carry the idea over to the case where $p$ is an ideal point.  However, since finite distances do not exist for ideal points, we must think of a different strategy.  For that, we note that if $p$ is a regular point, then the circles centered at $p$ are precisely the curves to which all lines through $p$ are perpendicular.  [This is easy to see by assuming we are in the Poincar\'e disk model and $p=0$.]

Thus, if $p$ is an ideal point, we define a \textbf{horocycle} (centered at $p$) to be a curve $\omega$ to which all lines meeting $p$ are perpendicular.  In this situation, the reader can readily verify that, (conversely), every line perpendicular to $\omega$ meets the ideal point $p$.

To understand the basic structure of a horocycle, we shall (again) assume we are in the Poincar\'e half-plane model, and that $p=\infty$.  In this case, the lines passing through $p$ are precisely the (Euclidean) vertical lines.  Thus we want $\omega$ to be perpendicular to all of the vertical lines.  In this case, every tangent vector to $\omega$ is perpendicular to the vertical line through the point, hence is horizontal (parallel to the $x$-axis).  Elementary calculus shows that $\omega$ is then a Euclidean horizontal line.
\begin{center}\includegraphics[scale=.25]{Horocycle1.png}\end{center}
As in the case of hypercycles, there are many clear consequences for general horocycles $\omega$:
\begin{itemize}
\item $\omega$ is a generalized circle, meeting the rim at only the point $p$.

\item A line through $\omega$ is perpendicular to $\omega$ if and only if it meets $p$.

\item The reflection over any line through $p$ (or perpendicular to $\omega$) sends $\omega$ to itself.

\item Every horolation centered at $p$ fixes the horocycle $\omega$.  [Because if $p=\infty$ in the half-plane, the horolations centered at $p$ are the Euclidean horizontal translations.]  Conversely, every orientation-preserving isometry which fixes $\omega$ is a horolation centered at $p$.
\end{itemize}
\begin{center}\includegraphics[scale=.25]{Horocycle2.png}\includegraphics[scale=.2]{Horocycle3.png}\end{center}
We will not make much use of hypercycles, but horocycles will have a role in the next section.

\subsection*{Exercises 4.4. (Hypercycles and Horocycles)} % Motivate hypercycles by first showing that the locus of points with a given distance from a line is not itself a line.
% Motivate horocycles by thinking of them as "circles centered at infinity."  Cover basic structure, e.g., the fact that in the Poincar\'e disk model, horocycles are Euclidean circles.
\begin{enumerate}
\item Explain why three distinct points in the hyperbolic plane (some of which may be ideal points) determine exactly one of the following things: a line; a hypercycle; a horocycle; or a circle.

\item Given an example of two hypercycles intersecting in two points.  Explain why they cannot intersect in three or more.

\item Show that when two (Euclidean) circles intersect in two points, the intersection points share the same angle measures.  Use this to give an alternative proof that a circle tangent to the rim in the Poincar\'e disk model is a horocycle centered at the point of tangency.

\item Show that the group of isometries acts transitively on each of the following sets:

(a) The set of hypercycles of radius $r$, where $r$ is a fixed positive real number;

(b) The set of all horocycles.

\item Let $p$ be an ideal point, and $p_1,p_2$ regular points.  Show that the following are equivalent:

~~~~(i) $p_1$ and $p_2$ lie on the same horocycle centered at $p$;

~~~~(ii) The triangle with vertices $p,p_1,p_2$ has congruent angles at $p_1,p_2$;

~~~~(iii) $p$ is on the perpendicular bisector of the line segment $\overline{p_1~p_2}$.

[Assume you are in the half-plane model and $p=\infty$.  Show that each statement is equivalent to saying that, as complex numbers, $p_1,p_2$ have the same imaginary part.]

\item If $p_1,p_2\in H^2(\mathbb R)$ are distinct regular points, then there are exactly two horocycles containing them.  [Use the previous exercise.]

\item (a) If $\ell$ is a line and $p$ is an ideal point not on the line, then there is a unique horocycle centered at $p$ tangent to $\ell$.  [Assume $p=\infty$ in the half-plane model.]

(b) If $\ell$ is a line and $p$ is a regular point on $\ell$, then there are two horocycles tangent to $\ell$ at $p$, on either side of $\ell$.

\item Suppose $\ell$ is a line, $p$ is a point on $\ell$.  For each $r>0$, let $\omega_r$ be the circle of radius $r$ tangent to $\ell$ at the point $p$.  In the Euclidean plane, $\lim_{r\to\infty}\omega_r$ is the line $\ell$.  In the hyperbolic plane, $\lim_{r\to\infty}\omega_r$ is the horocycle of part (b) of the previous exercise.

\item\emph{(Apeirogons.)} \---- The concept of an apeirogon in the hyperbolic plane will be introduced in this exercise.

Let $a,b>0$ be fixed real numbers.  Let $S$ be the set of points $\{nb+ia:n\in\mathbb Z\}$ in the half-plane model of the hyperbolic plane.  Connect each pair $(nb+ia,(n+1)b+ia),n\in\mathbb Z$ with a (hyperbolic) line segment.  The resulting figure is illustrated below.
\begin{center}\includegraphics[scale=.3]{Apeirogon1.png}\end{center}
(a) The figure is a graph with infinitely many edges and vertices in both directions.  Moreover, the vertices are part of a horocycle, and all of the angles are congruent.

(b) In terms of $a$ and $b$, find the common angle measure.  Conclude that this angle measure can be arranged to be any number strictly between $0$ and $\pi$.

(c) The orientation-preserving isometries of the figure consist of a discrete group of horolations centered at $\infty$, which is isomorphic to $\mathbb Z$.  The orientation-reversing isometries of the figure are reflections over lines containing $\infty$.

(After possibly applying an isometry), the figure above is called an \textbf{apeirogon}.  It is intuitively a polygon with infinitely many sides.  There are clever ways of tiling $H^2(\mathbb R)$ with it, as will be seen in the next section.
\begin{center}\includegraphics[scale=.25]{Apeirogon2.png}\end{center} % Fun fact: a *pseudogon* is obtained by connecting line segments along a hypercycle.  Fix a,b>0,r>1, and let \ell' be the line y=(a/b)x; then take the sequence of points (br^n,ar^n),n\in\mathbb Z and connect them by segments.  Since isometries fixing $\ell'$ are Euclidean dilations, they fix the points when the dilation is by a power of r.  I didn't bother covering this.
\end{enumerate}

\subsection*{4.5. Triangles, Polygons and Tilings}
\addcontentsline{toc}{section}{4.5. Triangles, Polygons and Tilings}
We recall (Exercise 4 of Section 2.2) the laws of sines and cosines for triangles in the Euclidean plane.  In this section, we shall find similar laws for hyperbolic triangles.  These laws will be noticeably trickier because, for example, there are no such things as similar triangles; any two triangles with identical angles are congruent by Proposition 4.9(vi).  Once we establish them, we will cover basic ways to construct various polygons and tilings in the hyperbolic plane.

The main concept needed to study hyperbolic triangles are the hyperbolic functions.  [One of them is the hyperbolic cosine, which has already been covered in Exercise 9 of Section 4.3.]  They are similar to the trigonometric functions, but instead of going round in circles, they (intuitively) shoot off on a hyperbola.  To motivate the concept, we will start by mentioning Euler's discovery about exponentials of imaginary numbers: $e^{ix}=\cos x+i\sin x$.\footnote{Taking $x=\pi$ entails $e^{i\pi}=-1$, since $\cos\pi=-1$ and $\sin\pi=0$.  Adding $1$ to both sides, one gets Euler's famous statement, $e^{i\pi}+1=0$.}  There are many ways to see this:

\begin{itemize}
\item One way is by taking the Taylor series (centered at $0$) of each function, and then summing things together.  Since
$$e^x=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\frac{x^4}{4!}+\dots$$
$$\cos x=1-\frac{x^2}{2!}+\frac{x^4}{4!}-\frac{x^6}{6!}+\dots$$
$$\sin x=x-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}+\dots$$
\begin{align*}
\text{We have }e^{ix}
&=1+ix+\frac{(ix)^2}{2!}+\frac{(ix)^3}{3!}+\frac{(ix)^4}{4!}+\dots\\
&=1+ix-\frac{x^2}{2!}-\frac{ix^3}{3!}+\frac{x^4}{4!}+\dots\\
&=\left(1-\frac{x^2}{2!}+\frac{x^4}{4!}-\frac{x^6}{6!}+\dots\right)+i\left(x-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}+\dots\right)\\
&=\cos x+i\sin x.
\end{align*}
\item Alternatively, define $f(x)=\cos x+i\sin x$.  Then, since $\frac d{dx}\cos x=-\sin x$ and $\frac d{dx}\sin x=\cos x$, we get
$$\frac d{dx}f(x)=-\sin x+i\cos x=i(\cos x+i\sin x)=if(x).$$
Since $f'(x)=kf(x)\implies f(x)=Ae^{kx}$ for some constant $A$, we conclude that we can write $f(x)=Ae^{ix}$ for some $A$.  Moreover, $A=f(0)=\cos 0+i\sin 0=1$.  Therefore, $f(x)=e^{ix}$, which proves that $e^{ix}=\cos x+i\sin x$.
\end{itemize}
\noindent Since $\cos x$ is an even function and $\sin x$ is an odd function, the reader can readily verify that $e^{-ix}=\cos x-i\sin x$.  Basic algebraic manipulation thus enables us to formulate the trigonometric functions in terms of complex numbers and the exponential function:
$$\cos x=\frac{e^{ix}+e^{-ix}}2~~~~~~\sin x=\frac{e^{ix}-e^{-ix}}{2i}$$
[These are analytic interpretations of the trigonometric functions.]  Once sine and cosine are derived, it is easy to get the rest of the trigonometric functions, because, for example, $\tan x=\frac{\sin x}{\cos x}$ and $\sec x=\frac 1{\cos x}$:
$$\tan x=\frac{-i(e^{ix}-e^{-ix})}{e^{ix}+e^{-ix}}~~~~~~\cot x=\frac{i(e^{ix}+e^{-ix})}{e^{ix}-e^{-ix}}$$
$$\sec x=\frac 2{e^{ix}+e^{-ix}}~~~~~~\csc x=\frac{2i}{e^{ix}-e^{-ix}}$$
We will not make much use of these formulas; however, we get the hyperbolic functions by imitating the formulas without using the imaginary unit $i$ as a factor.  Thus, we define them as follows:
$$\cosh x=\frac{e^x+e^{-x}}2~~~~~~\sinh x=\frac{e^x-e^{-x}}2$$
$$\tanh x=\frac{e^x-e^{-x}}{e^x+e^{-x}}~~~~~~\coth x=\frac{e^x+e^{-x}}{e^x-e^{-x}}$$
$$\operatorname{sech}x=\frac 2{e^x+e^{-x}}~~~~~~\operatorname{csch}x=\frac 2{e^x-e^{-x}}$$
$\cosh x$ is called the \textbf{hyperbolic cosine} of $x$, $\sinh x$ is called the \textbf{hyperbolic sine}, and so on.  In mathematical expressions, these functions are denoted by adding an ``h'' at the end of the trigonometric function name.

The reader is encouraged to verify the following identities for the hyperbolic functions:
\begin{itemize}
\item $\cosh x$ and $\operatorname{sech}x$ are even functions; the other four are all odd functions.

\item $\cosh^2x-\sinh^2x=1$.  [This, of course, is analogous to the Pythagorean identity $\sin^2x+\cos^2x=1$.  It indicates that, just as $t\mapsto(\cos t,\sin t)$ parametrizes a circle in the plane, $t\mapsto(\cosh t,\sinh t)$ parametrizes a branch of the hyperbola $x^2-y^2=1$.]

\item $\tanh x=\frac{\sinh x}{\cosh x}$.

\item $\coth x=\frac 1{\tanh x}$; $\operatorname{sech}x=\frac 1{\cosh x}$; $\operatorname{csch}x=\frac 1{\sinh x}$.

\item $\cosh(x\pm y)=\cosh x\cosh y\pm\sinh x\sinh y$, and $\sinh(x\pm y)=\sinh x\cosh y\pm\cosh x\sinh y$.  (Therefore, taking $x=y$, we get $\cosh(2x)=\cosh^2x+\sinh^2x=2\cosh^2x-1=2\sinh^2x+1$, and $\sinh(2x)=2\sinh x\cosh x$.)
\end{itemize}
Note that the corresponding sum formulas for the trigonometric functions have a swapped sign: $\cos(x\pm y)=\cos x\cos y\mp\sin x\sin y$.  The hyperbolic functions do not.
\begin{itemize}
\item $\tanh(x\pm y)=\frac{\tanh x\pm\tanh y}{1\pm\tanh x\tanh y}$.  In particular, taking $x=y$, $\tanh(2x)=\frac{2\tanh x}{1+\tanh^2x}$.

\item $1-\tanh^2x=\operatorname{sech}^2x$, and $\coth^2x-1=\operatorname{csch}^2x$.

\item $e^x=\cosh x+\sinh x$.  [This is analogous to $e^{ix}=\cos x+i\sin x$.]\\
\end{itemize}
\noindent Now that we are familiar with the hyperbolic functions, we are ready to see how they relate to the hyperbolic triangles.  Throughout this section, a triangle is assumed to have three regular vertices (triangles with ideal vertices are covered in Exercise 2).  A triangle is given as $\triangle ABC$ where the vertices are capital letters; $A,B,C$ denote the angle measures, and $a,b,c$ denote the side lengths opposite those respective vertices:
\begin{center}\includegraphics[scale=.25]{HypTriangleSample.png}\end{center}
\noindent\textbf{Proposition 4.13.} \emph{For the above triangle,} %https://www.maths.gla.ac.uk/wws/cabripages/hyperbolic/hypertrig.html

(i) \emph{The \textbf{hyperbolic law of cosines} holds: $\cosh c=\cosh a\cosh b-\sinh a\sinh b\cos C$.}

(ii) \emph{The \textbf{hyperbolic law of sines} holds: $\frac{\sinh a}{\sin A}=\frac{\sinh b}{\sin B}=\frac{\sinh c}{\sin C}$.}

(iii) \emph{The \textbf{second hyperbolic law of cosines} holds: $\cos C=-\cos A\cos B+\sin A\sin B\cosh c$.}\\

\noindent Several remarks are in order.  To begin with, (i) derives the angle measures from the side lengths, and (iii) derives the side lengths from the angle measures.  The special case of (i) where $C=\pi/2$ shows that if $a,b,c$ are the sides of a hyperbolic right triangle with $c$ the hypotenuse, then $\cosh c=\cosh a\cosh b$.

Recall the Euclidean law of cosines: by Exercise 4(c) of Section 2.2, if $\triangle ABC$ is a Euclidean triangle then $c^2=a^2+b^2-2ab\cos C$.  For both types of geometry, you can see that if $C=\pi$, (which means $\cos C=-1$), then $c=a+b$ (for the Euclidean case it follows from the binomial theorem; for hyperbolic triangles it follows from the sum formula for the hyperbolic cosine mentioned earlier).  In this case, $\angle C$ is a striaght angle and $A,B,C$ are collinear with $C$ in between, so this is a manifestation of the segment addition postulate.  Similarly, if $C=0$ then $c=|a-b|$.
\begin{proof}
(i) By applying an isometry, we may assume we are in the Poincar\'e disk model, vertex $C$ is at $0$ and vertex $A$ is at a positive real number $r$.  Write vertex $B$'s location as $se^{iC}$ with $s$ a positive real number (this is plausible because $C$ is the Euclidean measure of $\angle ACB$).  By Exercise 1(c), we have $b=2\tanh^{-1}r$, and $a=2\tanh^{-1}|se^{iC}|=2\tanh^{-1}s$.  Moreover, by Exercise 10 of Section 4.3,
$$c=\ln\frac{|rse^{iC}-1|+|r-se^{iC}|}{|rse^{iC}-1|-|r-se^{iC}|}=2\tanh^{-1}\frac{|r-se^{iC}|}{|rse^{iC}-1|}.$$
Therefore, $\tanh(c/2)=\frac{|r-se^{iC}|}{|rse^{iC}-1|}$, from which $\tanh^2(c/2)=\frac{|r-se^{iC}|^2}{|rse^{iC}-1|^2}$ follows.  Yet
$$\cosh c=\frac{\cosh c}1=\frac{\cosh^2(c/2)+\sinh^2(c/2)}{\cosh^2(c/2)-\sinh^2(c/2)}$$
$$=\frac{[\cosh^2(c/2)+\sinh^2(c/2)]/\cosh^2(c/2)}{[\cosh^2(c/2)-\sinh^2(c/2)]/\cosh^2(c/2)}=\frac{1+\tanh^2(c/2)}{1-\tanh^2(c/2)}$$
$$=\frac{|rse^{iC}-1|^2+|r-se^{iC}|^2}{|rse^{iC}-1|^2-|r-se^{iC}|^2}$$
As $|z|^2=z\overline z$ for $z\in\mathbb C$, we may rewrite $|rse^{iC}-1|^2=(rse^{iC}-1)(rse^{-iC}-1)=r^2s^2-rs(e^{iC}+e^{-iC})+1=r^2s^2-2rs\cos C+1$, and $|r-se^{iC}|^2=(r-se^{iC})(r-se^{-iC})=r^2-rs(e^{iC}+e^{-iC})+s^2=r^2-2rs\cos C+s^2$.\footnote{We recall that $\cos C=\frac{e^{iC}+e^{-iC}}2$.}  Hence,
$$\frac{|rse^{iC}-1|^2+|r-se^{iC}|^2}{|rse^{iC}-1|^2-|r-se^{iC}|^2}=\frac{(r^2s^2+1-2rs\cos C)+(r^2+s^2-2rs\cos C)}{(r^2s^2+1-2rs\cos C)-(r^2+s^2-2rs\cos C)}$$
$$=\frac{r^2s^2+r^2+s^2+1-4rs\cos C}{r^2s^2-r^2-s^2+1}=\frac{(1+r^2)(1+s^2)-4rs\cos C}{(1-r^2)(1-s^2)}$$

By Exercise 1(d), $\cosh b=\frac{1+r^2}{1-r^2}$ and $\sinh b=\frac{2r}{1-r^2}$.  Similarly, $\cosh a=\frac{1+s^2}{1-s^2}$ and $\sinh a=\frac{2s}{1-s^2}$.  By basic algebraic manipulation, we get
$$\frac{(1+r^2)(1+s^2)-4rs\cos C}{(1-r^2)(1-s^2)}=\cosh a\cosh b-\sinh a\sinh b\cos C,$$
proving (i) as desired.

(ii) By symmetry considerations, it suffices to show that $\frac{\sinh a}{\sin A}=\frac{\sinh b}{\sin B}$.  Let $\alpha=\cosh a,\beta=\cosh b,\gamma=\cosh c$.  Then by the identity $\cosh^2x-\sinh^2x=1$, we get $\sinh^2a=\alpha^2-1$ and $\sinh^2b=\beta^2-1$.  By part (i), $\cosh c=\cosh a\cosh b-\sinh a\sinh b\cos C$; in other words, $\gamma=\alpha\beta-\sinh a\sinh b\cos C$.  Consequently, $\sinh a\sinh b\cos C=\alpha\beta-\gamma$.  Squaring throughout,
$$(\alpha\beta-\gamma)^2=\sinh^2a\sinh^2b\cos^2C$$
and hence,
$$\sinh^2a\sinh^2b\sin^2C=\sinh^2a\sinh^2b(1-\cos^2C)=\sinh^2a\sinh^2b-\sinh^2a\sinh^2b\cos^2C$$
$$=(\alpha^2-1)(\beta^2-1)-(\alpha\beta-\gamma)^2=1-\alpha^2-\beta^2-\gamma^2+2\alpha\beta\gamma$$
Observe that this expression is symmetric in the three variables $\alpha,\beta,\gamma$.  Repeating the argument with $a,b,c$ permuted gives
$$\sinh^2a\sinh^2c\sin^2B=1-\alpha^2-\beta^2-\gamma^2+2\alpha\beta\gamma$$
$$\sinh^2b\sinh^2c\sin^2A=1-\alpha^2-\beta^2-\gamma^2+2\alpha\beta\gamma$$
from which $\sinh^2a\sinh^2c\sin^2B=\sinh^2b\sinh^2c\sin^2A$.  Dividing by $\sinh^2c$ throughout, $\sinh^2a\sin^2B=\sinh^2b\sin^2A$.  Taking square roots, and noting that $a,b$ are positive and $0<A,B<\pi$ (so that everything will be positive), $\sinh a\sin B=\sinh b\sin A$.  We divide by $\sinh a\sinh b$ to get the desired statement $\frac{\sinh a}{\sin A}=\frac{\sinh b}{\sin B}$.

(iii) Let $\alpha=\cosh a,\beta=\cosh b,\gamma=\cosh c$ as in part (ii).  In that part, we have (effectively) shown that
$$\sinh a\sinh b\sin C=\sinh a\sinh c\sin B=\sinh b\sinh c\sin A=\Delta,$$
where $\Delta=\sqrt{1-\alpha^2-\beta^2-\gamma^2+2\alpha\beta\gamma}$.  Part (i) entails:
$$\begin{array}{c l}\sinh a\sinh b\cos C=\alpha\beta-\gamma\\\sinh a\sinh c\cos B=\alpha\gamma-\beta\\\sinh b\sinh c\cos A=\beta\gamma-\alpha\end{array}$$
and as in part (ii), $\sinh^2c=\gamma^2-1$.  Furthermore,
$$\frac{\cos C+\cos A\cos B}{\sin A\sin B}=\frac{(\alpha\beta-\gamma)(\gamma^2-1)+(\beta\gamma-\alpha)(\alpha\gamma-\beta)}{\Delta^2}$$
as can be seen by multiplying the numerator and denominator of the left-hand side by $\sinh^2c\sinh a\sinh b$.  Yet the right-hand side readily simplifies to $\gamma=\cosh c$.  The statement (iii) thus follows.
\end{proof}

\noindent Many other properties of hyperbolic triangles can be easily proven using Proposition 4.13.  For starters, $\triangle ABC$ is equilateral (i.e., has three equal sides) if and only if it has three equal angles, as a consequence of the Isosceles Triangle Theorem (4.10).  Thus if $\triangle ABC$ is an equilateral triangle, it has some side length $s$ and some angle $\theta$.  We observe that by Proposition 4.13(i), % The Isosceles Triangle Theorem was Proposition 4.10.  I'll remind the reader :)
$$\cosh s=\cosh^2s-\sinh^2s\cos\theta$$
This, along with the fact that $\sinh^2s=\cosh^2s-1$, entails 
$$(1-\cos\theta)\cosh^2s-\cosh s+\cos\theta=0$$
When $\theta$ is fixed, this is a quadratic equation in the variable $u=\cosh s$, and it is readily seen that the roots are $u=1,\frac{\cos\theta}{1-\cos\theta}$.  [$u=1$ is manifestly a root, and Vieta's formulas can easily be used to find the other root.]  Yet $u=1$ entails $s=0$, which is not possible.  Therefore, $\cosh s=u=\frac{\cos\theta}{1-\cos\theta}$.  Incidentally, since $\cosh s>1$, we get $\cos\theta>\frac 12$ in the result; i.e., $\theta<60^\circ=\frac{\pi}3$, which follows anyway from Proposition 4.8.

Either using Proposition 4.13(iii), or directly deriving from $\cosh s=\frac{\cos\theta}{1-\cos\theta}$, one gets $\cos\theta=\frac{\cosh s}{\cosh s+1}$.  Thus:\\

\noindent\textbf{Proposition 4.14.} \emph{If a hyperbolic equilateral triangle has side length $s$ and angle measure $\theta$, then $\cosh s=\frac{\cos\theta}{1-\cos\theta}$ and $\cos\theta=\frac{\cosh s}{\cosh s+1}$.}\\

\noindent Now we turn our attention to right triangles.  We assume $\triangle ABC$ is a right triangle with $\angle C$ the right angle, so that $a,b$ are the lengths of the legs and $c$ is that of the hypotenuse.  As previously stated, Proposition 4.13(i) entails $\cosh c=\cosh a\cosh b$.  Furthermore, by Proposition 4.13(iii), (along with the fact that $C=90^\circ$, so that $\cos C=0$ and $\sin C=1$),
$$\cos B=-\cos A\cos C+\sin A\sin C\cosh b=\sin A\cosh b$$
from which we conclude that $\cosh b=\frac{\cos B}{\sin A}$.  We immediately have a formula for one of the lengths of the legs.  By symmetry considerations, $\cosh a=\frac{\cos A}{\sin B}$.  Hence, multiplying them, $\cosh c=\cosh a\cosh b=\frac{\cos A\cos B}{\sin A\sin B}=\cot A\cot B$.  [Note that for a \emph{Euclidean} right triangle, one would have $\cot A\cot B=1$ because $A$ and $B$ are complementary angles.  This is not so for hyperbolic triangles!]

Furthermore, Proposition 4.13(ii) implies $\frac{\sinh a}{\sin A}=\frac{\sinh c}{\sin C}=\sinh c$, from which we get $\sin A=\frac{\sinh a}{\sinh c}$.  Thus, for a hyperbolic right triangle, the sine of an acute angle is the hyperbolic sine of the opposite leg divided by the hyperbolic sine of the hypotenuse.  You probably recall that for Euclidean triangles, the sine of an acute angle is the opposite leg divided by the hypotenuse; now we know a simlar statement for hyperbolic triangles.

Furthermore,
$$\cos^2A=1-\sin^2A=1-\left(\frac{\sinh a}{\sinh c}\right)^2=1-\frac{\sinh^2a}{\sinh^2c}$$
$$=\frac{\sinh^2c-\sinh^2a}{\sinh^2c}=\frac{(\sinh^2c+1)-(\sinh^2a+1)}{\sinh^2c}=\frac{\cosh^2c-\cosh^2a}{\sinh^2c}$$
$$\overset{(*)}=\frac{\cosh^2a\cosh^2b-\cosh^2a}{\sinh^2c}=\frac{\cosh^2a(\cosh^2b-1)}{\sinh^2c}=\frac{\cosh^2a\sinh^2b}{\sinh^2c}$$
$$=\frac{\cosh^2a\sinh^2b\cosh^2b}{\sinh^2c\cosh^2b}\overset{(*)}=\frac{\cosh^2c\sinh^2b}{\sinh^2c\cosh^2b}=\frac{\tanh^2b}{\tanh^2c}$$
where the parts marked $(*)$ make use of the fact that $\cosh c=\cosh a\cosh b$.  Since $\cos A>0$ ($A$ is acute), we conclude that $\cos A=\frac{\tanh b}{\tanh c}$.  This is similar to the identity $\cos A=\frac bc$ for Euclidean right triangles, but also quite different.

Finally, we compute
$$\tan A=\frac{\sin A}{\cos A}=\frac{\sinh a/\sinh c}{\tanh b/\tanh c}=\frac{\sinh a\tanh c}{\tanh b\sinh c}=\frac{\sinh a}{\tanh b}\frac{\tanh c}{\sinh c}$$
and since $\frac{\sinh}{\cosh}=\tanh$,
$$\frac{\sinh a}{\tanh b}\frac{\tanh c}{\sinh c}=\frac{\sinh a}{\tanh b}\frac 1{\cosh c}=\frac{\sinh a\cosh b}{\sinh b}\frac 1{\cosh c}$$
$$=\frac{\sinh a\cosh b}{\sinh b}\frac 1{\cosh a\cosh b}=\frac{\sinh a}{\sinh b\cosh a}=\frac{\tanh a}{\sinh b}.$$
We have just shown\\

\noindent\textbf{Proposition 4.15.} \emph{Let $\triangle ABC$ be a hyperbolic right triangle with $\angle C$ the right angle, and let $a,b,c$ be the side lengths opposite $A,B,C$ respectively.  Then:}

(i) \emph{$\cosh c=\cosh a\cosh b$.}

(ii) \emph{$\cosh b=\frac{\cos B}{\sin A}$, $\cosh a=\frac{\cos A}{\sin B}$ and $\cosh c=\cot A\cot B$.}

(iii) \emph{$\sin A=\frac{\sinh a}{\sinh c}$, $\cos A=\frac{\tanh b}{\tanh c}$ and $\tan A=\frac{\tanh a}{\sinh b}$.}\\

\noindent Just as in the Euclidean plane, one can define more general polygons. % "New paragraph" - I always use \noindent right after a proposition.
An $n$-gon consists of an ordered $n$-tuple of points $A_1,\dots,A_n$ (called \textbf{vertices}) and the $n$ line segments $\overline{A_1A_2},\dots,\overline{A_{n-1}A_n},\overline{A_nA_1}$ (called \textbf{edges} or \textbf{sides}), such that no two segments intersect each other unless they share a vertex.  As usual, we will restrict ourselves to convex polygons (there is a point not touching the sides, which is simultaneously inside all of the angles).  By adapting Proposition 2.30, and using Proposition 4.8 in place of Proposition 2.11, it is easy to show that the measures of the angles of a regular $n$-gon add to $<180(n-2)^\circ$; i.e., less than $\pi(n-2)$.  The definitions of an equilateral/equiangular/regular polygon are identical to those of Section 2.3.

It can be shown (by mirroring the argument of Section 2.3) that if $A_1\dots A_n$ is a regular $n$-gon, then there is a unique circle passing through all its vertices (called the \textbf{circumscribed circle}); the center of this circle is called the center of the polygon.  There is a unique circle tangent to all the sides (called the \textbf{inscribed circle}),\footnote{The situation is actually trickier for $n$-gons with ideal vertices; they do not generally have full dihedral symmetry (unless $n=3$).  We will not delve into details here; the polygon in this paragraph is assumed to have regular vertices.} which has the same center.  Also, for every circle there are regular $n$-gons inscribed in the circle and circumscribed around it.\\

\noindent\textbf{TILINGS}\\

\noindent We conclude this section by showing how polygons can tile the hyperbolic plane.  These tilings are constructed in an unfamiliar fashion because, unlike in the Euclidean plane, we are not free to choose the side length of a regular polygon.  It is not actually hard to find the side length of a regular $n$-gon when given one of its angle measures (Exercise 6); however, for semiregular tilings which use multiple types of polygons with the same side length, we will not know the angle measures so easily.  The ideal helping hand for this is the \emph{triangle pattern}.

Triangle patterns apply equally to Euclidean geometry (Chapter 2), hyperbolic geometry (this chapter) and spherical geometry (Chapter 5).  However, they were not introduced in Chapter 2 because Euclidean tilings were extremely easy to establish without them.

The main idea is this: let $a,b,c$ be integers $\geqslant 2$.  We wish to start with a triangle whose angle measures are exactly $\pi/a,\pi/b,\pi/c$.  Depending on how the sum of these angle measures compares with $\pi$, the triangle exists in exactly one of the three aforementioned types of geometry:
\begin{itemize}
\item If the angles add to $\pi$ (or what is the same thing, $\frac 1a+\frac 1b+\frac 1c=1$), the triangle is in the Euclidean plane, in view of Proposition 2.11.

\item If the angles add to $<\pi$ (i.e., $\frac 1a+\frac 1b+\frac 1c<1$), the triangle is in the hyperbolic plane, in view of Proposition 4.8.  Note: in this case, we may allow $a,b,c$ to be $\infty$, so that the corresponding vertices of the triangle may be ideal points.

\item If the angles add to $>\pi$ (i.e., $\frac 1a+\frac 1b+\frac 1c>1$), the triangle is on the sphere, as will be seen in the next chapter.
\end{itemize}
Elementary arithmetic shows that if $a,b,c$ are integers $\geqslant 2$ such that $\frac 1a+\frac 1b+\frac 1c=1$, then the unordered triple $[a,b,c]$ has only three possibilities: $[2,4,4]$, $[2,3,6]$ and $[3,3,3]$.  These give, in the Euclidean plane, the isosceles right triangle, the 30-60-90 right triangle (which is half the equilateral triangle), and the equilateral triangle.

However, if $\frac 1a+\frac 1b+\frac 1c<1$, there are infinitely many possibilities for $[a,b,c]$, and hyperbolic triangles exist for all of them (Exercise 5).  This gives rise to many triangles exclusive to the hyperbolic plane.  The best-known example is $[2,3,7]$ ($\frac 12+\frac 13+\frac 17=\frac{41}{42}$), which yields many common tilings, such as the triheptagonal tiling, and the truncated order-7 triangular tiling (the setting of the Zeno Rogue game \emph{HyperRogue}).  By allowing $a$, $b$ or $c$ to be infinity, we will also be able to establish tilings which use regular apeirogons (see Exercise 9 of Section 4.4).

Once the triangle is constructed in any of the geometries, here's what to do.  Start by placing the triangle anywhere in the plane.  Then, construct the image of that triangle when reflected over one of the sides.  Now do this to the rest of the edges of the original triangle, without erasing any triangles that have already been constructed.  This is illustrated below, with the blue triangle the original one we started with:
\begin{center}\includegraphics[scale=.4]{TriangleReflecting.png}\end{center}
Now for the three triangles just constructed, reflect them over their own edges.  [Each triangle will already have one of its edge reflections in the picture, though.]  Continue reflecting triangles over their own edges until they take up all the space.  This involves infinitely many triangles (except in the spherical case).  Since each vertex of the triangle fits around a pivot an exact, even number of times (e.g., the $\pi/a$-angle vertex fits $2a$ times), it can be proven that the triangles tile perfectly, with any two triangles either disjoint, sharing a single vertex, or sharing an entire edge; the rigorous argument will be omitted here.

The final results, for the $[2,3,6]$ case of the Euclidean plane, and the $[2,3,7]$ case of the (Poincar\'e disk model of the) hyperbolic plane, are shown below.
\begin{center}
\includegraphics[scale=.25]{EucTrianglePattern.png}~~~~
\includegraphics[scale=.25]{HypTrianglePattern.png}
\end{center} % Unfortunately, we are not yet ready to include the spherical [2,3,5] with the family...
Effectively, constructing this tiling only requires a basic knowledge of isometries in the geometry, and the properties of triangles.  Yet once you have triangle tilings, it is easy to get pretty much any uniform tiling, by doing a simple, specific construction using one of the triangles, passing it over to the others by reflecting, and then erasing the original triangles.

For instance, given the Euclidean tiling of 30-60-90 triangles displayed above, suppose only the shortest leg is drawn for each triangle, and then the triangles are erased.  Then what you get is the hexagonal tiling.  If, on the other hand, only the altitudes to the hypotenuses of the original triangles are visible, a tiling emerges which is made up of hexagons and triangles, with two of each kind of polygon at each vertex.
\begin{center}
\includegraphics[scale=.25]{HexTilingForm1.png}
\includegraphics[scale=.25]{HexTilingForm2.png}
\end{center}
The same strategy can be used to obtain hyperbolic tilings.
\begin{itemize}
\item Take the $[2,3,7]$ triangle pattern up above.  Draw only the short leg of each triangle, and you get the heptagonal tiling, shown below.  It consists of heptagons with $2\pi/3=120^\circ$ angles.  By Exercise 6(a), the side length of these heptagons is $\cosh^{-1}\left(\frac{4\cos(2\pi/7)+1}3\right)\approx 0.566256$.
\begin{center}
\includegraphics[scale=.3]{HeptagonalTiling.png}
\end{center}
Here is a sample of the same tiling in the Poincar\'e half-plane model:
\begin{center}
\includegraphics[scale=.3]{HeptagonalTiling_HP.png}
\end{center}

\item Start again with the $[2,3,7]$ pattern.  When the altitude to the hypotenuse of each triangle is drawn, you get the triheptagonal tiling, made of heptagons and triangles, shown on the left.  It is noticeably difficult to find the angles of these polygons without the use of triangle patterns, as the angle of the triangle must be supplementary to that of the heptagon, but they must also have the same side length.  On the other hand, in the $[2,3,7]$ pattern, suppose you construct the following for each triangle:
\begin{center}
Start by \emph{considering} the angle bisector of the $2\pi/3$-angle vertex.  (Don't construct it.)  At the point where it meets the long leg of the right triangle, construct perpendicular line segments to the short leg and the hypotenuse.  [The line segment to the short leg is actually contained in the long leg, because it's a right triangle.]  Thus the construction consists of two identical-length line segments.
\end{center}
Then, you get the truncated order-7 triangular tiling, or the setting of \emph{HyperRogue}, shown on the right.
\begin{center}
\includegraphics[scale=.3]{TriheptagonalTiling.png}~~~~
\includegraphics[scale=.3]{HyperrogueTiling.png}
\end{center}

\item Start with the $[3,3,4]$ (resp., $[3,3,5]$) triangle pattern.  This pattern consists of isosceles triangles, with angle measures $60^\circ$, $60^\circ$ and $45^\circ$ (resp., $36^\circ$).  Take one of the triangles, and draw an altitude from one of the $60^\circ$ vertices (i.e., not the one that goes to the midpoint of the base).  Drawing just these will give you a tiling of triangles and squares (resp., pentagons), with three of each kind of polygon at each vertex, as shown below.
\begin{center}
\includegraphics[scale=.3]{HypExoticTiling1.png}~~~~
\includegraphics[scale=.3]{HypExoticTiling2.png}
\end{center}
\emph{Caution}: Since the triangles in the original pattern are isosceles, and actually have some symmetry, you must make sure that \emph{only reflections over the edges} are used to transfer the constructed altitude throughout the plane.  A $45^\circ$ (or $36^\circ$) rotation around the smallest-angle vertex of the triangle will not give the desired tiling.

\item The $[2,3,\infty]$ triangle pattern consists of triangles with two vertices measuring $90^\circ$ and $60^\circ$, and one ideal vertex.  By Exercise 2(c), the length of the side opposite the ideal vertex is $\ln\sqrt 3=\frac 12\ln 3$.

When a triangle has ideal vertices, its triangle pattern can be used to construct uniform tilings with regular apeirogons.  For example, drawing only the finite side opposite the ideal vertex for each triangle entails the regular order-3 apeirogonal tiling, seen on the left.  On the other hand, drawing the line segments from the incenter of the triangle perpendicular to the sides, one gets the semi-regular tiling on the right.
\begin{center}
\includegraphics[scale=.3]{ApeirogonRegularTiling.png}~~~~
\includegraphics[scale=.3]{ApeirogonTiling.png}
\end{center}
You could alternatively get the first tiling by considering the $[\infty,\infty,\infty]$ triangle pattern, which consists entirely of ideal triangles.  Connecting the incenters of the triangles with line segments will give you the regular tiling.
\end{itemize}
\noindent All in all, you can get (almost) any uniform tiling in one of the kinds of geometry, by starting with the triangle pattern, doing a specific construction in one of the triangles, then passing it over to the others.  This is known as the \textbf{Wythoff construction}.

\subsection*{Exercises 4.5. (Triangles, Polygons and Tilings)} % Introduce triangles, establish all relations between them (https://en.wikipedia.org/wiki/Hyperbolic_triangle) then go over
% triangle groups.  Show how to mathematically construct certain (semi)-regular tilings (e.g., the one with vertex config 3-5-3-5-3-5).  Don't bother classifying (semi)-regular tilings; we
% didn't do that in Section 2.3.
% POTENTIAL: Mention Escher's art.  I said I would in the chapter intro.
\begin{enumerate}
\item The inverse hyperbolic functions are defined on the real numbers as follows:
$$\cosh^{-1}x=\ln(x+\sqrt{x^2-1})~~~~~~\sinh^{-1}x=\ln(x+\sqrt{x^2+1})$$
$$\tanh^{-1}x=\frac 12\ln\frac{1+x}{1-x}~~~~~~\coth^{-1}x=\frac 12\ln\frac{x+1}{x-1}$$
$$\operatorname{sech}^{-1}x=\ln\frac{1+\sqrt{1-x^2}}x~~~~~~\operatorname{csch}^{-1}x=\ln\left(\frac 1x+\frac{\sqrt{1+x^2}}{|x|}\right)$$

(a) Show that each of these functions is a \emph{right} inverse of the corresponding hyperbolic function (e.g., $\cosh(\cosh^{-1}x)=x$ when $\cosh^{-1}x$ is a well-defined real number).

(b) Explain why they need not be left inverses of the hyperbolic functions.  Find domains for the hyperbolic functions which would make these two-sided inverses, just as the arc-sine would be an inverse of the sine if the sine's domain were only $[-\pi/2,\pi/2]$.

(c) If $z$ is a point in the Poincar\'e disk model of the hyperbolic plane (viewed as a complex number with absolute value $<1$), then the distance from $0$ to $z$ is $2\tanh^{-1}|z|$.  [Exercise 4 of Section 4.3.]

(d) For $-1<x<1$, we have $\cosh(\tanh^{-1}x)=\frac 1{\sqrt{1-x^2}}$, and $\cosh(2\tanh^{-1}x)=\frac{1+x^2}{1-x^2}$.

(e) $\sinh(\tanh^{-1}x)=\frac x{\sqrt{1-x^2}}$, and $\sinh(2\tanh^{-1}x)=\frac{2x}{1-x^2}$.

\item (a) Given any two ideal triangles, there exists an isometry sending one to the other; in other words, the isometries act transitively on the ideal triangles.  [Recall that an ideal triangle is a triangle all of whose vertices are ideal points.]

(b) Let $\triangle ABC$ and $\triangle A'B'C'$ be triangles with $A,B,A',B'$ ideal points and $C,C'$ regular points.  These are triangles with exactly two ideal vertices.  Show that there is an isometry sending one to the other $\iff\angle C\cong\angle C'$.

(c) Let $\triangle ABC$ be an omega triangle with $A$ the ideal vertex.  If $\alpha,\beta$ are the angles at the regular vertices $B,C$, show that the length of side $\overline{BC}$ is $\ln(\cot(\alpha/2)\cot(\beta/2))$.  [Assume $A=\infty$ in the half-plane model.]  Conclude that the angle measures at $B,C$ and side length $\overline{BC}$ come in only two degrees of freedom.

\item The \textbf{area} of a hyperbolic triangle is defined to be $180^\circ=\pi$ minus the sum of its angles. % You have given the integral definition of the area of a triangle, which the book does not intend to cover until Chapter 6!

(a) Prove that this notion of area satisfies Lemma 2.29. Conclude that the area of a hyperbolic $n$-gon can be defined the same way the area of a Euclidean polygon was in Section 2.3.

(b) The area of a (hyperbolic) $n$-gon is $\pi(n-2)$ minus the sum of its angles.

\item If $s$ and $\theta$ are the side length and angle measure of a hyperbolic equaliteral triangle, show that

(a) $\cos\theta=\frac{\tanh(s/2)}{\tanh s}$;

(b) $\cosh(s/2)=\frac{\cos(\theta/2)}{\sin\theta}=\frac 1{2\sin(\theta/2)}$.

\item The objective of this exercise is to show that if $\alpha,\beta,\gamma$ are positive real numbers such that $\alpha+\beta+\gamma<\pi=180^\circ$, a hyperbolic triangle with angles $\alpha,\beta,\gamma$ exists.

(a) Let $c$ be the side length opposite $\gamma$.  Show that $0<c<\cosh^{-1}\left(\frac{1+\cos\alpha\cos\beta}{\sin\alpha\sin\beta}\right)$.  [Use Proposition 4.13(iii).]

(b) Now suppose $\alpha,\beta$ are fixed, but $c$ can vary, making the opposite angle $\gamma$ vary.  [With $\alpha,\beta,c$ given, the triangle is easy to construct.]  As $c\to 0$, $\cos\gamma\to-\cos(\alpha+\beta)$, and hence $\gamma\to\pi-\alpha-\beta$.  As $c\to\cosh^{-1}\left(\frac{1+\cos\alpha\cos\beta}{\sin\alpha\sin\beta}\right)$, $\gamma\to 0$.  [Note that it can be shown that $\cosh^{-1}\left(\frac{1+\cos\alpha\cos\beta}{\sin\alpha\sin\beta}\right)=\ln(\cot(\alpha/2)\cot(\beta/2))$, which makes sense in view of Exercise 2(c).]

(c) Conclude, using the Intermediate Value Theorem, that any third angle $\gamma\in(0,\pi-\alpha-\beta)$ can be obtained for a particular positive real number $c$.

\item Suppose a regular $n$-gon in the hyperbolic plane has side length $s$ and angle measure $\theta$.

(a) Show that $\cosh s=\frac{2\cos(2\pi/n)+1+\cos\theta}{1-\cos\theta}$.  [Connecting two consecutive vertices to the center of the polygon gives a triangle with angles $2\pi/n,\theta/2,\theta/2$ (why?).  Now use Proposition 4.13(iii).]

(b) Determine the radii of the circumcircle and incircle of the polygon.  [A right triangle can be constructed by connecting the center of the polygon to a vertex, and dropping a perpendicular from the center of the polygon to an adjacent edge.  One of the legs is an inradius and the hypotenuse is a circumradius (why?).  The remaining leg is half the side of the polygon.  Use Proposition 4.15 and part (a).]

\item Explain how to construct each of the following tilings (presumably using the Wythoff construction).  All polygons are regular.
\begin{center}
\includegraphics[scale=.3]{TruncatedHeptagonalTiling.png}~~~~
\includegraphics[scale=.3]{RhombitriheptagonalTiling.png}\\
Truncated Heptagonal Tiling~~~~~~~~~Rhombitriheptagonal Tiling
\end{center}
[For the latter one, start by considering the angle bisector of the right angle in the triangle pattern, without constructing it.]

\item For each of the triangle patterns $[2,3,6]$, $[2,3,7]$, what tiling emerges when, for each triangle, you draw the perpendicular line segments from the incenter to each side?  [The incenter of a hyperbolic triangle is defined like how Exercise 17 of Section 2.1 defined the incenter of a Euclidean triangle.  All of the necessary facts hold in the hyperbolic case as well.  Note that the aforementioned perpendicular line segments must all have the same length.]

\item M.C.~Escher (1898-1972) was a talented mathematical artist who was known to challenge people's sense of space.\footnote{A.~Redhunt. ``Tessellations of the Hyperbolic Plane and M.C. Escher.''  \emph{The Geometric Viewpoint}, Colby.}  He has made many kinds of uniform art for the Euclidean plane, the surfaces of polyhedra, and the hyperbolic plane.  Below is his hyperbolic-tiling art, \emph{Circle Limit IV}.
\begin{center}
\includegraphics[scale=.45]{LW436-MC-Escher-Circle-Limit-IV-19601.jpg}
M.C.~Escher: Circle Limit IV
\end{center}
(a) Find a uniform (polygonal) tiling off of which this art could be based.  [Consider connecting angel feet to devil feet.]

(b) What kinds of symmetry does it have?
\end{enumerate}

\subsection*{4.6. Generalized Spheres and M\"obius Transforms in Higher Dimensions}
\addcontentsline{toc}{section}{4.6. Generalized Spheres and M\"obius Transforms in Higher Dimensions}
Just like the previous two kinds of geometry, we would like to generalize hyperbolic geometry to any number of dimensions.  However, it is not so clear how to do this, because the hyperbolic plane has been based off the complex plane, which cannot be generalized to arbitrary dimensions.  [In fact, $\mathbb R^3$ can't be seen as an extension of the complex numbers, but Hamilton discovered $\mathbb R^4$ as such an extension, which is what we now know as quaternions.  For most $n$, $\mathbb R^n$ is not a number system with all the desirable properties.] % OFC \mathbb R^n$ is a number system if you don't mind zero divisors, just add and multiply coordinatewise.

This brings a handicap to the concept of M\"obius transforms.  However, we will still see that the concept works fine, with the aid of Chapter 1's results.  If $n$ is a positive integer, we let $\overline{\mathbb R^n}=\mathbb R^n\sqcup\{\infty\}$, where $\infty$ is a symbolic point.  [Remember, this is not projective $n$-space unless $n=1$: it has only \emph{one} infinity point.]  We let $S^n$ be the hypersphere $\{\vec v\in\mathbb R^{n+1}:\|\vec v\|=1\}$, and we identify $\overline{\mathbb R^n}$ with the hyperplane in $\overline{\mathbb R^{n+1}}$ given by $x_{n+1}=0$.

As in Section 4.1, we define \textbf{stereographic projection} to be the map $\varphi:S^n\to\overline{\mathbb R^n}$, which sends $N=\vec e_{n+1}$ to $\infty$, and each other point $p\in S^n$ to the intersection with $\overline{\mathbb R^n}$ of the line through $N$ and $p$.  By imitating the arguments in Section 4.1 for the case $n=2$, one can verify the following formulas (the first one assumes $x_{n+1}\ne 1$):
\begin{equation}\tag{F1}
\varphi(x_1,\dots,x_n,x_{n+1})=\left(\frac{x_1}{1-x_{n+1}},\frac{x_2}{1-x_{n+1}},\dots,\frac{x_n}{1-x_{n+1}}\right)
\end{equation}
\begin{equation}\tag{F2}
\varphi^{-1}(x_1,\dots,x_n)=\left(\frac{2x_1}{x_1^2+\dots+x_n^2+1},\dots,\frac{2x_n}{x_1^2+\dots+x_n^2+1},\frac{x_1^2+\dots+x_n^2-1}{x_1^2+\dots+x_n^2+1}\right)
\end{equation}
We furthermore define the \textbf{central inversion} (or \textbf{hypersphere inversion}; sphere inversion in the case $n=3$) to be the map $\psi=\varphi\circ R\circ\varphi^{-1}:\overline{\mathbb R^n}\to\overline{\mathbb R^n}$ where $R$ is the reflection over $\overline{\mathbb R^n}$; i.e., $R(x_1,\dots,x_n,x_{n+1})=(x_1,\dots,x_n,-x_{n+1})$.  Imitating Section 4.1, the reader is encouraged to establish this formula:
$$\psi(x_1,\dots,x_n)=\left(\frac{x_1}{x_1^2+\dots+x_n^2},\dots,\frac{x_n}{x_1^2+\dots+x_n^2}\right)$$
In other words, whenever $\vec v\in\mathbb R^n$ is a nonzero vector, $\psi(\vec v)=\frac 1{\|\vec v\|^2}\vec v$.  Of course, we have $\psi(\vec 0)=\infty$ and $\psi(\infty)=\vec 0$.  In addition, $\psi$ fixes the unit hypersphere $S^{n-1}$.

The following properties can then be established, in pretty much the same way they were the first time around.  The verifications are left to the reader.  [$k$-planes are assumed to include the point $\infty$.]
\begin{itemize}
\item For $0\leqslant k<n$, stereographic projection sends $k$-spheres not containing $N$ to $k$-spheres, and $k$-spheres containing $N$ to $k$-planes.  Conversely, every $k$-sphere or $k$-plane in $\overline{\mathbb R^n}$ is the image of a $k$-sphere on $S^n$.

\item Central inversion fixes $k$-planes containing the origin.  It sends $k$-planes not through the origin to $k$-spheres through the origin, and vice versa.  It sends $k$-spheres not through the origin to other such $k$-spheres.

\item Stereographic projection and central inversion are conformal.  [A hint for proving this is to find a formula for the Jacobian of (F2), and then show that its columns are orthogonal and have the same magnitude.]
\end{itemize}
Influenced by these ideas, we define a \textbf{generalized $k$-sphere} in $\overline{\mathbb R^n}$ to be either a $k$-sphere in $\mathbb R^n$, or a set of the form $P\cup\{\infty\}$ where $P$ is a $k$-plane.  If $k=n-1$, a generalized $k$-sphere is called a \textbf{generalized hypersphere}.  A generalized $1$-sphere is called a \textbf{generalized circle}.  If $n=3$, then we shall call a generalized $2$-sphere a \textbf{generalized sphere} for simplicity.

In defining a M\"obius transform for arbitrary dimensions, our main fundamental result is this.\\

\noindent\textbf{Proposition 4.16 and Definition.} \emph{If $n\geqslant 2$ and $T:\overline{\mathbb R^n}\to\overline{\mathbb R^n}$ is a bijection, the following are equivalent:}

(i) \emph{$T$ is of one of the following forms:}
$$T(\vec v)=\alpha A\vec v+\vec\beta\text{~~~~or~~~~}T(\vec v)=\alpha A(\psi(\vec v-\vec\gamma))+\vec\beta$$
\emph{where $0\ne\alpha\in\mathbb R$, $\vec\beta,\vec\gamma\in\mathbb R^n$, $A\in O(n)$ and $\psi$ is the central inversion.}

(ii) \emph{$T$ is a conformal map which preserves generalized $k$-spheres for any $0\leqslant k<n$.}

(iii) \emph{$T$ is a conformal map which preserves generalized hyperspheres.}

\emph{Such a transformation $T$ is called a \textbf{M\"obius transform} of $\overline{\mathbb R^n}$.}\\

\noindent There is an unsatisfying inconsistency; according to Section 4.2, M\"obius transforms of the Riemann sphere $\mathbb C\sqcup\{\infty\}$ must be orientation-preserving; the orientation-reversing ones are called anti-M\"obius transforms.  However, when dealing with the general case, we shall use the term ``M\"obius transform'' regardless of what happens to the orientation.
\begin{proof}
(i) $\implies$ (ii). Bijections satisfying (ii) are clearly closed under function composition; yet they include dilations, orthogonal transformations, translations, and the central inversion (as previously mentioned).

(ii) $\implies$ (iii) is clear.

(iii) $\implies$ (i). Either $T(\infty)$ is equal to $\infty$ or it isn't.  Let us first suppose $T(\infty)=\infty$.  Then $T$ preserves (Euclidean) hyperplanes, because they are precisely the generalized hyperspheres containing $\infty$.  Let $\vec\beta=T(\vec 0)$.  Each coordinate axis of $\mathbb R^n$ gets mapped by $T$ to a line (through $\vec\beta$), because the coordinate axis is an intersection of $n-1$ hyperplanes, and the bijection $T$ commutes with this intersection.  Since $T$ is conformal, these output lines are perpendicular to each other.  We may thus let $\vec v_j=\frac{T(\vec e_j)-\vec\beta}{\|T(\vec e_j)-\vec\beta\|}$ for each $j$; the $\vec v_j$ are orthonormal vectors and hence
$$A=\begin{bmatrix}\uparrow&\uparrow&\dots&\uparrow\\\vec v_1&\vec v_2&\dots&\vec v_n\\\downarrow&\downarrow&\dots&\downarrow\end{bmatrix}$$
is an orthogonal matrix, i.e., $A\in O(n)$.  Define $S(\vec v)=A^{-1}(T(\vec v)-\vec\beta)$.  Then construction shows that $S$ satisfies (iii), $S(\vec 0)=\vec 0$ and for each $j$, $S(\vec e_j)=\lambda_j\vec e_j$ for some $\lambda_j\ne 0$ in $\mathbb R$.

As $S(\infty)=\infty$, $S$ preserves Euclidean hyperplanes.  Thus it must preserve the hyperplane $x_1=0$ (because this is the hyperplane spanned by $\vec 0,\vec e_2,\dots,\vec e_n$, whose images under $S$ are scalar multiples).  Consequently, it preserves the hyperplanes parallel to $x_1=0$, since a bijection preserves disjoint sets by elementary set theory; this means that it sends every hyperplane $x_1=c$ to a hyperplane of the form $x_1=d$, and thus the first coordinate $x_1$ of $S(\vec v)$ is completely determined by the first coordinate of $\vec v$.  The same argument holds for the rest of the coordinates.  Thus, we may write $S(x_1,\dots,x_n)=(f_1(x_1),\dots,f_n(x_n))$; the Jacobian $dS$ can then be written as
$$dS_{(x_1,\dots,x_n)}=\begin{bmatrix}\frac{\partial f_1}{\partial x_1}\\&\frac{\partial f_2}{\partial x_2}\\&&\ddots\\&&&\frac{\partial f_n}{\partial x_n}\end{bmatrix}$$
Since $S$ is conformal, this matrix has orthogonal columns with the same magnitude.  Yet the magnitude of the $j$-th column is a function of $x_j$ alone \---- this implies that this common magnitude is constant.  If the columns have a magnitude of $\alpha^2$ ($\alpha>0$), then each diagonal entry is either $a$ or $-a$; by multiplying $A$ by a suitable diagonal matrix with $\pm 1$'s on the diagonal, we may assume that the diagonal entries are all equal to $\alpha$.  In this case, $dS=\alpha I_n$, so that (since $S(\vec 0)=\vec 0$), $S(\vec v)=\alpha\vec v$, and $T(\vec v)=A(S(\vec v))+\vec\beta=\alpha A\vec v+\vec\beta$.

This completes the proof where $T(\infty)=\infty$.  Now suppose $T(\infty)\ne\infty$.  Let $\vec\gamma=T^{-1}(\infty)$, and define $S(\vec v)=T(\psi(\vec v)+\vec\gamma)$.  Then $S$ satisfies (iii) and $S(\infty)=\infty$.  Therefore, by the previous two paragraphs, $S(\vec v)=\alpha A\vec v+\vec\beta$ with $0\ne\alpha\in\mathbb R$, $\vec\beta\in\mathbb R^n$ and $A\in O(n)$.  Consequently, $T(\vec v)=S(\psi(\vec v-\vec\gamma))=\alpha A(\psi(\vec v-\vec\gamma))+\vec\beta$.
\end{proof}

\noindent\emph{Remark.} There is actually a deep theorem (due to Liouville) that states that if $n\geqslant 3$, then just $T$ being conformal guarantees that it is a M\"obius transform.  [This theorem is false for $n=2$; in fact, by the Riemann mapping theorem, there are conformal mappings from the disk to any homeomorphic region in $\mathbb R^2$ whatsoever.]  We will omit the details here.\\

\noindent By characterizing M\"obius transforms via property (ii) or (iii) in Proposition 4.16, it is clear that they form a group under function composition.  This group will be important to us later on.

We will depend heavily on M\"obius transforms in studying higher-dimensional hyperbolic spaces in the next section.

\subsection*{Exercises 4.6. (Generalized Spheres and M\"obius Transforms in Higher Dimensions)} % Introduce stereographic projection in higher dimensions, then go over generalized
% spheres and M\"obius transforms.  (Mention Liouville's theorem without trying to prove it)
\begin{enumerate}
\item Let $S\subset\mathbb R^n$ be a hypersphere which contains the origin (on its rim).  Show that when the central inversion in $\mathbb R^n$ is restricted to $S$, the result is stereographic projection from $S$ to a hyperplane, and the projection is taken from the origin.

\item Let $T$ be a M\"obius transform of $\overline{\mathbb R^n}$.

(a) If $T(p)=p$ for every $p\in\overline{\mathbb R^{n-1}}$, show that $T$ is either the identity or the reflection over $\overline{\mathbb R^{n-1}}$.  [According to statement (i) of Proposition 4.16, M\"obius transforms that fix $\infty$ are particularly easy to deal with.]

(b) If $T(p)=p$ for every $p\in S^{n-1}$, show that $T$ is either the identity or the central inversion.  [Establish a M\"obius transform sending $S^{n-1}\to\overline{\mathbb R^{n-1}}$, and then use it to conjugate $T$.]

\item Let $S$ be the hypersphere centered at $\vec c$ with radius $r>0$.  If $T(\vec v)=r^2\psi(\vec v-\vec c)+\vec c$, then $T$ is the unique M\"obius transform other than the identity which fixes every point of $S$.  [$T$ is called the \textbf{inversion via the hypersphere $S$}.]

\item Let $\varphi:S^n\to\overline{\mathbb R^n}$ be stereographic projection.

(a) If $A\in O(n+1)$ is viewed as a bijection of $S^n$, then conjugating it by stereographic projection ($\varphi\circ A\circ\varphi^{-1}:\overline{\mathbb R^n}\to\overline{\mathbb R^n}$) gives a M\"obius transform.  [Use statement (iii) in Proposition 4.16.]

(b) Show by example that a M\"obius transform of $\overline{\mathbb R^n}$ need not be conjugated to an orthogonal transformation.

\item Define $S:\overline{\mathbb R^n}\to\overline{\mathbb R^n}$ via $S(\vec 0)=\infty$, $S(\infty)=\vec 0$ and $S(\vec v)=-\frac 1{\|\vec v\|^2}\vec v$.

(a) $S$ is a M\"obius transform.

(b) When $S$ is conjugated via stereographic projection to a map $S^n\to S^n$, this map is the antipodal map $\vec p\mapsto -\vec p$ of the hypersphere. % For a second I thought you were saying the conjugate wasn't the antipodal map. kek

(c) If $T$ is a M\"obius transform of $\overline{\mathbb R^n}$, then $S\circ T=T\circ S$ if and only if $T$ is conjugated via stereographic projection to an orthogonal transformation of the hypersphere.  [If $S\circ T=T\circ S$, first show that the conjugation of $T$ preserves the great $(n-1)$-spheres of $S^n$, i.e., those which have the same Euclidean center as $S^n$.  Then use Proposition 1.9 to assume the transformation fixes certain ones.]  Use this exercise and the previous one to conclude that $O(n+1)$ is isomorphic to the centralizer of $S$ in the group of M\"obius transforms of $\overline{\mathbb R^n}$ [Exercise 11(b) of Section 1.2].

\item If $T$ is a M\"obius transform of $\overline{\mathbb R^n}$ and $\psi$ is the central inversion, then $T\circ\psi=\psi\circ T$ if and only if $T$ fixes the sphere $S^{n-1}$ (not necessarily pointwise).  [Use Exercise 2(b).]  Note that such a transformation $T$ need not fix the closed ball $\{\vec v\in\mathbb R^n:\|\vec v\|\leqslant 1\}$, because it may send the interior of the ball to the exterior (e.g., if $T=\psi$).

\item Let $\omega$ be a generalized $k$-sphere, $0\leqslant k<n$.  Show that the following are equivalent:

~~~~(i) The central inversion sends $\omega$ to itself;

~~~~(ii) $\omega$ meets the unit hypersphere orthogonally;

~~~~(iii) Either $\omega$ is a $k$-plane through the origin, or else it is the intersection of a $(k+1)$-plane\footnote{If $k=n-1$, we take this ``$(k+1)$-plane'' to be all of $\overline{\mathbb R^n}$.} through the origin with a hypersphere of radius $r$ and center $\vec a\in\mathbb R^n$ such that $\|\vec a\|^2-r^2=1$.

Furthermore, if these conditions hold and $\omega$ is a $k$-sphere, then the radius $r$ and center $\vec a$ of $\omega$ satisfy $\|\vec a\|^2-r^2=1$.  

\item Let $\omega$ be a generalized $k$-sphere, $0\leqslant k<n$.  Show that the following are equivalent:

~~~~(i) The central inversion sends $\omega$ to its own negation (i.e., $\omega$ is fixed by the transformation in Exercise 5);

~~~~(ii) The corresponding $k$-sphere on the $n$-sphere (via stereographic projection) shares the same Euclidean center as the $n$-sphere;

~~~~(iii) The corresponding $k$-sphere on the $n$-sphere contains at least one point and its antipode;

~~~~(iv) The corresponding $k$-sphere on the $n$-sphere is closed under taking antipodes;

~~~~(v) Either $\omega$ is a $k$-plane through the origin, or else it is the intersection of a $(k+1)$-plane through the origin with a hypersphere of radius $r$ and center $\vec a\in\mathbb R^n$ such that $r^2-\|\vec a\|^2=1$.

Furthermore, if these conditions hold and $\omega$ is a $k$-sphere, then the radius $r$ and center $\vec a$ of $\omega$ satisfy $r^2-\|\vec a\|^2=1$.

[Adapt Exercises 6-7 of Section 4.2.]

\item (a) Show that the dihedral angle between generalized spheres in $\overline{\mathbb R^3}$ is constant throughout their intersection.  The dihedral angle between surfaces at an intersection point is the angle between the tangent vectors to the surfaces at that point which are perpendicular to the intersection.

(b) Show that the dihedral angle between a generalized sphere and a generalized circle (when they intersect in two points) is the same at the two points.
\end{enumerate}

\subsection*{4.7. Hyperbolic $n$-space}
\addcontentsline{toc}{section}{4.7. Hyperbolic $n$-space}
Given the material from the previous section, we can readily study (or introduce) hyperbolic space in an arbitrary number of dimensions.  We do this merely by generalizing the material of Section 4.2, using the previous section in place of Section 4.1.  As before, the isometries are the M\"obius transforms, and will play a major role in the study.

We let $H^n(\mathbb R)$ be the open unit ball $\{\vec v\in\mathbb R^n:\|\vec v\|<1\}$, and $\overline{H^n}(\mathbb R)$ the closed unit ball $\{\vec v\in\mathbb R^n:\|\vec v\|\leqslant 1\}$.  We let $H^n_\infty(\mathbb R)=\overline{H^n}(\mathbb R)-H^n(\mathbb R)=\{\vec v\in\mathbb R^n:\|\vec v\|=1\}$.  $H^n(\mathbb R)$ is referred to as the \textbf{Poincar\'e ball model} of hyperbolic $n$-space, $H^n_\infty(\mathbb R)$ is referred to as the \textbf{rim} / \textbf{boundary at infinity}, and its elements are called \textbf{ideal points}.  Observe that $H^n_\infty(\mathbb R)$ is a generalized hypersphere.

For $0\leqslant k<n$, a \textbf{$k$-plane} is defined to be a set of the form $\omega\cap\overline{H^n(\mathbb R)}$, where $\omega$ is a generalized $k$-sphere which meets the rim orthogonally.  Generally, a $1$-plane is called a \textbf{line}, a $2$-plane is called a \textbf{plane}, and an $(n-1)$-plane is called a \textbf{hyperplane}.

Alternatively, if we take $H^n(\mathbb R)$ to be the upper half of $\mathbb R^n$ given by $\{(x_1,\dots,x_n)\in\mathbb R^n:x_n>0\}$, $H^n_\infty(\mathbb R)$ to be the generalized hypersphere $x_n=0$ (in particular, it contains $\infty$), and $\overline{H^n}(\mathbb R)$ to be $H^n(\mathbb R)\cup H^n_\infty(\mathbb R)$, then we get the \textbf{Poincar\'e half-space model} of hyperbolic $n$-space.  Its $k$-planes, as before, are sets of the form $\omega\cap\overline{H^n}(\mathbb R)$ where $\omega$ is a generalized $k$-sphere which meets the rim orthogonally; in particular, hyperplanes are Euclidean hyperplanes parallel to the $x_n$-axis and portions of hyperspheres centered on $x_n=0$.

For definiteness, $H^n(\mathbb R),\overline{H^n}(\mathbb R),H^n_\infty(\mathbb R)$ will refer to the ball model, unless otherwise specified.\\

\noindent\textbf{ISOMETRIES AND DISTANCE}\\

\noindent As before, the easiest step for now is to cover what the isometries are.  In either the Poincar\'e ball or half-space model, we would like an isometry to be a M\"obius transform which fixes the subset $H^n(\mathbb R)$.  It is clear that such a map fixes the generalized hypersphere $H^n_\infty(\mathbb R)$, and (since they are conformal), they therefore preserve $k$-spheres orthogonal to $H^n_\infty(\mathbb R)$: in other words, they preserve $k$-planes.

In Proposition 4.18, we will obtain a distance metric which is invariant under these transformations.  Since we cannot define cross ratios for $\mathbb R^n$ the way we did for the complex plane, we must define the notion of distance rather differently.  Our trick is to use Proposition 1.21, which tells us we need only define it on certain kinds of points, provided certain conditions are satisfied.\\

\noindent\textbf{Lemma 4.17.} \emph{Two points $p,q\in H^n(\mathbb R)$ determine a line.}
\begin{proof}
As in Section 4.3, it is routine to show that the group of M\"obius transforms acts transitively on $H^n(\mathbb R)$.  Thus, by applying a M\"obius transform, we may assume $p=0$, the Euclidean origin of the ball.  In that case, the Euclidean line $\ell$ through $0$ and $q$ is a diameter of the hypersphere, hence is orthogonal to the ball, i.e., a hyperbolic line.  If $\ell'$ is another line going through $0$ and $q$, then (assuming $\ell'\ne\ell$), $\ell'$ cannot be a Euclidean line, because $\ell$ is the \emph{unique} Euclidean line through $0$ and $q$.  Hence $\ell'$ is a Euclidean circle, and its radius $r$ and center $\vec a$ satisfy $\|\vec a\|^2-r^2=1$ by Exercise 7 of the previous section.  Yet, since $0\in\ell'$, we must have $\|\vec a\|=r$, which leads to a clear contradiction.
\end{proof}
\noindent\textbf{Proposition 4.18 and Definition.} \emph{Let $H^n(\mathbb R)$ be the half-space model for hyperbolic $n$-space.  There is a unique function $\rho:H^n(\mathbb R)\times H^n(\mathbb R)\to\mathbb R$ such that:}

(i) \emph{$\rho(T(p),T(q))=\rho(p,q)$ whenever $T$ is a M\"obius transform of $H^n(\mathbb R)$; in other words, $\rho$ is invariant under these M\"obius transforms.s}

(ii) \emph{$\rho(a\vec e_n,b\vec e_n)=|\ln(b/a)|$, where $a,b>0$ and $\vec e_n$ is the vector $(0,\dots,0,1)$.} % Re. your comment about \sqrt{dx^2+dy^2}/y, the calculussy metric is for Chapter 6!

\emph{This function gives the \textbf{distance} between two regular points in hyperbolic $n$-space.}\\

\noindent It is worth remarking that we are temporarily using the half-space model instead of the ball model to define distances.  However, this distance metric easily carries over to the ball model, via the conversion in Exercise 4.  (Similar, but tougher, reasoning can be used to prove Proposition 4.18 directly for the ball model, but that will be needless.)

\begin{proof}
Let $X=H^n(\mathbb R)\times H^n(\mathbb R)$, and let the group $G$ of M\"obius transforms act on $X$ via $g\cdot(p,q)=(g(p),g(q))$.  Observe that condition (i) states that, in terms of Section 1.6, $\rho$ is a function on orbits from the set $X$ on which the group acts.

Let $X'=\{(a\vec e_n,b\vec e_n):a,b>0\}\subset X$, and define $f:X'\to\mathbb R$ via $f(a\vec e_n,b\vec e_n)=|\ln(b/a)|$.  Then all that is left is to prove that conditions (i), (ii) of Proposition 1.21 are satisfied; it will follow that $f$ extends to a unique function on orbits $\varphi$ on $X$, completing the proof of this proposition.

To show (i), let $(p,q)\in X$.  By Lemma 4.17, we may let $\ell$ be the line determined by them, and let $c$ be an ideal point of $\ell$.  We let $\psi$ be the inversion via the unit hypersphere centered at $c$ [see Exercise 3 of the previous section]; then $\psi\in G$.  Moreover, $\psi$ sends $\ell$ to a generalized circle $\ell'$ containing $\infty$ (because $c\in\ell$ and $\psi(c)=\infty$), hence $\ell'$ is a Euclidean line, and is incidentally parallel to the $x_n$-axis.  Thus, $\ell'$ is of the form $\{(c_1,\dots,c_{n-1},t):t\geqslant 0\}$ with the $c_j$ fixed.  If $p'=\psi(p)$ and $q'=\psi(q)$, then $p',q'\in\ell'$.  Finally, the map $T:\vec v\mapsto\vec v-(c_1,\dots,c_{n-1},0)$ is clearly an M\"obius transform, And $T(\ell')$ is the $x_n$-axis.  Thus, $T(p'),T(q')$ are on the $x_n$-axis.  Therefore, $T\circ\varphi$ is an M\"obius transform which sends $(p,q)$ to two points on the $x_n$-axis, i.e., an element of $X'$, proving condition (i) of Proposition 1.21.

As for (ii), suppose that $(p,q),(p',q')\in X'$, say $p=a\vec e_n,q=b\vec e_n,p'=a'\vec e_n,q'=b'\vec e_n$, and let $T\in G$ be an M\"obius transform such that $T(p)=p',T(q)=q'$.  Then $f(p,q)=|\ln(b/a)|$ and $f(p',q')=|\ln(b'/a')|$; we wish to show that they are equal.  Let $\ell$ be the line determined by $p,q$ (Lemma 4.17); then $\ell$ is the $x_n$-axis, and since $p',q'\in\ell$, we have $T(\ell)=\ell$.  Moreover, $T$ either fixes $0,\infty$ or swaps them (as $0,\infty$ are the ideal points of $\ell$).

Suppose $T$ fixes $0,\infty$.  Then since $T(\infty)=\infty$, Proposition 4.16 shows that $T$ is of the form $T(\vec v)=\alpha A\vec v+\vec\beta$, with $\alpha\ne 0$ in $\mathbb R$, $A\in O(n)$, and $\vec\beta\in\mathbb R^n$.  Since $T(\vec 0)=\vec 0$, $\vec\beta=\vec 0$.  Since $T(\ell)=\ell$, it is clear that $A\vec e_n=\pm\vec e_n$; we may assume $A\vec e_n=\vec e_n$ by changing $\alpha$ and $A$ to their negatives otherwise.  Consequently, $T(p)=\alpha p$ and $T(q)=\alpha q$, since $p,q$ are scalar multiples of $\vec e_n$ which is fixed by $A$.  Thus, we have
$$\left|\ln\frac{b'}{a'}\right|=\left|\ln\frac{\alpha b}{\alpha a}\right|=\left|\ln\frac ba\right|$$
and so $f(p,q)=f(p',q')$ in this case.

Now suppose $T$ swaps $0,\infty$.  Then $\psi\circ T$ (were $\psi$ is the origin-centered central inversion) is another M\"obius transform which fixes the line $\ell$; let $p^*=\psi(p')=\frac 1{a'}\vec e_n$ $[=(\psi\circ T)(p)]$, and $q^*=\psi(q')=\frac 1{b'}\vec e_n$.  By construction, $\psi\circ T$ fixes $0$ and $\infty$.  Therefore, by the argument of the preceding paragraph, $\left|\ln\frac{1/b'}{1/a'}\right|=\left|\ln\frac ba\right|$.  Furthermore,
$$\left|\ln\frac ba\right|=\left|\ln\frac{1/b'}{1/a'}\right|=\left|-\ln\frac{1/a'}{1/b'}\right|=\left|\ln\frac{1/a'}{1/b'}\right|=\left|\ln\frac{b'}{a'}\right|$$
hence $f(p,q)=f(p',q')$ again.
\end{proof}

\noindent The M\"obius transforms that fix $H^n(\mathbb R)$ are hence called \textbf{isometries}.  By applying them and using Proposition 4.18(ii), it is clear that the segment addition postulate holds, and that points of a given distance are closer to the Euclidean eye when closer to the rim.

Though the M\"obius transforms preserve distances by Proposition 4.18, it is natural to ask whether they are the only global functions which do.  The answer is yes, but the proof will be omitted here, as it would digress from the main point of the matter.\\

\noindent\textbf{BASIC FACTS ABOUT HYPERBOLIC $3$-SPACE}\\

\noindent We conclude this section with a few basic properties about hyperbolic $3$-space.  First, we have already shown that two points determine a line.  We also have:\\

\noindent\textbf{Proposition 4.19.} \emph{In $H^3(\mathbb R)$:}

(i) \emph{Any two distinct planes are either disjoint in $\overline{H^3}(\mathbb R)$ (in which case we say they are \textbf{divergent-parallel}), intersect in one ideal point (in which case we say they are \textbf{convergent-parallel}), or intersect in a line.}

(ii) \emph{Three points that are not collinear determine a plane.}

(iii) \textsc{(Ultraparallel Theorem)} \emph{Given two divergent-parallel planes, there is a unique line simultaneously perpendicular to both of them.}\\

\noindent These results have analogues in higher-dimensional spaces; e.g., in $H^4(\mathbb R)$, four points that are not coplanar determine a hyperplane.
\begin{proof}
(i) If the planes meet at a regular point $p$, we may assume $p$ is the origin of the ball model.  By imitating the proof of Lemma 4.17, one sees that the planes are then Euclidean planes through the origin, hence they clearly intersect in a line.  If the planes meet at an ideal point $q$ but at no regular point, we may assume we are in the half-space model and $q=\infty$.  Then the planes are Euclidean planes parallel to the $z$-axis (as those are the only planes containing $\infty$), and since they do not intersect at any regular points, they must be parallel as Euclidean planes.  In this case, their intersection consists solely of the ideal point $\infty$.  Finally, if the planes do not intersect at all, there is nothing to prove.

(ii) Let $a,b,c$ be the points.  If all of them are ideal, we may assume we are in the half-space model and $a=\infty$.  Then the points $b,c$ in the $xy$-plane determine a line $\ell$, and then the set $\{(x,y,z):(x,y)\in\ell,z\geqslant 0\}\cup\{\infty\}$ is a (hyperbolic) plane containing $a,b,c$.  Now suppose at least one of $a,b,c$ is regular.  Without loss of generality, assume $a$ is regular; further assume we are in the ball model and $a=0$.  Then $a,b,c$ are contained in a Euclidean plane, and this plane is a hyperbolic plane as well (because it contains the origin).  This proves that a plane containing $a,b,c$ exists.

As for the uniqueness of the plane, copy the last paragraph of the proof of Proposition 2.43, using Proposition 4.19(i) in place of Proposition 2.43(i).

(iii) The proof is similar to that of Theorem 4.7.  Let $P_1$ and $P_2$ be the planes.  By applying isometries, we may assume that we are in the half-space model, $P_1$ is the $yz$-plane, and $P_2$ is a Euclidean hemisphere centered on the positive $x$-axis; say it is centered at $(a,0,0)$ and its Euclidean radius is $r$.  Since the planes are divergent-parallel, $0<r<a$.  Then the reader can readily verify through algebra that:
\begin{itemize}
\item A line $\ell$ is perpendicular to $P_1$ if and only if it is a semicircle arc centered on the $y$-axis, contained in a plane perpendicular to the $y$-axis; i.e., it can be parametrized via $t\mapsto(s\sin t,c,s\cos t),t\in[-\pi/2,\pi/2]$ (with $s>0,c$ constant).

\item If $\ell$ satisfies those conditions, $\ell\perp P_2$ if and only if $c=0$ and $s=\sqrt{a^2-r^2}$.
\end{itemize}
[Remember, if a curve is perpendicular to a surface, then it must go in the direction of the normal vector to the surface.]

From this, the statement is immediate.
\end{proof}

\subsection*{Exercises 4.7. (Hyperbolic $n$-space)}
\begin{enumerate}
\item Show that if $0\leqslant k<n$, then in $H^n(\mathbb R)$, a $k$-plane $P$ is isometric to $H^k(\mathbb R)$; i.e., there is a bijection $P\cong H^k(\mathbb R)$ which preserves distances, as well as lines and angles.  [Assume $P$ is a Euclidean $k$-plane in the half-space model.  Note that this statement also makes sense when ideal points are included in the situation.]  Conclude (by taking $k=2$) that the triangle theorems from Section 4.3 hold in arbitrary hyperbolic $n$-space.

\item In hyperbolic $3$-space, show that a plane and a line not contained in the plane intersect in at most one point.  [The point could be ideal; in this case we say the plane and line are convergent-parallel.]

\item Suppose $p=(a_1,\dots,a_n)$ and $q=(b_1,\dots,b_n)$ are regular points of the half-space model of hyperbolic $n$-space.  [Thus $a_n>0$ and $b_n>0$.]  Adapt Exercise 9 of Section 4.3 to show that their distance is
$$\rho(p,q)=\cosh^{-1}\left(1+\frac{(a_1-b_1)^2+(a_2-b_2)^2+\dots+(a_n-b_n)^2}{2a_nb_n}\right)$$

\item Define $f,g:\overline{\mathbb R^n}\to\overline{\mathbb R^n}$ as follows:
$$f(u_1,\dots,u_n)=\frac 1{u_1^2+\dots+u_{n-1}^2+(u_n+1)^2}\left(2u_1,\dots,2u_{n-1},u_1^2+\dots+u_n^2-1\right)$$
$$f(\infty)=(0,\dots,0,1),~~~~f(0,\dots,0,-1)=\infty$$
$$g(v_1,\dots,v_n)=\frac 1{v_1^2+\dots+v_{n-1}^2+(v_n-1)^2}\left(2v_1,\dots,2v_{n-1},1-v_1^2-\dots-v_n^2\right)$$
$$g(\infty)=(0,\dots,0,-1),~~~~g(0,\dots,0,1)=\infty$$
(a) $f$ and $g$ are M\"obius transforms, and $f\circ g=1_{\overline{\mathbb R^n}}=g\circ f$.  [To avoid hassle, left and right compose $f,g$ with translations to get maps sending $0\mapsto\infty\mapsto 0$.]

(b) $f$ sends the half-space model of hyperbolic $n$-space to the ball model, and $g$ sends the ball model to the half-space model.

(c) Conclude that $f$ and $g$ are isometries which convert between the two models of hyperbolic $n$-space.

\item Let $\vec v$ be a regular point in the ball model of hyperbolic $n$-space; i.e., $\|\vec v\|<1$.  Show that the hyperbolic distance from the origin to $\vec v$ is $\ln\frac{1+\|\vec v\|}{1-\|\vec v\|}$, or what is the same thing, $2\tanh^{-1}\|\vec v\|$.

\item\emph{(Categorization of isometries of $H^3(\mathbb R)$.)} \---- The aim of this exercise is to categorize isometries of $H^3(\mathbb R)$, the way isometries of $H^2(\mathbb R)$ were categorized at the end of Section 4.3.

(a) Suppose $\operatorname{Isom}(H^3(\mathbb R))$ is the group of isometries of the half-space model, and $G$ is the group of M\"obius and anti-M\"obius transforms of $\overline{\mathbb R^2}$.  Define $\varphi:\operatorname{Isom}(H^3(\mathbb R))\to G$ by taking an isometry of the space and restricting it to the rim at infinity.  Show that $\varphi$ is a group isomorphism. [Proposition 4.16 applies to both dimensions.]

(b) An isometry of $H^3(\mathbb R)$ is orientation-preserving if and only if its restriction to the rim is a M\"obius transform of $\mathbb C\sqcup\{\infty\}$; i.e., $z\mapsto\frac{az+b}{cz+d}$ with $ad-bc\ne 0$.

(c) Classify the M\"obius transforms of $\mathbb C\sqcup\{\infty\}$ up to conjugacy.  [The group of M\"obius transforms is isomorphic to $PGL_2(\mathbb C)\cong GL_2(\mathbb C)/\mathbb C^*$; consider writing matrices in Jordan normal form.]  Use this and parts (a)-(b) to classify the orientation-preserving isometries of $H^3(\mathbb R)$.

(d) Show that every orientation-reversing isometry is a composition of a reflection and an orientation-preserving isometry.  Use this and part (c) to classify the orientation-reversing isometries of $H^3(\mathbb R)$.  Some of them are \textbf{horolary reflections}, obtained by performing a horolation on a plane and then reflecting space over the plane.  [The tricky part is to use casework on how the reflection and the orientation-preserving isometry relate; e.g., if the orientation-preserving isometry is a rotation, one must look at its axis' intersection with the plane of reflection.]

\item (a) If $T$ is an isometry of $H^n(\mathbb R)$ and $T(\vec 0)=\vec 0$, show that $T$ is Euclidean orthogonal transformation in $O(n)$.  [Use Proposition 1.9 to make certain convenient assumptions.]

(b) Show that every isometry of $H^n(\mathbb R)$ is a finite composition of reflections.  [In the ball model, start by composing the isometry with a reflection so that the resulting isometry fixes the origin.  Now the isometry is in $O(n)$ by part (a), and the rest uses linear algebra.]

\item\emph{(Hypersheets, horosheets and space fillings.)} \---- (a) Let $P$ be a plane in $H^3(\mathbb R)$, $p$ a regular point outside $P$, and $Q$ the trajectory of $p$ along all translations that fix $P$.  Show that (after completion), $Q$ is a generalized sphere in the ball and half-space models, and that it has the same ideal points as $P$.  [We call $Q$ a \textbf{hypersheet}; it is a two-dimensional analogue of a hypercycle.]

(b) Explain why the isometry group of a hypersheet is isomorphic to that of a plane.

(c) Let $p$ be an ideal point, and let $S$ be a surface to which all lines through $p$ are perpendicular.  Show that $S$ is a generalized sphere which meets the rim $H^3_\infty(\mathbb R)$ at exactly one point.  [We call $S$ a \textbf{horosheet}.]

(d) Show that the isometry group of a horosheet is isomorphic to $\operatorname{Isom}(\mathbb R^2)$, the isometry group of the Euclidean plane. [Assume its ideal point is $\infty$ in the half-space model.] % Thus hyperbolic 3-space has models of all three metric geometries!  A hyperplane (also a hypersheet) has the geometry of the hyperbolic plane; a horosheet has the geometry of the Euclidean plane; and a sphere has the geometry of a spherical plane.

(e) Recall the five Platonic solids from Section 2.7.  Suppose you start with only the vertices of a Platonic solid centered at the origin, and require the vertices to be inside the (closed) Poincar\'e ball.  Show that if you connect them with hyperbolic line segments and plane portions, you get a hyperbolic-space model of the solid with the same isometry group as the original solid. % Section 2.7 gave algebraic coordinates for the Platonic solids centered at the origin.  Since isometries fixing the origin are the same as Euclidean isometries fixing the origin, they act transitively on the vertices, edges and faces, and the stabilizer of each n-gonal face is isomorphic to D_n.  So the faces are regular and the dihedral angles are constant, this doesn't need to be assumed.

(f) Let $P$ be a Platonic solid from part (e).  If the vertices of $P$ are ideal, then the angles of each face are zero, and the dihedral angles of the solid measure $\pi-\frac{2\pi}n$ where $n$ is the number of faces to a vertex.  [Think of the spheres as Euclidean spheres in $\mathbb R^3$.]

(g) Show that a regular ideal tetrahedron, cube, octahedron or dodecahedron may be used to fill the $3$-space without gaps or overlaps.  [Keep reflecting them over their own faces.]  Then by taking the centers of the cells and connecting two of them by an edge if and only if the cells meet by a face, you get a space-filling made up of uniform Euclidean tilings on horosheets [this makes sense in view of part (d)].

(h) Explain why a regular ideal icosahedron cannot fill the $3$-space.
\end{enumerate}

\subsection*{4.8. Beltrami-Klein Model}
\addcontentsline{toc}{section}{4.8. Beltrami-Klein Model}
The Beltrami-Klein model of $H^2(\mathbb R)$ is a disk-shaped model for which lines are Euclidean chords of the circle.  It is also known as the \textbf{Klein disk model}, \textbf{Cayley-Klein model}, or \textbf{projective model} (as we will eventually see that isometries are projective transformations).

% https://en.wikipedia.org/wiki/Beltrami%E2%80%93Klein_model#History
It was first discovered in two of Eugenio Beltrami's papers in 1868, first for the plane, then for hyperbolic space in an arbitrary number of dimensions.  Those papers were unnoticed for a while, until the model was named after Felix Klein.  This happened because in 1859, when Arthur Cayley found a way to derive Euclidean geometry from projective geometry, Klein became acquainted with his work.  Klein then realized that Cayley's ideas entailed a way to derive hyperbolic geometry from projective geometry.

We will start by studying this model for the hyperbolic plane (two dimensions).  We could do this from scratch, but then it would not be clear how to transform figures between this model and the Poincar\'e disk and half-plane models, so we wish to find the conversion first.  If $D$ is the closed disk $\{\vec v\in\mathbb R^2:\|\vec v\|\leqslant 1\}$, then our intuition tells us that we seek a function $\chi:D\to D$ satisfying the following two conditions:
\begin{itemize}
\item $\chi$ fixes every point on the unit circle; i.e., $\|\vec v\|=1\implies\chi(\vec v)=\vec v$.

\item $\chi$ sends every line in the Poincar\'e disk model (i.e., generalized circle peprpendicular to the circle) to a Euclidean chord of the circle.
\end{itemize}
\noindent These conditions determine the function rather easily.  To begin with, every diameter $\ell$ of the circle is a line in the Poincar\'e disk model, and so we have $\chi(\ell)=\ell$ \---- because the conditions imply that $\chi(\ell)$ is a Euclidean chord which shares the endpoints of $\ell$.  Therefore $\chi$ fixes every diameter of the circle; by intersecting two diameters, we get $\chi(\vec 0)=\vec 0$.

If $0<c<1$ is a real number, we next claim that $\chi((c,0))$ is uniquely determined.  After all, $(c,0)$ can be realized as the intersection of two Poincar\'e-disk lines: the $x$-axis, and the line $\ell^\perp$ through $(c,0)$ perpendicular to the $x$-axis, which is in the Euclidean sense a circle centered on the $x$-axis.  Suppose $\ell^\perp$ is centered at $(a,0),a>0$ and has radius $r$.  Since it is a line in the Poincar\'e disk, Exercise 6 of Section 4.2 implies $a^2-r^2=1$.  Yet $(c,0)$ is on the circle; hence, $a-r=c$.\footnote{We know it is not $a+r$ which is equal to $c$, because that would imply $a-r=\frac 1c>c$.}  Therefore, $a+r=\frac{a^2-r^2}{a-r}=\frac 1c$.  Algebraic manipulation shows $a=\frac{1+c^2}{2c}$ and $r=\frac{1-c^2}{2c}$; hence $\ell^\perp$ is given by an equation
$$\left(x-\frac{1+c^2}{2c}\right)^2+y^2=\left(\frac{1-c^2}{2c}\right)^2$$
or equivalently,
$$(x^2+y^2)-\frac{1+c^2}cx+1=0$$
One can check that this equation is satisfied by $(c,0)$.  Moreover, we can easily find the ideal points of $\ell^\perp$, by taking the conjunction of this equation with the unit circle's equation $x^2+y^2=1$.  With that done, we get $1-\frac{1+c^2}cx+1=0$; hence, $x=\frac{2c}{1+c^2}$.  Since $y^2=1-x^2$, we conclude $y=\pm\frac{1-c^2}{1+c^2}$, so that the ideal points of $\ell^\perp$ are $\left(\frac{2c}{1+c^2},\pm\frac{1-c^2}{1+c^2}\right)$.

Since $\chi$ fixes every ideal point, we conclude that $\chi(\ell^\perp)$ is the Euclidean line segment between $\left(\frac{2c}{1+c^2},\pm\frac{1-c^2}{1+c^2}\right)$.  Also, $\chi$ clearly maps the $x$-axis to itself.  Taking the intersection of these yields $\chi((c,0))=\left(\frac{2c}{1+c^2},0\right)$.  This argument can essentially be repeated for any point in the disk, by consideration of a Euclidean rotation; we get that $\chi$ must be given by:
$$\chi(\vec v)=\frac{2\vec v}{1+\|\vec v\|^2}\text{ for }\vec v\in D.$$
Note that $\chi$ is bijective, and its inverse is given by $\chi^{-1}(\vec u)=\frac{\vec u}{1+\sqrt{1-\|\vec u\|^2}}$; this can be directly verified.  We claim that $\chi$ successfully does satisfy the bullet points at the beginning of the section.

That $\|\vec v\|=1\implies\chi(\vec v)=\vec v$ is a direct computation.  To see where $\chi$ sends a line $\ell$ in the Poincar\'e disk model, we recall that if $\ell$ is a diameter of the circle, then $\chi(\ell)=\ell$ because $\chi$ sends vectors to scalar multiples of themselves.  Thus, $\chi(\ell)$ is a Euclidean chord of the circle.

We may henceforth assume $\ell$ is not a diameter of the circle, and that it is a Euclidean circle perpendicular to the circle.  Observe that in this case, $\ell$ can be given an equation $\|\vec v\|^2-\vec c\cdot\vec v+1=0$, where $\vec c$ is a fixed vector.  After all, if $\vec a$ is its center and $r$ is its radius, then $\|\vec a\|^2-r^2=1$, and $\ell$ is given by $\|\vec v-\vec a\|^2=r^2$: with that, we may take $\vec c=2\vec a$.

Moreover, the set $\chi(\ell)$ is equal to $\{\vec u\in D:\chi^{-1}(\vec u)\in\ell\}$.  The formula $\chi^{-1}(\vec u)=\frac{\vec u}{1+\sqrt{1-\|\vec u\|^2}}$ and the aforementioned equation for $\ell$ entails
$$\vec u\in\chi(\ell)\iff\frac{\vec u}{1+\sqrt{1-\|\vec u\|^2}}\in\ell\iff\big\|\frac{\vec u}{1+\sqrt{1-\|\vec u\|^2}}\big\|^2-\vec c\cdot\frac{\vec u}{1+\sqrt{1-\|\vec u\|^2}}+1=0$$
$$\iff\left(\frac{\|\vec u\|}{1+\sqrt{1-\|\vec u\|^2}}\right)^2-\vec c\cdot\frac{\vec u}{1+\sqrt{1-\|\vec u\|^2}}+1=0$$
$$\iff\frac{\|\vec u\|^2}{(1+\sqrt{1-\|\vec u\|^2})^2}-\frac{\vec c\cdot\vec u}{1+\sqrt{1-\|\vec u\|^2}}+1=0$$
$$\iff\|\vec u\|^2-(1+\sqrt{1-\|\vec u\|^2})(\vec c\cdot\vec u)+(1+\sqrt{1-\|\vec u\|^2})^2=0$$
$$\iff-(1+\sqrt{1-\|\vec u\|^2})(\vec c\cdot\vec u)+2+2\sqrt{1-\|\vec u\|^2}=0$$
$$\iff-(1+\sqrt{1-\|\vec u\|^2})(\vec c\cdot\vec u)+2(1+\sqrt{1-\|\vec u\|^2})=0$$
$$\iff-(\vec c\cdot\vec u)+2=0\iff\vec c\cdot\vec u=2,$$
and that last equation describes a Euclidean line perpendicular to $\vec c$.  Hence $\chi(\ell)$ is a Euclidean chord of the circle as desired.  We have thus established:\\

\noindent\textbf{Proposition 4.20 and Definition.} \emph{If $D$ is the closed unit disk, and $\chi:D\to D$ is defined by $\chi(\vec v)=\frac{2\vec v}{1+\|\vec v\|^2}$, then $\chi$ is the unique bijection such that}

(i) \emph{$\chi(\vec v)=\vec v$ for all $\|\vec v\|=1$;}

(ii) \emph{$\chi$ sends lines in the Poincar\'e disk model of the hyperbolic plane to Euclidean chords of the circle.}

\emph{This map $\chi$ is called the \textbf{conversion from the Poincar\'e disk to the Klein disk}.  Its inverse $\chi^{-1}:\vec u\mapsto\frac{\vec u}{1+\sqrt{1-\|\vec u\|^2}}$ is called the \textbf{conversion from the Klein disk to the Poincar\'e disk}.}

\emph{The \textbf{Beltrami-Klein model} of the hyperbolic plane is the model in $D$ where lines are Euclidean chords of the circle, for which $\chi^{-1}$ sends constructions to the corresponding constructions in the Poincar\'e disk model.}\\

\noindent We thus have, by moving the points around as above, a new way to display hyperbolic-plane constructions.  As an example, consider the picture equipped with Proposition 4.5, consisting of a line and several lines through a point parallel to this line.  The right-hand side illustrates what it would look like in the Beltrami-Klein model:\footnote{To distinguish the models, we will specifically use a fuschia color on the Beltrami-Klein model constructions.}
\begin{center}
\includegraphics[scale=.3]{HypParallels.png}~~~~
\includegraphics[scale=.3]{HypParallels_BK.png}\\
Poincar\'e Disk Model~~~~~~~~~~~~~~~~~~~~~Beltrami-Klein Model
\end{center}

Observe that in the Beltrami-Klein model, the Euclidean angle between two lines meeting at the rim is never zero (even though the hyperbolic angle is zero).  This is an immediate consequence of the lines being Euclidean straight lines.  In fact, the model is not conformal: \emph{angle measures between lines in the Beltrami-Klein model do not generally coincide with the Euclidean angle measures}.  For example, the lines below are perpendicular, but certainly don't look like they are to the Euclidean eye:
\begin{center}
\includegraphics[scale=.3]{Perp_BK.png}
\end{center}
In view of Proposition 4.8, the model can't be conformal anyway: if the Beltrami-Klein model were conformal, then (since triangles look exactly like Euclidean triangles), the angles of a triangle would add to $180^\circ=\pi$, which would contradict Proposition 4.8.

However, we claim we have a basic way of being able to tell when two lines are perpendicular.  We recall (see the end of Section 3.6) that if $\ell$ is a chord of a circle $\omega$, then the \textbf{pole point} of $\ell$ is defined by taking the points where the chord meets $\omega$, forming the tangent lines to $\omega$ at these points, and then taking their intersection.  The particularly interesting thing about the Beltrami-Klein model is that lines are perpendicular if and only if they contain each other's pole points:\\

\noindent\textbf{Proposition 4.21.} \emph{If $\ell_1$ and $\ell_2$ are lines in the Beltrami-Klein model of the hyperbolic plane, the following are equivalent:}

(i) \emph{The lines are perpendicular.}

(ii) \emph{$[\![a,b,c,d]\!]=2$, where $a,c$ are the ideal points of $\ell_1$, and $b,d$ are the ideal points of $\ell_2$, when viewed as complex numbers.}

(iii) \emph{$\ell_1$ contains the pole point of $\ell_2$, when everything is viewed in the projective plane $P^2(\mathbb R)$.}\\

\noindent There is a special reason that (iii) uses the projective plane.  First, as we will see later, the isometries of the Beltrami-Klein model are precisely the projective transformations which fix the circle.  Secondly, if $\ell_2$ is a diameter, its pole point is at infinity, and in this case (iii) states that $\ell_1\perp\ell_2$ in the Euclidean sense.
\begin{proof}
(i) $\iff$ (ii). The ideal points are transformed to themselves through the conversion map $\chi$; thus $a,b,c,d$ are also the ideal points of the lines in the Poincar\'e disk model.  Now use Exercise 16 of Section 4.3.

(ii) $\iff$ (iii). First suppose that $\ell_2$ is a diameter.  Then $b=-d$, and
$$[\![a,b,c,d]\!]=\frac{(c-a)(d-b)}{(d-a)(c-b)}=\frac{(c-a)(2d)}{(d-a)(c+d)}=\frac{2(c-a)d}{(d-a)(c+d)}$$
and hence
$$[\![a,b,c,d]\!]=2\iff\frac{2(c-a)d}{(d-a)(c+d)}=2\iff(c-a)d=(d-a)(c+d)$$
$$\iff cd-ad=cd+d^2-ac-ad\iff d^2=ac,$$
which is true if and only if $\ell_1$ is perpendicular to $\ell_2$ in the Euclidean sense (as one can see by arguing geometrically).  Since $\ell_2$ is a diameter, this is in turn equivalent to $\ell_1$ containing the pole point of $\ell_2$.  This proves (ii) $\iff$ (iii) when $\ell_2$ is a diameter.

Now suppose $\ell_2$ is not a diameter.  Then the pole point of $\ell_2$, as a complex number, is equal to $\frac{2bd}{b+d}$: this can be verified by subtracting $b$ from the expression, noting that the resulting complex number is perpendicular to $b$, concluding that $\frac{2bd}{b+d}$ is on the tangent line to the circle at $b$, then doing the same for $d$.  Also, a complex number $z$ is in the line extending $\ell_1$ if and only if $\frac{z-a}{c-a}\in\mathbb R$, because $z\mapsto\frac{z-a}{c-a}$ and sends $a\mapsto 0,c\mapsto 1$, hence $\ell_1$ to the real line.  Moreover, as the real numbers are precisely those equal to their own complex conjugates,
$$\frac{z-a}{c-a}\in\mathbb R\iff\frac{z-a}{c-a}=\overline{\left[\frac{z-a}{c-a}\right]}=\frac{\overline z-\overline a}{\overline c-\overline a}=\frac{ac(\overline z-\overline a)}{ac(\overline c-\overline a)}\overset{(*)}=\frac{ac\overline z-c}{a-c}=\frac{c-ac\overline z}{c-a}$$
where the equality $(*)$ uses the fact that $a,c$ are \emph{unit} complex numbers, so that $a\overline a=c\overline c=1$.  Taking $z$ to be the pole point $\frac{2bd}{b+d}$ of $\ell_2$,
$$\text{(iii)}\iff\frac{\frac{2bd}{b+d}-a}{c-a}\in\mathbb R\iff\frac{\frac{2bd}{b+d}-a}{c-a}=\frac{c-ac\overline{\left[\frac{2bd}{b+d}\right]}}{c-a}\iff$$
$$\frac{2bd}{b+d}-a=c-ac\overline{\left[\frac{2bd}{b+d}\right]}=c-ac\frac{2\overline b\overline d}{\overline b+\overline d}=c-ac\frac{bd(2\overline b\overline d)}{bd(\overline b+\overline d)}=c-ac\frac{2}{b+d}$$
where we have again used the fact that $b,d$ are unit complex numbers.  Multiplying by $b+d$ shows that last statement to be equivalent to
$$2bd-ab-ad=cb+cd-2ac,$$
and one readily verifies (by using the definition $[\![a,b,c,d]\!]=\frac{(c-a)(d-b)}{(d-a)(c-b)}$), that said statement is equivalent to $[\![a,b,c,d]\!]=2$, or (ii).  Thus, the proof of (ii) $\iff$ (iii) is complete.
\end{proof}

\noindent Given Proposition 4.21, we can readily establish formulas for the isometries in the Beltrami-Klein model.

We do this by first establishing a geometric construction for the reflection of the point $P$ over the line $\ell$, shown below.
\begin{center}
\includegraphics[scale=.3]{GeoReflect_BK.png}
\end{center}
The construction goes like this: Let $R$ be one ideal point of $\ell$.  Draw the line from $R$ that passes through $P$; let the other ideal point of this new line be $Q$.  Now draw the line through $Q$ and the pole point of $\ell$, and denote its other ideal point by $Q'$.  Draw the line from the $R$ to $Q'$.  Finally, draw the line through $P$ and the pole point of $\ell$, and let $P'$ be its intersection with $\overset{\longleftrightarrow}{RQ'}$.

Since $\overset{\longleftrightarrow}{PP'}$ and $\overset{\longleftrightarrow}{QQ'}$ contain the pole point of $\ell$, Proposition 4.21 shows that they are perpendicular to $\ell$ as lines in the Beltrami-Klein model of the hyperbolic plane.  Moreover, Exercise 18 of Section 4.3 shows that $\ell$ is the angle bisector of $\angle QRQ'=\angle PRP'$, and hence, (by transfering to the Poincar\'e disk model and assuming $\ell$ is the $x$-axis), we conclude that $P'$ is the reflection of $P$ over $\ell$.

Thus, the construction described in the paragraph following the above picture takes the point $P$ and reflects it over the line $\ell$ to get $P'$.  Yet, observe that it solely uses the intrinsic projective geometry, based off of the point $P$, the line $\ell$, and the conic which is the circle bounding the hyperbolic plane.  Thus, the entire construction commutes with projective transformations.  Yet by Exercise 2, there is a projective transformation $T$ sending $\ell$ to the $x$-axis, and applying $T$ to the constructions will conjugate the correspondence $P\mapsto P'$ by $T$.  The resulting correspondence reflects points over the $x$-axis in the Euclidean sense (why?), hence is a projective transformation.  Undoing the conjugation by $T$, we conclude that the original correspondence $P\mapsto P'$ was a projective transformation itself.

We have just shown that the reflection over any line, in the Beltrami-Klein model, is a projective transformation of $P^2(\mathbb R)$.  Thus we easily conclude\\

\noindent\textbf{Proposition 4.22.} \emph{In the Beltrami-Klein model of $H^2(\mathbb R)$, every isometry is a projective transformation of $P^2(\mathbb R)$ which fixes the unit circle.}\begin{proof}
We have proven this for reflections; now use Exercise 7(a) of Section 4.3.
\end{proof}

\noindent In fact, we also have the converse of Proposition 4.22; every projective transformation which fixes the unit circle is an isometry:\\

\noindent\textbf{Proposition 4.23.} \emph{A map from the Beltrami-Klein model to itself is an isometry if and only if it is a projective transformation of $P^2(\mathbb R)$ which fixes the unit circle.}\\

\noindent This is, in fact, why this model of hyperbolic geometry is sometimes called the ``projective model.''
\begin{proof}
Let $H$ be the group of isometries, and $G$ be the group projective transformations fixing the unit circle.  By Proposition 4.22, $H\subset G$, and hence $H$ is a subgroup of $G$.  We shall use Proposition 1.9 to show that $H=G$.

Let $T\in G$.  Then $T$ fixes all interior points of the circle (see Section 3.5), so that $T(\vec 0)$ is a regular point of the hyperbolic plane.  Exercise 1(a) of Section 4.3 further implies that there is an isometry $U\in H$ sending $T(\vec 0)\mapsto\vec 0$.  Then $U\circ T\in G$ and $(U\circ T)(\vec 0)=\vec 0$.  In view of Proposition 1.9, it suffices to show that $U\circ T$ is in $H$.  In other words, in showing $T\in G\implies T\in H$, we may assume $T(\vec 0)=\vec 0$.

Moreover, $T((1,0))$ is some point of the unit circle, say $(\cos\theta,\sin\theta)$.  If $R$ is a clockwise rotation by $\theta$ around the origin, then $R$ is also a rotation around the origin in the Euclidean sense (why?), and $R((\cos\theta,\sin\theta))=(1,0)$.  We further use Proposition 1.9 to reduce the proof to showing $R\circ T\in H$.  In other words, we may assume $T((1,0))=(1,0)$ as well.

Since $T$ fixes both $\vec 0=(0,0)$ and $(1,0)$, it fixes the line determined by them, i.e., the $x$-axis.  Consequently, since $T$ fixes the unit circle, $T$ must fix the points where the $x$-axis meets the circle; hence, $T$ fixes $(-1,0)$.  Using the intrinsic nature of the construction of a pole point, it is clear that projective transformations preserve pole points of lines via conics; therefore, since $T$ fixes the $x$-axis, it also fixes $[0:1:0]\in P^2(\mathbb R)$, which is the pole point of the $x$-axis via the unit circle.  Consequently, $T$ fixes the $y$-axis, because it is the line determined by $\vec 0$ and $[0:1:0]$ (and both points are fixed by $T$).

Furthermore, $T$ fixes intersection of the $y$-axis with the unit circle, which is $\{(0,1),(0,-1)\}$: but $T$ may either fix these points or swap them.  If $T$ fixes each of the points $(0,1),(0,-1)$, then $T$ must be the identity: otherwise, by consideration of the eigenspaces of the linear map $\mathbb R^3\to\mathbb R^3$ used to define $T$, the set of fixed points of $T$ would be contained in the union of a point and a line; this does not hold water if the five points $(0,0),(\pm 1,0),(0,\pm 1)$ are fixed.  If $T$ swaps $(0,1)$ and $(0,-1)$, then $T$ must be the (Euclidean) reflection over the $x$-axis, because composing $T$ with this reflection yields a projective transformation fixing $(0,0)$, $(\pm 1,0)$ and $(0,\pm 1)$ and so the previous argument applies to the new transformation.  In either case, $T$ is a hyperbolic isometry; i.e., $T\in H$.
\end{proof}

\noindent Along with the model not being conformal, we also claim that its circles are \emph{not} generally Euclidean circles.  We will now see what they really are.

If a circle is centered at the origin, it is clearly a Euclidean circle (for essentially the same reason it is in the Poincar\'e disk model).  Yet, any point $p$ can be sent to the origin through an isometry; such an isometry is a projective transformation by Proposition 4.22 (or 4.23), and hence preserves conics, as seen in Section 3.5.  Since this projective transformation sends a circle $\omega$ centered at $p$ to an origin-centered circle, the image of $\omega$ is a conic in $P^2(\mathbb R)$ (an origin-centered Euclidean circle), hence so is $\omega$ itself.  As $\omega$ is contained in the bounded unit disk, it clearly must be an ellipse (parabolas and hyperbolas do not fit inside the disk).

We can determine precisely what kind of ellipse it is.  First, the ellipse must be symmetric around the line through its center and the origin of the disk, because the (Euclidean) reflection over the line is a hyperbolic isometry, hence fixes any circle centered at one of its fixed points.  In fact, this line gives the minor axis of the ellipse.  This can be seen by starting in the Poincar\'e disk model, and drawing three lines tangent to the circle: two of them go through the origin, and the third embraces between the circle and the origin.  When passing over to the Beltrami-Klein model, the circle is still tangent to these lines, yet the first two lines remain the same and the third one becomes the Euclidean chord between the same ideal points:\footnote{Neither circle is tangent to the rim; if the one on the right appears to be, it is a graphical illusion because of how close they are to the Euclidean eye.}
\begin{center}
\includegraphics[scale=.3]{CircleAnalysis_PD.png}~~~~
\includegraphics[scale=.3]{CircleAnalysis_BK.png}\\
Poincar\'e Disk Model~~~~~~~~~~~~~~~~~~~~~Beltrami-Klein Model
\end{center}
This brushes the circle toward the rim, while the tangency to the first two lines implies that it gets wider.  Thus, the resulting ellipse is thinner in the direction of the diameter, and wider in the perpendicular dimension.  In other words, circles in the Beltrami-Klein model are ellipses which are ``flat'' towards the rim, and flatter when they are closer.  We will not give precise equations of these ellipses now, as the next section will introduce a few other models of the hyperbolic plane, along with their conversions; and from there, one can get a clearer picture.
\begin{center}
\includegraphics[scale=.3]{Circles_BK.png}\\Circles in the Beltrami-Klein model.
\end{center}

At this point it would be interesting to take some of the tilings from Section 4.5, and display them in the Beltrami-Klein model of the hyperbolic plane.
Here is the heptagonal tiling from Section 4.5, displayed side-by-side in both the Poincar\'e disk and Beltrami-Klein models.  Note that in the Klein model, most of the polygons near the rim look closer to flat ellipses; this is because that's what circles are in the Beltrami-Klein model.  Also, the tiling may appear to be on top of a hemisphere in birds-eye view: in the next section, we will see why this is so.
\begin{center}
\includegraphics[scale=.3]{HeptagonalTiling.png}~~~~
\includegraphics[scale=.3]{HeptagonalTiling_BK.png}\\
Poincar\'e Disk Model~~~~~~~~~~~~~~~~~~~~~Beltrami-Klein Model
\end{center}
Here are two other tilings from Section 4.5, the triheptagonal tiling and the truncated order-$7$ triangular tiling, each in the Beltrami-Klein model.
\begin{center}
\includegraphics[scale=.3]{TriheptagonalTiling_BK.png}~~~~
\includegraphics[scale=.3]{HyperrogueTiling_BK.png}\\
\end{center}

\noindent\textbf{HIGHER DIMENSIONS}\\

\noindent We conclude this section by discussing the Beltrami-Klein model of hyperbolic $n$-space.  It uses the unit $n$-ball $\{\vec v\in\mathbb R^n:\|\vec v\|\leqslant 1\}$, along with essentially the same conversion map from the Poincar\'e ball model, $\chi(\vec v)=\frac{2\vec v}{1+\|\vec v\|^2}$, but this time $\vec v$ is an $n$-dimensional vector.  By imitating the proofs in this section for the $2$-dimensional plane, the following can be verified; it is worth it for the reader to work these out.
\begin{itemize}
\item $\chi(\vec v)=\vec v$ for all $\|\vec v\|=1$.

\item $\chi$ sends $k$-planes ($0\leqslant k<n$) in the Poincar\'e ball model to the intersections of Euclidean $k$-planes with the unit ball (the generalization of chords).

\item Intersecting lines are perpendicular if and only if they contain each other's pole points in the $2$-dimensional cross section containing them.  [For this, the proof of (ii) $\iff$ (iii) in Proposition 4.21 need not be repeated: you just need basic manipulation of cross ratios of ideal points.]

\item A line and a hyperplane are perpendicular if and only if the line contains the pole point of the hyperplane.  [The \textbf{pole point} of the hyperplane $\Pi$ is obtained by taking the tangent hyperplanes to the hypersphere at all points where it meets $\Pi$, and then seeing where the hyperplanes meet.  The fact that they all meet in a common point can be visualized for $3$ dimensions, and then the intuition entails an easy proof for an arbitrary number of dimensions.]

\item A reflection across a hyperplane in this model is a projective transformation.  [This can be established through the same geometric construction given immediately after Proposition 4.21.]  Therefore, by Exercise 7(b) of the previous section, every isometry is a projective transformation.  Conversely, every projective transformation fixing the unit hypersphere is an isometry.

\item Hyperspheres in this model are hyper-ellipsoids, which possess all Euclidean symmetries that pointwise fix the line through the center and the origin.  [In particular, spheres in the Beltrami-Klein model of $3$-space are ellipsoids of revolution.]  They are ``flat'' towards the rim and flatter when they are closer.
\end{itemize}
\subsection*{Exercises 4.8. (Beltrami-Klein Model)} % Introduce the Beltrami-Klein model of the hyperbolic plane, by first noting that in the Poincar\'e disk, lines aren't Euclidean straight lines.
% Show the thing about perpendicular lines (and pole points); and reflections; use this to prove isometries are projective transformations.  Also say a thing about higher dimensions.
% POTENTIAL: Add the BK tilings that have been made in Turtle Graphics.
\begin{enumerate}
\item If $a$ and $b$ are points in the Beltrami-Klein model of the hyperbolic plane, construct the (Euclidean) chord through these points.  Let it meet the circle at points $p$ and $q$, with $p$ next to $a$.
\begin{center}\includegraphics[scale=.3]{HDistance_BK.png}\end{center}
Show that the distance between $a$ and $b$ is $\frac 12\ln[\![b,a,p,q]\!]$, where we are referring to the cross ratio of collinear points in $P^2(\mathbb R)$, defined in Section 3.3 right before Proposition 3.6.  [Explain why we may assume the chord is the $x$-axis and $a=0$.  Then use the formula for $\chi$ in Proposition 4.20.]

\item If $\omega$ is a circle in $P^2(\mathbb R)$, and $G$ is the group of projective transformations that fix $\omega$, then $G$ acts transitively on the chords of $\omega$.  [See Exercise 7 of Section 3.5; this is similar.] % By "nondegenerate" I meant the chord goes between two distinct points.  If a line segment from a point on the circle to itself counted as a chord, the action would not be transitive.

\item Use Proposition 4.21 to give an alternate proof of the Perpendicular Postulate (Proposition 4.6).  [Recall that in $P^2(\mathbb R)$, two points determine a line.]

\item Use Proposition 4.21 to give an alternate proof of the Ultraparallel Theorem (Theorem 4.7).

\item Let $\angle BAC$ be an angle in the Beltrami-Klein model.

(a) Explain why the angle bisector of $\angle BAC$ may not coincide with the Euclidean angle bisector.

(b) Let $B'$ and $C'$ be the respective ideal points of the rays $\overset{\longrightarrow}{AB}$ and $\overset{\longrightarrow}{AC}$.  Then, take the lines that are tangent to the unit circle at $B'$ and $C'$, and let $D$ be their intersection.  Draw the line from $D$ to $A$.  Show that this line is the angle bisector of $\angle BAC$.  [Use Proposition 4.21 and Exercise 18 of Section 4.3.]

\item Let $-1<a<1$ be a fixed real number.  Show that in the Beltrami-Klein model of $n$-space, the reflection over the hyperplane $x_n=a$ is given by
$$(x_1,\dots,x_n)\mapsto\left(\frac{(a^2-1)x_1}{2ax_n-a^2-1},\dots,\frac{(a^2-1)x_{n-1}}{2ax_n-a^2-1},\frac{(a^2+1)x_n-2a}{2ax_n-a^2-1}\right).$$
[Let $T$ be the reflection, viewed as a projective transformation.  Explain why $T$ must fix the pole point of the hyperplane $x_n=a$.  In particular, if $\varphi$ is a linear map of $\mathbb R^{n+1}$ inducing $T$, then the subspace $x_n=ax_{n+1}$ of $\mathbb R^{n+1}$ (which corresponds to the hyperplane) and the line corresponding to the pole point are eigenspaces for $\varphi$.  Also note that $T^2$ is the identity.  Now compute $T$, using a change-of-basis matrix.]

\item Use the previous exercise to find a formula for the translation along the $x_1$-axis by a (hyperbolic) distance of $d>0$, where the origin goes toward the positive $x_1$-axis.

\item (a) Show that a hypercycle with base line $\ell$ is an elliptical arc which is tangent to the unit circle at the ideal points of $\ell$.  [Assume $\ell$ is the $x$-axis, then use Exercise 1.]

(b) Since all these hypercycles are tangent to the unit circle, they make a nonzero constant Euclidean angle with $\ell$.  Yet, as the radius of the hypercycle approaches zero, the hypercycle approaches the line $\ell$ itself, so this Euclidean angle becomes zero.  Comment on how this does not actually make the model inconsistent.  [Compare, in both the Poincar\'e disk and Beltrami-Klein models, the interaction of the hypercycle with another line meeting one of $\ell$'s ideal points.]

(c) Show that a horocycle is an ellipse which ``osculates'' the unit circle; in other words, the unit circle and horocycle can be parametrized by functions $\varphi(t),\psi(t)$ respectively, such that
$$\varphi(0)=\psi(0),~~~~\varphi'(0)=\psi'(0),~~~~\varphi''(0)=\psi''(0).$$
[Express the horocycle as a converging limit of hypercycles.]
\end{enumerate}

\subsection*{4.9. Hyperboloid and Hemisphere Models: Model Conversion}
\addcontentsline{toc}{section}{4.9. Hyperboloid and Hemisphere Models: Model Conversion}
We know three models of hyperbolic $n$-space which can be viewed in Euclidean $n$-space: the Poincar\'e ball (or disk) model, the Poincar\'e half-space (or half-plane) model, and the Beltrami-Klein model.  Here, we shall introduce two more models that \emph{cannot} be viewed in Euclidean $n$-space, and then see a clever geometric way to relate the models.

The first such model is the hyperboloid model, also known as the Lorentz / Minkowski model.  It takes place on the upper branch of the hyperboloid in $\mathbb R^{n+1}$, given by
$$x_{n+1}=\sqrt{x_1^2+\dots+x_n^2+1},$$
and has the following properties.  [Note that the equation $x_{n+1}^2=x_1^2+\dots+x_n^2+1$ is \emph{not} quite right, as it gives both branches of the hyperboloid, and the model of hyperbolic space only takes place on the upper branch.]
\begin{itemize}
\item Its ideal points are \emph{not} directly visible.  However, the hyperboloid branch can be viewed as a subset of $P^{n+1}(\mathbb R)$ rather than $\mathbb R^{n+1}$.  In this case, the (topological) closure of the branch consists of an $(n-1)$-sphere of points at infinity, given by $\{[x_1:\dots:x_n:x_{n+1}:0]:x_{n+1}^2=x_1^2+\dots+x_n^2\}$.  In this case, these points at infinity are the ideal points of the $n$-space.  This makes perfect sense because ideal points of hyperbolic space are infinitely far away in the hyperbolic sense anyway.

\item $k$-planes in this model are intersections of the hyperboloid branch with $(k+1)$-planes in $\mathbb R^{n+1}$ through the origin.  In particular, if $n=2$, then lines in the model are intersections of the branch $z=\sqrt{x^2+y^2+1}$ with planes through the origin in $\mathbb R^3$.

\item $k$-spheres appear tall and skinny to the Euclidean eye when they are high up on the branch.

\item The model is not conformal; angle measures between lines do not generally coincide with the Euclidean angle measures.
\end{itemize}
To illustrate this model, the heptagonal tiling and the truncated order-$7$ triangular tiling are displayed below, on the hyperboloid branch $z=\sqrt{x^2+y^2+1}$ in the case $n=2$.
\begin{center}
\includegraphics[scale=.2]{HeptagonalTiling_Hyper.png}~~~~
\includegraphics[scale=.2]{HyperrogueTiling_Hyper.png}
\end{center}

The other model that cannot be viewed in Euclidean $n$-space is the hemisphere model.  It takes place on the upper half of the unit $n$-sphere, $x_{n+1}=\sqrt{1-x_1^2-\dots-x_n^2}$.  Significant properties of this model are:
\begin{itemize}
\item Its ideal points are on the equatorial $(n-1)$-sphere, given by $x_{n+1}=0$, $x_1^2+\dots+x_n^2=1$.

\item $k$-planes in this model are intersections of the hemisphere with $(k+1)$-planes in $\mathbb R^{n+1}$ parallel to the $x_{n+1}$ axis.  In particular, if $n=2$, then lines in the model are intersections of the hemisphere $z=\sqrt{1-x^2-y^2}$ with planes parallel to the $z$-axis; these are upright-standing semicircle arcs.

\item $k$-spheres are Euclidean $k$-spheres contained in the hemisphere, which do not touch the rim.

\item The model is conformal; every angle measure matches the Euclidean angle measure.
\end{itemize}
It is worth remarking that lines are \emph{not} geodesics in the hemisphere's own geometry.  If they were, then by the spherical nature of the hemisphere, the angles of a triangle would add to $>180^\circ$, contradicting Proposition 4.8.

To illustrate this model, the heptagonal tiling and the rhombitriheptagonal tiling are displayed below, on the hemisphere $z=\sqrt{1-x^2-y^2}$ in the case $n=2$.
\begin{center}
\includegraphics[scale=.3]{HeptagonalTiling_Hemis.png}~~~~
\includegraphics[scale=.3]{RhombitriheptagonalTiling_Hemis.png}
\end{center}
The hemisphere model is not usually used to construct things in hyperbolic geometry; however, it does provide a curious geometric link between the conversions.\\

\noindent Of course, the only way to identify these models with hyperbolic space as we know it is to construct the conversion maps.  Here's how it is done:

(1) Start with a point $(a_1,\dots,a_n,a_{n+1})$ in the hyperboloid model of hyperbolic $n$-space.  Thus, $a_{n+1}>0$ and $a_{n+1}^2=a_1^2+\dots+a_n^2+1$.

(2) Draw the line through that point and the origin, and see where it meets the hyperplane $x_{n+1}=1$.  This intersection point is $\left(\frac{a_1}{a_{n+1}},\dots,\frac{a_n}{a_{n+1}},1\right)$.  Observe that the sum of the squares of the first $n$ components is less than $1$, so that this point is in the unit $n$-ball of the hyperplane.  This is the corresponding point in the Beltrami-Klein model.

(3) Project this point vertically (parallel to the $x_{n+1}$-axis) to a point on the hemisphere $x_{n+1}=\sqrt{1-x_1^2-\dots-x_n^2}$.  In this case, the $x_{n+1}$-coordinate becomes
$$\sqrt{1-\left(\frac{a_1}{a_{n+1}}\right)^2-\dots-\left(\frac{a_n}{a_{n+1}}\right)^2}=\sqrt{\frac{a_{n+1}^2-a_1^2-\dots-a_n^2}{a_{n+1}^2}}=\sqrt{\frac 1{a_{n+1}^2}}=\frac 1{a_{n+1}},$$
and hence the resulting point is
\begin{equation}\tag{HS}\left(\frac{a_1}{a_{n+1}},\dots,\frac{a_n}{a_{n+1}},\frac 1{a_{n+1}}\right)\end{equation}
which is the corresponding point in the hemisphere model.

(4) Consider the point $S=(0,\dots,0,-1)$, which is the south pole of the $n$-sphere in which the hemisphere is contained.  [It is not actually in the hemisphere, as its last component is negative.]  Connect this point to the point (HS), and see where the resulting line meets the hyperplane $x_{n+1}=0$.  Basic algebra entails the resulting point is
$$\left(\frac{a_1}{a_{n+1}+1},\dots,\frac{a_n}{a_{n+1}+1},0\right).$$
If this point is regarded as a vector $\vec v$, then $\vec v\cdot\vec v=\frac{a_{n+1}-1}{a_{n+1}+1}$, and hence $\|\vec v\|<1$.  This means that the point is contained in the unit $n$-ball of the hyperplane $x_{n+1}=0$.  This is the corresponding point in the Poincar\'e ball model.

In other words, the Poincar\'e ball model is obtained by taking the stereographic projection from the hypersphere to the hyperplane $x_{n+1}=0$, via the point $S$.  It maps the hypersphere precisely to the unit $n$-ball.

It actually turns out that the line through $S$ and the point (HS) also meets the original point $(a_1,\dots,a_n,a_{n+1})$ on the hyperboloid.  The reader can readily check this.  This gives an alternative way to find the hemisphere and Poincar\'e ball points directly given the hyperboloid point.

(5) Finally, to get the Poincar\'e half-space model, we consider the point $P=(-1,0,\dots,0)$ which is on the rim of the hemisphere.  We stereographically project the hemisphere-model point $\left(\frac{a_1}{a_{n+1}},\dots,\frac{a_n}{a_{n+1}},\frac 1{a_{n+1}}\right)$ onto the hyperplane $x_1=0$ through this point.  The result is
$$\left(0,\frac{a_2}{a_1+a_{n+1}},\dots,\frac{a_n}{a_1+a_{n+1}},\frac 1{a_1+a_{n+1}}\right).$$
This is the only conversion which breaks free of the permutability of the points $x_1,\dots,x_n$, but it is rather important that it does.  Of course, one could get the half-space model by taking the stereographic projection from \emph{any} rim point of the hemisphere.\\

\noindent The reader can readily verify that all of the aforementioned conversions are consistent with what has been covered earlier in the chapter, along with the properties of the hyperboloid and hemisphere models that have been stated here.  For example, if $\vec v$ is a point in the Poincar\'e ball model, then $\frac{2\vec v}{1+\|\vec v\|^2}$ is the corresponding point in the Beltrami-Klein model.

The following diagram illustrates the conversions between the five models, by taking the cross section in the plane spanned by the $x_1$ and $x_{n+1}$ axes.  Note, however, the stereographic projection for the half-space model has \emph{not} been taken to the hyperplane $x_1=0$.  It has been taken to a different hyperplane off to the side, so that the reader can see all the models clearly.
\begin{center}
\includegraphics[scale=.5]{ModelConversion.png}
\end{center}
Note that these cross-sectional curves are lines in each of the hyperbolic-space models.  An interesting exercise is to parametrize one of them, and then convert the parametrization to the other models.  A typical parametrization of the hyperbola branch is $t\mapsto(\sinh t,\cosh t),t\in\mathbb R$.  Indeed, we have $\cosh t>0$ and $\cosh^2t=\sinh^2t+1$.

We leave it to the reader to verify the following facts, possibly using results from Section 4.5:
\begin{itemize}
\item The line on the hyperboloid model is parametrized via $t\mapsto(\sinh t,\cosh t)$.

\item The line on the Beltrami-Klein model [$y=1$] is parametrized via $t\mapsto(\tanh t,1)$.

\item The line on the hemisphere model is parametrized via $t\mapsto(\tanh t,\operatorname{sech}t)$.

\item The line on the Poincar\'e ball model is parametrized via $t\mapsto(\tanh(t/2),0)$.  [Hint: first show that $\tanh(t/2)=\frac{\sinh t}{1+\cosh t}$.]

\item The line on the half-space model is parametrized via $t\mapsto(0,e^{-t})$.
\end{itemize}
By examining these, one can see that $t$ measures arc length: the points corresponding to $t=a$ and $t=b$ have a hyperbolic distance of exactly $|a-b|$.

The exercises will cover two more models of hyperbolic $n$-space, which can be viewed in Euclidean $n$-space, but which are relatively rare.

\subsection*{Exercises 4.9. (Hyperboloid and Hemisphere Models: Model Conversion)} % Finally introduce these two models and explain how to convert between them.
% POTENTIAL EXERCISES: Band model, Gans model ( https://en.wikipedia.org/wiki/Hyperbolic_geometry#The_Gans_model )
% In the exercise re. the band model, mention the deep result that any (genuine) 2-dimensional region can be made into a conformal model of the hyperbolic plane.
\begin{enumerate}
\item (a) Show that there is a projective transformation $T:(x_1,\dots,x_n,x_{n+1})\mapsto\left(\frac{x_1}{x_{n+1}},\dots,\frac{x_n}{x_{n+1}},\frac 1{x_{n+1}}\right)$ of $P^{n+1}(\mathbb R)$, and that $T^2$ is the identity.

(b) Show that $T$ exchanges the hyperboloid branch and the hemisphere, and is in fact an isometry from the hyperboloid model of hyperbolic $n$-space to the hemisphere model.

(c) Use this to show that if $n=2$, then circles on the hyperboloid model are ellipses obtained by intersecting the branch with a (projective) plane.  Also, horocycles on the hyperboloid model are parabolas, and hypercycles on the hyperboloid model are hyperbolas obtained by intersecting the branch with planes that are not through the origin.

\item Let $T$ be a projective transformation of $P^{n+1}(\mathbb R)$ which fixes the upper hyperboloid branch.

(a) Show that $T$ fixes the set of points at infinity in the closure of the branch; i.e., the set $\{[x_1:\dots:x_n:x_{n+1}:0]:x_{n+1}^2=x_1^2+\dots+x_n^2\}$.  [It may help to think of the continuity of $T$.]

(b) Show that $T$ fixes the origin of $\mathbb R^{n+1}$.  [Explain why $T$ permutes the hyperplanes which are tangent to the hyperboloid at the infinity points in~(a).  Show that they all contain the origin.]

(c) Show that $T$ fixes the unit $n$-ball in the hyperplane $x_{n+1}=1$.

(d) Now show that $T$ is a hyperbolic isometry of the hyperboloid model, and conversely, every isometry is of this form.  [Because of the geometry behind the conversion between this model and the Beltrami-Klein model, it is clear that restrictions of $T$ to the hyperboloid and to the unit $n$-ball are conjugates under the conversion.  Now use Proposition 4.23, or more directly, its generalization to higher dimensions stated at the end of Section 4.8.]

(e) Show that a hyperbolic isometry of the hemisphere model is given by a projective transformation of $P^{n+1}(\mathbb R)$ which fixes the upper hemisphere.  [Exercise 1 may help.]

\item Explain why the argument of the previous exercise does \emph{not} imply that isometries of the Poincar\'e ball model are projective transformations.

\item\emph{(Band model.)} \---- The band model of the hyperbolic plane is a model which takes place on an infinite horizontal strip in $\mathbb R^2$.  Its regular points are of the form $(x,y)$ with $x,y\in\mathbb R$ and $|y|<\pi/2$.  Its rim consists of the points $(x,\pm\pi/2)$ for all $x\in\mathbb R$, along with \emph{two} symbolic infinity points, one on the left and one on the right.

The model can be obtained as follows: let $z$ be a point in the Poincar\'e half-plane model.  View $z$ as a complex number, such that $\operatorname{Im}(z)\geqslant 0$.  Rotate $z$ clockwise by $90$ degrees to get $-iz$; this point is in the right half of the complex plane where the real part is nonnegative.  Moreover, the argument of $-iz$ is a real number between $-\pi/2$ and $\pi/2$, and is the imaginary part of $\ln(-iz)$.  The point $\ln(-iz)$ in the complex plane is then the corresponding point of the band model.  The points corresponding to $z=0$ and $z=\infty$ are the symbolic infinity points of the band model; they will be referred to as $-\infty$ and $+\infty$ respectively.

Note that this model is conformal (because the conversion map $z\mapsto\ln(-iz)$ is holomorphic, and hence conformal).  Thus angle measures coincide with the Euclidean angle measures, and all lines meet the rim orthogonally.  Each line is either the $x$-axis (which connects the symbolic infinity points), or looks like one of these:
\begin{center}
\includegraphics[scale=.2]{BandModelLines.png}
\end{center}
(a) Show that every horizontal translation of the band by a real number is an isometry.  [Conjugate it via the conversion and see what map of the half-plane is entailed.]

(b) Show that for each $a\in\mathbb R$, the line from $(a,\pi/2)$ to $(a,-\pi/2)$ is simply the Euclidean line segment between these points.

(c) The line from $(0,\pi/2)$ to $+\infty$ is given by the equation $x=\ln\csc y$ ($y>0$).  [Do the construction in the half-plane model and then apply the conversion map.]

(d) The line from $(a,\pi/2)$ to $(-a,-\pi/2)$ is given by the equation $y=\sin^{-1}\frac{\sinh x}{\sinh a}$, with $x\in[-a,a]$.

(e) The line from $(a,\pi/2)$ to $(-a,\pi/2)$ is given by the equation $y=\sin^{-1}\frac{\cosh x}{\cosh a}$ with $x\in[-a,a]$.

(f) Explain how parts (a)-(e) can be used to derive the line between any two ideal points.

(g) If $|y_1|,|y_2|<\pi/2$, show that the hyperbolic distance between $(x_1,y_1)$ and $(x_2,y_2)$ is
$$\cosh^{-1}\left(\frac{e^{x_1-x_2}+e^{x_2-x_1}-2\sin y_1\sin y_2}{2\cos y_1\cos y_2} \right ).$$
[Pass them through the conversion, and then use Exercise 9 of Section 4.3.]

(h) Find equations for hypercycles, horocycles and circles.  [It would help to think of the preimage of any generalized circle under the exponential map $z\mapsto e^z$ from $\mathbb C\to\mathbb C$.]

Here is a sample of the triheptagonal tiling in the band model:
\begin{center}
\includegraphics[scale=.25]{TriheptagonalTiling_Band.png}
\end{center}
At this point it is worth mentioning a rather deep fact about conformal models of the hyperbolic plane.  You can take \emph{any} open region of $\mathbb R^2$ which is homeomorphic to the disk (such as an ellipse, a square, or even the shape of a pet labrador), and there will be a conformal model of the hyperbolic plane in that region.  This is due to the Riemann mapping theorem, which states that there is a conformal mapping from the open disk to any such region.  Such a conformal mapping can then be used as a conversion map between models.

\item\emph{(Gans model.)} \---- The Gans model is a nonconformal model of hyperbolic $n$-space, which uses the entire Euclidean space $\mathbb R^n$.  It was proposed in 1966 by David Gans, in the \emph{American Mathematical Monthly}.  This space does not contain the ideal points; however, they may be adjoined symbolically if one desires.

This model can easily be converted from the hemisphere model as follows: let $(x_1,x_2,\dots,x_{n+1})$ be a regular point in the hemisphere model; thus, $x_{n+1}>0$ and $x_1^2+\dots+x_n^2+x_{n+1}^2=1$.  Draw the line through the origin and this point and see where it meets the hyperplane $x_{n+1}=1$; this point of intersection is $\left(\frac{x_1}{x_{n+1}},\dots,\frac{x_n}{x_{n+1}},1\right)$, and (as a point of the hyperplane $x_{n+1}=1$), is the corresponding point of the Gans model.  [Note that this is not stereographic projection because the origin is not on the $n$-sphere.]

Alternatively, the model can be converted from the hyperboloid model through orthographic projection, i.e., $(x_1,\dots,x_n,x_{n+1})\mapsto(x_1,\dots,x_n,1)$ with the former point on the hyperboloid branch.  The results of this chapter readily show that this gives the same model.

In parts (a)-(b), assume $n=2$; in this case a point $(x,y,z)$ on the hemisphere corresponds to the point $(x/z,y/z)\in\mathbb R^2$.

(a) Every line is either a Euclidean line through the origin, or a branch of a hyperbola whose center is the origin.  In particular, lines that do not go through the origin are concave away from it.  [Consider the cone centered at the origin of $\mathbb R^3$, going through a line in the hemisphere model.]

(b) Hypercycles are Euclidean lines not through the origin and branches of hyperbolas not centered at the origin; horocycles are parabolas; and circles are (Euclidean) ellipses.

(c) A circle is tall in the direction of the Euclidean line through the origin and its center, and narrow in the perpendicular direction.

(d) More generally, in the Gans model of hyperbolic $n$-space, a $k$-plane is a Euclidean $k$-plane through the origin or a branch of a $k$-dimensional hyperboloid whose center is the origin.  Classify what a $k$-sphere is as well.

Here is a sample of the rhombitriheptagonal tiling in the Gans model of the hyperbolic plane:
\begin{center}
\includegraphics[scale=.25]{RhombitriheptagonalTiling_Gans.png}
\end{center}
\end{enumerate}







































\chapter{Spherical Geometry}

Spherical geometry is geometry which takes place on a sphere.  It is particularly common to navigation and astronomy, since the planet Earth is sphere-shaped.  It is similar to hyperbolic geometry, but rather ``opposite.''  For example, in hyperbolic geometry, there are numerous lines through a point parallel to a given line; whereas in spherical geometry, there are no parallel lines, as we will see.

The distinguishing feature of this geometry is that unlike Euclidean and hyperbolic geometry, line segments are not free to be as long as they want.  After a finite distance, they close upon themselves.  Thus, the study of the distance between points will be slightly different than expected.  For each point there will be another significant point called the \emph{antipode}, and these two points will relate in many ways.

Spherical geometry enables one to study and classify polyhedra, which essentially arise from tilings.  The spherical plane has the unique property that it is \emph{compact}, and its tilings consist of finitely many faces in total, unlike the Euclidean and hyperbolic geometries.  Spherical tilings will be covered in the second section.  The third and fourth sections will introduce polyhedra, and classify the Archimedean solids with the aid of Chapter 2's last result.  This chapter will end with asim kind of geometry ilar to spherical geometry which relates to projective space.

\subsection*{5.1. Basic Geometry and Stereographic Projection}
\addcontentsline{toc}{section}{5.1. Basic Geometry and Stereographic Projection}
We take our spherical plane to be the unit sphere $x^2+y^2+z^2=1$ in $\mathbb R^3$, and we denote it as $S^2(\mathbb R)$.  We shall start by defining certain terms directly on the sphere, and then we will pass the material to the extended plane $\overline{\mathbb R^2}=\mathbb R^2\sqcup\{\infty\}$ via stereographic projection.

In $S^2(\mathbb R)$, a \textbf{line} is defined to be a great circle: this is a circle (of radius $1$) centered at the origin in the ambient $\mathbb R^3$, and is a set of the form $S^2(\mathbb R)\cap\Pi$ with $\Pi$ a plane through the origin (i.e., a two-dimensional vector subspace of $\mathbb R^3$).  A \textbf{line segment} is defined to be a segment of the arc of such a circle.  These concepts are illustrated below.
\begin{center}
\includegraphics[scale=.2]{SphLine.png}~~~~
\includegraphics[scale=.2]{SphLineSeg.png}\\
~~~~~~Spherical Line~~~~~~~~~Spherical Line Segment
\end{center}
Note that each point of $S^2(\mathbb R)$ determines a radius: namely, the Euclidean line segment from that point to $\vec 0\in\mathbb R^3$.  We define the \textbf{distance} between $a,b\in S^2(\mathbb R)$, denoted as $ab$ or $\rho(a,b)$, to be the angle between the radii corresponding to $a$ and $b$ (in radians), as illustrated below.  Note that this is also the arc length of the shorter (spherical) line segment connecting the points.  Moreover, \emph{this distance is always $\leqslant\pi$}.  This strongly contrasts with the Euclidean and hyperbolic geometries, where line segments can be arbitrarily long.
\begin{center}
\includegraphics[scale=.2]{SphDistance.png}
\end{center}
Since a line is a great circle by definition, the radii from the points on a line all lie in one Euclidean plane.  From the angle addition postulate in Euclidean geometry, we \emph{almost} have the segment addition postulate for spherical geometry: if $a,b,c$ are on a line with $b$ between $a$ and $c$, and $\rho(a,b)+\rho(b,c)\leqslant\pi$, then $\rho(a,c)=\rho(a,b)+\rho(b,c)$.  However, the assumption that $\rho(a,b)+\rho(b,c)\leqslant\pi$ is crucial, and the statement may be false without it (why?).

Just like the Poincar\'e disk model of the hyperbolic plane, this geometry is conformal.  In other words, the \textbf{angle measure} between two intersecting lines is the Euclidean angle measure between them.  Again the angle addition postulate is clear, but we do not have an analogue of Axiom 2.9.

We may now cover basic starting theorems about spherical geometry.  They are surprisingly easier to prove than they were in hyperbolic geometry.  Points $p,q\in S^2(\mathbb R)$ are said to be \textbf{antipodes} if $p=-q$ as vectors in $\mathbb R^3$.\\

\noindent\textbf{Proposition 5.1.} \emph{In the spherical plane,}

(i) \emph{Two points are antipodes if and only if their (spherical) distance is equal to $\pi$.} % No, the Euclidean distance would be 2, the diameter of the sphere.

(ii) \emph{Any two lines intersect in exactly two points which are antipodes.  Hence, no parallel lines exist.}

(iii) \emph{Two points determine a line, unless the points are antipodes, in which case every line through one point also goes through the other.}\\

\noindent Here is an illustration of (ii):
\begin{center}
\includegraphics[scale=.2]{SphLines.png}
\end{center}
\begin{proof}
(i) is clear from the definitions, as straight angles (i.e., where the two rays are part of a common line) are precisely the angles with measure $\pi=180^\circ$.

(ii) Let $\ell_1$ and $\ell_2$ be distinct lines in $S^2(\mathbb R)$.  By definition, we may write $\ell_j=S^2(\mathbb R)\cap V_j$ where $V_1,V_2$ are planes through the origin.  With that, $V_1$ and $V_2$ are two-dimensional subspaces of the vector space $\mathbb R^3$.  Since $V_1\not\subset V_2$ (for obvious reasons), $V_1\cap V_2\subsetneq V_1$, so that $\dim(V_1\cap V_2)<2$.  If $\dim(V_1\cap V_2)=0$, then $V_1+V_2=V_1\oplus V_2$ and its dimension is $4$; contradiction, because no subspace of $\mathbb R^3$ can have dimension $4$.  Therefore, $\dim(V_1\cap V_2)=1$.  With that, the subspace $V_1\cap V_2$ can be written as the span of a unit vector $\vec u\in\mathbb R^3$.  From this it follows readily that $\ell_1\cap\ell_2=\{\vec u,-\vec u\}$. % Then I don't understand why it didn't here.  I've used \oplus all along.

(iii) Let $p,q\in S^2(\mathbb R)$ be distinct points.  Suppose first that they are not antipodes.  Then they are linearly independent vectors in $\mathbb R^3$, hence span a two-dimensional subspace $V$.  With that, $\ell=S^2(\mathbb R)\cap V$ is a spherical line through $p$ and $q$.  There are many ways to see that this line is unique: for instance, if $\ell'$ is another line through $p$ and $q$, then $p,q$ are both in $\ell\cap\ell'$: this contradicts part (ii), which states that $\ell\cap\ell'$ consists of a point and its antipode.

If $p$ and $q$ are antipodes, however, then $p=-q$.  With that, it is clear that any subspace of the vector space $\mathbb R^3$ \---- and hence any spherical line \---- contains $p$ if and only if it contains $q$. % How much more?  It proves that every line through either p or q also goes through the other, which is part of the last phrase.
\end{proof}

\noindent Note that there are no such thing as rays in spherical geometry, since they would close upon themselves and be lines.

Just as in the previous kinds of geometry, a spherical line segment has well-defined endpoints.  It can also have any length \emph{less than $2\pi$} \---- $2\pi$ is the circumference of an entire line, as a circle of radius $1$ in $\mathbb R^3$.  However, if its length is a number $\alpha$ with $\pi<\alpha<2\pi$, then $\alpha$ is \emph{not} the distance between the endpoints: $2\pi-\alpha$ is, since there is a shorter line segment connecting the same points.

Axiom 2.4(ii) and (iii) have already been covered for spherical geometry [(ii) is \emph{almost} true]; and as for Axiom 2.4(i), the closest true statement in the spherical case is this one which the reader may verify:
\begin{center}
\textbf{If $\ell$ is a line containing a point $a$, and $r$ is a real number such that $0<r<\pi$, there are exactly two points on $\ell$ whose distance from $a$ is equal to $r$.}
\end{center}
The well-known parallel postulate, however, is one of the axioms that differs in all three types of geometry.  Up to now, we have established that:
\begin{itemize}
\item \textbf{In Euclidean geometry}: Given a line and a point not on the line, there is exactly one line through the point parallel to the given line.  [Axiom 2.5.]

\item \textbf{In hyperbolic geometry}: Given a line and a point not on the line, there are multiple lines through the point parallel to the given line.  [Proposition 4.5.]

\item \textbf{In spherical geometry}: Given a line and a point not on the line, there is no line through the point parallel to the given line.  [By Proposition 5.1(ii), there are no parallel lines at all.]
\end{itemize}
Recall the remarks given immediately after Axiom 2.5.  These differences between the types of geometry are the main starting point of the matter.

In addition, we \emph{almost} have the perpendicular postulate, but there is an exception to it that must be noted.  If $p\in S^2(\mathbb R)$, we define its \textbf{attention line} to be the set $\{\vec v\in S^2(\mathbb R):\vec v\cdot\vec p=0\}$.  It is clear that this is a line, and that all of its points have a distance from $p$ of exactly $\pi/2$.  Each line is the attention line of two points which are antipodes, and these points are called the line's \textbf{center points}.\\

\noindent\textbf{Proposition 5.2.} (i) \emph{Lines $\ell_1,\ell_2$ are perpendicular if and only if $\ell_2$ contains the center points of $\ell_1$.}

(ii) \textsc{(Perpendicular postulate)} \emph{Given a line $\ell$ and a point $p$, if $p$ is not a center point of $\ell$ then there is a unique line through $p$ perpendicular to $\ell$.  However, if $p$ is a center point of $\ell$, every line through $p$ is perpendicular to $\ell$.}\\

\noindent It is worth remarking that part (i) is similar to Proposition 4.21 about the Beltrami-Klein model, which states that two lines are perpendicular if and only if one of them contains the pole point of the other.
\begin{proof}
(i) Let $\ell_j=S^2(\mathbb R)\cap V_j$ with the $V_j$ two-dimensional subspaces of $\mathbb R^3$.  Let $p$ be a center point of $\ell_1$.  Then by definition, $V_1=\{\vec v\in\mathbb R^3:\vec p\cdot\vec v=0\}$, so that $\vec p$ is a unit normal vector to $V_1$.  Consequently, $\ell_1\perp\ell_2\iff V_1\perp V_2\iff\vec p\in V_2\iff p\in\ell_2$.

(ii) Let $q$ be a center point of $\ell$.  By part (i), the lines perpendicular to $\ell$ are precisely the lines through $q$.  Thus, if $p$ is not a center point, then $p\ne q$ and $p,q$ are not antipodes; hence there is a unique line through them by Proposition 5.1(iii), and this is also the unique line through $p$ perpendicular to $\ell$.  If $p$ is a center point, then either $p=q$ or $p,q$ are antipodes; by Proposition 5.1 every line through $p$ also goes through $q$.
\end{proof}

\noindent Similar to Theorem 4.7, we also have this curious fact:\\

\noindent\textbf{Proposition 5.3.} \textsc{(Common Perpendicular Theorem)} \emph{If $\ell_1$ and $\ell_2$ are distinct lines, there is a unique line $\ell$ simultaneously perpendicular to both $\ell_1$ and $\ell_2$.}
\begin{proof}
If $p_1,p_2$ are center points of $\ell_1,\ell_2$ respectively, then $p_1,p_2$ are distinct non-antipodes.  The simultaneous perpendicularity is equivalent to saying $\ell$ goes through $p_1,p_2$: now use Proposition 5.1(iii).
\end{proof}

\noindent We now introduce triangles.  As before, a triangle consists of three noncollinear points, and (spherical) line segments connecting each pair of them.  Note that three noncollinear points cannot contain a point and its antipode: points are collinear if and only if they are linearly dependent vectors of $\mathbb R^3$, which is always the case if two of them are negatives of each other.  Thus, if $a,b,c$ are noncollinear points, each pair of them is connected by exactly two line segments, and either of them can be taken to be the side of the triangle between them.  Whenever we say $\triangle abc$, the sides will be the \emph{shorter} line segments between the points, unless otherwise specified.
\begin{center}
\includegraphics[scale=.2]{SphTriangle.png}
\end{center}
It is now natural to cover the triangle congruence theorems, and see how the sum of the angles of a triangle compares to $\pi=180^\circ$.  However, it may feel awkward to do these directly on the sphere, where every point ``faces a different direction'' and has a different tangent plane.  For the more general case, we would like to use a model which takes place (mostly) in the flat plane $\mathbb R^n$.  We have done this for hyperbolic geometry, anyway, and there is a curiously basic way to view spherical geometry using the results from Section 4.1 and 4.2.
The \textbf{stereographic projection model} is obtained as follows: let $N=(0,0,1)$ and $\Pi$ be the $xy$-plane, then use stereographic projection $S^2(\mathbb R)\to\Pi\sqcup\{\infty\}$ (where $N$ maps to $\infty$) to convert from the sphere to the plane:
\begin{center}
\includegraphics[scale=.2]{SphereLines1.png}~~~~
\includegraphics[scale=.2]{SphereLines2.png}%\\Three lines in the spherical plane.~~~~~~Stereographic projection.~~~~
\end{center}
We shall regularly identify $\Pi$ with the complex plane $\mathbb C$.  The following facts readily follow:
\begin{itemize}
\item Lines in the stereographic projection model are Euclidean lines through the origin, and circles with radius $r$ and center $a\in\mathbb C$, such that $r^2-|a|^2=1$.  [Exercise 7 of Section 4.2.]

\item Angle measures are the Euclidean angle measures.  [Exercise 4(b) of Section 4.1 shows that stereographic projection is conformal.]

\item When $z_1,z_2\in\mathbb C$ are viewed in the stereographic projection model of the spherical plane, their distance is $2\tan^{-1}\frac{|z_1-z_2|}{|\overline{z_1}z_2+1|}.$
In particular, the distance between $z$ and $0$ is $2\tan^{-1}|z|$.  [Exercise 2.  Note the similarity with Exercises 4 and 10 of Section 4.3; e.g., Exercise 10 shows that the distance between points $z_1,z_2$ in the Poincar\'e disk model is $2\tanh^{-1}\frac{|z_1-z_2|}{|\overline{z_1}z_2-1|}$.]
\end{itemize}
The main purpose of switching to stereographic projection is to be able to prove complicated results, and to note their similarities with hyperbolic geometry.  As seen above, basic geometry is surprisingly easy to do directly on the sphere itself.

For a first example, we will show that the angles of a triangle add to greater than $\pi$, or $180^\circ$.  This can actually be shown directly on the sphere, using tangent planes [Exercise 3]; however, we will not use this strategy; it steers away from the setting of (spherical) plane geometry.\\

\noindent\textbf{Proposition 5.4.} \emph{The measures of the angles of a spherical triangle add to $>180^\circ$.}
\begin{proof}
As in the proof of Proposition 4.8, we may assume we are in the stereographic projection model, and $a=0$.
\begin{center}
\includegraphics[scale=.3]{SphTriangleAngles.png}
\end{center}
The Euclidean triangle $\triangle abc$ shares the same line segments $\overline{ab},\overline{ac}$, but the Euclidean line segment from $b$ to $c$ lies inside the triangle, with the spherical triangle's side curving outwards.  The reason is that the spherical line $\overset{\longleftrightarrow}{bc}$ is a circle with a center $u$ and radius $r$ such that $r^2-|u|^2=1$.  The interior of such a circle is given by the equation $|z-u|<r$, hence contains the origin $0$ (because $r^2-|u|^2=1>0\implies|u|<r$); this implies that the Euclidean circle everywhere curves away from the origin.  Hence, the angles at $b$ and $c$ are larger than the corresponding angles of the Euclidean triangle; yet the angle at $a$ is the same.  This completes the proof since the Euclidean triangle's angles add to $180^\circ$, and the spherical triangle's angles have a larger sum.
\end{proof}

\noindent Moreover, we have the same concept of triangle congruence from Euclidean and hyperbolic geometry, $\triangle abc\cong\triangle a'b'c'$ if $\overline{ab}\cong\overline{a'b'}$, $\angle bac\cong\angle b'a'c'$, etc.  We have the same congruence theorems from the hyperbolic case; in particular, the angles determine the triangle up to congruence:\\

\noindent\textbf{Proposition 5.5.} (i) \textsc{(Side-side-side / SSS)} \emph{If $\overline{ab}\cong\overline{a'b'},\overline{bc}\cong\overline{b'c'},\overline{ca}\cong\overline{c'a'}$, then $\triangle abc\cong\triangle a'b'c'$.}

(ii) \textsc{(Side-angle-side / SAS)} \emph{If $\overline{ab}\cong\overline{a'b'}$, $\overline{ac}\cong\overline{a'c'}$, and $\angle a\cong\angle a'$, then $\triangle abc\cong\triangle a'b'c'$.}

(iii) \textsc{(Hypotenuse-leg / HL / RHS)} \emph{If $\angle b,\angle b'$ are right angles, $\overline{ab}\cong\overline{a'b'}$ and $\overline{ac}\cong\overline{a'c'}$, then $\triangle abc\cong\triangle a'b'c'$.}

(iv) \textsc{(Angle-side-angle / ASA)} \emph{If $\angle a\cong\angle a'$, $\overline{ab}\cong\overline{a'b'}$, $\angle b\cong\angle b'$, then $\triangle abc\cong\triangle a'b'c'$.}

(v) \textsc{(Angle-angle-angle / AAA)} \emph{If $\angle a\cong\angle a'$, $\angle b\cong\angle b'$, and $\angle c\cong\angle c'$, then $\triangle abc\cong\triangle a'b'c'$.}\\

\noindent Notice that AAS is missing.  Exercise 4 shows that, in fact, AAS is generally false in spherical geometry.
\begin{proof}
Each of these can be proved by applying convenient isometries to the triangles, which cause certain corresponding parts to coincide.  We have illustrated this in the hyperbolic case in Proposition 4.9; here, we shall leave it to the reader.
\end{proof}
\noindent\textbf{Proposition 5.6} \textsc{(Isosceles Triangle Theorem)} \emph{If $\triangle abc$ is a triangle, then $\overline{ab}\cong\overline{ac}$ if and only if $\angle b\cong\angle c$.}
\begin{proof}
Copy the proof of Proposition 2.15, using Proposition 5.5 in place of Axiom 2.13.
\end{proof}

\noindent We can take the attention point of two lines (which must meet anyway).  If $\ell_1,\ell_2$ are distinct lines, we may let $\ell$ be the unique line perpendicular to both of them (Proposition 5.3).  If $p_j\in \ell\cap\ell_j$ for $j=1,2$, there are two line segments occurring between $p_1$ and $p_2$ (those points can't be antipodes, because that would imply $\ell_1=\ell_2$).  The midpoints of these segments are antipodes which are not centers of $\ell_1$ or $\ell_2$; we call these points \textbf{attention points} of the pair of lines.  We again have the nice properties of transversals:\\

\noindent\textbf{Proposition 5.7.} \emph{Let $\ell_1$ and $\ell_2$ be distinct lines, $\ell_3$ a transversal, and $p$ an attention point of $\ell_1,\ell_2$.  Then the following are equivalent:}

(i) \emph{$p\in\ell_3$.}

(ii) \emph{Corresponding angles are congruent.}

(iii) \emph{Corresponding exterior angles are supplementary.}

(iv) \emph{Alternate exterior angles are congruent.}

(v) \emph{Corresponding interior angles are supplementary.}

(vi) \emph{Alternate interior angles are congruent.}
\begin{proof}
(i) $\iff$ (vi). Copy the argument in Proposition 4.11, using Proposition 5.2 in place of 4.6, and Proposition 5.5 in place of Proposition 4.9.

As usual, (ii) - (vi) are equivalent because angles in linear pairs are supplementary.
\end{proof}

\noindent We conclude this section by discussing circles.  The conscientious reader may be aware that isometries have not been covered yet, the way they were in Section 4.3 before the basic geometric results.  However, the isometry group of the sphere has already been studied in Chapter 2, and there is not much difficulty in understanding it in the spherical geometry setting.  See Exercise 1.

As before, if $r$ is a positive real number between $0$ and $\pi$, and $o\in S^2(\mathbb R)$, then the set of points $a$ whose distance to $o$ equals $r$ is called a circle.  [Technically, one can arrange for this to work with $r\geqslant\pi$, by traveling in each direction from $o$, geodesically a distance of $r$.  This will get them a circle all the same, but there could be useless looping.]  $r$ is called the \text{radius} of the circle and $o$ is called a \textbf{center}.

We note that for $\vec p\in S^2(\mathbb R)$, we get (since the distance $\rho(\vec p,\vec o)$ is the angle between the vectors),
$$\rho(\vec p,\vec o)=\cos^{-1}\frac{\vec p\cdot\vec o}{\|\vec p\|\|\vec o\|}=\cos^{-1}(\vec p\cdot\vec o),$$
from which it follows that with $0<r<\pi$, the circle is given by the equation $\vec p\cdot\vec o=\cos r$.  If this equation were for all vectors $\vec p\in\mathbb R^3$, it would give a plane perpendicular to $\vec o$ (not necessarily through the origin).  Restricting this to $S^2(\mathbb R)$, we get a circle on the sphere in the usual sense.

Thus the circles in spherical geometry are merely the circles in the usual sense (the intersection of $S^2(\mathbb R)$ with planes).  It is clear that, conversely, every circle on the sphere is a circle in spherical geometry.  Note that this means that \textbf{lines are circles in spherical geometry} \---- specifically, circles with radius $\pi/2$.  This is similar to the fact that ``hypercycles'' (Section 4.4) in the Euclidean plane are lines.

Since stereographic projection preserves (generalized) circles by Lemma 4.2, we conclude that the circles in the stereographic projection model are the generalized circles of $\overline{\mathbb C}$.  Every generalized circle is a circle; it's only a line if it's either a Euclidean line through the origin, or a circle with center $r$ and radius $a$ such that $r^2-|a|^2=1$.

\subsection*{Exercises 5.1. (Basic Geometry and Stereographic Projection)} % Start by going over spherical geometry directly on the sphere, introducing points and lines.
%attention point theorem => circles
% POTENTIAL: the isometry group----it's O(n+1) in this particular geometry, [while the stabilizer of a point in all three kinds of geometry is O(n)]
\begin{enumerate}
\item The isometries of $S^2(\mathbb R)$ are precisely the elements of $O(3)$, transforming the unit sphere in $\mathbb R^3$ in the usual sense.  The elements of $SO(3)$ are called \textbf{orientation-preserving isometries}, and the rest are called \textbf{orientation-reversing isometries}.

(a) Explain why isometries preserve distances, lines and angles.

(b) Show that the isometries act transitively on the set of points; and also on the set of lines.

(c) In the stereographic projection model, show that the orientation-preserving isometries are precisely the M\"obius transforms which commute with the map $z\mapsto -1/\overline z$.  Use this to show that they are the M\"obius transforms of the form $z\mapsto\frac{az-b}{\overline bz+\overline a}$.  What are the orientation-reversing isometries?

\item (a) Let $z\in\mathbb C$, regarded as a point in the stereographic projection model of the spherical plane.  Show that the distance between $z$ and $0$ is $2\tan^{-1}|z|$.  [Find the corresponding points on the sphere, then it should be fairly easy.]

(b) If $z_1,z_2\in\mathbb C$ are viewed in the stereographic projection model, show that their distance is
$$\rho(z_1,z_2)=2\tan^{-1}\frac{|z_1-z_2|}{|\overline{z_1}z_2+1|}.$$
[By Exercise 1(b), the transformation $z\mapsto\frac{z-z_1}{\overline{z_1}z+1}$ is an isometry.  Pass $z_1,z_2$ through this isometry, then use part (a).]

(c) It is possible for the expression in (b) to be undefined.  When does this happen?  What is the distance between the points in this case? % Referring to when z_1,z_2 are antipodes, which entails that \overline{z_1}z_2 + 1 = 0, and the expression has a division by zero.  In this case, the distance is \pi; this makes sense in view of \tan(\pi/2) being undefined.

\item Here is an alternate proof of Proposition 5.4, using tangent planes.  Let $\triangle abc$ be a triangle in $S^2(\mathbb R)$.  Construct the tangent planes $\Pi_a,\Pi_b,\Pi_c$ to the sphere at the points $a,b,c$ respectively.

(a) Let $\ell_1=\Pi_a\cap\Pi_b$.  Show that $\ell_1$ is a line which is perpendicular to the Euclidean plane through the origin containing $\overline{ab}$.  [Use basic linear algebra and the fact that the tangent plane to a point is perpendicular to the radius at that point.]  Similarly with $\ell_2=\Pi_b\cap\Pi_c$ and $\ell_3=\Pi_c\cap\Pi_a$.

(b) The planes $\Pi_a,\Pi_b,\Pi_c$ have a common intersection point $p$ (in $\mathbb R^3$, not the sphere), and for each plane there is an angle at $p$ between the lines established in part (a).  Show that the angle on $\Pi_a$ is supplementary to the angle of $a$ in the spherical triangle, and similarly for the other angles.  [The angle between two lines on the sphere is the dihedral angle between the planes through the origin containing them: why?]

(c) Show that the angles between the planes sum to less than $2\pi=360^\circ$.  [Think of the planes as three hinged pieces of rigid, flat material; remove one hinge and flatten everything.]

(d) Conclude from parts (b) and (c) that the sum of the angles of $\triangle abc$ exceeds $\pi=180^\circ$.

\item Let $0<\alpha<\pi$ be any real number.

(a) Construct a triangle $\triangle abb'$ for which $m\angle b=\alpha$ and $m\angle b'=\pi-\alpha$.  [Remember, lines always meet no matter what.]

(b) Extend $\overline{bb'}$ on one side and let $c$ be a point on the line with $b'$ between $b,c$.  Show that $m\angle abc=m\angle ab'c=\alpha$, $\angle acb\cong\angle acb'$ and $\overline{ac}\cong\overline{ac}$, but $\triangle abc\not\cong\triangle ab'c$.  This is a counterexample to the assertion of AAS congruence in spherical geometry.

\item Show that a circle centered at $o$ with radius $r,0<r<\pi$ is also a circle centered at the antipode of $o$.  What is the radius in this situation?

\item\emph{(Lambert azimuthal equal-area projection.)} \---- The Lambert azimuthal equal-area projection is a projection of $S^2(\mathbb R)$ on a disk of radius $2$, obtained as follows.   Let the plane $\Pi$ be tangent to the sphere at the south pole $S=(0,0,-1)$, and identify $(x,y)$ with $(x,y,-1)$ on this plane.

Take any point $\vec p\in S^2(\mathbb R)$.  If $\vec p=S$, then $\vec p$ just maps to the origin $(0,0)$.  If $\vec p=N=(0,0,1)$, the north pole, then $\vec p$ does not map to any well-defined point; thus the north pole is excluded.  Otherwise, let $\Pi_1$ be the plane through $\vec p$ and the $z$-axis, and rotate the Euclidean line segment from $S$ to $\vec p$ in $\Pi_1$, centered at $S$, away from the $z$-axis, until it lies in the plane $\Pi$.  The following diagram illustrates:
\begin{center}
\includegraphics[scale=.3]{LAEAP_diagram.png}
\end{center}
Thus the projection of $\vec p$ is the point in $\Pi_1\cap\Pi$ whose distance from $S$ is the Euclidean distance from $S$ to $\vec p$ in the ambient $\mathbb R^3$.

(a) Show that the formula $\varphi:S^2(\mathbb R)-\{N\}\to\Pi$ for Lambert azimuthal equal-area projection is given by
$$\varphi(x,y,z)=\left(\sqrt{\frac 2{1-z}}x,\sqrt{\frac 2{1-z}}y\right),$$
and that its range is the origin-centered open disk of radius $2$, henceforth to be called $D_2(0)$.  [The distance from $(x,y,z)$ to $S$ is $\sqrt{x^2+y^2+(z+1)^2}=\sqrt{2(1+z)}$; show that this is also the distance from $S$ to the given point.  The rest should be clear.  As for the statement about the range, the diameter of $S^2(\mathbb R)$ is $2$.]

(b) Show that the inverse of $\varphi$, $\varphi^{-1}:D_2(0)\to S^2(\mathbb R)-\{N\}$ is given by
$$\varphi^{-1}(u,v)=\left(\sqrt{1-\frac{u^2+v^2}4}u,\sqrt{1-\frac{u^2+v^2}4}v,\frac{u^2+v^2}2-1\right).$$
[It suffices to show that this formula gives a two-sided inverse for the one in part (a).]

(c) Show that this projection is \emph{not} conformal.  [Exercise 4(a) of Section 4.1.]  The projection, however, preserves area; see Exercise 8 of Section 6.3.

(d) Show that at the origin \---- and only at the origin \---- lines are Euclidean straight lines, and angle measures are the Euclidean angle measures.  [This is what ``azimuthal'' means.  Note that this is also true for the stereographic projection model of the sphere, and the Poincar\'e disk and Beltrami-Klein models of the hyperbolic plane.]

Here is an illustration of two lines in the Lambert azimuthal equal-area projection of the spherical plane.  Note that the origin-centered one is a Euclidean circle, but the other one is not.
\begin{center}
\includegraphics[scale=.25]{LAEAP1.png}
\end{center}
\end{enumerate}

\subsection*{5.2. Triangles, Polygons and Tilings}
\addcontentsline{toc}{section}{5.2. Triangles, Polygons and Tilings}
Now that we have covered basic geometry, we wish to study triangles, exactly as we did in the Euclidean and hyperbolic planes.  The trigonometric laws for spherical triangles will be similar to those in Proposition 4.13, but they will not use the hyperbolic functions.  Instead, they will use the trigonometric functions.

Thus, we wish to remind ourselves of plenty of properties of trigonometric functions:
\begin{itemize}
\item $\cos x$ and $\sec x$ are even functions; $\sin x,\tan x,\cot x,\csc x$ are all odd functions.

\item $\sin^2x+\cos^2x=1$ (the Pythagorean identity).

\item $\tan x=\frac{\sin x}{\cos x}$.

\item $\cot x=\frac 1{\tan x}$; $\sec x=\frac 1{\cos x}$; $\csc x=\frac 1{\sin x}$.

\item $\cos(x\pm y)=\cos x\cos y\mp\sin x\sin y$, and $\sin(x\pm y)=\sin x\cos y\pm\cos x\sin y$.  (Therefore, taking $x=y$, we get $\cos(2x)=\cos^2x-\sin^2x=2\cos^2x-1=1-2\sin^2x$, and $\sin(2x)=2\sin x\cos x$.)

\item $\tan(x\pm y)=\frac{\tan x\pm\tan y}{1\mp\tan x\tan y}$.  In particular, taking $x=y$, $\tan(2x)=\frac{2\tan x}{1-\tan^2x}$.

\item $\tan^2x+1=\sec^2x$, and $\cot^2x+1=\csc^2x$.
\end{itemize}
For the conscientious reader who would like to prove these identities, several strategies are applicable: one could (i) recall that most of the identities have been proven in the early sections of Chapter 2, and then readily derive the rest; or (ii) recall the expressions for the trigonometric functions in terms of the exponential function and the imaginary unit (given in Section 4.5); then the proofs use basic algebra.

Throughout this chapter, a triangle is assumed to have side lengths and angles strictly between $0$ and $\pi$, and is given as $\triangle ABC$ where the vertices are capital letters, $A,B,C$ denote the angle measures, and $a,b,c$ denote the side lengths opposite those respective vertices:
\begin{center}
\includegraphics[scale=.25]{SphTriangleSample.png}
\end{center}
As previously mentioned, the spherical triangle laws imitate the hyperbolic triangle laws:\\

\noindent\textbf{Proposition 5.8.} \emph{For the above triangle,}

(i) \emph{The \textbf{spherical law of cosines} holds: $\cos c=\cos a\cos b+\sin a\sin b\cos C$.}

(ii) \emph{The \textbf{spherical law of sines} holds: $\frac{\sin a}{\sin A}=\frac{\sin b}{\sin B}=\frac{\sin c}{\sin C}$.}

(iii) \emph{The \textbf{second spherical law of cosines} holds: $\cos C=-\cos A\cos B+\sin A\sin B\cos c$.}\\

\noindent As before, (i) derives the angle measures from the side lengths, and (iii) derives the side lengths from the angle measures.  The special case of (i) where $C=\pi/2$ shows that if $a,b,c$ are the sides of a spherical right triangle with $c$ the hypotenuse, $\cos c=\cos a\cos b$.  Again we have that if $C=\pi$ then $c=a+b$ by part (i), and if $C=0$ then $c=|a-b|$ \---- the manifestation of the segment addition postulate.
\begin{proof}
(i) By applying an isometry, we may assume we are in the stereographic projection model, vertex $C$ is at $0$ and vertex $A$ is at a positive real number $r$.  Then sides $\overline{AC},\overline{BC}$ are Euclidean line segments and $C$ is the Euclidean angle measure between them; hence the location of vertex $B$ is of the form $se^{iC}$ where $s$ is a positive real number.

By Exercise 2 of the previous section, $a=2\tan^{-1}s$, $b=2\tan^{-1}r$ and $c=2\tan^{-1}\frac{|r-se^{iC}|}{|rse^{iC}+1|}$.  Thus, $\tan(c/2)=\frac{|r-se^{iC}|}{|rse^{iC}+1|}$, from which $\tan^2(c/2)=\frac{|r-se^{iC}|^2}{|rse^{iC}+1|^2}$ follows.  Yet,
$$\cos c=\frac{\cos c}1=\frac{\cos^2(c/2)-\sin^2(c/2)}{\cos^2(c/2)+\sin^2(c/2)}=\frac{1-\tan^2(c/2)}{1+\tan^2(c/2)}$$
$$=\frac{|rse^{iC}+1|^2-|r-se^{iC}|^2}{|rse^{iC}+1|^2+|r-se^{iC}|^2}$$
Since $|z|^2=z\overline z$ for $z\in\mathbb C$, we have $|rse^{iC}+1|^2=(rse^{iC}+1)\overline{(rse^{iC}+1)}=(rse^{iC}+1)(rse^{-iC}+1)=r^2s^2+2rs\cos C+1$, and similarly, $|r-se^{iC}|^2=(r-se^{iC})(r-se^{-iC})=r^2-2rs\cos C+s^2$.  Therefore
$$\cos c=\frac{(r^2s^2+2rs\cos C+1)-(r^2-2rs\cos C+s^2)}{(r^2s^2+2rs\cos C+1)+(r^2-2rs\cos C+s^2)}$$
$$=\frac{r^2s^2-r^2-s^2+1+4rs\cos C}{r^2s^2+r^2+s^2+1}=\frac{(1-r^2)(1-s^2)+4rs\cos C}{(1+r^2)(1+s^2)};$$
since Exercise 1(b) shows that $\cos a=\frac{1-s^2}{1+s^2}$, $\sin a=\frac{2s}{1+s^2}$, $\cos b=\frac{1-r^2}{1+r^2}$ and $\sin b=\frac{2r}{1+r^2}$, we conclude $\cos c=\cos a\cos b+\sin a\sin b\cos C$ as desired.

(ii) As in Proposition 4.13(ii) it suffices to show that $\frac{\sin a}{\sin A}=\frac{\sin b}{\sin B}$.  Let $\alpha=\cos a,\beta=\cos b,\gamma=\cos c$.  Then by the Pythagorean identity, we get $\sin^2a=1-\alpha^2$ and $\sin^2b=1-\beta^2$.  By part (i), $\cos c=\cos a\cos b+\sin a\sin b\cos C$, or what is the same thing, $\gamma=\alpha\beta+\sin a\sin b\cos C$.  Hence, $\sin a\sin b\cos C=\gamma-\alpha\beta$.  Squaring throughout,
$$(\gamma-\alpha\beta)^2=\sin^2a\sin^2b\cos^2C$$
and hence
$$\sin^2a\sin^2b\sin^2C=\sin^2a\sin^2b(1-\cos^2C)=\sin^2a\sin^2b-\sin^2a\sin^2b\cos^2C$$
$$=(1-\alpha^2)(1-\beta^2)-(\gamma-\alpha\beta)^2=1-\alpha^2-\beta^2-\gamma^2+2\alpha\beta\gamma$$
The expression on the right-hand side is symmetric in $\alpha,\beta,\gamma$; hence by repeating the argument with $a,b,c$ permuted, it is also equal to $\sin^2a\sin^2c\sin^2B$ and $\sin^2b\sin^2c\sin^2A$.  Therefore, $\sin^2a\sin^2c\sin^2B=\sin^2b\sin^2c\sin^2A$.  Dividing by $\sin^2c$ throughout, $\sin^2a\sin^2B=\sin^2b\sin^2A$.  Taking square roots, and noting that everything is positive (because $a,b,A,B\in(0,\pi)$, we get $\sin a\sin B=\sin b\sin A$, and therefore $\frac{\sin a}{\sin A}=\frac{\sin b}{\sin B}$.

(iii) As in part (ii), let $\alpha=\cos a,\beta=\cos b,\gamma=\cos c$.  We have shown that $\Delta=\sqrt{1-\alpha^2-\beta^2-\gamma^2+2\alpha\beta\gamma}$ is equal to
$$\Delta=\sin a\sin b\sin C=\sin a\sin c\sin B=\sin b\sin c\sin A.$$
Moreover, by part (i),
$$\sin a\sin b\cos C=\gamma-\alpha\beta$$
$$\sin a\sin c\cos B=\beta-\alpha\gamma$$
$$\sin b\sin c\cos A=\alpha-\beta\gamma$$
and as before, $\sin^2c=1-\gamma^2$.  Therefore
$$\frac{\cos C+\cos A\cos B}{\sin A\sin B}=\frac{(1-\gamma^2)(\gamma-\alpha\beta)-(\beta-\alpha\gamma)(\alpha-\beta\gamma)}{\Delta^2},$$
as can be seen by multiplying the numerator and denominator of the left-hand side by $\sin^2c\sin a\sin b$.  Yet as before, the right-hand side readily simplifies to $\gamma=\cos c$, proving (iii).
\end{proof}

\noindent Now that we have the fundamental properties, we can analyze many other properties of spherical triangles.  For example, if $\triangle ABC$ is equilateral (i.e., has three equal sides), it has three equal angles by the Isosceles Triangle Theorem.  We can let $s$ be its side length and $\theta$ its angle.  By Proposition 5.8(i),
$$\cos s=\cos^2s+\sin^2s\cos\theta$$
This, along with the fact that $\sin^2s=1-\cos^2s$, entails
$$(1-\cos\theta)\cos^2s-\cos s+\cos\theta=0$$
When $\theta$ is fixed, this is a quadratic equation in $u=\cos s$, and it is readily seen that the roots are $u=1,\frac{\cos\theta}{1-\cos\theta}$.  Yet $u=1$ entails $s=0$, absurd.  Therefore, we have $\cos s=\frac{\cos\theta}{1-\cos\theta}$.  Incidentally, since $\cos s<1$, we get $\cos\theta<\frac 12$, so that $\theta>60^\circ=\frac{\pi}3$ \---- the reader should be able to see how this is inferred.  Either using Proposition 5.8(iii) or directly deriving from that equation, we get $\cos\theta=\frac{\cos s}{1+\cos s}$.  Hence\\

\noindent\textbf{Proposition 5.9.} \emph{If a spherical equilateral triangle has side length $s$ and angle measure $\theta$, then $\cos s=\frac{\cos\theta}{1-\cos\theta}$ and $\cos\theta=\frac{\cos s}{1+\cos s}$.}\\

\noindent Now we turn our attention to right triangles.  We assume $\triangle ABC$ is a right triangle with $\angle C$ the right angle, so that $a,b$ are the lengths of the legs and $c$ is the length of the hypotenuse.  As previously stated, $\cos c=\cos a\cos b$.  Furthermore, by Proposition 5.8(iii), we get
$$\cos A=-\cos B\cos C+\sin B\sin C\cos a=\sin B\cos a$$
(because $C$ is a right angle), and hence $\cos a=\frac{\cos A}{\sin B}$, and we have a formula for one of the lengths of the legs.  Moreover, $\cos b=\frac{\cos B}{\sin A}$ by symmetry considerations, and hence $\cos c=\cos a\cos b=\frac{\cos A\cos B}{\sin A\sin B}=\cot A\cot B$.  [This is $<1$ this time, because $A+B>90^\circ=\pi/2$.  For a hyperbolic right triangle, those inequalities would be opposite.]

By Proposition 5.8(ii), $\frac{\sin a}{\sin A}=\frac{\sin c}{\sin C}=\sin c$, from which we get $\sin A=\frac{\sin a}{\sin c}$.  This shows that the sine of an angle in a right triangle is the quotient of the \emph{sine} of the opposite leg over that of the hypotenuse.  Moreover, the reader is encouraged to verify these, by imitating the arguments in Section 4.5:
\begin{itemize}
\item $\cos A=\frac{\tan b}{\tan c}$.

\item $\tan A=\frac{\tan a}{\sin b}$.
\end{itemize}
There are many other interesting identities for spherical triangles, which will be covered in Exercise 3.  However, in practice, we will only deal with a few at a time.\\

\noindent\textbf{Proposition 5.10.} \emph{Let $\triangle ABC$ be a spherical right triangle with $\angle C$ the right angle, and let $a,b,c$ be the side lengths opposite $A,B,C$ respectively.  Then:}

(i) \emph{$\cos c=\cos a\cos b$.}

(ii) \emph{$\cos b=\frac{\cos B}{\sin A}$, $\cos a=\frac{\cos A}{\sin B}$ and $\cos c=\cot A\cot B$.}

(iii) \emph{$\sin A=\frac{\sin a}{\sin c}$, $\cos A=\frac{\tan b}{\tan c}$ and $\tan A=\frac{\tan a}{\sin b}$.}\\

\noindent As before, we can define more general polygons.  An $n$-gon consists of an ordered $n$-tuple of points $A_1,\dots,A_n$ (called \textbf{vertices}) equipped with the $n$ line segments $\overline{A_1A_2},\dots,\overline{A_{n-1}A_n},\overline{A_nA_1}$ (called \textbf{edges} or \textbf{sides}), such that no two segments intersect with each other unless they share a vertex.  As before, we restrict ourselves to convex polygons.  By adapting Proposition 2.30 and using Proposition 5.4, one readily shows that the measures of the angles of a regular $n$-gon add to $>180(n-2)^\circ=\pi(n-2)$.  The definitions of an equilateral/equiangular/regular polygon are identical to those of Section 2.3.

It can also be shown that if $A_1,\dots,A_n$ is a regular $n$-gon, then one can form the \textbf{circumscribed circle} and the \textbf{inscribed circle}, and that every circle which is not a line has regular $n$-gons inscribed in the circle and circumscribed around.\\

\noindent\textbf{TILINGS}\\

\noindent As usual, now that we have studied triangles and polygons in detail, we may show how they cover the spherical plane.  For this, we recall the concept of a triangle pattern from Section 4.5.

Let $a,b,c$ be integers $\geqslant 2$.  We start with a triangle whose angle measures are exactly $\pi/a,\pi/b,\pi/c$.  Depending on how $\pi/a+\pi/b+\pi/c$ compares with $\pi$ (i.e., how $\frac 1a+\frac 1b+\frac 1c$ compares with $1$), the triangle exists in either spherical, Euclidean or hyperbolic geometry: spherical if the sum is greater than $\pi$ (in view of Proposition 5.4), hyperbolic if the sum is less than $\pi$ (in view of Proposition 4.8), and Euclidean if the sum equals $\pi$ (in view of Proposition 2.11).  Given this triangle, we may reflect it over its edges, as shown below, and then reflect all subsequently formed triangles over their own edges until they cover the plane.
\begin{center}\includegraphics[scale=.4]{TriangleReflecting.png}\end{center}
As previously shown, the only Euclidean ones ($\frac 1a+\frac 1b+\frac 1c=1$) are, up to ordering, $[2,4,4]$, $[2,3,6]$ and $[3,3,3]$.  These give the isosceles right triangle, the 30-60-90 right triangle (which is half the equilateral triangle), and the equilateral triangle.  However, there are infinitely many possible triangles in the hyperbolic case, where $a,b,c$ can be arbitrarily large.  There are also infinitely many in the spherical case, but they can be more simply classified.

If $a,b,c$ are integers $\geqslant 2$ such that $\frac 1a+\frac 1b+\frac 1c>1$, then one of the integers must be $2$: if they were all $\geqslant 3$, we would have $\frac 1a+\frac 1b+\frac 1c\leqslant\frac 13+\frac 13+\frac 13=1$.  Suppose, without loss of generality, that $a=2$.  Then $\frac 1b+\frac 1c>\frac 12$.  Either $b$ or $c$ must be $<4$ (if both were $\geqslant 4$, the inequality would fail); so suppose, again without loss of generality, that $b<4$.  Either $b=2$ or $b=3$.  If $b=2$, then $c$ can be anything, and the inequality will always hold.  If $b=3$, then we are left with $\frac 1c>\frac 16$, so that $c<6$.  From this it follows that the only possible unordered triples of $[a,b,c]$ are:
$$[2,2,n]\text{ for integers }n\geqslant 2;~~~~~~~~[2,3,3],~~~~~~~~[2,3,4],~~~~~~~~[2,3,5].$$
In the case $[2,2,n]$, we get a tiling of triangles with two right angles and an angle of $\pi/n$: it is obtained by taking the union of the sphere's equator with $2n$ equally spaced longitudes/meridians.  In the case $[2,3,3]$, we get a tiling of $24$ triangles; $[2,3,4]$, we get a tiling of $48$ triangles with octahedral symmetry, and in the case $[2,3,5]$ we get a tiling of $120$ triangles with icosahedral symmetry.

Now that we have covered all three types of geometry, we may display an example of a triangle pattern in each one.  In each case, the numbers in the brackets refer to the valencies of the vertices divided by $2$.
\begin{center}
\includegraphics[scale=.18]{SphTrianglePattern.png}
\includegraphics[scale=.2]{EucTrianglePattern.png}
\includegraphics[scale=.2]{HypTrianglePattern.png}\\
$[2,3,5]$ (spherical)~~~~~~~~~$[2,3,6]$ (Euclidean)~~~~~~~~~$[2,3,7]$ (hyperbolic)
\end{center}
We also recall how to use these to construct various uniform tilings: we do a simple, specific construction in one of the triangles, pass it over to the others by reflecting, then erase the original triangles (a.k.a, the Wythoff construction).  For instance, in the 30-60-90 triangle tiling in the Euclidean plane, constructing the altitude to the hypotenuse of a triangle then passing it over to the others gives the trihexagonal tiling (made up of hexagons and triangles):
\begin{center}
\includegraphics[scale=.18]{HexTilingForm2.png} % Sure, Space Structures is a relevant book, but I never used it to write this... so would it make sense to cite it?
\end{center}
We now apply this strategy in spherical geometry.
\begin{itemize}
\item Take the $[2,3,5]$ triangle pattern up above.  Draw only the short leg of each triangle, and you get the (spherical) dodecahedron, shown below.  It consists of twelve pentagons with $2\pi/3=120^\circ$ angles.  By Exercise 7(a), its side length is $\cos^{-1}\left(\frac{4\cos(2\pi/5)+1}{3}\right)\approx 0.729728$.

Below are two perspectives of the spherical dodecahedron: the one on the left is directly based off of the former illustration of the triangle pattern, and the one on the right is naturally comprehensible.
\begin{center}
\includegraphics[scale=.2]{SphDodecahedron_Persp2.png}~~~~
\includegraphics[scale=.2]{SphDodecahedron.png} % Fair point, the 2D projections are nearly rotations of one another.  Then again, people recognize a dodecahedron better with pentagons right at the bases, right?
\end{center}
Let us display the same tiling in the stereographic projection model and the Lambert azimuthal equal-area projection model (Exercise 6 of the previous section):
\begin{center}
\includegraphics[scale=.2]{Dodecahedron_StereoProj.png}~~~~
\includegraphics[scale=.22]{Dodecahedron_LAEAP.png}
\end{center}
\item Start with the $[2,3,5]$ pattern again.  But this time, we do the following to the triangle: we start by considering the angle bisector of the $2\pi/3$-angle vertex, without constructing it.  At the point where it meets the long leg of the right triangle, we construct perpendicular line segments to the short leg and the hypotenuse.  [Due to the right angle, the line segment to the short leg is actually contained in the long leg.]  Thus the construction consists of two identical-length line segments.

When we reflect it over the rest of the triangles, we get the \emph{truncated icosahedron}.  It consists of $20$ hexagons and $12$ pentagons, with one pentagon and two hexagons at each vertex.  As a spherical polyhedron, this is seen a lot in real life, as a soccer ball / buckyball.  As a soccer ball, its pentagons are black, and its hexagons are white, as shown on the right side.
\begin{center}
\includegraphics[scale=.2]{Buckyball.png}~~~~
\includegraphics[scale=.2]{Buckyball2.png}
\end{center}
More uniform tilings which use the $[2,3,5]$ pattern are outlined in Exercises 9-10.

\item We can do all of those things with the $[2,3,4]$ tiling as well:
\begin{center}
\includegraphics[scale=.2]{Sph234Pattern.png}
\end{center}
Drawing only the short leg of each triangle gives us the cube, and doing the construction of the previous bullet point gives us the \emph{truncated octahedron}, shown on the right.
\begin{center}
\includegraphics[scale=.2]{SphCube.png}~~~~
\includegraphics[scale=.2]{SphTruncatedOctahedron.png}
\end{center}

\item For the $[2,2,n]$ tiling, many things can be constructed.  For instance, drawing only the longest sides of each triangle results in a ``digonal polyhedron,'' an exotic kind of structure which only exists nondegenerately on the sphere.  Taking the incenter of each triangle, and drawing perpendiculars to the sides, results in a $(2n)$-gonal prism.  The following diagrams assume $n=7$.
\begin{center}
\includegraphics[scale=.2]{SphAxialTiling1.png}~~~~
\includegraphics[scale=.2]{SphAxialTiling2.png}
\end{center}
\end{itemize}
Uniform tilings can actually be classified pretty easily, but in the next section, we will use spherical geometry to introduce a more general concept.  Then in Section 5.4, all of the Catalan and Archimedean solids will be classified.

\subsection*{Exercises 5.2. (Triangles, Polygons and Tilings)} % Introduce triangles, establish all relations between them then recall triangle groups from Section 4.5.
% Show how to mathematically construct certain (semi)-regular tilings.  Don't bother classifying (semi)-regular tilings; that will be done next section.
\begin{enumerate}
\item Let $x\in\mathbb R$.

(a) Show that $\cos(\tan^{-1}x)=\frac 1{\sqrt{1+x^2}}$, and $\sin(\tan^{-1}x)=\frac x{\sqrt{1+x^2}}$.

(b) Moreover, $\cos(2\tan^{-1}x)=\frac{1-x^2}{1+x^2}$ and $\sin(2\tan^{-1}x)=\frac{2x}{1+x^2}$.  [Use the double angle formulas.]

\item (a) Show that:
$$\cos x\cos y=\frac 12[\cos(x-y)+\cos(x+y)],~~~~\sin x\sin y=\frac 12[\cos(x-y)-\cos(x+y)]$$
$$\sin x\cos y=\frac 12[\sin(x+y)+\sin(x-y)],~~~~\cos x\sin y=\frac 12[\sin(x+y)-\sin(x-y)]$$
[Use the sum formulas for sine and cosine.]

(b) Use part (a) to conclude that:
$$\cos a+\cos b=2\cos\frac{a+b}2\cos\frac{a-b}2,~~~~\sin a+\sin b=2\sin\frac{a+b}2\cos\frac{a-b}2,$$
$$\cos a-\cos b=2\sin\frac{b+a}2\sin\frac{b-a}2,~~~~\sin a-\sin b=2\sin\frac{a-b}2\cos\frac{a+b}2.$$

\item Let $\triangle ABC$ be a triangle where $A,B,C$ are the angle measures of the vertices, and $a,b,c$ are the side lengths opposite the respective vertices.

(a) Show that $\cos a\cos B=\cot c\sin a-\sin B\cot C$.  [This is called a \textbf{cotangent four-part formula.}]  [By Proposition 5.8(i), $\cos c=\cos a\cos b+\sin a\sin b\cos C=\cos a(\cos a\cos c+\sin a\sin c\cos B)+\sin a\sin b\cos C$, and by Proposition 5.8(ii) and cross-multiplying, $\sin b\sin C=\sin B\sin c$, and hence $\sin b\cos C=\sin B\sin c\frac{\cos C}{\sin C}=\sin B\sin c\cot C$.  Hence, $\cos c=\cos^2a\cos c+\cos a\sin a\sin c\cos B+\sin a\sin B\sin c\cot C$.  Now subtract $\cos^2a\cos c$ from both sides, and divide out $\sin a\sin c$.]

(b) If $s=\frac{a+b+c}2$ and $S=\frac{A+B+C}2$, then show that
$$\sin\left(\frac 12A\right)=\sqrt{\frac{\sin(s-b)\sin(s-c)}{\sin b\sin c}},~~~~\sin\left(\frac 12a\right)=\sqrt{\frac{-\cos S\cos(S-A)}{\sin B\sin C}}$$
$$\cos\left(\frac 12A\right)=\sqrt{\frac{\sin s\sin(s-a)}{\sin b\sin c}},~~~~\cos\left(\frac 12a\right)=\sqrt{\frac{\cos(S-B)\cos(S-C)}{\sin B\sin C}}$$
$$\tan\left(\frac 12A\right)=\sqrt{\frac{\sin(s-b)\sin(s-c)}{\sin s\sin(s-a)}},~~~~\tan\left(\frac 12a\right)=\sqrt{\frac{-\cos S\cos(S-A)}{\cos(S-B)\cos(S-C)}}$$

[The double-angle formula $\cos(2x)=1-2\sin^2x$ entails $2\sin^2(A/2)=1-\cos A$.  Now use Proposition 5.8(i) to get $\cos A=\frac{\cos a-\cos b\cos c}{\sin b\sin c}$, and use this and the sum formula to get $2\sin^2(A/2)=\frac{\sin b\sin c+\cos b\cos c-\cos a}{\sin b\sin c}=\frac{\sin(b+c)-\cos a}{\sin b\sin c}$.  Now conclude, using the previous exercise, to get the expression for $\sin(A/2)$ given here.  Similar arguments apply for the rest.]

\item If $A_1A_2\dots A_n$ is a spherical polygon, show that the following are equivalent:

~~~~(i) The vertices are coplanar in the ambient $\mathbb R^3$;

~~~~(ii) The polygon has a circumscribed circle (i.e., a circle meeting all the vertices);

~~~~(iii) The tangent planes to the sphere at the vertices all meet at a common point.

[(i) $\iff$ (ii) because planes intersect spheres to circles.  (i) $\iff$ (iii): recall the concept of pole points from the end of Section 4.8.]

Moreover, show that regular polygons, and arbitrary triangles, always satisfy these conditions.  Such a polygon is said to be \textbf{flattenable}.

\item A \textbf{lune} consists of antipodes $p,q$ and two line segments between them that do not lie in a common line.
\begin{center}
\includegraphics[scale=.2]{Lune.png}
\end{center}
(a) Explain why $p,q$ have to be antipodes.

(b) Show that the lune consists of two angles of equal measure, and that this angle can have any measure between $0$ and $\pi$.

(c) Explain why it is called a digon.

\item We will show that if $0<\alpha,\beta,\gamma<\pi$ are real numbers, then a spherical triangle with angles $\alpha,\beta,\gamma$ exists if and only if:
$$\alpha+\beta+\gamma>\pi=180^\circ,$$
$$\alpha+\beta-\gamma<\pi,~~~~\beta+\gamma-\alpha<\pi,~~~~\gamma+\alpha-\beta<\pi.$$

(a) Let $c$ be the length opposite $\gamma$.  With $\alpha,\beta,c$ given, the triangle is easy to construct.  Suppose $\alpha,\beta$ are fixed, but $c$ can vary, making the opposite angle vary.  As $c\to 0$, $\cos\gamma=-\cos(\alpha+\beta)$, and hence $\gamma\to\pi-\alpha-\beta$.  As $c\to\pi$, $\cos\gamma\to-\cos(\alpha-\beta)$.  [Use Proposition 5.8(iii).]

(b) Conclude from part (a).

(c) Show that spherical triangles exist for $[2,2,n],n\geqslant 2$, $[2,3,3]$, $[2,3,4]$ and $[2,3,5]$.  Show that for any $\frac{\pi}3<\theta<\pi$, an equilateral triangle with angle $\theta$ exists.

(d) Show that the spherical triangle does \emph{not} exist if $\alpha+\beta-\gamma\geqslant\pi$.  [If $a,b,c$ are the side lengths opposite $\alpha,\beta,\gamma$ respectively, then by Proposition 5.8(iii), $\cos\alpha=-\cos\beta\cos\gamma+\sin\beta\sin\gamma\cos a$.  Use this to conclude that $\cos a\leqslant -1$ and derive a contradiction.]  By symmetry considerations, we have necessity of all the conditions at the beginning of the problem.

\item Suppose a regular $n$-gon in the hyperbolic plane has side length $s$ and angle measure $\theta$.

(a) Show that $\cos s=\frac{2\cos(2\pi/n)+1+\cos\theta}{1-\cos\theta}$.  [Use Proposition 5.8(iii) on suitable triangles.]

(b) Determine the radii of the circumcircle and incircle of the polygon.

\item The \textbf{area} of a spherical triangle is to be the sum of its angles minus $180^\circ=\pi$.  Show that this notion of area satisfies Lemma 2.29, so that one can define the area of a general polygon.  Moreover, show that the area of a spherical $n$-gon is the sum of its angles minus $\pi(n-2)$. % I'll remove the word "defined", kek, but remember, that integral requires the differential kind of metric, which is only covered in Section 6.10!

\item For each of the following constructions in the $90^\circ-60^\circ-36^\circ$ triangle (used to build the $[2,3,5]$ triangle pattern), calculate and explain the resulting spherical tiling.

(a) Construct the altitude to the hypotenuse; i.e., the perpendicular segment from the right angle to the opposite side.

(b) Consider the angle bisector of the right angle, but don't construct it.  Let $p$ be the point where it meets the hypotenuse, and construct perpendiculars from $p$ to each of the legs of the right triangle.

\item Explain how to construct each of the following tilings using the $[2,3,5]$ triangle pattern.  (All polygons are regular.)
\begin{center}
\includegraphics[scale=.2]{SphTruncatedDodecahedron.png}~~~~~~~~~~~~
\includegraphics[scale=.2]{SphGreatRhombicosidodecahedron.png}~~~~\\
Truncated Dodecahedron~~~~~~Great Rhombicosidodecahedron
\end{center}
\item Starting with the $[2,2,n]$ triangle pattern, explain how to use the Wythoff construction to obtain an $n$-gonal prism.  (This should work whether $n$ is even or odd.)  [Start by taking an angle bisector of a right angle.]
\end{enumerate}

\subsection*{5.3. Spherical Polyhedra}
\addcontentsline{toc}{section}{5.3. Spherical Polyhedra}
Spherical tilings have the unique property that they have only finitely many faces: for example the dodecahedron shown in the previous section has only 12 faces total.  Euclidean and hyperbolic tilings have infinitely many faces.  The reason for this discrepancy is that the sphere is \emph{compact}, and it has finite surface area, whereas the other types of geometry do not.  Thus, uniform spherical tilings have become their own field of study.  They also relate to polyhedra, which were first seen in places like Egypt, and then started being described in writing in Greek civilizations.

In this section, we will study the notion of a \emph{spherical polyhedron}, which is a special kind of spherical tiling.  Then we will focus on Euclidean polyhedra, which are topologically like spherical polyhedra, but are embedded in $\mathbb R^3$ so that the faces are contained in Euclidean planes.  We recall that a spherical polygon is said to be \textbf{flattenable} if its vertices are coplanar in the ambient $\mathbb R^3$ [Exercise 4 of the previous section.]\\

\noindent\textbf{Definition.} \emph{A \textbf{spherical polyhedron} is a spherical tiling for which every face is flattenable, has at least $3$ sides, and no two faces have their vertices all in a common plane in $\mathbb R^3$.}\\

\noindent\textbf{Examples.}

(1) We recall the dodecahedron and cube (as spherical tilings) from the previous section.  More generally, all five Platonic solids are spherical polyhedra, since they are made up of regular polygons.
\begin{center}
\includegraphics[scale=.13]{SphTetrahedron.png}
\includegraphics[scale=.13]{SphOctahedron.png}
\includegraphics[scale=.13]{SphIcosahedron.png}
\includegraphics[scale=.13]{SphCube.png}
\includegraphics[scale=.13]{SphDodecahedron.png}\\
Tetrahedron~~~~~Octahedron~~~~~Icosahedron~~~~~~Cube~~~~~~Dodecahedron
\end{center}
The truncated icosahedron and truncated octahedron are also spherical polyhedra:
\begin{center}
\includegraphics[scale=.2]{Buckyball.png}~~~~
\includegraphics[scale=.2]{SphTruncatedOctahedron.png}
\end{center}

(2) Here is an example of a spherical polyhedron which does not use regular polygons.  Let $a>b>c>0$ be real numbers such that $a^2+b^2+c^2=1$.  Then there are eight vectors of the form
$$(\pm a,\pm b,\pm c)$$
and they are all unit vectors.  Connect two of them by a spherical segment if and only if they differ in exactly one component.  Doing so will get you the \emph{rectangular prism} shown below.  Each face is a rectangle, and is flattenable, as the reader can see.  But the faces are not squares, even though the polyhedron is graph-theoretically the same as the cube.
\begin{center}
\includegraphics[scale=.2]{SphRectangularPrism.png}
\end{center}

(3) The rhombic dodecahedron, obtained by connecting the vertices of the cube and those of the octahedron together as shown below, is \emph{not} a spherical polyhedron.  Its faces are not flattenable, as they cannot be inscribed in circles.
\begin{center}
\includegraphics[scale=.2]{SphRhombicDodecahedron.png}
\end{center}

(4) The following tiling, obtained by taking a cube and drawing one diagonal of each face, is not a spherical polyhedron.  Though all faces are flattenable (because they are triangles), the triangles that make up each square have four coplanar vertices altogether.  This causes the Euclidean version to have degenerate edges (whose dihedral angles are $180^\circ$).
\begin{center}
\includegraphics[scale=.2]{SphCrazySample.png}
\end{center}

\noindent\textbf{Proposition 5.11.} (i) \emph{A tiling made up of regular polygons is a spherical polyhedron.}

(ii) \emph{Given a spherical polyhedron, the vertices can be connected to form a Euclidean polyhedron, whose faces correspond bijectively to the tiling's faces.}\\

\noindent It is worth remarking that (ii) is false for tilings that are not spherical polyhedra.  If a face of the tiling is not flattenable, it will correspond to multiple faces of the Euclidean polyhedron, with angles strictly $<180^\circ$.
\begin{proof}
(i) By Exercise 4 of the previous section, every face is flattenable, and obviously each face has at least $3$ sides [lunes are not considered regular polygons].  If two faces had their vertices all in a common plane in $\mathbb R^3$, there would be a circle passing through all the vertices; hence the faces have the same circumscribed circle, hence the same center, contradicting that their interiors are disjoint.

(ii) For each pair of vertices connected by an edge, connect them by a Euclidean line segment in $\mathbb R^3$.  Each face of the tiling is flattenable, hence the Euclidean line segments corresponding to its edges lie in a Euclidean plane, hence bound a flat polygon, which can be taken to be the corresponding face.  This face is nondegenerate, since it has at least three sides and no three points on $S^2(\mathbb R)$ are collinear.  Finally, since no two faces of the spherical tiling have their vertices all in a common plane, no two faces of the Euclidean polyhedron lie in a common plane, which prevents any faces from merging into a single face.
\end{proof}

\noindent The polyhedron in part (ii) is called the \textbf{inscribed Euclidean polyhedron}.  It will be introduced in a different light in the next section.  That section will also show how to circumscribe a Euclidean polyhedron, as well as to classify the Archimedean and Catalan solids.

\subsection*{Exercises 5.3. (Spherical Polyhedra)} % In the 2018 fall, I realized how to convert between Archimedean solids and Catalan solids.  Specifically, there is a genuine
% mathematical conversion between a spherical polyhedron, its flat-face counterpart (by connecting the vertices) and its flat-face dual (by taking tangent planes to the vertices).
% This is to be covered in Section 5.4.  Right here, introduce & prove properties for spherical polyhedra.
\begin{enumerate}
\item Suppose $P$ is a convex Euclidean polyhedron.  The \textbf{vertex angle defect} of a vertex of $P$ is equal to $360^\circ=2\pi$ minus the sum of the angles.  Note that this can be obtained by cutting around the vertex, cutting one of the edges, flattening the result on a table, and seeing what angle the cut edge becomes.  [Intuitively, the vertex angle defect measures how far the vertex is from being flat.]

The aim of this exercise is to show that the vertex angle defects of $P$ add to $720^\circ=4\pi$.

(a) Form a spherical tiling as follows: For each face of $P$, draw the outward unit normal vector.  This is a point on the unit sphere; construct it as a vertex of the tiling.  Then, connect two vertices by an edge if and only if the corresponding faces meet at an edge. [This need not be a spherical polyhedron.]

Show that faces of this tiling correspond bijectively to vertices of $P$, edges correspond to edges, and vertices correspond to faces.  [In this case we say the spherical tiling is \textbf{dual} to $P$.  The concept of duality is a bit combinatorial, however, so it will not be elaborated.]

(b) Given a face-vertex pair where the vertex is on the face, either on $P$ or the spherical tiling, one can measure the angle inside the vertex at the face.  A face-vertex pair on the tiling corresponds to one on the polyhedron by part (a).  Show that for corresponding pairs, the angles are supplementary.  [It helps to assume, by rotating everything in $\mathbb R^3$, that the vertex on the tiling which possesses the angle is at the north pole.]

(c) Recall Exercise 8 of the previous section: the area of an $n$-gon is the sum of its angles minus $\pi(n-2)$.  Show that the sum of the areas of the faces of the tiling is $4\pi$.  [You do not need to know why the formulas actually give the area: it suffices to understand the invariance under the cutting and pasting of polygons.  Then you can simply change the arbitrary tiling to a more convenient one.] % Though my ambition is to show that the formulas actually give the area in Chapter 6.  I found a PDF with the Gauss-Bonnet theorem, doesn't seem like the proof will be as hard as it may seem.

(d) Show that the area of a single face of the tiling equals the vertex angle defect of the corresponding vertex of $P$.  Conclude.

\item When studying uniform Euclidean polyhedra (which will be defined in the next section), there are two important infinite families of them.  One are \textbf{prisms}, obtained by taking two $n$-gons, and joining them by a belt of $n$ squares, each meeting its top edge on one $n$-gon and its bottom edge on the other, while their left and right edges meet each other.  The other are \textbf{antiprisms}, obtained by taking two $n$-gons, and joining them by a belt of $2n$ equilateral triangles; $n$ triangles meet only the top $n$-gon by the edges, the other triangles meet only the bottom $n$-gon by the edges, and types of triangles alternate around the belt. % Added a parenthesized thing.  (This exercise doesn't actually deal with the general uniform polyhedron concept.)

The following are illustrations for the case $n=6$.
\begin{center}
\includegraphics[scale=.2]{HexPrism.png}~~~~
\includegraphics[scale=.2]{HexAntiprism.png}\\
~~~Prism~~~~~~~~~~~~~~~~~~~~~Antiprism
\end{center}
(a) If $n\geqslant 3$, show that, up to rigid motions and rescaling, there is a unique $n$-gonal prism (resp., antiprism) made out of regular polygons.

(b) The cube is a square prism; the octahedron is a triangular antiprism.

(c) For sufficiently large $n$, the symmetry group of the $n$-gonal prism is the group (vi) in Theorem 2.54, and the symmetry group of the $n$-gonal antiprism is the group (vii) in Theorem 2.54.  [Note that part (b) implies that this fails for a few small values of $n$.]  This is why they are called ``prismatic'' and ``antiprismatic'' symmetry, as mentioned after the proof of Theorem 2.54.

(d) What kinds of prisms and antiprisms \---- as spherical tilings \---- are spherical polyhedra?  [The question refers to arbitrary tilings which are graph-theoretically isomorphic to the prisms and antiprisms.]
\end{enumerate}

\subsection*{5.4. Inscribed and Circumscribed Euclidean Polyhedra}
\addcontentsline{toc}{section}{5.4. Inscribed and Circumscribed Euclidean Polyhedra}
In the previous section, we have shown (Proposition 5.11(b)) that a spherical polyhedron has an inscribed Euclidean polyhedron obtained by connecting the vertices.  Here, we shall show how to circumscribe a Euclidean polyhedron, and we shall cover the concept of inscription-circumscription duality.

Suppose we are given a spherical polyhedron on $S^2(\mathbb R)$.  To each vertex, construct the tangent plane at $S^2(\mathbb R)$ to the vertex.  Whenever two vertices meet by an edge, the corresponding tangent planes intersect in a line, which we declare to be an edge of the circumscribed polyhedron.  Finally, for each face on the sphere, the tangent planes to the vertices are concurrent by Exercise 4 of Section 5.2.  Their common intersection point is declared to be a vertex of the circumscribed polyhedron.  Finally, we restrict each face to consist only of the portion of the plane inside its edges and vertices.

The polyhedron described in the previous paragraph is called the \textbf{circumscribed Euclidean polyhedron}.  By the same token that the inscribed polyhedron's edges are convex, the circumscribed ones have edges of positive length.  Note how, unlike the inscribed polyhedron, the circumscribed polyhedron is \emph{not} combinatorially isomorphic to the spherical tiling: it is isomorphic to its dual, as its faces correspond to the tiling's vertices and its vertices correspond to the tiling's faces.

For example, given the dodecahedron (as a spherical polyhedron) from Section 5.2, its circumscribed Euclidean polyhedron is a regular icosahedron.

The reader is encouraged to verify that for each edge of the spherical tiling, there are three values, each of which determines the other two: (i) the length of the corresponding edge of the inscribed Euclidean polyhedron; (ii) the spherical length of the edge in the tiling; (iii) the dihedral angle of the corresponding edge of the circumscribed polyhedron.  See Exercise 1.\\

\noindent At this point it is natural to define the concept of a uniform polyhedron.  A \textbf{uniform polyhedron} is a convex Euclidean polyhedron satisfying the following conditions:
\begin{itemize}
\item Every face is a regular polygon.  [Note that since edges are shared, each of these polygons must have the same side length.]

\item The (global) isometry group of the polyhedron acts transitively on the vertices.  In other words, if $v_1$ and $v_2$ are any two vertices, there is a global isometry of the polyhedron sending $v_1\mapsto v_2$.
\end{itemize}
For example, the Platonic solids are uniform polyhedra; in fact, they are precisely the uniform polyhedra with only one type of regular polygon for the faces.  Prisms and antiprisms (Exercise 2 of the previous section) are also uniform polyhedra.  In this section we shall show that there are exactly $13$ other uniform polyhedra in addition to these;\footnote{Technically, two of them have no orientation-reversing symmetry.  If the reflections of these polyhedra are counted separately, there are $15$.} they are called the \textbf{Archimedean solids} and were first enumerated by Archimedes. % Ah, I see now. https://en.wikipedia.org/wiki/Archimedean_solid (first sentence)

If $G$ is the isometry group of a uniform polyhedron, then by Proposition 2.49, there exists a point $p\in\mathbb R^3$ fixed by $G$ entirely.  Moreover, since $G$ acts transitively on the vertices, every vertex has the same distance from $p$: indeed, if $v_1$ and $v_2$ are vertices, then there exists $T\in G$ such that $T(v_1)=v_2$, but also $T(p)=p$, and so $\rho(p,v_1)=\rho(T(p),T(v_1))=\rho(p,v_2)$.  If $r$ is this common distance, then the sphere centered at $p$ of radius $r$ contains all the vertices of the polyhedron.  Thus the polyhedron is inscribed in the sphere, and can even be seen as the inscribed polyhedron given by a spherical polyhedron (why?).\\

\noindent\emph{Remark}. It is tempting to think it suffices that every face be regular, and all vertices be equal in angles and dihedral angles around the loop.  That would imply that between any two vertices, there is a \emph{local} isometry sending a neighborhood of one vertex to a neighborhood of the other.  Under these weaker conditions, however, Branko Gr\"unbaum (2009) discovered an extra polyhedron: it is called the \emph{pseudo-rhombicuboctahedron}, obtained by taking a rhombicuboctahedron and rotating its bottom cap $45^\circ$, as shown below.
\begin{center}
\includegraphics[scale=.2]{Pseudorhombicuboctahedron.png}
\end{center}
Gr\"unbaum pointed out the flaw that many authors used, where they used the definition of an Archimedean solid where the vertices only needed to be locally isometric, but they omitted the pseudo-rhombicuboctahedron.\\

\noindent\textbf{ARCHIMEDEAN SOLIDS}\\

\noindent We are now in a position where we can classify the Archimedean solids; i.e., the uniform polyhedra which are not Platonic solids, prisms or antiprisms.  To do this, we let $P$ be an Archimedean solid, inscribed in the unit sphere, and we let $G$ be its isometry group.  Then $G$ is a finite subgroup of $O(3)$, hence is one of the groups of Theorem 2.54, and we may use casework.  Since $G$ acts transitively on the vertices of $P$, the set of vertices must be exactly one of the orbits of the action of $G$.  This fact makes the classification easier than it may seem at first glance.

First, we claim that $G$ cannot be among the families of axial groups (i)-(vii) of Theorem 2.54.  Indeed, if $G$ were any of them, then we may assume $G$ fixes the standard equator $z=0$.  Then if any vertex is on the equator, all of them are, which is obviously not possible.  If a vertex has a $z$-coordinate of $r>0$, then by the transitivity, all vertices would have $z$-coordinates of $\pm r$.  Moreover, either there are at most two vertices in the plane $z=r$ (from which an easy contradiction is derived), or the vertices in that plane form an $n$-gon for some $n$ \---- this must be regular by uniformity of $P$.  Consequently, the plane $z=-r$ would also have a regular $n$-gon, and $P$, being the convex hull of the set of vertices, is readily seen to be either an $n$-gonal prism or an $n$-gonal antiprism  (depending on whether the polygons are aligned in the $z$-direction).  This is against the rules, hence $G$ is not an axial group.

We thus restrict the casework to the seven groups (viii)-(xiv).

If $G=\operatorname{Isom}(\mathbf{Tet})$ (which is (ix)), then we may get a spherical tiling by connecting each pair of vertices of $\mathbf{Tet}$ (via spherical line segments), and then in each triangle, drawing the three altitudes to the opposite edges (these are also the medians and angle bisectors, since the triangles are equilateral).  The result is as shown (and this turns out to be the $[2,3,3]$ triangle pattern):
\begin{center}
\includegraphics[scale=.2]{TetrTiling.png} % T = Stri_tiling(3,3,2); sphereDesign([t.vlist() for t in T],STD_ICOS_PERSP_POINT)
\end{center}
There are $24$ triangles in this tiling and $|\operatorname{Isom}(\mathbf{Tet})|=24$; moreover, the ele\-ments of $G$ send each triangle to all the distinct triangles.  Note also that $G$ does not have $90^\circ$ or $60^\circ$ rotations; it \emph{reflects} each triangle to each other one adjacent by an edge.  Once we know where a vertex of $P$ is located in one of these triangles, the rest of the vertices must be essentially located in that same position in the other triangles (reflecting over edges).  Then we will know what $P$ is, being the convex hull of the set of vertices. % TeX put the hyphen and line break in automatically, but okay.

Thus, we restrict our attention to one triangle, noting that the triangles have angles $90^\circ,60^\circ,60^\circ$.  Where can a vertex of the polyhedron be located?
\begin{center}
\includegraphics[scale=.5]{Tri606090.png}
\end{center}
If the vertex of $P$ is at $B$ or $C$, then it is fixed by six elements of $G$, and the upshot is that $P$ has only four vertices and is a tetrahedron.  This contradicts our assumption that $P$ is not a Platonic solid.  If the vertex is at $A$, then $P$ is an octahedron, again a Platonic solid.  So the vertex can't be at any of the vertices of this triangle.

If the vertex is on the edge $\overline{BC}$, then $P$ has a rectangular face at each $4$-fold vertex, hence a square face by uniformity, so the vertex must be at the midpoint of $\overline{BC}$.  But in this case the reflection of the triangle over the angle bisector from $A$ is an extra isometry of $P$ which is outside $G$; contradiction.  If the vertex is on edge $\overline{AC}$ (or $\overline{AB}$), then each face of $P$ is either a triangle or a hexagon, (as can be seen visually), and the diameter of the sphere which is perpendicular to the face goes through a $6$-fold vertex of the tiling.  In this case the uniformity of $P$ determines where the vertex is located, and we have the \emph{truncated tetrahedron}, consisting of four triangles and four hexagons, with one triangle and two hexagons to each vertex.
\begin{center}
\includegraphics[scale=.5]{VertexSample.png}~~~~
\includegraphics[scale=.2]{ArchimedeanSolids/TruncatedTetrahedron.png}
\end{center}
Finally, if the vertex is in the interior of $\triangle ABC$, then the faces of $P$ are hexagons and squares, and furthermore, the vertex must be the incenter of $\triangle ABC$.  But this again implies that $P$ has an extra isometry outside $G$, obtained by reflecting the triangle over $A$'s angle bisector, so this case is impossible.

Thus, if $G=\operatorname{Isom}(\mathbf{Tet})$, then $P$ must be the truncated tetrahedron.

The above reasoning suggests a general principle when classifying all possible locations of one vertex.  Whenever two triangles in the spherical tiling meet along an edge, their corresponding vertices are connected by an edge in $P$ (unless they coincide, which is true if and only if the vertex is on the tiling's edge).  When the spherical edge between the vertices is drawn, it must be perpendicular to the triangle tiling's edge (by symmetry considerations).  Since $P$ is uniform, it must also have the same length no matter which side of the tiling's triangle it goes towards.  Thus we have established
\begin{center}
[*] \textbf{Suppose we are given a triangle pattern with $|G|$ triangles, such that the elements of $G$ send each triangle to all the distinct triangles. % "G acts transitively on the triangles" is a weaker assertion.  I want to state that G acts transitively on the triangles *and* the stabilizer of each triangle is trivial.
Suppose also that triangles that are adjacent by an edge reflect to one another via $G$.  Then wherever $P$'s vertex is located in one triangle, it must have the same perpendicular distance to every side of the triangle it does not touch (because this distance is half the edge length of $P$'s spherical version).}
\end{center}
This principle easily boils down the possible vertex positions; in fact, it means the vertex position is determined by the specification of which edges it is on.

With this in mind, we now consider the case $G=\operatorname{Isom}^+(\mathbf{Tet})$, which is (viii) in Theorem 2.54.  We cannot exactly apply our principle, because $G$ has no reflections ($G\subset SO(3)$), but we can still invoke the idea.  To do this, we consider this alternate tiling, obtained by taking the tetrahedron, and drawing on each face three line segments from the vertices to the center of the triangle.
\begin{center}
\includegraphics[scale=.2]{TetrTiling2.png}
\end{center}
Given the location of one vertex of $P$ in one of the triangles, the rest are obtained by rotating the point $120^\circ$ around each 3-fold and 6-fold vertex, and $180^\circ$ around the midpoint of each long edge (of the original tetrahedron), and then iterating.  We can use this to answer the question of whether any Archimedean solids are possible.

The answer is, in fact, \emph{no}: if the vertex of $P$ is on the altitude to the base, \emph{or} on any edge, the reader can see why orientation-reversing elements of $\operatorname{Isom}(\mathbf{Tet})$ are isometries, contradicting that $G=\operatorname{Isom}^+(\mathbf{Tet})$.  Thus the vertex must be in the interior of the tiling's triangle and cannot be on the altitude to the base.  The reader is then encouraged to use the above illustration as a guide, and observe that $P$ is made up of triangles, with five to each vertex: hence (in order to be uniform), $P$ would be the regular icosahedron, which obviously has way more isometries than $G$.  Thus, $G$ cannot be $\operatorname{Isom}^+(\mathbf{Tet})$.

We leave it to the reader to follow a similar argument (with an appropriate spherical tiling) to show that $G$ cannot be the pyritohedral symmetry group (xiv).  Thus we have covered the cases (viii), (ix), (xiv) of Theorem 2.54.

We now consider the case $G=\operatorname{Isom}(\mathbf{Oct})$ [which is (xi)].  We claim that there are actually five Archimedean solids which fall into this case, and we may easily apply the principle [*] to the $[2,3,4]$ triangle pattern, because the hypotheses are clearly held:
\begin{center}
\includegraphics[scale=.2]{Sph234Pattern.png}
\end{center}
Once we know the location of one vertex, $P$ can be obtained by reflecting it over all the triangles' edges then taking the convex hull of the result.

The angles of each triangle are $90^\circ,60^\circ,45^\circ$; we draw one of these triangles to investigate.
\begin{center}
\includegraphics[scale=.5]{Tri456090.png}
\end{center}
If the vertex of $P$ is at $C$, it is fixed by eight elements of $G$ (forming a subgroup $\cong D_4$); moreover, there are six vertices and $P$ is an octahedron; against our assumptions.  If the vertex of $P$ is at $B$, we get a similar contradiction; this time $P$ is a cube.

However, the vertex of $P$ \emph{can} be at $A$ without a problem.  In this case, the vertices of $P$ are precisely at the $4$-fold vertices of the triangle pattern, and $P$ consists of six squares and eight equilateral triangles, with the pattern square-triangle-square-triangle around each vertex.  This solid is called the \emph{cuboctahedron}, pictured below on the left.

What if $P$'s vertex is on $\overline{AC}$ without being at either endpoint?  Then by our principle, it must have the same perpendicular distance to $\overline{AB}$ and $\overline{BC}$ \---- (note that since $\angle A$ is a right angle, the perpendicular distance to $\overline{AB}$ is just the distance to $A$).  This determines where the vertex is located,\footnote{Note by the way, that the vertex of $P$ is located where $\angle B$'s angle bisector meets $\overline{AC}$.} and by examining the triangle pattern, the vertices of $P$ form a square around each eight-fold vertex and a hexagon around each six-fold vertex.  In fact, each vertex of $P$ has around it two hexagons and a square, yielding the \emph{truncated octahedron}, the second solid from the left in the picture below.

By similar reasoning, if $P$'s vertex is on $\overline{AB}$ then the polyhedron is made up of two octagons and a triangle at each vertex; we have the \emph{truncated cube}, the third solid from the left in the picture below.

If $P$'s vertex is on $\overline{BC}$, then $P$ must have the same perpendicular to $\overline{AC}$ and $\overline{AB}$, and these perpendiculars make up the edges of $P$.  In this case, we have the \emph{rhombicuboctahedron}, which has three squares and a triangle to each vertex, and is the fourth solid from the left (or second from the right).  [Note that it has 18 squares total, but $G$ does not act transitively on the squares: one orbit of the action consists of the six squares meeting triangles at all four corners, and another orbit consists of the other twelve squares.]

The final case is where $P$ is in the interior of $\triangle ABC$.  In this case, its perpendiculars to all three sides must have equal length by our principle [*], so $P$ must actually be the incenter of the triangle.  Two vertices of $P$ connect by an edge if and only if the corresponding faces of the triangle pattern meet by an edge, and from this it is readily visualized that each vertex of $P$ has three distinct polygons occurring once: a square, a hexagon and an octagon.  This gives us the \emph{great rhombicuboctahedron}, or \emph{truncated cuboctahedron}, the fifth (or rightmost) solid in the picture.
\begin{center}
\includegraphics[scale=.15]{ArchimedeanSolids/Cuboctahedron.png}~
\includegraphics[scale=.15]{ArchimedeanSolids/TruncatedOctahedron.png}~
\includegraphics[scale=.15]{ArchimedeanSolids/TruncatedCube.png}~
\includegraphics[scale=.15]{ArchimedeanSolids/Rhombicuboctahedron.png}~
\includegraphics[scale=.15]{ArchimedeanSolids/GreatRhombicuboctahedron.png}
\end{center}
These are the five Archimedean solids with isometry group $\operatorname{Isom}(\mathbf{Oct})$.  The following diagram summarizes which location of $P$'s vertex in the triangle entails which solid:
\begin{center}
\includegraphics[scale=.5]{Tri456090_expl.png}
\end{center}
\emph{Remark}. Some authors call the rhombicuboctahedron the \emph{small rhombicuboctahedron} to distinguish it from the great one.\\

\noindent Now we tackle the case $G=\operatorname{Isom}^+(\mathbf{Oct})$.  We claim that there is one Archimedean solid (up to isometries including orientation-reversing ones) which falls into this case.

For this, we consider the $[2,3,3]$ triangle pattern, this time drawing a spherical cube and drawing the two diagonals of each face.  As before, each triangle is sent by the elements of $G$ to all the distinct triangles, so we may consider a vertex as it appears in just one of the triangles, and transfer it to the others using only rotations ($G$ is chiral).
\begin{center}
\includegraphics[scale=.2]{TetrakisTiling.png}
\end{center}
As in the case of $\operatorname{Isom}^+(\mathbf{Tet})$, the vertex of $P$ cannot be on either the triangle's boundary or the altitude to the hypotenuse, because that would imply that $G$ has orientation-reversing isometries.  When the vertex is placed randomly, and then rotated according to $G$'s elements, the front four triangles of the spherical tiling (the square with the X in it facing us) consist of four of $P$'s vertices forming a tilted square.

Using the above diagram as a guide, one can see that for each six-fold vertex (each vertex of the original cube), $P$ has a triangle.  Also, on each edge of the original cube (the hypotenuses of the triangles in the pattern), $P$ has two triangles, with $180^\circ$-rotational symmetry around the edge's midpoint.  When an adjacent square of $P$ is drawn, one of the triangles meets it by an edge and the other only meets it by a vertex.  With that, each vertex of $P$ consists of a square and four triangles, determining the solid to be the \emph{snub cube}:
\begin{center}
\includegraphics[scale=.2]{SnubCubeLeft.png}~~~~\includegraphics[scale=.2]{ArchimedeanSolids/SnubCube.png}\\
Left snub cube~~~~~~~~~~Right snub cube
\end{center}
One checks that $P$ has no orientation-reversing symmetry, yet it has all orientation-\emph{preserving} symmetries of the octahedron: its isometry group is exactly $\operatorname{Isom}^+(\mathbf{Oct})$.  Due to the lack of orientation-reversing symmetry, when we reflect $P$ (or apply any orientation-reversing isometry of $\mathbb R^3$), we get another form of the snub cube which cannot possibly be rotated rigidly to get the original one.

These forms are called the \emph{two chiral forms} of the snub cube (see them side-by-side above).  In practice, they are not counted separately in the family of Archimedean solids; thus, when we add this to the five solids with full octahedral symmetry, we have, in our hands, six Archimedean solids with octahedral symmetry.

At this point, only the cases $G=\operatorname{Isom}(\mathbf{Icos}),\operatorname{Isom}^+(\mathbf{Icos})$ remain [(xiii) and (xii) of Theorem 2.54 respectively].  But this is relatively easy for the reader now, because it merely copies the classification for octahedral symmetry.  For $G=\operatorname{Isom}(\mathbf{Icos})$, by taking the $[2,3,5]$ triangle pattern (where the angles of each triangle are $90^\circ,60^\circ,36^\circ$), and using casework on where $P$'s vertex is located inside the triangle, one gets five new Archimedean solids, exactly as in the octahedral case.

And in the case $G=\operatorname{Isom}^+(\mathbf{Icos})$, one can take the spherical dodecahedron and get a triangular tiling by connecting the vertices of each pentagon to the center of the pentagon.  One last Archimedean solid thus arises, resulting in six with icosahedral symmetry.  Adding these to the six with octahedral symmetry, and the one (truncated tetrahedron) with tetrahedral symmetry, we end up with a total of 13.

We will not give all the details, but will give a chart all of the Archimedean solids.  [The vertex configuration $(n_1.n_2.\dots.n_k)$ indicates that, around a vertex, the faces are (in order) an $n_1$-gon, an $n_2$-gon and so on up to an $n_k$-gon.]
\begin{center}
\begin{tabular}{ccccccc}
\parbox{2.5cm}{Name (vertex\\configuration)}&Picture&\#Faces&\# Edges&\# Vertices&\parbox{2cm}{Symmetry\\Group}\\\hline\hline

\parbox{2.5cm}{Truncated\\Tetrahedron\\(3.6.6)}&
\includegraphics[scale=.1]{ArchimedeanSolids/TruncatedTetrahedron.png}&
8 \parbox{2cm}{(4 triangles,\\4 hexagons)}&
18&
12&
$\operatorname{Isom}(\mathbf{Tet})$\\\hline

\parbox{2.5cm}{Cuboctahedron\\(3.4.3.4)}&
\includegraphics[scale=.1]{ArchimedeanSolids/Cuboctahedron.png}&
14 \parbox{2cm}{(8 triangles,\\6 squares)}&
24&
12&
$\operatorname{Isom}(\mathbf{Oct})$\\\hline

\parbox{2.5cm}{Truncated\\Cube\\(3.8.8)}&
\includegraphics[scale=.1]{ArchimedeanSolids/TruncatedCube.png}&
14 \parbox{2cm}{(8 triangles,\\6 octagons)}&
36&
24&
$\operatorname{Isom}(\mathbf{Oct})$\\\hline

\parbox{2.5cm}{Truncated\\Octahedron\\(4.6.6)}&
\includegraphics[scale=.1]{ArchimedeanSolids/TruncatedOctahedron.png}&
14 \parbox{2cm}{(6 squares,\\8 hexagons)}&
36&
24&
$\operatorname{Isom}(\mathbf{Oct})$\\\hline

\parbox{2.5cm}{Rhombicuboct-\\ahedron\\(3.4.4.4)}&
\includegraphics[scale=.1]{ArchimedeanSolids/Rhombicuboctahedron.png}&
26 \parbox{2cm}{(8 triangles,\\18 squares)}&
48&
24&
$\operatorname{Isom}(\mathbf{Oct})$\\\hline

\parbox{2.5cm}{Great\\Rhombicuboct-\\ahedron\\(4.6.8)}&
\includegraphics[scale=.1]{ArchimedeanSolids/GreatRhombicuboctahedron.png}&
26 \parbox{2cm}{(12 squares,\\8 hexagons,\\6 octagons)}&
72&
48&
$\operatorname{Isom}(\mathbf{Oct})$\\\hline

\parbox{2.5cm}{Snub Cube\\(3.3.3.3.4)}&
\includegraphics[scale=.1]{ArchimedeanSolids/SnubCube.png}&
38 \parbox{2cm}{(32 triangles,\\6 squares)}&
60&
24&
$\operatorname{Isom}^+(\mathbf{Oct})$\\\hline

\parbox{2.5cm}{Icosidodecahedron\\(3.5.3.5)}&
\includegraphics[scale=.1]{ArchimedeanSolids/Icosidodecahedron.png}&
32 \parbox{2.2cm}{(20 triangles,\\12 pentagons)}&
60&
30&
$\operatorname{Isom}(\mathbf{Icos})$\\\hline

\parbox{2.5cm}{Truncated\\Dodecahedron\\(3.10.10)}&
\includegraphics[scale=.1]{ArchimedeanSolids/TruncatedDodecahedron.png}&
32 \parbox{2cm}{(20 triangles,\\12 decagons)}&
90&
60&
$\operatorname{Isom}(\mathbf{Icos})$\\\hline

\parbox{2.5cm}{Truncated\\Icosahedron\\(5.6.6)}&
\includegraphics[scale=.1]{ArchimedeanSolids/TruncatedIcosahedron.png}&
32 \parbox{2.3cm}{(12 pentagons,\\20 hexagons)}&
90&
60&
$\operatorname{Isom}(\mathbf{Icos})$\\\hline
\end{tabular}

\begin{tabular}{ccccccc}
\parbox{2.5cm}{Name (vertex\\configuration)}&Picture&\#Faces&\# Edges&\# Vertices&\parbox{2cm}{Symmetry\\Group}\\\hline\hline

\parbox{2.5cm}{Rhombicosi-\\dodecahedron\\(3.4.5.4)}&
\includegraphics[scale=.1]{ArchimedeanSolids/Rhombicosidodecahedron.png}&
62 \parbox{2.2cm}{(20 triangles,\\30 squares,\\12 pentagons)}&
120&
60&
$\operatorname{Isom}(\mathbf{Icos})$\\\hline

\parbox{2.5cm}{Great\\Rhombicosi-\\dodecahedron\\(4.6.10)}&
\includegraphics[scale=.1]{ArchimedeanSolids/GreatRhombicosidodecahedron.png}&
62 \parbox{2cm}{(30 squares,\\20 hexagons,\\12 decagons)}&
180&
120&
$\operatorname{Isom}(\mathbf{Icos})$\\\hline

\parbox{2.5cm}{Snub\\Dodecahedron\\(3.3.3.3.5)}&
\includegraphics[scale=.1]{ArchimedeanSolids/SnubDodecahedron.png}&
92 \parbox{2.2cm}{(80 triangles,\\12 pentagons)}&
150&
60&
$\operatorname{Isom}^+(\mathbf{Icos})$\\\hline
\end{tabular}
\end{center}

To remember these solids and their names, it helps to know what some of the terms mean.

First, \textbf{truncation} of a polyhedron is obtained by ``slicing off'' each vertex, so that a new face arises with as many sides as there were edges to the vertex.  The following illustration shows this for a cube:
\begin{center}
\includegraphics[scale=.3]{Truncation.png}
\end{center}
Alternatively, draw points on the edges near the vertex, connect each pair of points with an edge if and only if the edges they're on share a face, and then erase inside the bounded region to make a new face.  [This makes more sense for tilings on a smooth surface; recall the truncated heptagonal tiling from Exercise 7 of Section 4.5.]

Each Platonic solid can be truncated, resulting in one of the Archimedean solids.  Note that the slices have to be positioned in a particular way for the result to have regular-polygon faces. % By "This is relatively easy to prove without considering each Platonic solid," I meant that you can prove that truncating any of them yields a uniform polyhedron, without casework on each of the actual 5 solids.
To get an Archimedean, the lengths must be chosen so that the face formed each original vertex is perpendicular to the segment from the center to that vertex, and the side lengths of these faces are equal to the resulting lengths of the shortened edges.
\begin{center}
\begin{tabular}{ccccc}
\includegraphics[scale=.1]{ArchimedeanSolids/TruncatedTetrahedron.png}&
\includegraphics[scale=.1]{ArchimedeanSolids/TruncatedOctahedron.png}&
\includegraphics[scale=.1]{ArchimedeanSolids/TruncatedIcosahedron.png}&
\includegraphics[scale=.1]{ArchimedeanSolids/TruncatedCube.png}&
\includegraphics[scale=.1]{ArchimedeanSolids/TruncatedDodecahedron.png}\\
Truncated&
Truncated&
Truncated&
Truncated&
Truncated\\
Tetrahedron&
Octahedron&
Icosahedron&
Cube&
Dodecahedron
\end{tabular}
\end{center}
This careful truncation makes sure none of the slices touch each other, so that some of each edge of the original polyhedron is still left.  We could be greedy and get rid of the entire edges through the truncation, though; this happens when we slice each vertex through the midpoints of the edges meeting it.  When we do this, we get the \textbf{rectification} (or \textbf{degenerate truncation}), whose vertices correspond bijectively with the edges of the original polyhedron (even though in the careful truncation, there is a two-to-one correspondence from the vertices to the original polyhedron's edges).

The interesting graph-theoretic thing about the rectification is this: a polyhedron and its dual have the same rectification.  The reader who is interested in the graph-theoretic viewpoint of polyhedra is encouraged to wing a proof.  Thus, the cube and octahedron have the same rectification; as do the icosahedron and dodecahedron.

The rectification of a tetrahedron is merely an octahedron, which is another Platonic solid.  But the rectifications of the other Platonic solids are Archimedean solids, the cuboctahedron for the cube and octahedron; and the icosidodecahedron for the icosahedron and dodecahedron:
\begin{center}
\includegraphics[scale=.2]{ArchimedeanSolids/Cuboctahedron.png}~~~~\includegraphics[scale=.2]{ArchimedeanSolids/Icosidodecahedron.png}
\end{center}
Now, on the above two polyhedra, disconnect the faces and rotate each face at an angle of $\pi/n$ where $n$ is the number of sides; this sends the vertices to where the edges' midpoints used to be.  Then, translate the faces in $\mathbb R^3$ until each pair of vertices corresponding to formerly meeting edges coincides.  You thus have two frames; one consists of 8 triangles and 6 squares, and the other consists of 20 triangles and 12 pentagons, but they only meet at vertices.  At each vertex of the original polyhedron, a empty spot with four edges emerges.

Putting squares in these empty spots yields the rhombicuboctahedron and rhombicosidodecahedron:
\begin{center}
\includegraphics[scale=.2]{ArchimedeanSolids/Rhombicuboctahedron.png}~~~~\includegraphics[scale=.2]{ArchimedeanSolids/Rhombicosidodecahedron.png}
\end{center}
Or, if you twist the faces in such a way that each spot can fit a pair of hinged triangles, you get the snubs (the ones with no orientation-reversing symmetry):
\begin{center}
\includegraphics[scale=.2]{ArchimedeanSolids/SnubCube.png}~~~~\includegraphics[scale=.2]{ArchimedeanSolids/SnubDodecahedron.png}
\end{center}
Finally, take the rhombicuboctahedron and rhombicosidodecahedron, and consider the squares that ``filled in the spots.''  For the rhombicosidodecahedron, that's all 30 squares; for the rhombicuboctahedron, that's only the twelve squares which meet the triangles along edges.  Suppose we wish to expand these squares out so they don't meet each other by the vertices.  Then each vertex turns into two vertices and an edge that connects the spot-filling squares.  The edges add more sides to the rest of the faces, doubling the quantities.  We thus get the great rhombicuboctahedron and great rhombicosidodecahedron:
\begin{center}
\includegraphics[scale=.2]{ArchimedeanSolids/GreatRhombicuboctahedron.png}~~~~\includegraphics[scale=.2]{ArchimedeanSolids/GreatRhombicosidodecahedron.png}
\end{center}
Observe that, graph-theoretically, these are the truncations of the cuboctahedron and icosidodecahedron respectively!  The cuboctahedron and icosidodecahedron are two Archimedean solids satisfying a unique property, making this work; see Exercise 2.  Thus, the two polyhedra above are sometimes called the \emph{truncated cuboctahedron} and \emph{truncated icosidodecahedron}, respectively.  However, if you directly slice a vertex of the cuboctahedron (resp., icosidodecahedron) in $\mathbb R^3$, you do not get a square; instead, you get a rectangle whose side lengths are in the ratio $\sqrt 2$ (resp., $\phi$).\\

\noindent\textbf{CATALAN SOLIDS}\\

\noindent We now find ourselves interested in the dual of a uniform polyhedron.  We know what it is graph-theoretically, but what is the best way to embed it into $\mathbb R^3$?

The first easy observation is that the dual of an Archimedean solid can't be an Archimedean solid.  Indeed, if $P$ is a uniform polyhedron whose dual is another uniform polyhedron $Q$, then $Q$'s isometry group acts transitively on the vertices.  By duality, $P$'s isometry group would act transitively on the faces, which would make it a Platonic solid.

The dual of an Archimedean solid has identical faces (due to the Archimedean solid having identical vertices), but it has different numbers of edges surrounding different vertices, so no matter how it is drawn, its isometry group cannot act transitively on the vertices.  However, one can ask for regular vertex figures,\footnote{A vertex figure is regular if all the angles of the faces meeting it are equal, and all the dihedral angles meeting it (inside the polyhedron) are equal.  Note that this is equivalent to the vertex's local rotations acting transitively on the edges.} just as an Archimedean solid has regular polygons for faces.

This is essentially all we could wish for, and yet it is easy to construct the dual with these properties.  To do this, recall that a uniform polyhedron $P$ can be inscribed in a sphere.  Simply let $Q$ be the circumscribed polyhedron given by $P$ (as a spherical polyhedron).  Then since $Q$ has the same isometry group as $P$, the group acts transitively on $Q$'s faces.  Moreover, by Exercise 1, since $P$ has identical edge lengths, $Q$ has identical dihedral angles.  The reader is then encouraged to check that $Q$ has regular vertex figures.

Thus, the dual of a uniform polyhedron has the following properties:
\begin{itemize}
\item Isometry group acts transitively on the faces;

\item Every vertex has a regular vertex figure.
\end{itemize}
Conversely, such a polyhedron is circumscribed around a sphere, and the corresponding inscribed Euclidean polyhedron is uniform.  This shows how to easily convert between uniform polyhedra and their duals as polyhedra with the above two properties.

Platonic solids dualize to each other; they are both uniform and have the above two properties, as the reader can easily see.  Prisms and antiprisms dualize to other infinite families of polyhedra; see Exercise 5.  Finally, in this fashion, the Archimedean solids dualize to the \emph{Catalan solids}, first described by Eug\`ene Catalan in 1865.  Here is a chart with the Catalan solids; they are listed in the same order as their duals in the previous chart.
\begin{center}
\begin{tabular}{cccccccc}
Name&Picture&\# Faces&Face Type&\# Edges&\# Vertices&\parbox{2cm}{Symmetry\\Group}\\\hline\hline

\parbox{2.5cm}{Triakis\\Tetrahedron}&
\includegraphics[scale=.1]{CatalanSolids/TriakisTetrahedron.png}&
12&
\parbox{2cm}{isosceles\\triangle}&
18&
8&
$\operatorname{Isom}(\mathbf{Tet})$\\\hline

\parbox{2.5cm}{Rhombic\\Dodecahedron}&
\includegraphics[scale=.1]{CatalanSolids/RhombicDodecahedron.png}&
12&
rhombus&
24&
14&
$\operatorname{Isom}(\mathbf{Oct})$\\\hline

\parbox{2.5cm}{Triakis\\Octahedron}&
\includegraphics[scale=.1]{CatalanSolids/TriakisOctahedron.png}&
24&
\parbox{2cm}{isosceles\\triangle}&
36&
14&
$\operatorname{Isom}(\mathbf{Oct})$\\\hline

\parbox{2.5cm}{Tetrakis\\Cube}&
\includegraphics[scale=.1]{CatalanSolids/TetrakisCube.png}&
24&
\parbox{2cm}{isosceles\\triangle}&
36&
14&
$\operatorname{Isom}(\mathbf{Oct})$\\\hline

\parbox{2.5cm}{Deltoidal\\Icositetrahedron}&
\includegraphics[scale=.1]{CatalanSolids/DeltoidalIcositetrahedron.png}&
24&
kite&
48&
26&
$\operatorname{Isom}(\mathbf{Oct})$\\\hline

\parbox{2.5cm}{Disdyakis\\Dodecahedron}&
\includegraphics[scale=.1]{CatalanSolids/DisdyakisDodecahedron.png}&
48&
\parbox{2cm}{scalene\\triangle}&
72&
26&
$\operatorname{Isom}(\mathbf{Oct})$\\\hline

\parbox{2.5cm}{Pentagonal\\Icositetrahedron}&
\includegraphics[scale=.1]{CatalanSolids/PentagonalIcositetrahedron.png}&
24&
pentagon&
60&
38&
$\operatorname{Isom}^+(\mathbf{Oct})$\\\hline

\parbox{2.5cm}{Rhombic\\Triacontahedron}&
\includegraphics[scale=.1]{CatalanSolids/RhombicTriacontahedron.png}&
30&
rhombus&
60&
32&
$\operatorname{Isom}(\mathbf{Icos})$\\\hline

\parbox{2.5cm}{Triakis\\Icosahedron}&
\includegraphics[scale=.1]{CatalanSolids/TriakisIcosahedron.png}&
60&
\parbox{2cm}{isosceles\\triangle}&
90&
32&
$\operatorname{Isom}(\mathbf{Icos})$\\\hline

\parbox{2.5cm}{Pentakis\\Dodecahedron}&
\includegraphics[scale=.1]{CatalanSolids/PentakisDodecahedron.png}&
60&
\parbox{2cm}{isosceles\\triangle}&
90&
32&
$\operatorname{Isom}(\mathbf{Icos})$\\\hline
\end{tabular}

\begin{tabular}{cccccccc}
Name&Picture&\# Faces&Face Type&\# Edges&\# Vertices&\parbox{2cm}{Symmetry\\Group}\\\hline\hline

\parbox{2.5cm}{Deltoidal\\Hexecontahedron}&
\includegraphics[scale=.1]{CatalanSolids/DeltoidalHexecontahedron.png}&
60&
kite&
120&
62&
$\operatorname{Isom}(\mathbf{Icos})$\\\hline

\parbox{2.5cm}{Disdyakis\\Triacontahedron}&
\includegraphics[scale=.1]{CatalanSolids/DisdyakisTriacontahedron.png}&
120&
\parbox{2cm}{scalene\\triangle}&
180&
62&
$\operatorname{Isom}(\mathbf{Icos})$\\\hline

\parbox{2.5cm}{Pentagonal\\Hexecontahedron}&
\includegraphics[scale=.1]{CatalanSolids/PentagonalHexecontahedron.png}&
60&
pentagon&
150&
92&
$\operatorname{Isom}^+(\mathbf{Icos})$\\\hline
\end{tabular}
\end{center}

A quadrilateral $ABCD$ where $AB=BC$ and $CD=DA$, which implies reflective symmetry over $\overset{\longleftrightarrow}{BD}$, is called a kite.  Also, the prefix ``icositetra-'' merely means 24; it does not mean the icosahedron and tetrahedron play the main roles (like in ``icosidodecahedron'').

\subsection*{Exercises 5.4. (Inscribed and Circumscribed Euclidean Polyhedra)} % Given a spherical polyhedron, show how to get the inscribed Euclidean polyhedron
% (by connecting the vertices), and the circumscribed Euclidean polyhedron (by taking tangent planes to the vertices).
% Use this to define Archimedean solids and Catalan solids and classify them all.
% POTENTIAL EXERCISES: Some of the mathematical calculations for the Catalan/Archimedean solids I did in F2018.
% POTENTIAL EXERCISE: Johnson solids [of course, we're not going to classify them, but it's interesting to know there are exactly 92]
\begin{enumerate}
\item Let $P$ be a spherical polyhedron, and $E$ one of its edges.  Let $d$ be the spherical length of $E$; let $\ell$ be the length of the corresponding edge of the inscribed polyhedron; and let $\theta$ be the dihedral angle of the corresponding edge of the circumscribed polyhedron.  Show that $\ell=\sqrt{2-2\cos d}$ and $d+\theta=\pi$.  Conclude that each of the values $d,\ell,\theta$ determines the other two, so that if any two edges have one of these values equal, they have all of them equal.

\item Among all Archimedean solids, only the cuboctahedron and icosidodecahedron have isometry groups acting transitively on the edges.  [Because of this, those two solids are considered \textbf{quasi-regular}, while the other eleven are \textbf{semi-regular}.]  Explain why this enables them to be (graph-theoretically) truncated, still resulting in Archimedean solids.

\item The great rhombicuboctahedron and great rhombicosidodecahedron are the only Archimedean solids where the \emph{orientation-preserving} isometry group fails to act transitively on the vertices.  [Transitivity of the orientation-preserving isometry group on the vertices is a stronger condition which is not required for uniformity.]

\item\emph{(Euler's formula.)} \---- Given a convex polyhedron with $F$ faces, $E$ edges and $V$ vertices, prove that $F-E+V=2$. [Convert it to a spherical polyhedron, and then use induction on $E$, noting the invariance of the formula when certain edges are deleted.  Care must be taken to avoid ``bogus'' faces with holes in them.]  This fact has first been stated by Leonhard Euler.

\item\emph{(The duals of prisms and antiprisms.)} \---- In this exercise we determine the dual of prisms and antiprisms, as polyhedra with congruent faces and regular vertex figures.

(a) Show that the dual of an $n$-gonal prism is an $n$-gonal \textbf{bipyramid}; i.e., it is obtained by taking two pyramids and joining their bases together.

(b) Show that the dual of an $n$-gonal antiprism has kite-shaped faces, with $n$ on top and $n$ on the bottom, similarly to the bipyramid.  It is called a \textbf{deltohedron} (or \textbf{trapezohedron}).

Here is the special case of each with $n=7$.
\begin{center}
\includegraphics[scale=.2]{Bipyramid.png}~~~~~
\includegraphics[scale=.2]{Deltohedron.png}\\
~~~Bipyramid~~~~~~Deltohedron
\end{center}
(c) If $n=4$, the bipyramid is an octahedron; and if $n=3$, the deltohedron is a cube.

\item The aim of this exercise is to show how certain Archimedean/Catalan solids can be constructed in practice.

(a) 24 points in $\mathbb R^3$ are permutations of $(\pm 2,\pm 1,0)$ [even and odd permutations are permitted].  Show that their convex hull is a truncated octahedron.

Now we show how to construct its dual, the tetrakis cube.

(b) Explain why we may assume that the tetrakis cube has vertices $(\pm 1,\pm 1,\pm 1)$ and $(\pm r,0,0),(0,\pm r,0),(0,0,\pm r)$ for some $r>0$.

(c) The dihedral angle between two faces is supplementary to the central angle given by the perpendiculars from the origin to the faces.  Use this to show that the dihedral angles of this tetrakis cube are equal to $\cos^{-1}\left[-\frac{2(r-1)}{r^2-2r+2}\right]$ and $\cos^{-1}\left[-\frac 1{r^2-2r+2}\right]$.

(d) Conclude that (since a Catalan solid has constant dihedral angles), $r=\frac 32$.

(e) Explain how the tetrakis cube could have been alternatively constructed from part (a).  [Consider inscribing the truncated octahedron in a sphere.]

Now we give an example involving a Catalan solid with icosahedral symmetry.  Recall (Section 2.7) that the twelve points of the form
$$(\pm\phi,\pm 1,0),~~~~(0,\pm\phi,\pm 1),~~~~(\pm 1,0,\pm\phi)$$
form a regular icosahedron, and the twenty points of the form
$$(\pm 1,\pm 1,\pm 1),~~~~(\pm\phi^{-1},\pm\phi,0),~~~~(\pm\phi,0,\pm\phi^{-1}),~~~~(0,\pm\phi^{-1},\pm\phi)$$
form a regular dodecahedron.  We dilate these polyhedra with variable scalars $a,b$ so that the icosahedron's vertices are even permutations of $(\pm a\phi,\pm a,0)$, and the dodecahedron's vertices are $(\pm b,\pm b,\pm b)$ and even permutations of $(\pm b\phi^{-1},\pm b\phi,0)$.

(f) Show that the 32 points above form a rhombic triacontahedron if and only if the points
$$(a\phi,a,0),~~~~(b\phi,0,b\phi^{-1}),~~~~(a\phi,-a,0),~~~~(b\phi,0,-b\phi^{-1})$$
are coplanar.  [Two of them form an edge of the icosahedron, and two of them form the dodecahedron's corresponding edge.]

(g) Show that the points form a rhombic triacontahedron if and only if $a=b$.  Furthermore, conclude that the faces of a rhombic triacontahedron are \textbf{golden rhombi}; in other words, the ratio of their diagonals is $\phi$.

(h) Now explain how to get the icosidodecahedron.

This strategy can be used to obtain the coordinates of any Catalan and Archimedean solid, but many of the constructions involve nontrivial polynomial solving.  Those with mathematical software are encouraged to try to construct these solids. % Which I, the author of Various Geometry, did in the fall of 2018!  (Also, I don't think they're all quadratic irrationalities, as several times I used Newton's method to approximate roots of a high-degree polynomial)

\item\emph{(Johnson solids.)} \---- A \textbf{Johnson solid} is a strictly convex polyhedron (i.e., all dihedral angles are strictly less than $180^\circ=\pi$), with regular polygons as faces, which is not uniform.  In other words, its global isometry group is \emph{not} to act transitively on the vertices.  An example is the square pyramid, obtained by putting four equilateral triangles around a vertex and then closing the base with a square.  Another example is the pseudo-rhombicuboctahedron mentioned before the classification of Archimedean solids.

Come up with many ways to create Johnson solids.  [Consider attaching pyramids to uniform polyhedra, removing parts of uniform polyhedra, and attaching prisms/antiprisms.  These are respectively called \textbf{augmenting}, \textbf{diminishing} and \textbf{elongating}/\textbf{gyroelongating}.]

In 1966, Norman Johnson listed 92 of these solids and gave names and an ordering for them.  He did not yet know whether there were any others, but in 1969, Victor Zalgaller confirmed that he had all of them.
\end{enumerate}

\subsection*{5.5. Spherical $n$-space}
\addcontentsline{toc}{section}{5.5. Spherical $n$-space}
As usual, we wish to generalize the geometry to higher dimensions.  We let $S^n(\mathbb R)$ be the $n$-sphere $\{\vec v\in\mathbb R^{n+1}:\|\vec v\|=1\}$, and after defining certain notions, we will call this \textbf{spherical $n$-space}.

It may seem that two dimensions is the highest number of dimensions of spherical geometry which can be visualized, since in three dimensions, one uses the hypersphere in $\mathbb R^4$.  However, we recall the \emph{stereographic projection model} from Section 5.1.  It enables the $n$-sphere to be realized as $\mathbb R^n$ (with one point left out), which is perfectly visualizable when $n=3$.\\

\noindent The basic geometry of $S^n(\mathbb R)$ is fairly easy, for the most part.  For $0\leqslant k<n$, a \textbf{$k$-sphere} is defined to be a $k$-sphere on $S^n(\mathbb R)$: this is the intersection with $S^n(\mathbb R)$ of a $(k+1)$-plane in $\mathbb R^{n+1}$, and we require the intersection to have more than one point (so that in particularly it has infinitely many points).  A $k$-plane is defined to be a $k$-sphere centered at the origin, or what is the same thing, a $k$-sphere with largest possible radius.

Isometries of $S^n(\mathbb R)$ are, once again, elements of $O(n+1)$.  The elements of $SO(n+1)$ are orientation-preserving, and the rest are orientation-reversing.

The distance between two points in $S^n(\mathbb R)$ is, as before, the measure of the central angle given by the radii to the points.  It may also be realized as the arc length of the line segment connecting the points.

Before carrying on, it would help to understand the stereographic projection model (the best way for spherical 3-space to be visualized), but that is fairly self-explanatory.  In Section 4.6, we defined stereographic projection $S^n(\mathbb R)\to\mathbb R^n\sqcup\{\infty\}$, and we may merely use this map to convert a construction in the $n$-sphere to one of the flat $n$-space.  The reader is left to verify:
\begin{itemize}
\item $k$-spheres are generalized $k$-spheres in $\overline{\mathbb R^n}$.  In other words, (unless they contain $\infty$), they are Euclidean $k$-spheres, but their centers generally differ from the Euclidean centers.

\item A $k$-plane is the intersection of a $(k+1)$-plane through the origin, with a hypersphere of radius $r$ and center $\vec a\in\mathbb R^n$ such that $r^2-\|\vec a\|^2=1$. [Exercise 8 of Section 4.6.]
\end{itemize}
This study is relatively easy, unlike the other kinds of geometry.\\

\noindent\textbf{BASIC FACTS ABOUT SPHERICAL $3$-SPACE}\\

\noindent We conclude this section with a few basic properties about spherical 3-space.  First, since the isometry group is $O(4)$, it is clear that isometries act transitively on the points.  We also have the transitivity of isometries on planes, lines, spheres of a given radius, and circles of a given radius.  But we haven't proven that two points determine a line, etc.  That is what we shall tackle right now.\\

\noindent\textbf{Proposition 5.12.} \emph{In $S^3(\mathbb R)$,}

(i) \emph{Two points determine a line unless the points are antipodes, in which case every line through one point also goes through the other.}

(ii) \emph{Any two distinct planes intersect in a line.}

(iii) \emph{Three points that are not collinear determine a plane.}

(iv) \emph{If $\Pi$ is a plane and $\ell$ a line not contained in it, then $\Pi$ and $\ell$ intersect in two antipodal points.}\\

\noindent Remarks are in order.  First, (i) holds in any dimensional space $S^n(\mathbb R)$, and the proof can readily be carried over.  Secondly, in part (iii), the hypothesis implies that no two of the points are antipodes, because if they were, then the three points would be collinear no matter what the third point is.

Finally, two lines in $S^3(\mathbb R)$ need not intersect; for instance, the intersections of the planes $x=y=0$ and $w=z=0$ with $S^3(\mathbb R)$ are two nonintersecting lines.  [See how to view them in the stereographic projection model?]

\begin{proof}
(i) Let $p$ and $q$ be distinct points of $S^3(\mathbb R)$.  If $p,q$ are antipodes, then $p=-q$, so that every line going through either $p$ or $q$ also goes through the other (because lines are cross sections of two-dimensional linear subspaces of $\mathbb R^4$).  If $p,q$ are not antipodes, then they are linearly independent vectors of $\mathbb R^4$ (why?), hence span a plane $\Pi$.  With that, $\Pi\cap S^3(\mathbb R)$ is the unique line going through $p$ and $q$.

Alternatively, one could assume we are in the stereographic projection model and $p=0$.  Then the antipode of $p$ is $\infty$.  Moreover, if $q\ne\infty$, then there is a unique Euclidean line $\ell$ through $p,q$, and (since it contains the origin) it must be a spherical line.  If $\ell'\ne\ell$ is another line through $p,q$, then $\ell'$ is not a Euclidean line, hence it is a cross section of a circle with center $\vec a$ and radius $r$ such that $r^2-\|\vec a\|^2=1$: but $0\in\ell'$ implies $\|\vec a\|=r$ and so $r^2-\|\vec a\|^2=0$, contradiction.

(ii) The planes are cross sections of three-dimensional (linear) subspaces of $\mathbb R^4$, say $V$ and $W$.  Then $V+W=\mathbb R^4$ (why?).  Moreover, $6=\dim(V)+\dim(W)=\dim(V+W)+\dim(V\cap W)=4+\dim(V\cap W)$, so $\dim(V\cap W)=2$.  Moreover, $[V\cap W]\cap S^3(\mathbb R)$ is a spherical line, and is the intersection of the given planes.

(iii) If $p,q,r\in S^3(\mathbb R)$ are noncollinear points, then (since there are three of them), they are contained in a three-dimensional subspace of $\mathbb R^4$, say $V$.  With that, $V\cap S^3(\mathbb R)$ is a plane containing the points.  This plane is unique because if $p,q,r$ were in another plane, then they would be in the intersection of the planes, which is a line by part (ii): contradiction, because $p,q,r$ are noncollinear.

(iv) This mirrors the proof of (ii), but this time $\dim(V)=3$ and $\dim(W)=2$.  We leave the argument to the reader.
\end{proof}

\subsection*{Exercises 5.5. (Spherical $n$-space)} % Introduce higher dimensional spaces, and use the stereographic projection to the aid as before.
\begin{enumerate}
\item Show that if $0\leqslant k<n$, then in $S^n(\mathbb R)$, a $k$-plane $P$ is isometric to $S^k(\mathbb R)$; i.e., there is a bijection $P\cong S^k(\mathbb R)$ which preserves distances, as well as lines and angles.  [Assume $P$ is a Euclidean $k$-plane in the stereographic projection model.]

\item In all three types of geometry (spherical, Euclidean and hyperbolic), a $k$-sphere has an isometry group canonically isomorphic to that of $S^k(\mathbb R)$ [i.e., $O(k+1)$].

\item Let $\vec v\in\mathbb R^n$, regarded as in the stereographic projection model of spherical $n$-space.  Show that the hyperbolic distance from the origin to $\vec v$ is $2\tan^{-1}\|\vec v\|$.

\item Recall the regular 4-polytopes from Exercise 3 of Section 2.7.  In this exercise, we shall show that they all exist.

(a) In the stereographic projection model of spherical 3-space, take any Platonic solid centered at the origin.  Then erase its Euclidean edges and replace them with the spherical line segments connecting the vertices.  Likewise, allow the new faces to be taken from planes of the spherical space.

Show that the resulting figure has exactly the same symmetry (in the spherical space) as the original Platonic solid.  Conclude that it has constant dihedral angles.

(b) Let $\delta$ be the dihedral angle of the Platonic solid in Euclidean 3-space (Exercise 3(b) of Section 2.7).  Show that the dihedral angle $\alpha$ of the figure of part (a) satisfies $\delta<\alpha<\pi$, and that any real number in that range is possible.

(c) If $\alpha=2\pi/n$ for some integer $n$, then repeatedly reflecting the figure over its own faces eventually fills the spherical space with no gaps or overlaps.

(d) By transfering the space filling in part (c) directly to $S^3(\mathbb R)$, and then connecting the vertices by Euclidean line segments in $\mathbb R^4$, show that we get the desired regular polytope.

\item Suppose $T$ is an isometry of $S^n(\mathbb R)$, and this space is viewed via stereographic projection.  If $T(\vec 0)=\vec 0$, show that $T$ is a Euclidean orthogonal transformation in $O(n)$.  [Use Proposition 1.9.]  Moreover, every isometry of $S^n(\mathbb R)$ is a finite composition of reflections.
\end{enumerate}

\subsection*{5.6. Relation to Projective Space: Elliptic Geometry}
\addcontentsline{toc}{section}{5.6. Relation to Projective Space: Elliptic Geometry}
We recall from the first section that lines in $S^2(\mathbb R)$ are great circles, and any two of them intersect them in two antipodal points.  But it is really satisfying for lines to intersect in two points?  Certainly not, in some moods; for one thing, it ruins the statement that two points determine a line.  (They don't determine a line if they are antipodes.)  It would really be to our benefit if two lines had one intersection point.

There is a surprisingly easy approach we can use to achieve this.  Start with $S^2(\mathbb R)$, and let $\sim$ be the equivalence relation on $S^2(\mathbb R)$ where $p\sim q$ if and only if $p=\pm q$ in $\mathbb R^3$.  Quotient out this equivalence relation, and let $E^2(\mathbb R)$ be the result.  Then $E^2(\mathbb R)$ is called the \textbf{real elliptic plane}.  It is like the sphere, except two points which used to be antipodes are now the same point.  Meanwhile, there is an obvious two-to-one correspondence from $S^2(\mathbb R)\to E^2(\mathbb R)$.

Basic spherical geometry still holds in this setting: a line is a great circle of the sphere; a line segment is an arc of such a great circle; and a circle is a circle of the sphere.  However, the notions of a line segment and a circle are slightly different, because antipodes are identified together, and hence if any point is in a set its ``antipode'' must be as well.  Thus, for instance, a circle in $E^2(\mathbb R)$ (if it is not a line) really appears as \emph{two} circles on the sphere, for which each point on one circle is identified with its opposite point on the other.  And as before, there are two line segments between points, but both have length $<\pi$: if either had length exceeding $\pi$, it would contain antipodes on the sphere, so that its image in $E^2(\mathbb R)$ would really be an entire line.
\begin{center}
\includegraphics[scale=.2]{EllipLineSeg.png}~~~~
\includegraphics[scale=.2]{EllipCircle.png}
\end{center}
We recall from Section 5.1 that in spherical geometry, two points $p,q$ have a distance $\leqslant\pi$, because they are connected by two segments, whose lengths add to $2\pi$, and the distance is the shorter length.  In the elliptic plane, the two segments have lengths that add to $\pi$, because if you try to draw the segments on the sphere, they will share \emph{one} of the points $p,q$, but as for the other point, one segment has the point and the other has its antipode as an endpoint.  Hence, their distance (the shorter length) is $\leqslant\pi/2$.
\begin{center}
\includegraphics[scale=.2]{EllipDifferentSegs.png}\\
\emph{Two points making two segments (the labeled point in the back and its antipode are the same).}
\end{center}
In spherical geometry, two lines intersect in two points which are antipodes.  Here, now that we are identifying a pair of antipodes to a single point, we have\\

\noindent\textbf{Proposition 5.13.} \emph{In $E^2(\mathbb R)$, any two lines intersect in a unique point.}\\

\noindent Also, if two points are distinct in $E^2(\mathbb R)$, then they cannot be antipodes, and therefore by Proposition 5.1(iii),\\

\noindent\textbf{Proposition 5.14.} \emph{In $E^2(\mathbb R)$, any two points determine a line.}\\

\noindent At this point, we begin to imagine that the properties look familiar.  We recall the projective plane $P^2(\mathbb R)$ from Chapter 3, as the quotient space of $\mathbb R^3-\{\vec 0\}$ by the equivalence relation $\vec v\sim\vec w\iff\vec v=\lambda\vec w~\exists\lambda\in\mathbb R$.  Propositions 5.13 and 5.14, in fact, state that the elliptic plane satisfies the same basic properties of points and lines that the projective plane does (Proposition 3.1).  This is because they are canonically identified, as we now see.\\

\noindent\textbf{Proposition 5.15 and Definition.} (i) \emph{Define a map $\vartheta:S^2(\mathbb R)\to P^2(\mathbb R)$ by $\vartheta((x,y,z))=[x:y:z]$.  Then $\vartheta$ is a surjective function such that $\vartheta(p)=\vartheta(q)$ if and only if $p=\pm q$ for $p,q\in S^2(\mathbb R)$.  Moreover, $\vartheta$ induces a bijection $\overline{\vartheta}$ from $E^2(\mathbb R)\to P^2(\mathbb R)$.}

(ii) \emph{$\vartheta$ sends each $(x,y,z),z\ne 0$ to the point $(x/z,y/z)$ of the Euclidean plane.  We refer to this (resp., $\overline{\vartheta}$) as the \textbf{gnomonic projection} of the sphere (resp., elliptic plane).}

(iii) \emph{Under the correspondence $\overline{\vartheta}$, elliptic lines correspond to projective lines.}\\

\noindent It is worth remarking that by (iii), gnomonic projection sends lines to Euclidean lines (except the one it sends to the line at infinity).  Of course, this is not true for stereographic projection, where most of the lines are Euclidean circles.
\begin{proof}
(i) $\vartheta$ is well-defined because the only time $[x:y:z]$ is undefined is when $x=y=z=0$, and $S^2(\mathbb R)$ does not contain the origin.  $\vartheta$ is surjective, because each point of $P^2(\mathbb R)$ can be written as $[x:y:z]$, which is
$$\vartheta\left(\frac 1{\sqrt{x^2+y^2+z^2}}(x,y,z)\right).$$
If $\vartheta(p)=\vartheta(q)$, then $[\vec p]=[\vec q]$ in $P^2(\mathbb R)$.  Hence, by definition, $\vec p=\lambda\vec q$ for some $\lambda\ne 0$ in $\mathbb R$.  Since $\vec p,\vec q\in S^2(\mathbb R)$, their magnitudes are $1$, hence
$$1=\|\vec p\|=\|\lambda\vec q\|=|\lambda|\|\vec q\|=|\lambda|,$$
so that $\lambda=\pm 1$ and $p=\pm q$.  The converse is clear: if $p=\pm q$ then $\vartheta(p)=\vartheta(q)$.

Clearly $\overline{\vartheta}$ can be defined by sending $\overline p\in E^2(\mathbb R)$ to $[\vec p]$, and it is a bijection.

(ii) is clear because if $z\ne 0$, then $[x:y:z]=[x/z:y/z:1]=(x/z,y/z)$.

(iii) Spherical lines are intersections with $S^2(\mathbb R)$ of planes through the origin, i.e., two-dimensional subspaces of $\mathbb R^3$.  The image of such a spherical line in $P^2(\mathbb R)$ is this plane, and hence a projective line.  Conversely, a projective line is a two-dimensional subspace of $\mathbb R^3$, and its restriction to the sphere is a great circle, i.e., a spherical line.
\end{proof}

\noindent Thus, Propositions 5.13 and 5.14 can alternatively be proven using Propositions 5.15 and 3.1, as Proposition 5.15 shows that the elliptic plane has the same points and lines as the projective plane.  However, this time there is a metric on pairs of points (two points have a distance $\leqslant\pi/2$).

It is clear that isometries of $S^2(\mathbb R)$ can be restricted to $E^2(\mathbb R)$, because if $T$ is an isometry then $T(-p)=-T(p)$; and hence the map $p\mapsto\overline{T(p)}$ from $S^2(\mathbb R)\to E^2(\mathbb R)$ sends $\pm p$ to the same thing and one can identify antipodes in the domain. % Basically, set-theoretic injectification.
Moreover, we recall that a projective transformation of $P^2(\mathbb R)$ is given by restricting any nonsingular linear operator on $\mathbb R^3$.  Since $T$ is given by a nonsingular linear operator (an orthogonal one), it becomes a projective transformation of the gnomonic projection.  Thus
\begin{center}
\textbf{In the gnomonic projection of the elliptic plane, isometries are projective transformations.  However, not all projective transformations are isometries, as a projective transformation does not generally preserve distances (they act transitively on pairs of distinct points).}
\end{center}
Similarly, a circle in $E^2(\mathbb R)$ which is not a line, gives rise to a cone in $\mathbb R^3$, and hence a conic in $P^2(\mathbb R)$.  Each circle has exactly one center (because the spherical circle has two centers which are antipodes), and again, not all conics are circles in $E^2(\mathbb R)$.\\

\noindent\textbf{HIGHER DIMENSIONS}\\

\noindent As before, we expect to be able to generalize these results to higher dimensions.

It is fairly easy; we do the same thing to $S^n(\mathbb R)$ which we did to $S^2(\mathbb R)$.  We let $\sim$ be the equivalence relation where $p\sim q$ if and only if $p=\pm q$, and quotient it out to get elliptical $n$-space, $E^n(\mathbb R)$.  As in Proposition 5.15, the map $S^n(\mathbb R)\to P^n(\mathbb R)$ given by $(x_1,x_2,\dots,x_{n+1})\mapsto[x_1:x_2:\dots:x_{n+1}]$ induces a bijection $E^n(\mathbb R)$, and the reader is left to verify:
\begin{itemize}
\item $k$-planes in $E^n(\mathbb R)$ correspond to projective $k$-planes;

\item Isometries are projective transformations, but not conversely;

\item $k$-spheres which are not $k$-planes are $k$-dimensional ellipsoids, paraboloids and hyperboloids of revolution in $P^n(\mathbb R)$.\\
\end{itemize}
\noindent At this point it is interesting to compare projective space with elliptic space, and tell how much homogeneity is lost when bringing in the elliptic metric.  We can answer this by finding the number of degrees of freedom each has in picking isometries.\footnote{More rigorously, the group of isometries of any of the spaces we've gone over (spherical, Euclidean, hyperbolic, projective, elliptic) is a \emph{Lie group}, as it locally has the same topological structure as a concrete space $\mathbb R^d$.  The number $d$, called the \emph{dimension} of the Lie group, is the number of degrees of freedom.  We will not delve into details here.}  To begin with, $P^n(\mathbb R)$'s isometry group has $n^2+2n$ degrees of freedom: indeed, there are $(n+1)^2=n^2+2n+1$ degrees of freedom in picking an element of $GL_{n+1}(\mathbb R)$ [one for each entry: the requirement that the matrix be nonsingular is not equational, hence does not hinder any degree of freedom].  But when we quotient out the subgroup of scalar multiples of the identity matrix to get $PGL_{n+1}(\mathbb R)$, one degree of freedom is lost, leaving us with $n^2+2n$.

Now we consider the isometry group of $E^n(\mathbb R)$.  It is obtained by taking $S^n(\mathbb R)$'s isometry group, $O(n+1)$, and quotienting out the subgroup $\{I_{n+1},-I_{n+1}\}$.  Since we are quotienting out a discrete subgroup, no degrees of freedom are lost to it.  The number of degrees of freedom in choosing an element of $O(n+1)$ is equal to $\frac{n(n+1)}2$, as can be seen in many ways:
\begin{itemize}
\item There are $n$ degrees in picking the first column of the matrix, because it can be any \emph{unit} $(n+1)$-dimensional vector.  Then there are $n-1$ degrees in picking the second column, because it must be a unit vector, but it must also be orthogonal to the first column, and this condition takes away a degree of freedom.  Then there are $n-2$ degrees in picking the third column, because it must be orthogonal to the first two: the linear independence of the first two guarantees that there will always be \emph{exactly} two degrees of freedom biting the dust.  The argument iterates until the last column, where there are no degrees of freedom left, and only two possible vectors that can be placed.  Hence the total number of degrees of freedom is $n+(n-1)+\dots+1+0=\frac{n(n+1)}2$.

\item Take $m=n+1$ for simplicity of notation.  The map $A\mapsto AA^T$ is a smooth map from $m\times m$ matrices to $m\times m$ symmetric matrices, and its differential is surjective at every matrix $A$ which maps to $I_m$ (i.e., every orthogonal matrix).  The domain has $m^2$ degrees of freedom for obvious reasons.  The symmetric matrices have $\frac{m(m+1)}2$ degrees of freedom, because you can choose any elements for the upper triangular part including the diagonal, and then the rest is determined.  Hence, since $I_m$ is a regular value, its inverse image \---- which is $O(m)$ \---- is a regular manifold of $m^2-\frac{m(m+1)}2=\frac{m(m-1)}2=\frac{n(n+1)}2$ dimensions.
\end{itemize}
Since projective space has $n^2+2n=n(n+2)$ degrees worth of isometries, and elliptic space has $\frac{n(n+1)}2$, we conclude that the preserving the metric takes away $[n^2+2n]-\frac{n(n+1)}2=\frac{n^2+3n}2=\frac 12n(n+3)$ degrees of freedom.  Hence, whether a projective transformation preserves the metric can be characterized by $\frac 12n(n+3)$ equations.  It is an extremely relevant study to see what kinds of equations they are, but that is a topic beyond the scope of this book.

\subsection*{Exercises 5.6. (Relation to Projective Space: Elliptic Geometry)} % Identify antipodes together to get a new kind of geometry, where
% two lines intersect exactly *once*.  Realize that this is projective space but with a metric equipped, and restrictions on isometries.
\begin{enumerate}
\item (a) Let $(x_1,y_1),(x_2,y_2)\in\mathbb R^2$, viewed as points in the gnomonic projection of the elliptic plane.  Show that their distance is
$$\cos^{-1}\frac{|x_1x_2+y_1y_2+1|}{\sqrt{x_1^2+y_1^2+1}\sqrt{x_2^2+y_2^2+1}}$$
[Convert to the sphere.]  In particular, the distance from $(x,y)$ to the origin is $\cos^{-1}\frac 1{\sqrt{x^2+y^2+1}}$.

(b) Let $(u_1,\dots,u_n),(v_1,\dots,v_n)\in\mathbb R^n$, viewed as points in the gnomonic projection of elliptic $n$-space.  What is their distance?

\item (a) If $p\in E^2(\mathbb R)$, the set of points whose distance from $p$ is equal to $\pi/2$ is a line.  Moreover, when (and only when) a line segment from $p$ goes past this line, it stops being the shortest segment between the points.  [This line is called the \textbf{cut locus} of $p$.]

(b) More generally, if $p\in E^n(\mathbb R)$, the set of points whose distance from $p$ is equal to $\pi/2$ is a hyperplane in $E^n(\mathbb R)$.  [Again called the \textbf{cut locus}.] % It's "hyperplane" for the same reason it's "line" in part (a).

(c) The cut locus of $\vec p\in S^n(\mathbb R)\subset\mathbb R^{n+1}$ is $S^n(\mathbb R)\cap V$, where $V$ is the orthogonal complement of the span of $\vec p$.

\item In the gnomonic projection of $E^n(\mathbb R)$, a projective transformation $T$ is an isometry if and only if it preserves cut loci, i.e., if $C$ is the cut locus of $p$, then $T(C)$ is the cut locus of $T(p)$.

\item In the gnomonic projection of $E^2(\mathbb R)$, when is the center of a circle the origin?  When is it a point at infinity?

\item In $E^2(\mathbb R)$, each of the triangle patterns $[2,3,3],[2,3,4],[2,3,5]$ still exist.  However, they have $12$, $24$ and $60$ triangles respectively, half of what they would have in the spherical case.  [Remember, antipodes are identified.]

Here is an illustration of the $[2,3,5]$ tiling in the gnomonic projection:
\begin{center}
\includegraphics[scale=.2]{EllipPattern235.png}
\end{center}
\item Every Platonic solid except the tetrahedron can be constructed in $E^2(\mathbb R)$, but it will have half as many faces as in the spherical case.

\item Which Archimedean and Catalan solids can be carried over to $E^2(\mathbb R)$?

\item Is gnomonic projection conformal?  [Do not be fooled just because it sends lines to projective lines!]
\end{enumerate}







































\chapter{Introduction to Differential Geometry} %\emph{The material and exposition for this chapter follows Do Carmo, M.P., \emph{Differential Geometry of Curves \& Surfaces}, Second Edition, Chs. 1-4.}\\

\noindent % Opportunity: bring up some of the material from Fall 2019's graduate course!  It'll help generalize spherical, Euclidean, hyperbolic spaces in all dimensions.
In the previous chapters of this book, we have studied various types of ``homogeneous'' geometry: Euclidean, projective, hyperbolic and spherical.  We have defined them purely algebraically, which made lines and circles, etc.~straightforward and easy to define; but this has its unsatisfying limitations.  It is difficult to generalizing the space.  The notion of the length of a curve segment is undefined, unless the curve is from a line.  Third, the area of a region is undefined (in the three kinds of geometry, we gave our own definitions of triangle area, but they are impractical since they do not generalize to arbitrary regions).

In this final chapter, we will study geometric spaces using analysis (calculus), which will enable us to make these generalizations.  We will start by considering regular curves and surfaces in $\mathbb R^3$, and learn how to deal with them by parametrizing them and using a few pieces of equipment (no reference to the ambient 3-space).  Then we will go into further detail about regular surfaces, and establish the ``straight lines'' on them (which are technically called \textbf{geodesics}).  The Gaussian curvature of a surface will also be covered in Proposition 6.24, and in Corollary 6.29 we will relate this to the sum of the angle measures of a triangle (compares Propositions 2.11, 4.8 and 5.4). % I wouldn't say it's analysis.  Analysis means you're really going rigorous about continuity and (e.g.) proving that certain things are equal by showing they're within a distance of an arbitrary close number.  This chapter uses calculus significantly without going into that rigor.

Finally, Section 6.10 will cover how to customize a surface (presumably an open set of $\mathbb R^2$) with any metric which need not be the usual Euclidean one.  This will provide new ways of viewing the Poincar\'e disk, half-plane and Beltrami-Klein models of the hyperbolic plane, as well as the stereographic projection and Lambert azimuthal equal-area projection models of the spherical plane.

Due to the vast generality of the concepts, it is fairly difficult for beginners to study just the two-dimensional case, and plainly more convoluted if we generalize to higher dimensions at once.  A few concepts barely even generalize without extra assumptions being imposed.  Thus, each section of this chapter (except the first) sticks to the two-dimensional case, but has guided exercises which transfer the material to higher dimensions.

\emph{Throughout this chapter, ``differentiable'' and ``smooth'' shall mean infinitely differentiable, also known as $\mathcal C^\infty$.}% "We will specify if it means anything else."

\subsection*{6.1. Regular Curves.  Frenet Trihedron}
\addcontentsline{toc}{section}{6.1. Regular Curves.  Frenet Trihedron}
To get a good general feel of how we will do things, it is useful to start with regular curves.  Not only are they a warmup, but we will use them when we deal with surfaces.

Think of a particle moving in space along a smooth path which we have chosen.  It may twist around, or it may stay in one plane.  It may go in a straight line, but because it is differentiable, it cannot have corners.  It may have either limited or unlimited length.

To make this notion rigorous, we let $I$ be an open interval of $\mathbb R$, which could be any of the following things:
$$(a,b)\text{ with }a<b;~~~~(a,\infty);~~~~(-\infty,b);~~~~(-\infty,\infty)=\mathbb R.$$
Then a \textbf{curve} in $\mathbb R^n$ is defined as a differentiable map $\alpha:I\to\mathbb R^n$.  The parameter in $I$ is commonly referred to as $t$ for ``time.''  \\

\noindent\textbf{Examples.}

(1) Basic examples are the curves that have generally been dealt with in Chapters 2-5; i.e., lines, circles and conics.  For example, $\alpha(t)=(2t,t+1)$ parametrizes the line $x-2y=-2$, as shown on the left; and $\alpha(t)=(3\cos t,2\sin t)$ parametrizes the ellipse $\frac{x^2}9+\frac{y^2}4=1$, as shown on the right.
\begin{center}
\includegraphics[scale=.3]{ParamLine.png}~~~~
\includegraphics[scale=.3]{ParamEllipse.png}
\end{center}
This can be seen by plugging in the formulas for $x$ and $y$ in terms of $t$: if $(x,y)=(2t,t+1)$ then $x-2y=2t-2(t+1)=2t-(2t+2)=-2$, and if $(x,y)=(3\cos t,2\sin t)$ then $\frac{x^2}9+\frac{y^2}4=\frac{(3\cos t)^2}9+\frac{(2\sin t)^2}4=\frac{9\cos^2t}9+\frac{4\sin^2t}4=\cos^2t+\sin^2t=1$.

We know $\alpha(t)=(\cos t,\sin t)$ parametrizes the unit circle, and $\alpha(t)=(\cosh t,\sinh t)$ parametrizes a branch of a hyperbola.  [The entire hyperbola can be parametrized via $\alpha(t)=(\sec t,\tan t)$ with $t\notin\left\{\frac{\pi}2+n\pi:n\in\mathbb Z\right\}$, but we shall not deal with curves whose domains are disconnected.]\\

(2) Sometimes we will take only a certain interval for the domain instead of all of $\mathbb R$.  For instance, the line segment from $(-2,0)$ to $(4,3)$ can be parametrized as $\alpha(t)=(2t,t+1)$, for $t\in[-1,2]$.  Also, if you parametrize $\alpha(t)=(\cos t,\sin t)$ only for $t\in[0,\pi]$, you get the semicircle arc in the upper half of the plane.

You may have noticed that we asked for the domain $I$ to be an \emph{open} interval, but we're using closed intervals in this example.  The fact of the matter is that the closed intervals make it easier to connect to earlier in the book (because, for instance, line segments in Section 2.1 were considered to include their endpoints).  However, in differential geometry, we want to be able to differentiate things at any point, and this can only be done if the function is defined on an open neighborhood of the point.\\

(3) Here is an example of a curve which may not seem familiar to you.  The cycloid is given by $\alpha(t)=(t-\sin t,1-\cos t)$:
\begin{center}
\includegraphics[scale=.3]{Cycloid1.png}
\end{center}
It is the locus of a point on a circle of radius $1$ rolling on the $x$-axis without slipping:
\begin{center}
\includegraphics[scale=.3]{Cycloid1_with_circle.png}
\end{center}
You can get different curves by taking the locus of a point \emph{inside} or \emph{outside} the circle as well.  Examples of these are $\left(t-\frac 12\sin t,1-\frac 12\cos t\right)$ and $\left(t-\frac 32\sin t,1-\frac 32\cos t\right)$, which look like:
\begin{center}
\includegraphics[scale=.3]{Cycloid2.png}~~~~\includegraphics[scale=.3]{Cycloid3.png}
\end{center}
Notice how the one on the right has loops, and the one on the left is simple.  It is an interesting exercise to try to figure out why this is so exactly.\\

(4) Curves exist in higher dimensions as well.  For example, if $a,b>0$, there is the curve $\alpha:\mathbb R\to\mathbb R^3$ given by $\alpha(t)=(a\cos t,a\sin t,bt)$.  This is called the \textbf{helix}.  There is also the \textbf{moment curve} $\alpha(t)=(t,t^2,t^3)$.\\

\noindent It is tempting to think that a curve in $\mathbb R^n$ must be a smooth path without any corners along the way.  However, without further assumptions, this may not be the case.  Consider, for instance, the curve $\alpha:\mathbb R\to\mathbb R^2$ given by $\alpha(t)=(t^3,t^2)$; this is what it looks like:
\begin{center}
\includegraphics[scale=.3]{SingularPoint.png}
\end{center}
This curve is not smooth at the origin; it has a visible corner.  The cycloid from Example 3 also has this issue, (unlike the other two curves in the example).  This is a curious phenomenon to the reader who notices that all of the coordinates are still differentiable, so what is causing it?

Let us take a look at the derivative $\alpha'(t)$ at each of the ``corner'' times.  For the curve $\alpha(t)=(t^3,t^2)$, we have $\alpha'(t)=(3t^2,2t)$ so that $\alpha'(0)=(0,0)$.  Similarly, for $\alpha(t)=(t-\sin t,1-\cos t)$, the corners occur when $t=2\pi n$ for some $n\in\mathbb Z$, and $\alpha'(t)=(1-\cos t,\sin t)$ which is zero if and only if $t=2\pi n$.

This suggests that sharp corners can only occur at points where $\alpha'(t)=\vec 0$.  This is indeed true: for if $\alpha'(t)\ne\vec 0$, then it is a tangent vector to the curve at $\alpha(t)$, and no parts of the curve can go in any other directions.  If a part of a curve went in another direction, its starting velocity would be a vector in that direction, and this is a contradiction if the velocity is nonzero because then it is contained in only one line.  Thus, it seems reasonable to demand that the derivative $\alpha'(t)$ never be zero.\footnote{If $\alpha'(t)\ne\vec 0$ then the curve is nonsingular (i.e., traveling in one pair of opposite directions) at $t$, but not conversely: the curve $\alpha:\mathbb R\to\mathbb R^2$ given by $\alpha(t)=(t^3,t^3)$ has a zero derivative at $t=0$, yet its image is the line $y=x$, which is nonsingular.}  We would like to stick with curves satisfying this particular property, so we can avoid the anomaly of a corner or cusp.  Thus we give these curves a definition.\\

\noindent\textbf{Definition.} \emph{If $I$ is an open interval in $\mathbb R$, a \textbf{(parametrized) regular curve} is defined as a differentiable map $\alpha:I\to\mathbb R^n$, such that $\alpha'(t)\ne\vec 0$ for all $t\in I$.  $t$ is called the \textbf{parameter}, and the points $\alpha(t)$ are called the \textbf{values} of the curve.}\\

\noindent Thus a regular curve is a smooth path, even though it may have self-intersections.  Its \textbf{velocity} is the vector $\alpha'(t)$ (which we always assume nonzero), and its \textbf{speed} is the magnitude $\|\alpha'(t)\|$ of the velocity.  The \textbf{acceleration} of $\alpha$ is its second derivative, $\alpha''(t)$, which may be zero, unlike the velocity [e.g., take $\alpha(t)=(2t,t+1)$ from Example 2].

These terms may be familiar from an elementary introduction to physics.  However, we are more interested in the curve itself than the time parameter used to form it.  Thus, our ambition in the rest of this section is to study the intrinsics of the curve, which have nothing to do with $t$.

The first fundamental step is to find a canonical way to parametrize curves which does not rely on any particular time parameter.  To do this, we fix $t_0\in I$, and define
$$\varphi(t)=\int_{t_0}^t\|\alpha'(u)\|\,du.$$
Then $\varphi$ is a diffeomorphism from $I$ to some interval.  Indeed, $\varphi(t_0)=0$ and $\varphi'(t)=\|\alpha'(t)\|$ (by the first fundamental theorem of calculus), which is strictly positive (since $\alpha$ is regular, and hence $\alpha'(u)\ne\vec 0$).  It follows that, $\varphi$ is an increasing function from $I$ to some interval $J\subset\mathbb R$, and we may let $\psi:J\to I$ be its inverse, which is also a differentiable increasing function (why?).  We set $\alpha_1=\alpha\circ\psi:J\to\mathbb R^n$, which is an alternate way to parametrize the same curve.

Now, what can we say about $\alpha_1'(s)$ for $s\in J$?  Well, first, $\varphi(\psi(s))=s$ for all $s$; differentiating both sides and using the Chain Rule gives $\varphi'(\psi(s))\psi'(s)=1$, and so $\psi'(s)=\frac 1{\varphi'(\psi(s))}=\frac 1{\|\alpha'(\psi(s))\|}$.  Furthermore, again by the Chain Rule,
$$\alpha_1'(s)=\alpha'(\psi(s))\psi'(s)=\frac{\alpha'(\psi(s))}{\|\alpha'(\psi(s))\|},$$
which is a \emph{unit vector} \---- in fact, it is exactly the normalization of the vector $\alpha'(\psi(s))$, which is the velocity of $\alpha$ on the same point of the curve.  Hence, $\|\alpha_1'(s)\|=1$ for all $s\in J$.

We have just shown that every regular curve can be parametrized so that the speed is equal to $1$ everywhere.  As we will see later, this is actually a convenient assumption on the parametrization, as it both simplifies the study of curve intrinsics, and makes computations less complicated.  These kinds of parametrizations have a special name.\\

\noindent\textbf{Definition.} \emph{A regular curve $\alpha:I\to\mathbb R^n$ is said to be \textbf{parametrized by arc length} (or to have \textbf{unit speed}) provided that $\|\alpha'(s)\|=1$ for all $s\in I$.  In this case, the parameter in $I$ is usually denoted as $s$ instead of $t$.}\\

\noindent The name ``arc length'' is explained in Exercise 3, which shows that $\varphi(t)$ is actually the length of the curve from $\alpha(t_0)$ to $\alpha(t)$.

Using the preceding argument, it is easy to get examples of arc length parametrizations of curves.  An example is the unit circle $\alpha(s)=(\cos s,\sin s)$, because $\|\alpha'(s)\|=\|(-\sin s,\cos s)\|=\sin^2s+\cos^2s=1$.  For general $r>0$, the circle of radius $r$ can be parametrized via $\alpha(t)=(r\cos t,r\sin t)$, but this is not an arc length parametrization unless $r=1$, because $\|\alpha'(t)\|=r$.  However, the circle of radius $r$ does have another parametrization by arc length:
$$\alpha(s)=(r\cos(s/r),r\sin(s/r)).$$
This is an arc length parametrization because $\frac d{ds}\left(r\sin\frac sr\right)=r\frac 1r\cos\frac sr=\cos\frac sr$, and $\frac d{ds}\left(r\cos\frac sr\right)$ can be found similarly; hence $\alpha'(s)=(-\sin(s/r),\cos(s/r))$, a unit vector.  Note that $\alpha$ is periodic with period $2\pi r$, the circumference of the circle.

We first claim that arc length parametrizations have an essential uniqueness property.  For the purpose of this, we define a \textbf{reparametrization} of $\alpha$ as a composition $\alpha\circ\psi$ where $\psi:J\to I$ is a differentiable bijection with strictly positive derivative.\\

\noindent\textbf{Proposition 6.1.} \emph{Every regular curve $\alpha:I\to\mathbb R^n$ has a reparametrization by arc length.  If, moreover, $\alpha_1:J_1\to\mathbb R^n$ and $\alpha_2:J_2\to\mathbb R^n$ are two such reparametrizations, there exists $c\in\mathbb R$ such that $J_2=\{r+c:r\in J_1\}$ and $\alpha_1(s)=\alpha_2(s+c)$ for all $r\in J_1$.}
\begin{proof}
We have already seen an example of such a reparametrization: fix $t_0\in I$ and let $\varphi(t)=\int_{t_0}^t\|\alpha'(u)\|\,du$; then $\alpha\circ\varphi^{-1}$ is a reparametrization by arc length.  Now suppose $\alpha_1:J_1\to\mathbb R^n$ and $\alpha_2:J_2\to\mathbb R^n$ are two reparametrizations of $\alpha$, say $\alpha_i=\alpha\circ\psi_i$ for $j=1,2$.  Then since the $\psi_i:J_i\to I$ are bijections with strictly positive derivative, so is
$$\zeta=\psi_2^{-1}\circ\psi_1:J_1\to J_2.$$
By construction, $\psi_2\circ\zeta=\psi_1$, and hence $\alpha_2\circ\zeta=\alpha_1$.  Moreover, differentiating $\alpha_2(\zeta(s))=\alpha_1(s)$ for $s\in J_1$ entails
$$\alpha_1'(s)=\alpha_2'(\zeta(s))\zeta'(s);$$
since $\|\alpha_1'(s)\|=\|\alpha_2'(\zeta(s))\|=1$ [$\alpha_1,\alpha_2$ are arc length parametrizations], taking the magnitude of both sides entails $|\zeta'(s)|=1$.  Therefore, $\zeta'(s)=1$ for all $s\in J_1$ (because it is strictly positive).  Elementary integration then shows that there exists $c\in\mathbb R$ such that $\zeta$ is the map $s\mapsto s+c$.  Since $\zeta$ is a bijection $J_1\to J_2$, it follows that $J_2=\{r+c:r\in J_1\}$, and $\alpha_1(s)=\alpha_2(s+c)$ by construction.
\end{proof}

\noindent Despite what has just been proven, there are parametrizations of the curve which are not technically reparametrizations; they traverse the curve in the opposite direction.  Specifically, if $\alpha:I\to\mathbb R^n$, one can define $I_-=\{a\in\mathbb R:-a\in I\}$ and then define $\alpha_-:I_-\to\mathbb R^n$ via $\alpha_-(t)=\alpha(-t)$.  By elementary calculus, $\alpha_-'(t)=-\alpha'(-t)$, and so $\|\alpha_-'(t)\|=\|\alpha'(t)\|$; hence $\alpha_-$ is an arc length parametrization if and only if $\alpha$ is.

If $\sigma:I_-\to I$ is the map $t\mapsto -t$, then $\sigma$ is differentiable with a strictly \emph{negative} derivative, and $\alpha_-=\alpha\circ\sigma$.  Thus, $\alpha_-$ is parametrizes the same curve, but as if it were going the other way.  The direction in which a curve travels (technically called its \emph{orientation}) will be needed for most of the results in the rest of this section to be rigorous, hence we will only admit ``positive'' reparametrizations which keep the curve going in the same direction.\\

\noindent\textbf{CURVATURE AND TORSION OF A CURVE}\\

\noindent For the rest of this section, we shall focus on regular curves in $\mathbb R^3$, where they can be visualized easily.  We let $\alpha:J\to\mathbb R^3$ be a regular curve parametrized by arc length, and construct an orthonormal basis of $\mathbb R^3$ which varies differentiably with the parameter $s\in J$.  We will then use it to define the curvature, which indicates \emph{curviness} of the curve, or the strength at which it curves (see Exercises 9-12 and 16-17 below).  We will also use it to define the torsion, which indicates \emph{twisting} of the curve, or how far it is from being a plane curve (see Exercises 8 and 17 below).

We start by taking $\boldsymbol\tau(s)=\alpha'(s)$.  This is a unit vector, since $\alpha$ is an arc length parametrization.  Hence $\boldsymbol\tau(s)$ is a perfectly good first vector for our orthonormal basis.  For the second vector, we note that by Exercise 4(b), $\boldsymbol\tau(s)\cdot\boldsymbol\tau'(s)=0$.  Thus we may define $\boldsymbol\nu(s)=\frac{\boldsymbol\tau'(s)}{\|\boldsymbol\tau'(s)\|}$ (unless $\boldsymbol\tau'(s)=\vec 0$, which we will assume is not the case).  Given this data, $\boldsymbol\tau,\boldsymbol\nu$ are orthonormal.  Finally, we take the third vector, $\boldsymbol\beta(s)$ to be the cross product, $\boldsymbol\tau(s)\times\boldsymbol\nu(s)$.  Then $\{\boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta\}$ is a positive orthonormal basis (which varies with $s$), by Exercise 5.  This is called the \textbf{Frenet trihedron} (or \textbf{Frenet frame}) of the space curve; they are to be visualized at $\alpha(s)$ as $s$ varies.

$\boldsymbol\tau(s)$ is called the \textbf{tangential unit vector} to the curve at $\alpha(s)$ (for clear reasons), $\boldsymbol\nu(s)$ is called the \textbf{normal unit vector} and $\boldsymbol\beta(s)$ is called the \textbf{binormal vector}.  We shall study how they vary with respect to arc length, as that will effectively tell us some of the structures of the curve.

The first important observation comes from the orthonormality of the three vectors:
\begin{equation}\tag{1}\boldsymbol\tau\cdot\boldsymbol\tau=1,~~~~\boldsymbol\nu\cdot\boldsymbol\nu=1,~~~~\boldsymbol\beta\cdot\boldsymbol\beta=1,\end{equation}
$$\boldsymbol\tau\cdot\boldsymbol\nu=0,~~~~\boldsymbol\tau\cdot\boldsymbol\beta=0,~~~~\boldsymbol\nu\cdot\boldsymbol\beta=0.$$
Differentiating each equation in (1) with respect to $s$, and using Exercise 4, gives
\begin{equation}\tag{2}\boldsymbol\tau\cdot\boldsymbol\tau'=0,~~~~\boldsymbol\nu\cdot\boldsymbol\nu'=0,~~~~\boldsymbol\beta\cdot\boldsymbol\beta'=0,\end{equation}
$$\boldsymbol\tau'\cdot\boldsymbol\nu+\boldsymbol\tau\cdot\boldsymbol\nu'=0,~~~~\boldsymbol\tau'\cdot\boldsymbol\beta+\boldsymbol\tau\cdot\boldsymbol\beta'=0,~~~~\boldsymbol\nu'\cdot\boldsymbol\beta+\boldsymbol\nu\cdot\boldsymbol\beta'=0.$$
We define $\kappa:J\to\mathbb R$ by $\kappa(s)=\|\boldsymbol\tau'(s)\|=\|\alpha''(s)\|$; then $\boldsymbol\tau'(s)=\kappa(s)\boldsymbol\nu(s)$ by how we defined $\boldsymbol\nu(s)$ earlier.  $\kappa(s)$ is a positive differentiable function (wherever $\boldsymbol\tau'(s)\ne\vec 0$), called the \textbf{curvature} of the curve.  From this we conclude that $\boldsymbol\tau'\cdot\boldsymbol\beta=\kappa(\boldsymbol\nu\cdot\boldsymbol\beta)=0$, and hence $\boldsymbol\tau\cdot\boldsymbol\beta'=0$ by (2).

As $\boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta$ is an orthonormal basis, we have, for any $\mathbf x\in\mathbb R^3$,
\begin{equation}\tag{3}\mathbf x=(\mathbf x\cdot\boldsymbol\tau)\boldsymbol\tau+(\mathbf x\cdot\boldsymbol\nu)\boldsymbol\nu+(\mathbf x\cdot\boldsymbol\beta)\boldsymbol\beta,\end{equation} % Re. the difficulty of distinguishing \tau from \boldsymbol\tau, the reader should be able to infer whether each \tau is a vector or a scalar (e.g., we can't have vector\cdot scalar).  Also, I didn't find such a symbol in https://detexify.kirelabs.org/classify.html
because you can write $\mathbf x$ as a linear combination of the basis vectors, and then taking dot products of both sides instantly reveal what the coefficients are.  In particular, taking $\mathbf x=\boldsymbol\beta'$, we have $\boldsymbol\beta'=(\boldsymbol\beta'\cdot\boldsymbol\tau)\boldsymbol\tau+(\boldsymbol\beta'\cdot\boldsymbol\nu)\boldsymbol\nu+(\boldsymbol\beta'\cdot\boldsymbol\beta)\boldsymbol\beta=(\boldsymbol\beta'\cdot\boldsymbol\nu)\boldsymbol\nu$, since $\boldsymbol\beta'\cdot\boldsymbol\tau=\boldsymbol\beta'\cdot\boldsymbol\beta=0$.  We define $\tau(s)=-\boldsymbol\beta'(s)\cdot\boldsymbol\nu(s)$, from which we have $\boldsymbol\beta'(s)=-\tau(s)\boldsymbol\nu(s)$.  $\tau(s)$ is called the \textbf{torsion} of the curve.

The equations (2) imply $\boldsymbol\nu'\cdot\boldsymbol\beta=-\boldsymbol\nu\cdot\boldsymbol\beta'=\tau(s)$, and $\boldsymbol\nu'\cdot\boldsymbol\tau=-\boldsymbol\nu\cdot\boldsymbol\tau'=-\boldsymbol\nu\cdot(\kappa\boldsymbol\nu)=-\kappa$.  Taking $\mathbf x=\boldsymbol\nu'$ in (3) then yields $\boldsymbol\nu'(s)=-\kappa(s)\boldsymbol\tau(s)+\tau(s)\boldsymbol\beta(s)$.

Thus we have expressed the derivative of each basis vector with respect to the basis itself:
$$\boldsymbol\tau'(s)=\kappa(s)\boldsymbol\nu(s)$$
$$\boldsymbol\nu'(s)=-\kappa(s)\boldsymbol\tau(s)+\tau(s)\boldsymbol\beta(s)$$
$$\boldsymbol\beta'(s)=-\tau(s)\boldsymbol\nu(s)$$
These are called the \textbf{Frenet equations}, and they can be rephrased in this matrix form:
$$\begin{bmatrix}\leftarrow\boldsymbol\tau'\rightarrow\\\leftarrow\boldsymbol\nu'\rightarrow\\\leftarrow\boldsymbol\beta'\rightarrow\end{bmatrix}=\begin{bmatrix}0&\kappa&0\\-\kappa&0&\tau\\0&-\tau&0\end{bmatrix}\begin{bmatrix}\leftarrow\boldsymbol\tau\rightarrow\\\leftarrow\boldsymbol\nu\rightarrow\\\leftarrow\boldsymbol\beta\rightarrow\end{bmatrix}$$
Here $\boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta$ and their derivatives are viewed as row vectors, which is what the arrows $\leftarrow$ and $\rightarrow$ mean.

Using these relations, we can show that given any differentiable functions $\kappa$ and $\tau$ on the arc length parameter, there is a curve with curvature $\kappa$ and torsion $\tau$, which is unique up to rigid motions.  However, this fact depends on the fundamental theorem of differential equations, which is beyond the scope of this book.  Since this fundamental theorem will spark significant results several times in the chapter, we will state it here without proof.\\

\noindent\textbf{Theorem 6.2.} \textsc{(Fundamental Theorem of Differential Equations)} \emph{If $f_i(t,x_1,\dots,x_n),1\leqslant i\leqslant n$ are differentiable functions which are defined for $t$ in an open interval $J$, and $t_0$ is a fixed element of $J$, then the system of differential equations}
$$\frac{dx_1}{dt}=f_1(t,x_1,x_2,\dots,x_n)$$
$$\frac{dx_2}{dt}=f_2(t,x_1,x_2,\dots,x_n)$$
$$\vdots$$
$$\frac{dx_n}{dt}=f_n(t,x_1,x_2,\dots,x_n)$$
\begin{center}
\emph{with given initial conditions $x_i(t_0)=a_i$ ($a_i\in\mathbb R$)}
\end{center}
\emph{has a unique solution in some open neighborhood of $t_0$ in $J$.  If, moreover, the system is linear; i.e., we have}
$$f_i(t,x_1,x_2,\dots,x_n)=a_{i1}(t)x_1+\dots+a_{in}(t)x_n+a_i(t)$$
\emph{for some differentiable functions $a_{ij},a_i$, then the solution exists throughout $J$.}\\

\noindent For a proof, see A.L.~Cauchy and G.~Peano [Peal], \'E.~Picard [Pi] or E.~Lindel\"of [Lind1, Lind2]. %https://link.springer.com/chapter/10.1007/978-1-4612-1506-6_1

It is readily seen that elements of $\operatorname{Isom}^+(\mathbb R^3)$ keep $\kappa$ and $\tau$ unchanged, as one can verify (using, e.g., the linearity of the derivative) that $[A,\vec v]\in\operatorname{Isom}^+(\mathbb R^3)$ merely applies the underlying $A\in SO(3)$ to the Frenet trihedron.  However, if you apply an orientation-\emph{reversing} isometry of $\mathbb R^3$ to the curve, $\kappa$ stays the same and $\tau$ becomes negative.\\

\noindent\textbf{Proposition 6.3.} \emph{Let $J\subset\mathbb R$ be an open interval, and $\kappa,\tau:J\to\mathbb R$ differentiable functions with $\kappa(s)>0$ for all $s\in J$.  Then there exists a curve $\alpha:J\to\mathbb R^3$ parametrized by arc length, with curvature $\kappa$ and torsion $\tau$.  Any two such curves are related by an element of $\operatorname{Isom}^+(\mathbb R^3)$.}
\begin{proof}
Let us first write the Frenet trihedron (the one for the curve $\alpha$ that we wish to define) in the form of scalar functions:
$$\boldsymbol\tau(s)=(\tau_1(s),\tau_2(s),\tau_3(s))$$
$$\boldsymbol\nu(s)=(\nu_1(s),\nu_2(s),\nu_3(s))$$
$$\boldsymbol\beta(s)=(\beta_1(s),\beta_2(s),\beta_3(s))$$
Consider the Frenet equations
$$\begin{bmatrix}\leftarrow\boldsymbol\tau'\rightarrow\\\leftarrow\boldsymbol\nu'\rightarrow\\\leftarrow\boldsymbol\beta'\rightarrow\end{bmatrix}=\begin{bmatrix}0&\kappa&0\\-\kappa&0&\tau\\0&-\tau&0\end{bmatrix}\begin{bmatrix}\leftarrow\boldsymbol\tau\rightarrow\\\leftarrow\boldsymbol\nu\rightarrow\\\leftarrow\boldsymbol\beta\rightarrow\end{bmatrix}.$$
They can be rephrased as a system of differential equations in the nine scalar functions given above,
\begin{equation}\tag{A}\tau_1'(s)=\kappa(s)\nu_1(s),~~~~\tau_2'(s)=\kappa(s)\nu_2(s),~~~~\tau_3'(s)=\kappa(s)\nu_3(s),\end{equation}
$$\nu_1'(s)=-\kappa(s)\tau_1(s)+\tau(s)\beta_1(s),~~~~\nu_2'(s)=-\kappa(s)\tau_2(s)+\tau(s)\beta_2(s),$$
$$\nu_3'(s)=-\kappa(s)\tau_3(s)+\tau(s)\beta_3(s),$$
$$\beta_1'(s)=-\tau(s)\nu_1(s),~~~~\beta_2'(s)=-\tau(s)\nu_2(s),~~~~\beta_3'(s)=-\tau(s)\nu_3(s).$$
Let us fix $s_0\in J$ and impose initial conditions via $\boldsymbol\tau(s_0)=\vec e_1,\boldsymbol\nu(s_0)=\vec e_2,\boldsymbol\beta(s_0)=\vec e_3$.  In other words, $\tau_1(s_0)=\nu_2(s_0)=\beta_3(s_0)=1$ and the other six functions send $s_0\mapsto 0$.  By Theorem 6.2, this system has a unique solution, which is defined throughout $J$ since the system is linear.  Such a solution then becomes a trio $\boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta$ of vector-valued functions $J\to\mathbb R^3$, satisfying the Frenet equations and the initial conditions.

However, we do not yet know if $\boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta$ are orthonormal vectors; so far we only know they are orthonormal at $s=s_0$, where they are the standard basis vectors.  But we can easily use this fact.  Define functions $\delta_i:J\to\mathbb R$ via
$$\delta_1:s\mapsto\boldsymbol\tau(s)\cdot\boldsymbol\tau(s),~~~~\delta_2:s\mapsto\boldsymbol\nu(s)\cdot\boldsymbol\nu(s),~~~~\delta_3:s\mapsto\boldsymbol\beta(s)\cdot\boldsymbol\beta(s),$$
$$\delta_4:s\mapsto\boldsymbol\tau(s)\cdot\boldsymbol\nu(s),~~~~\delta_5:s\mapsto\boldsymbol\nu(s)\cdot\boldsymbol\beta(s),~~~~\delta_6:s\mapsto\boldsymbol\tau(s)\cdot\boldsymbol\beta(s).$$
Then using the Frenet equations, one can readily see that the $\delta_i$ are a solution to the following system of differential equations with initial conditions:
$$\frac{d\zeta_1}{ds}=2\kappa(s)\zeta_4(s),~~~~\frac{d\zeta_2}{ds}=2(-\kappa(s)\zeta_4(s)+\tau(s)\zeta_5(s)),~~~~\frac{d\zeta_3}{ds}=-2\tau(s)\zeta_5(s),$$
$$\frac{d\zeta_4}{ds}=\kappa(s)[\zeta_2(s)-\zeta_1(s)]+\tau(s)\zeta_6(s),~~~~\frac{d\zeta_5}{ds}=-\kappa(s)\zeta_6(s)+\tau(s)[\zeta_3(s)-\zeta_2(s)],$$
$$\frac{d\zeta_6}{ds}=\kappa(s)\zeta_5(s)-\tau(s)\zeta_4(s),$$
$$\zeta_1(s_0)=\zeta_2(s_0)=\zeta_3(s_0)=1,~~~~\zeta_4(s_0)=\zeta_5(s_0)=\zeta_6(s_0)=0.$$
But the constant functions $\zeta_1(s)=\zeta_2(s)=\zeta_3(s)=1,\zeta_4(s)=\zeta_5(s)=\zeta_6(s)=0$ clearly satisfy this system as well.  Therefore the uniqueness part of Theorem 6.2 entails that the $\delta_i$ are said constant functions, and so $\boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta$ are orthonormal throughout $J$.  They also form a \emph{positive} orthonormal basis, because $\boldsymbol\tau\cdot(\boldsymbol\nu\times\boldsymbol\beta)$ is a smooth function on $J$, taking only the values $\pm 1$, and sending $s_0\mapsto 1$; hence it must be $1$ everywhere.

Now define a curve $\alpha:J\to\mathbb R^3$ via
$$\alpha(s)=\int_{s_0}^s\boldsymbol\tau(u)\,du.$$
Then $\alpha'(s)=\boldsymbol\tau(s)$ by the first fundamental theorem of calculus, and hence $\alpha(s)$ is an arc length parametrization (because $\|\boldsymbol\tau(s)\|=1$) and $\boldsymbol\tau(s)$ is its tangential unit vector.  By the Frenet equation $\boldsymbol\tau'(s)=\kappa(s)\boldsymbol\nu(s)$ and the hypothesis $\kappa(s)>0$, we get that $\|\boldsymbol\tau'(s)\|=\kappa(s)$, and hence $\kappa(s)$ is the curvature of the curve.  Moreover, $\boldsymbol\nu(s)=\frac{\boldsymbol\tau'(s)}{\kappa(s)}=\frac{\boldsymbol\tau'(s)}{\|\boldsymbol\tau'(s)\|}$, hence is the normal unit vector.  Since $\{\boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta\}$ is a positive orthonormal basis, we have $\boldsymbol\beta(s)=\boldsymbol\tau(s)\times\boldsymbol\nu(s)$, hence is the binormal unit vector.  Finally, since $\boldsymbol\beta'(s)=-\tau(s)\boldsymbol\nu(s)$, we have $\tau(s)=-\boldsymbol\beta'(s)\cdot\boldsymbol\nu(s)$ by orthonormality, and so $\tau(s)$ is the torsion of the curve.

This proves the existence of the curve $\alpha$.  Now suppose $\alpha_1:J\to\mathbb R$ is another curve parametrized by arc length, with the same curvature $\kappa$ and torsion $\tau$.  Let $A$ be the matrix $\begin{bmatrix}\leftarrow\boldsymbol\tau(s_0)\rightarrow\\\leftarrow\boldsymbol\nu(s_0)\rightarrow\\\leftarrow\boldsymbol\beta(s_0)\rightarrow\end{bmatrix}$, and let $\vec v=-A(\alpha_1(s_0))$.  Then $A\in SO(3)$, and the isometry $[A,\vec v]\in\operatorname{Isom}^+(\mathbb R^3)$ passes the curve $\alpha_1$ to a curve $\alpha_2:J\to\mathbb R$, also with curvature $\kappa$ and torsion $\tau$, and with the same initial conditions $\boldsymbol\tau(s_0)=\vec e_1,\boldsymbol\nu(s_0)=\vec e_2,\boldsymbol\beta(s_0)=\vec e_3$.  Note also $\alpha(s_0)=\alpha_2(s_0)=0$.  We claim that $\alpha_2=\alpha$, and that will imply that $\alpha,\alpha_1$ are related by an element of $\operatorname{Isom}^+(\mathbb R^3)$.

The trick is to notice that the Frenet trihedra of $\alpha$ and $\alpha_2$ both satisfy the \emph{same} differential equations (A), with the same initial conditions at $s_0$ [because they have the same functions for curvature and torsion].  By the uniqueness of solutions to differential equations, the Frenet trihedra of $\alpha$ and $\alpha_2$ coincide.  Hence $\alpha$ and $\alpha_2$ have the same tangential unit vector, so that (since they have the same derivative with respect to $s$), we have $\alpha_2(s)=\alpha(s)+c$ for some fixed $c\in\mathbb R$.  Finally, taking $s=s_0$ we get $0=0+c$, hence $c=0$.  Therefore $\alpha_2=\alpha$, completing the proof.
\end{proof}

\subsection*{Exercises 6.1. (Regular Curves.  Frenet Trihedron)} % This section intends to get the reader used to dealing with a ton of calculus...
% along with a bunch of other obvious factors.  Basically cover regular curves, and the Frenet trihedron.
% Also, bring in the fundamental theorem of differential equations, and cite some source which proves it (proving it in this book would be a complete throw-off).
% Use this to show that genuine functions \kappa,\tau in the arc length s determine the curve up to an element of \operatorname{Isom}^+(\mathbb R^3).
\begin{enumerate}
\item Which of the following functions $\alpha:I\to\mathbb R^n$ are regular curves?

(a) $I=(0,\infty),\alpha(t)=(\sqrt t,t^2)$

(b) $I=\mathbb R,\alpha(t)=(e^t,t^2)$

(c) $I=\mathbb R,\alpha(t)=(t-e^t,t^2,t^3)$

(d) $I=(-1,1),\alpha(t)=(\sin^{-1}t,\cos^{-1}t)$

(e) $I=\mathbb R,\alpha(t)=(2t^3,3t^5,5t^7)$

\item (a) Parametrize the regular curve $\alpha(t)=(7\cos t,7\sin t,3t)$ by arc length.  Sketch this helix.

(b) Parametrize $\alpha(t)=(t,t^2)$ by arc length.  [To integrate $\int_0^t\sqrt{1+4u^2}\,du$, make the substitution $u=\frac 12\sinh v$.]

(c) Parametrize $\alpha(t)=(t,\cosh t)$ by arc length.  [First show that $\|\alpha'(t)\|=\cosh t$.]

\item Let $\alpha:I\to\mathbb R^n$ be a regular curve, and $a,b\in I$.

(a) Show that the length of the curve from $\alpha(a)$ to $\alpha(b)$ is equal to $\int_a^b\|\alpha'(t)\|\,dt$.  [It would help to think of the integral using Riemann approximations.]

(b) If $\alpha$ is parametrized by arc length, the length of the curve from $\alpha(a)$ to $\alpha(b)$ is equal to $b-a$.

\item Let $I$ be an interval, and $\vec v,\vec w:I\to\mathbb R^n$ differentiable vector-valued functions.

(a) Show that $\frac{d}{dt}(\vec v(t)\cdot\vec w(t))=\vec v'(t)\cdot\vec w(t)+\vec v(t)\cdot\vec w'(t)$.

(b) Use part (a) to show that $\|\vec v(t)\|$ is constant if and only if $\vec v(t)\cdot\vec v'(t)=0$ for all $t\in I$.

(c) If $n=3$, show that $\frac{d}{dt}(\vec v(t)\times\vec w(t))=\vec v'(t)\times\vec w(t)+\vec v(t)\times\vec w'(t)$.

\item If $\vec v,\vec w$ are orthonormal vectors in $\mathbb R^3$, show that $\{\vec v,\vec w,\vec v\times\vec w\}$ is a positive orthonormal basis.

\item Let $\alpha:I\to\mathbb R^3$ be a regular curve, not necessarily parametrized by arc length.  Let $\vec v(t)$ be the velocity $\alpha'(t)$.

If $\varphi:I\to J$ is a bijection such that $J\overset{\varphi^{-1}}\to I\overset{\alpha}\to\mathbb R^3$ is a reparametrization by arc length, we shall say $\boldsymbol\tau(t),t\in I$ when we mean the tangential unit vector at $s=\varphi(t)\in J$ [i.e., at the point $\alpha(t)$ on the curve's image]; similarly for $\boldsymbol\nu,\boldsymbol\beta,\kappa,\tau$.

(a) Show that the acceleration $\alpha''(t)$ is equal to $a_T\boldsymbol\tau+a_N\boldsymbol\nu$, where $a_T=\frac d{dt}\|\vec v(t)\|$ and $a_N=\kappa(t)\|\vec v(t)\|^2$.  [Note that $\alpha'(t)=\|\alpha'(t)\|\boldsymbol\tau(t)$ and use the Product Rule for differentiation.  Be careful, however: $\boldsymbol\tau'(t)$ is \emph{not} equal to $\kappa(t)\boldsymbol\nu(t)$; that is the derivative of $\boldsymbol\tau$ with respect to the \emph{arc length parameter $s$}.  But given this, and the fact that $ds/dt=\|\alpha'(t)\|$, one can readily find $\boldsymbol\tau'(t)$ using the Chain Rule.]

$a_T$ is called the \textbf{tangential acceleration} of the parametrization, and $a_N$ is called the \textbf{normal acceleration}.

(b) Conclude that $\|\alpha''(t)\|=\sqrt{a_T^2+a_N^2}$.

(c) What are $a_T$ and $a_N$ if $\alpha$ is an arc length parametrization?

\item Let $\alpha:I\to\mathbb R^3$ be a regular curve, not necessarily parametrized by arc length.  Show that

(a) $\boldsymbol\tau(t)=\frac{\alpha'(t)}{\|\alpha'(t)\|}$.

(b) $\boldsymbol\nu(t)=\frac{1}{\|\boldsymbol\tau'(t)\|}\boldsymbol\tau'(t)$.

(c) $\kappa(t)=\frac 1{\|\alpha'(t)\|}\|\boldsymbol\tau'(t)\|=\frac{\|\alpha'(t)\times\alpha''(t)\|}{\|\alpha'(t)\|^3}$.  [Use Exercise 6.]

(d) Show by example that $\alpha''(t)$ need not be perpendicular to $\boldsymbol\tau(t)$.

(e) $\tau(t)=-\frac 1{\|\alpha'(t)\|}\boldsymbol\beta'(t)\cdot\boldsymbol\nu(t)=-\frac{\alpha'(t)\cdot(\alpha''(t)\times\alpha'''(t))}{\|\alpha'(t)\times\alpha''(t)\|^2}$.

The above formulas give easy ways to compute curvature, torsion and the trihedron, for any parametrization of the curve, without referring to the arc length parametrization (which can be difficult to find for most curves).  However, if we had given the formulas just like this from the start, it would have been far from obvious that the curvature and torsion are independent of the particular parametrization.

\item If $\alpha:J\to\mathbb R^3$ is a regular curve, show that the following are equivalent.

~~~~(i) $\alpha$ is a plane curve; i.e., contained in a plane.

~~~~(ii) The binormal vector $\boldsymbol\beta(s)$ is constant.

~~~~(iii) The torsion $\tau(s)$ is identically zero.

[(i) $\implies$ (ii): Show that $\boldsymbol\beta(s)$ must always be orthogonal to the plane. (ii) $\implies$ (i): Suppose $\boldsymbol\beta(s)=\boldsymbol\beta_0$, and fix $s_0\in J$.  Show that $s\mapsto[\alpha(s)-\alpha(s_0)]\cdot\boldsymbol\beta_0$ is identically zero, by showing its derivative is identically zero.  Conclude that the curve is contained in the plane through $\alpha(s_0)$ perpendicular to $\boldsymbol\beta_0$. (ii) $\iff$ (iii): Use the Frenet equations.]

\item\emph{(Signed curvature of a plane curve.)} \---- Let $\alpha:J\to\mathbb R^2$ be a regular curve parametrized by arc length.  Let $\boldsymbol\tau(s)=\alpha'(s)$ for $s\in J$; then $\|\boldsymbol\tau(s)\|=1$, and hence $\boldsymbol\tau(s)\cdot\boldsymbol\tau'(s)=0$ by Exercise 4(b).

(a) Let $\boldsymbol\nu(s)=\begin{bmatrix}0&-1\\1&0\end{bmatrix}\boldsymbol\tau(s)$.  In other words, if $\boldsymbol\tau(s)=(a(s),b(s))$ then $\boldsymbol\nu(s)=(-b(s),a(s))$.  Show that $\{\boldsymbol\tau(s),\boldsymbol\nu(s)\}$ is a positive orthonormal basis of $\mathbb R^2$, for each $s\in J$.

(b) Show that there is a unique differentiable function $\kappa:J\to\mathbb R$, such that $\boldsymbol\tau'(s)=\kappa(s)\boldsymbol\nu(s)$ for all $s\in J$.  Note that $\kappa$ can be negative, unlike the case for curves in $\mathbb R^3$.  $\kappa(s)$ is called the \textbf{signed curvature} of the plane curve.

(c) Show that there exists a differentiable function $\theta:J\to\mathbb R$, such that $\boldsymbol\tau(s)=(\cos\theta(s),\sin\theta(s))$ for all $s\in J$.  [$\theta$ tells the angle from the positive $x$-axis at which the curve is pointing.]  Then show that $\theta'(s)=\kappa(s)$.  Hence the signed curvature measures how much turning happens relative to the arc length.

(d) If $\alpha:J\to\mathbb R^3$ is a space curve, but its image happens to be contained in the $xy$-plane, how do the vectors $\boldsymbol\tau$, $\boldsymbol\nu$ and the scalar function $\kappa$ in this problem relate to those for space curves covered in this section?

\item Suppose $\alpha:I\to\mathbb R^2$ is a regular curve, not necessarily parametrized by arc length.  Show that $\boldsymbol\tau(t)=\frac{\alpha'(t)}{\|\alpha'(t)\|}$, and $\kappa(t)=\frac 1{\|\alpha'(t)\|}\|\boldsymbol\tau'(t)\|$.

\item\emph{(Osculating circles.)} \---- Let $\alpha:J\to\mathbb R^2$ be a regular curve, parametrized by arc length.  Suppose for some $s\in J$ that $\kappa(s)\ne 0$.  Then we define $\rho(s)=1/\kappa(s)$.  The absolute value $|\rho(s)|$ is called the \textbf{radius of curvature}.

(a) Show that the radius of curvature of a circle is the radius of the circle.

(b) The \textbf{center of curvature} is defined to be the point $\alpha(s)+\rho(s)\boldsymbol\nu(s)$.  The \textbf{osculating circle} at $\alpha(s)$ is the circle whose center is the center of curvature and whose radius is $|\rho(s)|$.  Show that this is the unique circle that can be parametrized via $\psi:\mathbb R\to\mathbb R^2$ in such a way that:
$$\psi(0)=\alpha(s),~~~~\psi'(0)=\alpha'(s),~~~~\psi''(0)=\alpha''(s).$$
[For this reason we say that $\alpha$ and the circle have \textbf{contact of order $\geqslant 2$}.]
\begin{center}
\includegraphics[scale=.3]{OscCircle.png}
\end{center}

(c) Is there an $\alpha$ besides a circle where the osculating circle is pretty easy to compute?

\item\emph{(Evolute and approaching normal lines.)} \---- Let $\alpha:I\to\mathbb R^2$ be a regular curve, not necessarily parametrized by arc length.  The locus of the center of curvature, i.e., the curve $\beta(t)=\alpha(t)+\rho(t)\boldsymbol\nu(t)$, is called the \textbf{evolute} of $\alpha$.  Though it is clear this is differentiable, it is not necessarily regular; e.g., if $\alpha$ is a circle then $\beta$ is constant. % How did evolute get its name?  Maybe it says somewhere here? https://en.wikipedia.org/wiki/Evolute

(a) Show that the tangent line to the evolute at $t\in I$ is the normal line to $\alpha$ at this value of $t$.  [$\beta(t)$ is clearly on the normal line to $\alpha$, hence it suffices to show the lines are parallel; i.e., $\alpha'(t)\perp\beta'(t)$.]

(b) If $\alpha(t)=(x(t),y(t))$, show that the signed curvature of $\alpha$ is
$$\kappa(t)=\frac{y''(t)x'(t)-x''(t)y'(t)}{[x'(t)^2+y'(t)^2]^{3/2}}.$$
[Let $v(t)=\|\alpha'(t)\|$ and write $x'(t)=v(t)\cos\theta(t),y'(t)=v(t)\sin\theta(t)$.  Then work out the above expression.  Note that the derivative of $\theta$ with respect to the arc length parameter is $\kappa$ by Exercise 9(c), and hence, $\theta'(t)=\kappa(t)v(t)$ when we differentiate with respect to $t$.]

(c) Let $\ell_1$ and $\ell_2$ be normal lines to $\alpha$, at two close-together points, $t_1$ and $t_2$.  As $t_2\to t_1$ and $\ell_2$ remains the normal line to $\alpha$ at $t_2$, show that $\ell_1\cap\ell_2$ approaches the center of curvature at $t_1$.  [Explain why one may assume $\alpha$ is an arc-length parametrization.  That should simplify the process.]

\item\emph{(Polar coordinates.)} \---- Suppose $A\subset\mathbb R$ is an open interval and $r(\theta)$ is a differentiable function for $\theta\in A$, and that $r(\theta)>0$ throughout $A$.

(a) Show that $\alpha(t)=(r(t)\cos t,r(t)\sin t)$ is a regular curve, and that
$$\alpha'(t)=(r'(t)\cos t-r(t)\sin t,r'(t)\sin t+r(t)\cos t).$$

(b) Use part (a) to show that $\alpha'(t)=\sqrt{r'(t)^2+r(t)^2}$.

(c) Use Exercise 12(b) to find a formula for the signed curvature $\kappa(t)$.

\item Let $\kappa:J\to\mathbb R$ be any differentiable function.  Fix $s_0\in J$.

For some $\mathbf x_0\in\mathbb R^2$ and $\psi\in\mathbb R$, define a curve $\alpha:J\to\mathbb R^2$ as follows:
$$\alpha(s)=\mathbf x_0+\int_{s_0}^s(\cos\theta(u),\sin\theta(u))\,du,\text{ with }\theta(s)=\psi+\int_{s_0}^s\kappa(u)\,du.$$

(a) Show that $\alpha$ is an arc length parametrization, and its signed curvature is $\kappa$.  [Use Exercise 9(c).]  Moreover, $\alpha(s_0)=\mathbf x_0$ and $\alpha'(s_0)=(\cos\psi,\sin\psi)$.

(b) Show that every curve parametrized by arc length, with signed curvature $\kappa$, is of the above form for some $\mathbf x_0$ and $\psi$.

\item The graph $y=f(x)$ of an explicit function can be expressed as a plane curve via $\alpha(t)=(t,f(t))$.  Show that $\alpha$ is regular and find its signed curvature.

\item Let $\alpha:J\to\mathbb R^3$ be a regular curve, parametrized by arc length.  For $s\in J$, the \textbf{radius of curvature} is defined as $\rho(s)=1/\kappa(s)>0$, and the \textbf{center of curvature} is defined to be $\alpha(s)+\rho(s)\boldsymbol\nu(s)$.  The \textbf{osculating plane} refers to the plane through $\alpha(s)$, parallel to the linear span of $\boldsymbol\tau(s),\boldsymbol\nu(s)$.  Finally, the \textbf{osculating circle} is the intersection of the osculating plane with the sphere of radius $\rho(s)$ centered at $\alpha(s)+\rho(s)\boldsymbol\nu(s)$.

(a) Show that the osculating circle is the unique circle with contact of order $\geqslant 2$ at $\alpha(s)$; i.e., it can be parametrized via $\psi:\mathbb R\to\mathbb R^3$ in such a way that $\psi(0)=\alpha(s),\psi'(0)=\alpha'(s),\psi''(0)=\alpha''(s)$.

(b) More generally, a plane through $\alpha(s)$ is the osculating plane if and only if it contains some curve meeting $\alpha(s)$ with contact of order $\geqslant 2$.

\item\emph{(Space curves with constant curvature and torsion.)} \---- Suppose $\alpha:J\to\mathbb R^3$ is an arc-length-parametrized curve, with constant curvature $\kappa>0$ and constant torsion $\tau$.  Show that there is an isometry in $\operatorname{Isom}^+(\mathbb R^3)$ sending $\alpha$ to this curve:
$$s\mapsto\left(\frac{\kappa}{\kappa^2+\tau^2}\cos(s\sqrt{\kappa^2+\tau^2}),\frac{\kappa}{\kappa^2+\tau^2}\sin(s\sqrt{\kappa^2+\tau^2}),\frac{\tau}{\sqrt{\kappa^2+\tau^2}}s\right).$$
[Use Proposition 6.3.]  Conclude that $\alpha$ is a circle if $\tau=0$ and a helix if $\tau\ne 0$.  The helix models constant curvature as it curves around a central axis (here the $z$-axis), and models constant torsion as it rises steadily in the $z$-direction while it curves around.

\item\emph{(Tractrix.)} \---- The \textbf{tractrix} is defined to be the curve $\alpha:(0,\pi)\to\mathbb R^2$ given by
$$\alpha(t)=(\sin t,\cos t+\ln(\tan(t/2))).$$
(a) Show that $\alpha'(t)=(\cos t,-\sin t+\csc t)$. [Use familiar differentiation rules and trigonometric laws to show $\frac{d}{dt}\ln(\tan(t/2))=\frac 1{\tan(t/2)}\cdot\frac 12\sec^2(t/2)=\csc t$.]  Moreover, $\alpha$ is regular on $(0,\pi/2)$ and $(\pi/2,\pi)$, but stationary at $t=\pi/2$.

(b) The tangent line at $t=t_0$ may be parametrized via $t\mapsto\alpha(t_0)+t\alpha'(t_0)$.  Show that this line meets the $y$-axis at $t=-\tan t_0$, at coordinates $(0,\ln(\tan(t/2)))$.

(c) Use part (b) to show that the distance from the point of tangency to where the tangent line meets the $y$-axis is equal to $1$ for all tangent lines.

\begin{center}
\includegraphics[scale=.3]{TractrixPicture.png}
\end{center}
\end{enumerate}

\subsection*{6.2. Regular Surfaces and Parametrization}
\addcontentsline{toc}{section}{6.2. Regular Surfaces and Parametrization}
In Section 6.1 we covered regular curves.  In this section, we shall introduce regular surfaces.  Surfaces are significantly different in the following sense: between any two regular curves, there are local transformations that preserve lengths of all arcs (since regular curves can be parametrized by arc length), but this is not true for surfaces.  Even more, it does not do good to define a surface using just one ``interval'' in $\mathbb R^2$ for the parameters; a surface can be \emph{locally} viewed as a subset of $\mathbb R^2$, but not always globally [take the closed sphere, for instance].

A regular surface in $\mathbb R^3$ is a smooth two-dimensional region.  The smoothness enables various notions to be defined, such as the tangent plane, normal vectors, and the Dupin indicatrix.

Just like curves, it is best to define surfaces via parametrizations, but as previously mentioned we should not require ourselves to get the entire surface in one shot.  Here is the technical definition.  A subset $U$ of $\mathbb R^n$ is said to be \textbf{open} if for any $\vec x\in U$, there exists $\varepsilon>0$ such that
$$B_{\varepsilon}(\vec x)=\{\vec v\in\mathbb R^n:\|\vec v-\vec x\|<\varepsilon\}\subset U.$$
\noindent\textbf{Definition.} \emph{A nonempty subset $S$ of $\mathbb R^3$ is said to be a \textbf{regular surface} provided that for every point $p\in S$, there exist open sets $p\in W\subset\mathbb R^3$ and $0\in U\subset\mathbb R^2$, along with a differentiable map $\mathbf x:U\to W\cap S$ such that:}

(i) \emph{At every point in $U$, the differential of $\mathbf x$ has rank $2$; in particular, it is injective as a linear transformation $\mathbb R^2\to\mathbb R^3$;}

(ii) \emph{$\mathbf x$ has a continuous inverse from $W\cap S$ to $U$.}

(iii) \emph{$\mathbf x(0)=p$.}

\emph{In this case, the function $\mathbf x$ is called a \textbf{local coordinate chart}.  A collection of local coordinate charts whose images cover $S$ (i.e., the union of the images is $S$) is called an \textbf{atlas}.}\\

\noindent If $\mathbf x(u,v)=(x(u,v),y(u,v),z(u,v))$, then the differential of $\mathbf x$, of course, refers to the linear map given by the matrix $\begin{bmatrix}\frac{\partial x}{\partial u}&\frac{\partial x}{\partial v}\\\frac{\partial y}{\partial u}&\frac{\partial y}{\partial v}\\\frac{\partial z}{\partial u}&\frac{\partial z}{\partial v}\end{bmatrix}$.

The importance of the map having rank $2$ is essentially identical to the importance of $\alpha'(t)\ne\vec 0$ for curves $\alpha:I\to\mathbb R^n$.  For one, if $\mathbf x$ did not need to have rank $2$, it could be given for example by $\mathbf x(u,v)=(u^3,v,u^2)$; this ``surface'' has a crease in the middle:
\begin{center}
\includegraphics[scale=.3]{SingularSurface.png}
\end{center}
The crease occurs where $u=0$.  And indeed, the differential of $\mathbf x$ is $\begin{bmatrix}3u^2&0\\0&1\\2u&0\end{bmatrix}$; this clearly has rank $2$ for $u\ne 0$, but if $u=0$ then $d\mathbf x_{(u,v)}=\begin{bmatrix}0&0\\0&1\\0&0\end{bmatrix}$ which has rank $1$.

For a more obvious example of what could go wrong when $d\mathbf x_p$ is not injective, consider $\mathbf x(u,v)=(\cos u,\sin u,u)$.  The image is a helix, a curve, not a surface. % (You basically suggested I add my comment to the book. LOL)  In this case we have a helix, this is a curve
Note that the condition that $d\mathbf x_p$ be injective is sometimes called the \textbf{regularity condition}.

There is also an important reason for condition (ii) [$\mathbf x$ having a continuous inverse].  Without it, one could have self-intersections, leading to there being more than one tangent plane to a point and causing confusion.  For example, suppose $\mathbf x(u,v)=(u^3-u,v,u^2-1)$, as shown below.
\begin{center}
\includegraphics[scale=.2]{SurfaceLoop.png}
\end{center}
The differential of $\mathbf x$ is $\begin{bmatrix}3u^2-1&0\\0&1\\2u&0\end{bmatrix}$, which can readily be seen to have rank $2$ for all $u,v\in\mathbb R$.  Thus condition (i) is met.  However, when $u=\pm 1$, $\mathbf x(u,v)$ is on the line where the two parts of the surface pass through [one part where $u$ is near $1$, and the other where $u$ is near $-1$].  No matter what open neighborhood $W$ of $\mathbf x(\pm 1,v)$ is involved, it will meet the surface in an open set on both parts.  It is hence impossible for $\mathbf x$ to possess a continuous inverse on $W\cap S$.  Indeed, $\mathbf x(\pm 1,v)$ is a limit of points in $W$ on each sheet part, hence continuity would imply $\mathbf x^{-1}$ sends that point to both $(1,v)$ and $(-1,v)$, so $\mathbf x^{-1}$ would not be a well-defined function.

This anomaly also entails the non-uniqueness of tangent planes; as we will see later, the tangent plane at a point $\mathbf x(u,v)\in U$ is parallel to the image of $d\mathbf x_{(u,v)}$ (as a subspace).  But the point $\mathbf x(\pm 1,v)$ in the example above has two ``tangent planes,'' $\operatorname{span}\left(\begin{bmatrix}1\\0\\1\end{bmatrix},\begin{bmatrix}0\\1\\0\end{bmatrix}\right)$ and $\operatorname{span}\left(\begin{bmatrix}1\\0\\-1\end{bmatrix},\begin{bmatrix}0\\1\\0\end{bmatrix}\right)$.  These different results are obtained by thinking of $u$ as $1$ or $-1$.

Now that we have covered the issues that could occur in the absence of the conditions, let us give some examples of surfaces which \emph{do} satisfy the conditions.\\

\noindent\textbf{Examples.}

(1) If $\vec a,\vec b\in\mathbb R^3$ are linearly independent vectors and $\vec x\in\mathbb R^3$ is any vector, the image of $\mathbf x:\mathbb R^2\to\mathbb R^3$ given by $\mathbf x(u,v)=u\vec a+v\vec b+\vec x$ is a regular surface.  It is a plane, and it has in fact been parametrized in one shot.  The reader can readily verify that $\vec a,\vec b$ are the columns of the differential of $\mathbf x$, hence the differential has rank $2$.  It is easy to prove using linear algebra that $\mathbf x$ has a continuous inverse.\\

(2) More generally, let $U\subset\mathbb R^2$ be open and $f:U\to\mathbb R$ a differentiable function.  Then the image of $\mathbf x:U\to\mathbb R^3$ given by $\mathbf x(u,v)=(u,v,f(u,v))$ is a regular surface, called the \textbf{graph of $f$}.  Indeed, $d\mathbf x_{(u,v)}=\begin{bmatrix}1&0\\0&1\\\frac{\partial f}{\partial u}&\frac{\partial f}{\partial v}\end{bmatrix}$, which has rank $2$ everywhere because the upper $2\times 2$ submatrix is nonsingular.  As for condition (ii), the restriction of $(x,y,z)\mapsto(x,y)$ to $\mathbf x$'s image is a continuous inverse of $\mathbf x$.\\

(3) Here is an example of a surface which cannot be parametrized in just a single coordinate chart.  Let $S^2=\{(x,y,z)\in\mathbb R^3:x^2+y^2+z^2=1\}$ be the unit sphere.  Then there are many \emph{partial} parametrizations, such as stereographic projection,
$$\mathbf x:\mathbb R^2\to S^2,\mathbf x(u,v)=\left(\frac{2u}{u^2+v^2+1},\frac{2v}{u^2+v^2+1},\frac{u^2+v^2-1}{u^2+v^2+1}\right),$$
which parametrizes the whole sphere except the North Pole.
There is also the spherical-coordinate parametrization
$$\mathbf x(u,v)=\left(\sin u\cos v,\sin u\sin v,\cos u\right)\text{ for }u\in(0,\pi),v\in(0,2\pi),$$
which uses the fact that $U=(0,\pi)\times(0,2\pi)$ is open in $\mathbb R^2$.  We shall mostly stick with the latter parametrization when dealing with the sphere.

The function $\mathbf x:U\to\mathbb R^3$ given by $\mathbf x(u,v)=\left(\sin u\cos v,\sin u\sin v,\cos u\right)$ is readily seen to have a differential of rank $2$.  We have
$$d\mathbf x_{(u,v)}=\begin{bmatrix}\cos u\cos v&-\sin u\sin v\\\cos u\sin v&\sin u\cos v\\-\sin u&0\end{bmatrix}$$
and $u\in(0,\pi)$ implies $\sin u>0$, so we can confirm that the rank is $2$ as follows.  The upper $2\times 2$ submatrix has determinant $\cos u\sin u$, hence if $\cos u\ne 0$ then this submatrix is nonsingular and the differential has rank $2$.  So suppose $\cos u=0$?  Then $u=\pi/2$ and $\sin u=1$, so that the differential is $\begin{bmatrix}0&-\sin v\\0&\cos v\\-1&0\end{bmatrix}$ \---- this obviously has rank $2$ because $\cos v,\sin v$ are not both zero.

As for a continuous inverse, we leave it to the reader to show that the image of $\mathbf x$ is $V=\{(x,y,z)\in S^2:y\ne 0\text{ or }x<0\}$, and on this set we may define an inverse via
$$\mathbf x^{-1}(x,y,z)=\left(\tan^{-1}_2(\sqrt{x^2+y^2},z),\tan^{-1}_2(y,x)\right)$$
where $\tan^{-1}_2(b,a)$ is the argument of the complex number $a+bi$ in the range $[0,2\pi)$; this can be regarded as a version of $\tan^{-1}(b/a)$ where the signs of $a,b$ both matter.  [See Exercise 1 for the basic properties of $\tan^{-1}_2(b,a)$.]

The parametrization $\mathbf x$ does have a continuous inverse, but its image is missing several points on the sphere; specifically the points $(x,y,z)\in S^2$ where $x\geqslant 0,y=0$.  However, it is clear that a few other parametrizations (also with missing points in the image) can be used along with $\mathbf x$, to form an atlas.\\%so that every point on the sphere is covered by at least one parametrization.

(4) Start with the $x$-axis in $\mathbb R^3$.  Then rotate it steadily around the $z$-axis, while translating it steadily along the $z$-axis at the same time.  The surface thus traced is called a \textbf{helicoid}.  It can be thought of as starting with a helix $(\cos t,\sin t,t)$ then scaling it in the $x$ and $y$ directions by all possible scalar multiples.  Specifically, it is parametrized via
$$\mathbf x(u,v)=(u\cos v,u\sin v,v).$$
We leave it to the reader to check conditions (i) and (ii) in the definition of a regular surface.  Hence the helicoid may actually be parametrized in one shot (i.e., one coordinate chart covering the whole surface).\\ % Helicoid

(5) \emph{(Surfaces of revolution.)} \---- Graphs of explicit functions (Example 2) are an important general class of surfaces.  Another important general class are \emph{surfaces of revolution}, which are obtained by taking a curve in the $xz$-plane, and rotating it all around the $z$-axis.  Specifically, let $\alpha(t)=(f(t),g(t))$ be any \emph{embedded} regular curve $I\to\mathbb R$ [Exercise 5(g) below], such that $f(t)>0$ for all $t\in I$.  Then define $\mathbf x:I\times(0,2\pi)\to\mathbb R^3$ as follows:
$$\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u)).$$
You may notice that since $v$ cannot touch either $0$ or $2\pi$, $\mathbf x$ is missing the section in the part of the $xz$-plane where $x$ is positive.  This is inevitable, because our definition requires $\mathbf x$ to have a continuous inverse (which it would not if its domain contained both $0$ and $2\pi$).  Fortunately, once this parametrization is studied, it will be clear that one more parametrization can be used to get an atlas, i.e., cover the surface.  Indeed, if $\theta\in(0,2\pi)$ is a constant, then substituting $v+\theta$ for $v$ in the above formula will do.

To show that $\mathbf x$ has rank $2$, we first compute that its differential is
$$d\mathbf x_{(u,v)}=\begin{bmatrix}f'(u)\cos v&-f(u)\sin v\\f'(u)\sin v&f(u)\cos v\\g'(u)&0\end{bmatrix}.$$
The upper $2\times 2$ submatrix has determinant $f'(u)f(u)$, and $f(u)$ is nonzero because it is positive.  Hence if $f'(u)\ne 0$, we have a $2\times 2$ submatrix with a nonzero determinant, so that the matrix has rank $2$.  If $f'(u)=0$, then, because $\alpha$ is regular, $g'(u)\ne 0$ and then it is clear that neither column is a scalar multiple of the other (because, for example, $\sin v$ or $\cos v$ is nonzero), so the matrix has rank $2$.

As for condition (ii), we use the assumption that $\alpha$ is an embedding, hence has a continuous inverse $\alpha^{-1}:\alpha(I)\to I$.  We then use the generalized arc tangent $\tan^{-1}_2$ (with range $[0,2\pi)$) to formulate $\mathbf x^{-1}$:\\
$$\mathbf x^{-1}(x,y,z)=(\alpha^{-1}(\sqrt{x^2+y^2},z),\tan^{-1}_2(y,x)).$$
Some examples of this are the sphere (when $\alpha$ is the circular arc $t\mapsto(\sin t,\cos t)$ for $t\in(0,\pi)$), the cylinder (when $\alpha$ is the line $t\mapsto(1,t)$ for $t\in\mathbb R$), the cone (when $\alpha$ is the line $t\mapsto(t,t)$ for $t>0$ \---- this time the line is not parallel to the axis of rotation), and the hyperboloid (when $\alpha$ is the hyperbola branch $t\mapsto(\cosh t,\sinh t)$ for $t\in\mathbb R$).

Another example is the \emph{catenoid}, which is the surface of revolution given by the hyperbolic cosine curve, $\alpha(t)=(\cosh t,t)$.  We will see later that, in fact, the catenoid is locally isometric to the helicoid.

Note that since regular curves can be parametrized by arc length, one may assume $(f')^2+(g')^2=1$ throughout $I$.  This will provide many advantages to studying surfaces of revolution, as will be seen in later sections.\\

\noindent In the rest of the section, we will be covering the concept of the tangent plane.  Many other concepts, along with generalizations to higher dimensions, will be covered in the exercises.\\

\noindent\textbf{Definition.} \emph{Let $S$ be a regular surface and $p\in S$.  The \textbf{tangent plane} at $p$, denoted $T_pS$, is defined as follows: Let $\mathbf x:U\to W\cap S$ be a local coordinate chart with $\mathbf x(0)=p$.  Then the tangent plane is the image of the linear map $d\mathbf x_0$, depicted as a plane with its ``origin'' at $p$.  Elements of the tangent plane are called \textbf{tangent vectors}.}\\

\noindent Note also that the tangent plane at $p$ is the span of the vectors $\frac{\partial\mathbf x}{\partial u}\big|_{(u,v)=(0,0)}$ and $\frac{\partial\mathbf x}{\partial v}\big|_{(u,v)=(0,0)}$, as these vectors are the columns of $d\mathbf x_0$.

To verify the definition we must check two things: (a) the span is indeed a plane, and (b) it is independent of the particular coordinate chart.  (a) follows from condition (i) of a regular surface, stating that $d\mathbf x_0$ is injective as a linear map; hence its image is isomorphic to its domain and has dimension $2$.

As for (b), let $\mathbf x_1:U_1\to W_1\cap S$ be another local coordinate chart with $\mathbf x_1(0)=p$; we may assume $W=W_1$ by restricting both of them to $W\cap W_1$ and reassigning $U=\mathbf x^{-1}(W\cap W_1),U_1=\mathbf x_1^{-1}(W\cap W_1)$.  Let $\psi=\mathbf x_1^{-1}\circ\mathbf x:U\to U_1$ ($\mathbf x_1^{-1}$ exists by condition (ii) of a regular surface); then $\psi$ is a diffeomorphism by Exercise 2(b), and $\mathbf x_1\circ\psi=\mathbf x$.  Note also $\psi(0)=0$.  Differentiating at $0$ and using the Chain Rule,
$$d(\mathbf x_1)_0\circ d\psi_0=d\mathbf x_0.$$
Since $d\psi_0$ is a linear isomorphism, it follows that $d\mathbf x_0$ and $d(\mathbf x_1)_0$ have the same image, hence the different choice of chart entails the same tangent plane.\\

\noindent In the rest of this section we shall give examples of tangent planes.

We recall that if $\vec a,\vec b$ are linearly independent vectors and $\vec x$ is any vector, then $\mathbf x(u,v)=u\vec a+v\vec b+\vec x$ parametrizes a plane.  Its tangent plane at any point is just the plane itself, as it is spanned by $d\mathbf x$'s columns, $\vec a$ and $\vec b$.

If $U\subset\mathbb R^2$ is open, $f:U\to\mathbb R$ is a differentiable function and $\mathbf x:U\to\mathbb R^3$ is the graph of $f$ [$(u,v)\mapsto(u,v,f(u,v))$], then the tangent plane at a point $p$ is spanned by the vectors $\left(1,0,\frac{\partial f}{\partial u}(p)\right)$ and $\left(0,1,\frac{\partial f}{\partial v}(p)\right)$.  This probably looks familiar from multivariable calculus; the partial derivatives give the slope of the tangent lines in each direction, and the tangent plane is spanned by those lines.

Finally, consider the sphere $S^2$.  Its standard spherical-coordinate parametrization is $\mathbf x(u,v)=(\sin u\cos v,\sin u\sin v,\cos u)$ for $0<u<\pi,0<v<2\pi$.  [Of course, this is missing several points, but the tangent planes at those points will be equally easy to find using another parametrization.]  $d\mathbf x_{(u,v)}=\begin{bmatrix}\cos u\cos v&-\sin u\sin v\\\cos u\sin v&\sin u\cos v\\-\sin u&0\end{bmatrix}$, so the columns of this matrix span the tangent plane at $\mathbf x(u,v)$.  We have already shown this matrix to have rank $2$ (so that its image is a plane), and it can be shown (either directly from here or using Exercise 4(b) of Section 6.1) that each column of the matrix is perpendicular to $\mathbf x(u,v)$.  Hence the tangent plane at a point $p\in S^2$ is the orthogonal complement of that point regarded as a unit vector; i.e., the vector space $\{\vec v\in\mathbb R^3:\vec p\cdot\vec v=0\}$.  The set of points $\vec v$ such that $\vec p\cdot\vec v=1$ is an affine plane which is literally tangent to the sphere at $\vec p$.

\subsection*{Exercises 6.2. (Regular Surfaces and Parametrization)} % Introduce the concept of a regular surface and parametrizations used to study them.
% Do not customize the metric; that is to be saved for Section 6.10.  Remember to cover tangent planes.
\begin{enumerate}
\item For $x,y\in\mathbb R$, not both zero, define $\tan^{-1}_2(y,x)$ to be the argument of the complex number $x+yi$ in the range $[0,2\pi)$.  Show that

(a) $\cos\tan^{-1}_2(y,x)=\frac x{\sqrt{x^2+y^2}}$.

(b) $\sin\tan^{-1}_2(y,x)=\frac y{\sqrt{x^2+y^2}}$.

(c) If $y>0$, then $0<\tan^{-1}_2(y,x)<\pi$.

(d) If $x\ne 0$ and $y>0$, then $\tan^{-1}_2(y,x)=\tan^{-1}(y/x)$.  Show by example that this may be false if $y\leqslant 0$.  [Also note that $\tan^{-1}_2(1,0)=\pi/2$, but $\tan^{-1}(1/0)$ is undefined.]

\item (a) Let $S$ be a regular surface, and $\mathbf x:U\to W\cap S$ a local coordinate chart.  Show that $\mathbf x^{-1}$ is differentiable.  [Remember that $\mathbf x$ has injective differentials at all points.]

(b) Suppose $\mathbf x_1:U_1\to W\cap S$ is another local coordinate chart using the same open set of $S$.  Then $\mathbf x_1^{-1}\circ\mathbf x:U\to U_1$ is a diffeomorphism.  [It is smooth by part (a); now diagnose its inverse by swapping the roles of $\mathbf x,\mathbf x_1$.]

\item Let $S_1$ and $S_2$ be regular surfaces.  A function $f:S_1\to S_2$ is said to be \textbf{smooth} / \textbf{differentiable} provided that whenever $U\subset\mathbb R^2$ is an open set and $g:U\to S_1$ is a differentiable function, then $f\circ g:U\to S_2$ is differentiable.

(a) Show that $f$ is smooth if and only if it composes with every local coordinate chart of $S_1$ to a differentiable map.

(b) Moreover, if $\{U_\alpha,\mathbf x_\alpha\}_{\alpha\in A}$ is an atlas, $f$ is smooth if and only if every $f\circ\mathbf x_\alpha$ is differentiable.

(c) Composition of smooth functions and the identity map on a regular surface are smooth.

(d) If $f:S_1\to S_2$ is smooth and bijective, show by example that $f^{-1}:S_2\to S_1$ need not be smooth.

For $p\in S_1$, the \textbf{differential} of $f$ at $p$, denoted $df_p$, is a linear map $T_pS_1\to T_{f(p)}S_2$ defined as follows.  Let $\mathbf x:U\to S_1$ be any local coordinate chart sending $0\mapsto p$.  Then $d\mathbf x_0$ has rank $2$, hence is an \emph{isomorphism} $\mathbb R^2\to T_pS_1$.  $df_p$ is thereby defined to be $d(f\circ\mathbf x)_0\circ d\mathbf x_0^{-1}$ (noting that $f\circ\mathbf x$ is differentiable but its differential may not have rank $2$).

(e) Show that $df_p$ is independent of the particular local coordinate chart.

(f) Prove the \textbf{Chain Rule}: if $f:S_1\to S_2$ and $g:S_2\to S_3$ are smooth and $p\in S_1$, then $d(f\circ g)_p=dg_{f(p)}\circ df_p$.

(g) Use part (f) to show that if $I\subset\mathbb R$ is an interval and $\alpha:I\to S_1$ is a (differentiable curve), such that $\alpha(t_0)=p$ and $\alpha'(t_0)=\vec v$, then $df_p(\vec v)=(f\circ\alpha)'(t_0)$.  Hence, the differential of $f$ sends tangent vectors of a curve to tangent vectors of the target curve.

(h) If $f:S_1\to S_2$ is smooth, show that $f$ has a smooth inverse if and only if it is bijective and its differential at every point is a linear isomorphism.  [Such a map $f$ is said to be a \textbf{diffeomorphism}.]

\item We can take curves \emph{on} the regular surfaces.  Let us suppose $S$ is a regular surface, and $\alpha:I\to S$ is a differentiable function.

(a) Assume $\mathbf x:U\to S$ is a local coordinate chart, and $\alpha$ is given by $\alpha(t)=\mathbf x(u(t),v(t))$.  Then $\alpha'(t)=\vec 0$ if and only if $u'(t)=v'(t)=0$.  [See condition (i) in the definition of a local coordinate chart.]  Thus $\alpha$ is a regular curve in $\mathbb R^3$ if and only if $t\mapsto(u(t),v(t))$ is a regular curve in $\mathbb R^2$.

(b) Let $\alpha:I\to S$ be a regular curve.  Show by example that its normal vector (from its Frenet trihedron) may not be normal to the surface $S$.

(c) Let $\mathbf N=\frac{\mathbf x_u\times\mathbf x_v}{\|\mathbf x_u\times\mathbf x_v\|}$ applied to $(u,v)\in U$.  Show that $\mathbf N$ is a unit vector normal to the surface at $\mathbf x(u,v)$ (i.e., perpendicular to the tangent plane $T_{\mathbf x(u,v)}S$).  [$\mathbf x_u$, of course, means $\frac{\partial\mathbf x}{\partial u}$.  Note that $\mathbf N$ is defined because $d\mathbf x$ has rank $2$.]

(d) If $\alpha:J\to S$ is a regular curve parametrized by arc length, let $\kappa(s)$ be its curvature and $\boldsymbol\nu(s)$ its normal vector.  Then $\alpha''(s)=\kappa(s)\boldsymbol\nu(s)$.  Show that there is a unique expression $\alpha''(s)=\kappa_g\mathbf n+\kappa_n\mathbf N$ where $\mathbf n$ is a unit vector, tangent to $S$ and normal to $\alpha$, pointing in the direction of $\alpha''(s)$.  [Take the orthogonal projection of $\alpha''(s)$ to the tangent plane.]  $\kappa_g$ is called the \textbf{geodesic curvature} of $\alpha$ and $\kappa_n$ its \textbf{normal curvature}.

\item The aim of this exercise is to generalize the results of this chapter to arbitrary dimensions.  A typical course on differential geometry would actually do the general case in the first place, but since $\mathbb R^n$ can only be visualized in real life when $n\leqslant 3$, this chapter will generally start in the low-dimensional visualizable areas.

Fix nonnegative integers $m\leqslant n$.  A subset $S\subset\mathbb R^n$ is defined to be a \textbf{regular manifold of dimension $m$}, provided that for each $p\in S$, there exist open sets $U\subset\mathbb R^m,p\in W\subset\mathbb R^n$ along with a differentiable map $\mathbf x:U\to W\cap S$, such that (i) at each $q\in U$ the differential $d\mathbf x_q$ has rank $m$; i.e., is injective as a linear map; and (ii) $\mathbf x$ has a continuous inverse $W\cap S\to U$.  [$\mathbf x$ is called a \textbf{local coordinate chart}, just as in the case for surfaces.]

If $m=n-1$, a regular manifold of dimension $m$ is called a \textbf{hypersurface}.  For example, $m$-planes and $m$-spheres in $\mathbb R^n$ are regular manifolds of dimension $m$, and hyperplanes and hyperspheres are hypersurfaces.

(a) Show that the connected regular manifolds of dimension $1$ are the embedded regular curves.

(b) We will assume the \textbf{Inverse Function Theorem}: if $U\subset\mathbb R^n$, $f:U\to\mathbb R^n$ is a differentiable function, $p\in U$ and $df_p$ is a linear isomorphism, then there exist open sets $p\in V\subset U$ and $W\subset\mathbb R^n$ such that $f|_V$ is a diffeomorphism from $V$ to $W$.  [The proof of this fact involves analysis which is beyond the scope of this book.]  Show that the regular manifolds of dimension $n$ in $\mathbb R^n$ are precisely the open sets.

(c) If $S$ is a regular $m$-dimensional manifold, come up with a definition for the tangent space $T_pS$ for any point $p\in S$, by imitating the definition of a tangent plane in the case $n=3,m=2$ covered in the text.  [It is an $m$-dimensional vector subspace of $\mathbb R^n$.]  Show that the tangent space of an $m$-plane is the vector space parallel to this $m$-plane.  Also, the tangent space of the hypersphere $S^{n-1}=\{\vec v\in\mathbb R^n:\|\vec v\|=1\}$ at a point $p$ is the set of vectors perpendicular to $p$.  [Of course, all of these tangent spaces are depicted with the origin at $p$.] % Hope this helps.

(d) If $M$ and $M'$ are manifolds with respective dimensions $m,m'$, a function $f:M\to M'$ is said to be \textbf{smooth} provided that whenever $U\subset\mathbb R^m$ is an open set and $g:U\to M$ is differentiable, then $f\circ g:U\to M'$ is differentiable.  [Note that $M$ and $M'$ could have different dimensions and they could be embedded into different dimensional Euclidean spaces.]

Generalize Exercise 3(a)-(c) to smooth maps of manifolds of arbitrary dimension.

(e) Imitate Exercise 3 to define the \textbf{differential} of a smooth function $f:M\to M'$ at $p\in M$; this is a linear map of the tangent spaces $T_pM\to T_{f(p)}M'$.  Then generalize Exercise 3(e)-(h) to arbitrary dimensions.

(f) A smooth map $f:M\to M'$ is said to be an \textbf{immersion} if $df_p$ is injective for all $p\in M$.  Show by example that an immersion of manifolds need not be injective.

(g) Show by example that if $f:M\to M'$ is an injective immersion, $f$ need not be a homeomorphism from $M$ to $f(M)$.  [Let $M$ be an open interval and $M'=\mathbb R^2$.  Then bend the interval in a figure 6 shape, so that one of its endpoints (note that they are excluded) would coincide with one of its interior points.]  If $f$ is a homeomorphism from $M$ to $f(M)$, $f$ is called an \textbf{embedding}.

It is worth noting a few facts that involve topics beyond the scope of the book: % "Rather than putting these at the end of the exercises, how about end of the section?"  Because, the section doesn't cover arbitrary-dimensional manifolds.  It only covers 2-dimensional ones.  This exercise introduces higher-dimensional analogues.  Aren't these facts worth stating for arbitrary dimensions?  Best not barge a mention of that into the section just to place these facts.

~~~~(i) If $M$ is compact, every injective immersion $M\to M'$ is an embedding.

~~~~(ii) Define $p\in M$ to be a \textbf{regular point} if $df_p$ is surjective (note that this is only possible if $m\geqslant m'$), and a \textbf{singular point} otherwise.  Define $x\in M'$ to be a \textbf{singular value} if there exists a singular point $p\in M$ such that $f(p)=x$, and a \textbf{regular value} otherwise (i.e., every point $p$ such that $f(p)=x$ is a regular point).  Then if $x$ is a regular value, $f^{-1}(\{x\})$ is a regular manifold of dimension $m-m'$.

~~~~(iii) A \textbf{Lie group}\footnote{Named after 19th-century Norwegian mathematician, Marius Sophus Lie.} is a regular manifold equipped with a group structure, for which the maps $(g,h)\mapsto gh$ and $g\mapsto g^{-1}$ are smooth.  See Exercise 5 of Section 6.8 to learn more about them.

~~~~(iv) Usually, regular manifolds [a.k.a., smooth manifolds] are not defined as subsets of Euclidean space.  They are usually defined as topological spaces where every point has an open neighborhood, homeomorphic to $\mathbb R^m$, and the transition functions between neighborhoods are diffeomorphisms.  (E.g., the projective space $P^m(\mathbb R)$ is readily seen to satisfy this.)  Hassler Whitney in 1944 proved that they can always be embedded into Euclidean space, specifically with twice the dimension of the manifold.  This is known as the \textbf{Whitney embedding theorem}.
\end{enumerate}

\subsection*{6.3. The First Fundamental Form.  Length and Area}
\addcontentsline{toc}{section}{6.3. The First Fundamental Form.  Length and Area}
In the previous section, we introduced regular surfaces.  The advantage to parametrizing a surface, is that we will be able to do define mathematical concepts (such as lengths of curves) on the surface, just using the surface parameters, without referring to the ambient space $\mathbb R^3$.  This will be especially important when customized surfaces are defined in Section 6.10.

The first natural step is to know how to compute dot products of tangent vectors.  Suppose $S$ is a regular surface, $p\in S$ and $\mathbf x:U\to S$ is a local coordinate chart sending $0\mapsto p$.  Then since $d\mathbf x_0$ is a linear isomorphism from $\mathbb R^2$ to $T_pS$ (by regularity), a tangent vector to $S$ at $p$ can be uniquely expressed as $d\mathbf x_0((a,b))$ with $(a,b)\in\mathbb R^2$.  It is clear that $d\mathbf x_0(\vec e_1)=\frac{\partial x}{\partial u}\big|_{(u,v)=0}=\mathbf x_u(0)$ and $d\mathbf x_0(\vec e_2)=\frac{\partial x}{\partial v}\big|_{(u,v)=0}=\mathbf x_v(0)$ \---- after all, applying any matrix to the standard basis vectors gives the columns of the matrix.  Hence, we have
$$d\mathbf x_0((a,b))=a\mathbf x_u(0)+b\mathbf x_v(0)$$
as a consequence of the linearity.

Similarly, for any other $\zeta\in U$, if we let $q=\mathbf x(\zeta)$, each element of $T_qM$ is uniquely of the form $a\mathbf x_u(\zeta)+b\mathbf x_v(\zeta)$ with $a,b\in\mathbb R$.  The reader probably observed this before, because the linear independence of $\mathbf x_u$ and $\mathbf x_v$ implies that they form a basis of the tangent plane.

The natural question to ask is this.  Given the coordinate chart, what is the \emph{best} way to formulate the dot product of any two tangent vectors $a_1\mathbf x_u+a_2\mathbf x_v$ and $b_1\mathbf x_u+b_2\mathbf x_v$ at the same point?  We would like a basic formula in terms of $a_1,a_2,b_1,b_2$ so that we don't need to keep looking back at the differential of the parametrization.  Fortunately, such a formula is easy to get due to the bilinearity of the dot product:
$$(a_1\mathbf x_u+a_2\mathbf x_v)\cdot(b_1\mathbf x_u+b_2\mathbf x_v)=(a_1\mathbf x_u)\cdot(b_1\mathbf x_u+b_2\mathbf x_v)+(a_2\mathbf x_v)\cdot(b_1\mathbf x_u+b_2\mathbf x_v)$$
$$=(a_1\mathbf x_u)\cdot(b_1\mathbf x_u)+(a_1\mathbf x_u)\cdot(b_2\mathbf x_v)+(a_2\mathbf x_v)\cdot(b_1\mathbf x_u)+(a_2\mathbf x_v)\cdot(b_2\mathbf x_v)$$
$$=a_1b_1\mathbf x_u\cdot\mathbf x_u+a_1b_2\mathbf x_u\cdot\mathbf x_v+a_2b_1\mathbf x_v\cdot\mathbf x_u+a_2b_2\mathbf x_v\cdot\mathbf x_v$$
$$=a_1b_1[\mathbf x_u\cdot\mathbf x_u]+(a_1b_2+a_2b_1)[\mathbf x_u\cdot\mathbf x_v]+a_2b_2[\mathbf x_v\cdot\mathbf x_v],$$
using the symmetry of the dot product in the last step.  Note that $\mathbf x_u\cdot\mathbf x_u$, $\mathbf x_u\cdot\mathbf x_v$ and $\mathbf x_v\cdot\mathbf x_v$ are scalar functions on the base $U$, which are determined by the local coordinate chart; they do not depend on the particular tangent vectors we are thinking about.  They hereby inherit a title.\\

\noindent\textbf{Definition.} \emph{Let $S$ be a regular surface, $\mathbf x:U\to S$ a local coordinate chart.  Then the differentiable functions $\mathbf x_u\cdot\mathbf x_u$, $\mathbf x_u\cdot\mathbf x_v$ and $\mathbf x_v\cdot\mathbf x_v$ from $U$ to $\mathbb R$ are denoted $E$, $F$ and $G$ respectively, and are called the \textbf{coefficients of the first fundamental form}.}\\

\noindent These coefficients give us the immediate formula for the dot product of tangent vectors:
\begin{equation}\tag{*}(a_1\mathbf x_u+a_2\mathbf x_v)\cdot(b_1\mathbf x_u+b_2\mathbf x_v)=Ea_1b_1+F(a_1b_2+a_2b_1)+Ga_2b_2\end{equation}
Several observations are in order.

(1) If $E,F,G$ are the coefficients of the first fundamental form, then $E>0$ and $EG-F^2>0$.  The first inequality is clear, because $E=\mathbf x_u\cdot\mathbf x_u=\|\mathbf x_u\|^2>0$ (strict because, by regularity, $\mathbf x_u$ cannot be zero).  As for the second inequality, we recall [Exercise 4(h) of Section 2.5] that for $\vec v,\vec w\in\mathbb R^3$, $\|\vec v\|^2\|\vec w\|^2=(\vec v\cdot\vec w)^2+\|\vec v\times\vec w\|^2$.  In particular, it follows that
$$EG-F^2=\|\mathbf x_u\|^2\|\mathbf x_v\|^2-(\mathbf x_u\cdot\mathbf x_v)^2=\|\mathbf x_u\times\mathbf x_v\|^2>0;$$
again the inequality is strict because $\mathbf x_u,\mathbf x_v$ are linearly independent.

These can alternatively be viewed by observing that for $p\in U$,
$$(d\mathbf x_p)^T(d\mathbf x_p)=\begin{bmatrix}E&F\\F&G\end{bmatrix}(p)$$
since the product clearly consists of dot products of $d\mathbf x_p$'s columns.  Moreover, the symmetric matrix $\begin{bmatrix}E&F\\F&G\end{bmatrix}$ must be positive definite by Exercise 8 of Section 3.5, and by Sylvester's Criterion this is tantamount to saying $E>0,EG-F^2>0$.

(2) $E,F,G$ depend on the parametrization.  For instance, if the $xy$-plane is parametrized via $\mathbf x(u,v)=(u,v,0)$ then $E=G=1$ and $F=0$ (constant functions).  However, if we parametrize the same plane by $\mathbf x(u,v)=(2u+v,3v,0)$, we have $E=4$, $F=2$ and $G=10$.  [Because $\mathbf x_u=(2,0,0)$ and $\mathbf x_v=(1,3,0)$.]

If we use polar coordinates, $\mathbf x(u,v)=(u\cos v,u\sin v,0)$ for $u>0$, then the situation is different yet: $\mathbf x_u=(\cos v,\sin v,0)$ and $\mathbf x_v=(-u\sin v,u\cos v,0)$, from which we derive $E=1$, $F=0$ and $G=u^2$.  So in this case, $G$ is not even constant.

The important point is that the coefficients can be used to derive things which don't depend on the parametrization; this will be seen in sections 6.6-6.9.

(3) There are many concepts that can be defined directly from the a local coordinate chart and the first fundamental form, and this chapter will cover some of them.  These concepts will have meaning in Section 6.10, when only the chart and the form are given.  Mathematicians call this \emph{intrinsic geometry}.\\

\noindent Let us work out some important examples.

Suppose $U\subset\mathbb R^2$ is open and $f:U\to\mathbb R$ is differentiable.  Then we recall that $\mathbf x(u,v)=(u,v,f(u,v))$ is a parametrization for a regular surface.  Moreover, $\mathbf x_u=(1,0,f_u)$ and $\mathbf x_v=(0,1,f_v)$, where $f_u,f_v$ are the partial derivatives of $f$.  We have the first fundamental form coefficients by direct computation:
$$E=1+f_u^2,~~~~F=f_uf_v,~~~~G=1+f_v^2.$$
Then $EG-F^2=1+f_u^2+f_v^2=1+\|\vec\nabla f\|^2$ (where $\vec\nabla f$ is the gradient of $f$).  Hence $E$ and $EG-F^2$ are both positive, as previously stated.

(4) Similarly, suppose $\alpha(t)=(f(t),g(t))$ is an embedded regular curve with $f(t)>0$.  Then we have the surface of revolution parametrized by
$$\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u)),$$
and we compute
$$\mathbf x_u=(f'(u)\cos v,f'(u)\sin v,g'(u))$$
$$\mathbf x_v=(-f(u)\sin v,f(u)\cos v,0)$$
$$\therefore E=f'(u)^2+g'(u)^2,~~~~F=0,~~~~G=f(u)^2$$
If we assume $\alpha$ is an arc length parametrization (i.e., $(f')^2+(g')^2=1$), then $E$ will be $1$, and $G$ will still be $f(u)^2$.\\

\noindent It usually helps to take the dot product of a tangent vector with itself.  After all, this is essentially the only kind of thing a quadratic form involves, and by integration it also provides a formula for the lengths of curves.  It even determines the rest of the dot products (by the identity $\vec v\cdot\vec w=\frac 12(\|\vec v+\vec w\|^2-\|\vec v\|^2-\|\vec w\|^2)$, so we may focus on the quadratic form.

By (*), the dot product of a vector $a_1\mathbf x_u+a_2\mathbf x_v$ (tangent at a certain point) with itself is equal to
$$\|a_1\mathbf x_u+a_2\mathbf x_v\|^2=Ea_1^2+2Fa_1a_2+Ga_2^2.$$
This quadratic form is called the \textbf{first fundamental form}, and the expression will henceforth be denoted as $I_p(a_1\mathbf x_u+a_2\mathbf x_v)$ [or $I_p((a_1,a_2))$ if it is clear the point is coming from $U$].  The I is a Roman numeral 1.

Suppose $\alpha:I\to S$ is a curve, say $\alpha(t)=\mathbf x(u(t),v(t))$.  Then its length is given by $\int_I\|\alpha'(t)\|\,dt$.  Now, by the Chain Rule, $\alpha'(t)=u'(t)\mathbf x_u+v'(t)\mathbf x_v$, hence the above formula entails
$$\|\alpha'(t)\|^2=E(u')^2+2Fu'v'+G(v')^2.$$
Thus the length of the curve can be readily computed via
$$L(\alpha)=\int_I\sqrt{E(u')^2+2Fu'v'+G(v')^2}\,dt.$$
If $s$ is the arc length parameter, then we have $\frac{ds}{dt}=\|\alpha'(t)\|$, and so
$$\frac{ds}{dt}=\sqrt{E(u')^2+2Fu'v'+G(v')^2},$$
which may be rewritten as
$$\left(\frac{ds}{dt}\right)^2=E\left(\frac{du}{dt}\right)^2+2F\frac{du}{dt}\frac{dv}{dt}+G\left(\frac{dv}{dt}\right)^2.$$
Mathematicians then use a notational trick, of multiplying the equation by $(dt)^2$ throughout to eliminate denominators.  Thus, they would write the first fundamental form in this fashion:
$$ds^2=E\,du^2+2F\,du\,dv+G\,dv^2.$$
Though it is not precise semantics, it is a useful way to remember the first fundamental form of a parametrization.

As the first fundamental form simplifies the finding of dot products, it simplifies the finding of angles.  We recall that if $\vec v,\vec w$ are nonzero vectors, then the angle in between them is $\cos^{-1}\frac{\vec v\cdot\vec w}{\|\vec v\|\|\vec w\|}$.  Thus, using (*),
\begin{center}
The angle between $a_1\mathbf x_u+a_2\mathbf x_v$ and $b_1\mathbf x_u+b_2\mathbf x_v$ is equal to
\end{center}
$$\cos^{-1}\frac{Ea_1b_1+F(a_1b_2+a_2b_1)+Ga_2b_2}{\sqrt{Ea_1^2+2Fa_1a_2+Ga_2^2}\sqrt{Eb_1^2+2Fb_1b_2+Gb_2^2}}.$$
To illustrate this principle, we shall do two things with the sphere, which we recall is (mostly) parametrized via
$$\mathbf x(u,v)=(\sin u\cos v,\sin u\sin v,\cos u),0<u<\pi,0<v<2\pi.$$
Taking $f(u)=\sin u,g(u)=\cos u$ in example (4) above (the surface of revolution) entails
\begin{equation}\tag{*}E=1,~~~~F=0,~~~~G=\sin^2u,\end{equation}
or what is the same thing,
$$ds^2=du^2+\sin^2u\,dv^2.$$
We fix $0<u_0<\pi$ and find the length of the latitude $\alpha(t)=\mathbf x(u_0,t),0<t<2\pi$.  This is an easy integration now that we have that metric; note that $u(t)=u_0$ and $v(t)=t$, so that $u'=0$ and $v'=1$.
$$L(\alpha)=\int_0^{2\pi}\sqrt{(u')^2+\sin^2u\,(v')^2}\,dt=\int_0^{2\pi}\sqrt{0+\sin^2u\cdot 1}\,dt=\int_0^{2\pi}\sqrt{\sin^2u}\,dt$$
$$=\int_0^{2\pi}\sin u\,dt=\int_0^{2\pi}\sin u_0\,dt=2\pi\sin u_0.$$
This is maximized at $2\pi$ when $u_0=\pi/2$; in this case the latitude is the equator.

Likewise, one can let fix $v_0$ and find the length of the meridian $\alpha_1(t)=\mathbf x(t,v_0),0<t<\pi$.  This results in
$$L(\alpha_1)=\int_0^\pi\sqrt{(u')^2+\sin^2u\,(v')^2}\,dt=\int_0^\pi\sqrt{1+\sin^2u\cdot 0}\,dt=\int_0^\pi 1\,dt=\pi,$$
and surely one can tell that all meridians have the same length.

Now we will demonstrate an example of what the angle formula can find for us.  Let $0<\beta<\pi/2$ be a fixed angle.  A \textbf{loxodrome} (or \textbf{rhumb line}) of a sphere is a curve meeting all of the meridians at the constant angle $\beta$, as shown below.
\begin{center}
\includegraphics[scale=.5]{loxodromie_1.png} %https://www.mathcurve.com/courbes3d.gb/loxodromie/sphereloxodromie.shtml
\end{center}
Note that a loxodrome never meets the north pole, and in fact spins around it infinitely many times while converging there.  After all, since the loxodrome makes a constant angle with the meridian, it can never be tangent to the meridian unless they coincide; and any curve at the north pole is tangent to a meridian no matter which way it goes.

We wish to find a curve $\alpha:I\to S^2$ which meets every meridian at an angle of $\beta$.  To do this, we first extend $\mathbf x$ as follows:
$$\mathbf x(u,v)=(\sin u\cos v,\sin u\sin v,\cos u)\text{ for }0<u<\pi,v\in\mathbb R$$
In other words, we let the revolution parameter $v$ go throughout the real number line.  Now this function is not injective, yet it still restricts to local coordinate charts at all points in its domain, so it makes sense to talk about $E,F,G$ applied to a point $(u,v),0<u<\pi$.

We then write $\alpha(t)=\mathbf x(u(t),v(t))$ for some differentiable functions $u,v$ such that $0<u<\pi$.  Then at each $t=t_0$, we want the angle between the curve and the meridian going through the point to be $\beta$.  To find the equation, we let $u_0=u(t_0)$, $v_0=v(t_0)$; the meridian $v=v_0$ can be parametrized via
$$\mu(t)=\mathbf x(t,v_0)\text{ for }0<t<\pi,$$
From this it is clear that at $(u_0,v_0)$, $\mu'(u_0)=\mathbf x_u$, and $\alpha'(t_0)=u'\mathbf x_u+v'\mathbf x_v$.  The angle between these tangent vectors, which is supposed to be $\beta$, is
$$\cos^{-1}\frac{\mu'\cdot\alpha'}{\|\mu'\|\|\alpha'\|}=\cos^{-1}\frac{\mathbf x_u\cdot(u'\mathbf x_u+v'\mathbf x_v)}{\|\mathbf x_u\|\|u'\mathbf x_u+v'\mathbf x_v\|}=\cos^{-1}\frac{Eu'+Fv'}{\sqrt E\sqrt{E(u')^2+2Fu'v'+G(v')^2}}$$
$$=\cos^{-1}\frac{u'}{\sqrt{(u')^2+\sin^2u(v')^2}},$$
using the formulas (*) for the first fundamental form coefficients.

Of course, it is very hard to establish any specifics about $\alpha$, because the parametrization is arbitrary; any re-parametrization of $\alpha$ will also meet the meridians at the same angles that $\alpha$ does.  For our sake we shall assume $\alpha$ is parametrized with respect to the latitude variable $v$ (i.e., $v(t)=t$); your intuition should tell you that this is feasible.

Under said assumption, $v'=1$, and hence $\beta=\cos^{-1}\frac{u'}{\sqrt{(u')^2+\sin^2u}}$.  We then arrange our work to find $u$ as follows:

(1) Clearly $\cos\beta=\frac{u'}{\sqrt{(u')^2+\sin^2u}}$.

(2) Squaring both sides, $\cos^2\beta=\frac{(u')^2}{(u')^2+\sin^2u}$.

(3) Furthermore, $1+\tan^2\beta=\sec^2\beta=\frac 1{\cos^2\beta}=\frac{(u')^2+\sin^2u}{(u')^2}=1+\frac{\sin^2u}{(u')^2}$.  Moreover, $\tan^2\beta=\frac{\sin^2u}{(u')^2}$.

(4) By assuming $\alpha$ goes in the upward direction, we may assume $u'>0$.  Then since $0<\beta<\pi$ and $0<u<2\pi$, we may take square roots of (3) to get $\tan\beta=\frac{\sin u}{u'}$; our hypotheses imply both sides of this are positive.

(5) By basic algebraic manipulation, $u'=\frac{\sin u}{\tan\beta}=\sin u\cot\beta$.

(6) This differential equation may be solved using separation of variables.  After all, it can be rewritten as $u'\csc u=\cot\beta$, and to integrate both sides, we take the left-hand side to be an antiderivative of $\csc u$ with respect to $u$.  Well, $\ln(\tan(u/2))$ is such an antiderivative, as one can check using familiar differential laws.  Hence integrating both sides (with respect to $t$) lands us with $\ln(\tan(u/2))=t\cot\beta+A$ for some $A$.  Consequently,
$$u=2\tan^{-1}(e^{t\cot\beta+A}),$$
parametrizing the loxodrome as $\alpha(t)=\mathbf x(2\tan^{-1}(e^{t\cot\beta+A}),t)$.  Or, it could just be easier to think of it as $v\cot\beta+A=\ln(\tan(u/2))$.

In Section 6.4, we will cover \textbf{Mercator's projection}, which is a projection of the sphere that passes loxodromes to straight lines.  It is useful for geographers, since the compass directions are constant throughout the map and angle measures at them coincide.\\

\noindent\textbf{AREAS OF REGIONS ON REGULAR SURFACES}\\

\noindent Just like arc length, computing the area of a region on a surface is not hard when you have the first fundamental form.  To illustrate this point, let $\mathbf x:U\to S$ be a local coordinate chart with $U$ bounded, and $E,F,G$ its first fundamental form coefficients.

In calculus, it is learned that the area under a graph $y=f(x)$ can be approximated by thin rectangles of the form $[x_0,x_0+\varepsilon]\times[0,f(x_0)]$.  Similar things apply to multivariable calculus.  And we can do something likewise with $\mathbf x$: if $U$ is divided into miniature squares of side length $\varepsilon$, then $\mathbf x$ sends them to miniature ``quadrilaterals,'' which approach the parallelograms spanned by $\mathbf x_u,\mathbf x_v$ as the squares become infinitesimal.  We can multiply each parallelogram's area by $\varepsilon^2$ (the area of the squares in $U$), and add them together to approximate the surface area.

Taking the limit as the number of squares in the division approaches infinity, we conclude that integrating (with respect to $u$ and $v$) the area of the parallelogram spanned by $\mathbf x_u,\mathbf x_v$ gives the surface area of $\mathbf x(U)$.  By Exercise 4(i) of Section 2.6, this area is $\|\mathbf x_u\times\mathbf x_v\|$.  Furthermore, $\|\mathbf x_u\times\mathbf x_v\|=\sqrt{EG-F^2}$, as shown in observation (1) following the definition of the first fundamental form.

Thus we have proven\\

\noindent\textbf{Proposition 6.4.} \textsc{(Area from first fundamental form)} \emph{Let $S$ be a regular surface, and $\mathbf x:U\to S$ a local coordinate chart with $U$ bounded.  Then the (surface) area of $\mathbf x(U)$ is equal to}
$$\int_U\sqrt{EG-F^2}\,du\,dv.$$

\noindent For example, taking $\mathbf x(u,v)=(u,v,f(u,v))$ (graph of $z=f(x,y)$), we recall that [Example 3 above] $EG-F^2=1+f_u^2+f_v^2=1+\|\vec\nabla f\|^2$.  Hence the surface area is obtained by integrating $\sqrt{1+\|\vec\nabla f\|^2}$ over the input-variable region. % I don't usually indent the paragraph right after a Proposition?

As a final illustration, we shall find the surface area of the sphere.  We recall the standard parametrization
$$\mathbf x(u,v)=(\sin u\cos v,\sin u\sin v,\cos u),0<u<\pi,0<v<2\pi$$
and the first fundamental form, $E=1,F=0,G=\sin^2u$.
This parametrization has no overlaps.  It does leave some points missing, but those points form a one-dimensional line segment which clearly has a surface area of zero.  Thus we may merely apply Proposition 6.4 to $U=(0,\pi)\times(0,2\pi)$, in order to get the surface area of the sphere:
$$\sqrt{EG-F^2}=\sqrt{1\sin^2u-0^2}=\sqrt{\sin^2u}=\sin u$$
$$\therefore\int_U\sqrt{EG-F^2}\,du\,dv=\int_0^{2\pi}\int_0^\pi\sin u\,du\,dv=\int_0^{2\pi}\left(\int_0^\pi\sin u\,du\right)\,dv.$$
$$\int_0^\pi\sin u\,du=-\cos u\big|_0^\pi=-\cos\pi-(-\cos 0)=-(-1)-(-1)=2$$
$$\therefore\int_U\sqrt{EG-F^2}\,du\,dv=\int_0^{2\pi}2\,dv=2v\big|_0^{2\pi}=4\pi.$$
It is clear that one can adapt the same argument to find the surface area of a sphere with arbitrary radius.  If $R>0$ is fixed, then
$$\mathbf x(u,v)=(R\sin u\cos v,R\sin u\sin v,R\cos u)$$
($0<u<\pi,0<v<2\pi$) parametrizes a sphere of radius $R$, and its first fundamental form coefficients are $E=R^2,F=0,G=R^2\sin^2u$.  From this one derives the surface area to be $4\pi R^2$.

\subsection*{Exercises 6.3. (The First Fundamental Form.  Length and Area)} % Explain that we want to be able to do metric things
% using coordinate charts and not the ambient 3-space.  Then bring up the FFF, and use it to establish length, area, angles...
% Also, establish the formula for loxodromes on the sphere.
% POTENTIAL EXERCISES: Similar exercises for the cylinder and cone;  surface; Pseudosphere; Use the notion of area to show LAEAP is area-preserving.
\begin{enumerate}
\item Find the first fundamental form for each of the following surface parametrizations:

(a) $\mathbf x(u,v)=(3u,u+2v,2)$

(b) $\mathbf x(u,v)=(u\cos v,u\sin v,v)$

(c) $\mathbf x(u,v)=(u,v,u^2+\sin v)$

(d) $\mathbf x(u,v)=(\cosh u\cos v,\cosh u\sin v,\sinh u)$

\item Let $\mathbf x:U\to S$ be a local coordinate chart, $p\in U$ and let $E,F,G$ be the coefficients of the first fundamental form.  Show that the following are equivalent:

~~~~(i) $\mathbf x$ is conformal at $p$, i.e., for all $\vec v,\vec w\in\mathbb R^2$ tangent to $p$, the angle between $d\mathbf x_p(\vec v)$ and $d\mathbf x_p(\vec w)$ equals the angle between $\vec v$ and $\vec w$;

~~~~(ii) $E(p)=G(p)$ and $F(p)=0$.

[Recall that $d\mathbf x_p((a_1,a_2))=a_1\mathbf x_u+a_2\mathbf x_v$.]

Universally quantifying these statements over all $p\in U$, we get that $\mathbf x$ is conformal $\iff$ $E=G$ and $F=0$ on $U$.  In this case $\mathbf x$ is said to give \textbf{isothermal} (or \textbf{conformal}) \textbf{coordinates}.

\item If $\mathbf x:U\to S$ is a local coordinate chart, show that the following are equivalent:\\

~~~~(i) $\mathbf x$ preserves curve lengths: whenever $\alpha:I\to U$ is a curve in $U$, $\alpha$ and $\mathbf x\circ\alpha$ (in $S$) have the same length;

~~~~(ii) $\mathbf x$ preserves dot products: for $p\in U$ and $\vec v,\vec w$ tangent to $p$, $\mathbf dx_p(\vec v)\cdot\mathbf dx_p(\vec w)=\vec v\cdot\vec w$;

~~~~(iii) $E=G=1$ and $F=0$.

In this case, $\mathbf x$ is said to give \textbf{isometric coordinates}.

\item If $\mathbf x:U\to S$, then the \textbf{coordinate curves} are the curves of the form $u\mapsto\mathbf x(u,v_0)$ and $v\mapsto\mathbf x(u_0,v)$ for $u_0,v_0\in U$ fixed.  Note that these are two families of curves; each family consists of curves where any two are disjoint, but each curve in one family typically intersects each curve in the other.

The parametrization is said to be a \textbf{Chebyshev net} if any quadrilateral made up of coordinate curves has opposite sides of the same length.  Show that this holds if and only if $\frac{\partial E}{\partial v}=0=\frac{\partial G}{\partial u}$.

\item\emph{(A line segment is the shortest path between points in $\mathbb R^n$.)} \---- Let $\vec p,\vec q\in\mathbb R^n$ be distinct points.  Then our ambition is to show that line segment
$$\alpha(t)=(1-t)\vec p+t\vec q,~~~~0\leqslant t\leqslant 1$$
has the shortest length out of all paths from $\vec p$ to $\vec q$.

(a) Show that the length of the segment $\alpha$ is $\|\vec q-\vec p\|$.  Do not apply any isometries; do this directly using the integral.

Now we take an arbitrary path $\alpha:I\to\mathbb R^n$ from $\vec p$ to $\vec q$, and show that its length is $\geqslant\|\vec q-\vec p\|$.  We may assume $\alpha:[0,1]\to\mathbb R^n$, with $\alpha(0)=\vec p,\alpha(1)=\vec q$.

(b) Show that if $\vec v$ is a constant unit vector ($\|\vec v\|=1$), then
$$\int_0^1\alpha'(t)\cdot\vec v\,dt\leqslant\int_0^1\|\alpha'(t)\|.$$
[You may assume that integration preserves ordering, i.e., $f\leqslant g$ implies $\int f\leqslant\int g$.]

(c) Show that $\int_0^1\alpha'(t)\cdot\vec v\,dt=(\vec q-\vec p)\cdot\vec v$.  [Explain why the dotting with $\vec v$ may be moved outside the integral.  Then use the fundamental theorem of calculus.]  Hence, by part (b), $(\vec q-\vec p)\cdot\vec v$ is less than or equal to $\int_0^1\|\alpha'(t)\|$, the length of $\alpha$.

(d) Now conclude by taking $\vec v=\frac{\vec q-\vec p}{\|\vec q-\vec p\|}$.

\item Find the first fundamental form for each of the following surfaces: % Parametrize cylinder, cone, Enneper's surface, and pseudosphere, and ask for FFF coefficients

(a) Cylinder: $\mathbf x(u,v)=(\cos v,\sin v,u)$.

(b) Cone: $\mathbf x(u,v)=(u\cos v,u\sin v,u)$ with $u>0$.

(c) \emph{Enneper's surface}: $\mathbf x(u,v)=\left(u-\frac 13u^3+uv^2,v-\frac 13v^3+vu^2,u^2-v^2\right)$.  [Once you find the coefficients, see what is implied by Exercise 2.]

(d) \emph{Pseudosphere}: $\mathbf x(u,v)=(\sin u\cos v,\sin u\sin v,\cos u+\ln(\tan(u/2)))$ with $\pi/2<u<\pi$.  Note that this is the surface of revolution given by the tractrix (Exercise 18 of Section 6.1). % Loxodrome example?  But the loxodrome is a curve, not a surface.

\item (a) Let $S$ be the cylinder $x^2+y^2=1$, parametrized via $\mathbf x(u,v)=(\cos v,\sin v,u)$.  Consider the ``meridian lines'' $u\mapsto\mathbf x(u,v_0)$.  Show that any curve on $S$ which makes a constant angle with all the meridian lines is either a helix or a ``latitude'' circle.

(b) Now let $C$ be the cone $z^2=x^2+y^2$ ($z>0$), parametrized via $\mathbf x(u,v)=(u\cos v,u\sin v,u)$ with $u>0$.  Consider the lines $u\mapsto\mathbf x(u,v_0)$.  If a curve on $C$ makes a constant angle of $\beta$ with all those lines, show that the curve has an equation of the form $u=Ae^{v\cot\beta/\sqrt 2}$ for some $A$.

\item The aim of this exercise is to show that the Lambert azimuthal equal-area projection of the sphere [Exercise 6 of Section 5.1] preserves area, hence is true to its name.

(a) If $\mathbf x:U\to S$ is a local coordinate chart of a surface, then $\mathbf x$ preserves areas of two-dimensional regions if and only if $EG-F^2=1$, where $E,F,G$ are the first fundamental form coefficients. [Recall how area is computed directly from the coordinates.]

(b) Recall [Exercise 6(b) of Section 5.1] that Lambert azimuthal equal-area projection is given by this parametrization of $S^2$:
$$\mathbf x(u,v)=\left(\sqrt{1-\frac{u^2+v^2}4}u,\sqrt{1-\frac{u^2+v^2}4}v,\frac{u^2+v^2}2-1\right),~~~~u^2+v^2<4.$$
Compute the first fundamental form coefficients for this parametrization, and use part (a) to conclude that area is preserved.

\item\emph{(Surface area of a surface of revolution.)} \---- Let $\alpha(s)=(f(s),g(s))$ be an embedded regular curve parametrized by arc length [$(f')^2+(g')^2=1$] and such that $f>0$.  Let $S$ be the surface of revolution generated by $\alpha$,
$$\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$$
Recall that the first fundamental form coefficients are $E=1,F=0,G=f(u)^2$.  Use this to show that the surface area of $\alpha$ \---- all around the $z$-axis for $s\in J$ \---- is equal to
$$\int_J 2\pi f(s)\,ds.$$

\item (a) Let $A$ be an $n\times m$ matrix.  If $A$ has linearly independent columns, show that $A$ admits a unique factorization $A=QR$ where $Q$ is an $n\times m$ matrix with orthonormal columns, and $R$ is an $m\times m$ upper triangular matrix with positive diagonal entries.  [Use the Gram-Schmidt process to turn $A$'s columns into an orthonormal basis.]

(b) If $A=\begin{bmatrix}\uparrow&\dots&\uparrow\\\vec v_1&\dots&\vec v_m\\\downarrow&\dots&\downarrow\end{bmatrix}$ is an $n\times m$ matrix, let $P$ be the parallelepiped with $A$'s columns as the side lengths; this is
$$P=\{c_1\vec v_1+c_2\vec v_2+\dots+c_m\vec v_m:c_1,c_2,\dots,c_m\in[0,1]\}.$$
Then the $m$-dimensional volume of this parallelepiped is $\sqrt{\det(A^TA)}$.  [If $A$ has linearly dependent columns, it should be clear that $A^TA$ is singular and the parallelepiped is contained in an $(m-1)$-plane, hence both values are zero.  Thus, assume $A$ has linearly independent columns.  Then write the factorization $A=QR$ of part (a); show that $A^TA=R^TR$ and $\sqrt{\det(A^TA)}=\det R$.  Since $R$ is upper triangular, $\det R$ is the product of its diagonal entries.  Now use the familiar properties of volume (rescaling in one dimension results in the volume being multiplied by that amount, and linear shearing does not change the volume).]

\item Fix real numbers $a>b>0$.  The \textbf{torus} is obtained by taking a circle of radius $b$ contained in the $xz$-plane, with center $(a,0,0)$, then revolving it around the $z$-axis.  In other words, it is the surface of revolution with generating curve $\alpha(t)=(a+b\cos t,b\sin t)$, so that it has the parametrization
$$\mathbf x(u,v)=((a+b\cos u)\cos v,(a+b\cos u)\sin v,b\sin u)\text{ for }0<u<2\pi,0<v<2\pi.$$
Find the first fundamental form of the torus, and use it to show that its surface area is $4\pi^2ab$.

Does it make sense that the answer is $(2\pi a)(2\pi b)$?

\item This exercise generalizes the results of this chapter to higher dimensions.  Recall [Exercise 5 of the previous section] that a subset $S\subset\mathbb R^n$ is a regular manifold of dimension $m$, if for each $p\in S$ there exist open sets $U\subset\mathbb R^m,p\in W\subset\mathbb R^n$ along with a differentiable map $\mathbf x:U\to W\to S$, such that the differential has rank $m$ everywhere and $\mathbf x$ has a continuous inverse.

If $\mathbf x$ is such a local coordinate chart, the \textbf{first fundamental form coefficients} refer to the scalar functions $F_{ij}=\mathbf x_{u_i}\cdot\mathbf x_{u_j}$ for $1\leqslant i,j\leqslant m$.  Note that the symmetry of the dot product entails $F_{ij}=F_{ji}$, hence there are really $\frac{m(m+1)}2$ such coefficients (for $1\leqslant i\leqslant j\leqslant m$).  It is worth observing that in the case $n=3,m=2$ covered in the section, we used $E=F_{11},F=F_{12},G=F_{22}$. % Do Carmo uses g_{ij} for F_{ij} but I personally think that's a bit dumb

(a) The symmetric matrix $F=\begin{bmatrix}F_{ij}\end{bmatrix}_{1\leqslant i,j\leqslant m}$ is positive definite at every point of $U$.  [Use Exercise 8 of Section 3.5.]

(b) If $\mathbf x$ is a bounded coordinate chart, the $m$-dimensional volume of $\mathbf x(U)$ is
$$\int_U\sqrt{\det F}\,du_1\dots du_n.$$
[Approximate the volume using parallelepipeds.  Then use Exercise 10(b), noting that $F=(d\mathbf x)^T(d\mathbf x)$.]

(c) Suppose $S$ is a regular manifold of dimension $m$.  If $m'\leqslant m$, $V\subset\mathbb R^{m'}$ and $\alpha:V\to S$ is a differentiable function.  Assume $\alpha$ factors as $\mathbf x\circ\beta$ with $\beta:V\to U$ differentiable.  Show that $\alpha$'s differential is injective everywhere if and only if $\beta$'s is.

(d) Moreover, the $m'$-dimensional volume of $\alpha(V)$ is equal to
$$\int_V\sqrt{\det((d\beta)^TF(d\beta))}\,dv_1\dots dv_{m'}.$$
[Apply part (b) to $\alpha$ directly, as giving an $m'$-dimensional manifold.]  In particular, taking $m'=1$, the length of a curve $\alpha:I\to S$ is equal to $\int_I\sqrt{\beta'(t)\cdot F\beta'(t)}\,dt$, where $\beta:I\to U$ is vector-valued in the single variable $t$.

(e) Find the three-dimensional volume of the 3-sphere $\{(x,y,z,w)\in\mathbb R^4:x^2+y^2+z^2+w^2=1\}$.  [Parametrize it via
$$\mathbf x(u_1,u_2,u_3)=(\sin u_1\sin u_2\cos u_3,\sin u_1\sin u_2\sin u_3,\sin u_1\cos u_2,\cos u_1)$$
with $0<u_1<\pi,0<u_2<\pi,0<u_3<2\pi$.] % It's 2\pi^2, by the way.  [The (unit) n-sphere's rim volume is a rational number times \pi^{\lceil n/2\rceil}.  Since the n-sphere's interior volume is exactly 1/(n+1) of that, it's also a rational number times \pi^{\lceil n/2\rceil}]

(f) Assume $m=n-1$ and $\mathbf x(u_1,\dots,u_{n-1})=(u_1,\dots,u_{n-1},f(u_1,\dots,u_{n-1}))$ is a graph of an explicit function.  Then $F_{ij}=\delta_{ij}+f_{u_i}f_{u_j}$,\footnote{We recall that the Kronecker delta $\delta_{ij}$ is equal to $1$ if $i=j$ and $0$ otherwise.} and $\det F=1+f_{u_1}^2+\dots+f_{u_{n-1}}^2=1+\|\vec\nabla f\|^2$.  [To find $\det F$, first note that $F=I_{n-1}+(\vec\nabla f)(\vec\nabla f)^T$ where $\vec\nabla f$ is considered as a column vector.  Clearly $(\vec\nabla f)(\vec\nabla f)^T$ has rank $1$ (unless $\vec\nabla f=0$, in which case it is zero), so that $n-2$ of its eigenvalues are zero.  The remaining eigenvalue is the sum of the eigenvalues, which is the trace, which is $\|\vec\nabla f\|^2$ (why?).  Consequently, $(\vec\nabla f)(\vec\nabla f)^T$ has characteristic polynomial $x^{n-1}-\|\vec\nabla f\|^2x^{n-2}$.  From here it should be easy to find the determinant of $F$.]
\end{enumerate}

\subsection*{6.4. Isometries and Conformal Mappings}
\addcontentsline{toc}{section}{6.4. Isometries and Conformal Mappings}
We recall (from earlier chapters) that the best way to study intrinsics of geometry in a plane is to understand the isometries of the plane.  This is not exactly so for differential geometry, but it would still help to know how isometries are defined.

By an isometry is meant a map which preserves distances.  However, we cannot directly transfer this definition because we do not yet have a notion of distance on a surface, and even if we did, it would deviate from the purpose of this chapter.  The lengths of curves and areas of regions were computed using calculus and infinitesimal data.  Thus we want a definition of isometries in terms of infinitesimal data, rather than just defining it to be a map preserving a distance metric $S\times S\to\mathbb R_{\geqslant 0}$. % Yep, Chapter 6's purpose is to study geometry with differential metrics, after the previous chapters used purely algebra. ^^

The ``infinitesimal piece of data'' giving the length of a curve (provided the curve is parametrized) is the magnitude of its derivative vector, as we recall that the length of $\alpha:I\to S$ is equal to $\int_I\|\alpha'(t)\|\,dt$.  Thus our desire is for isometries to preserve the magnitudes of tangent vectors.  This is a surprisingly easy property to deal with.\\

\noindent\textbf{Proposition 6.5 and Definition.} \emph{Let $S_1$ and $S_2$ be regular surfaces, and $\varphi:S_1\to S_2$ a bijective smooth map.  Then the following are equivalent:}

(i) \emph{$\varphi$ preserves dot products; for all $p\in S_1$ and $\vec v,\vec w\in T_pS_1$, we have $d\varphi_p(\vec v)\cdot d\varphi_p(\vec w)=\vec v\cdot\vec w$.}

(ii) \emph{$\varphi$ preserves lengths of tangent vectors; for all $p\in S_1$ and $\vec v\in T_pS_1$, we have $\|d\varphi_p(\vec v)\|=\|\vec v\|$.}

(iii) \emph{$\varphi$ preserves lengths of curves; if $\alpha:I\to S_1$ is a regular curve, then the length of $\varphi\circ\alpha:I\to S_2$ is equal to the length of $\alpha$.}

\emph{Under these conditions, $\varphi$ is said to be a \textbf{(global) isometry} from $S_1$ to $S_2$.}\\
\begin{proof}
(i) $\implies$ (ii) by taking $\vec v=\vec w$ in (i), since $\|\vec v\|=\sqrt{\vec v\cdot\vec v}$.

(ii) $\implies$ (i): Recall the formula $\vec v\cdot\vec w=\frac 12(\|\vec v+\vec w\|^2-\|\vec v\|^2-\|\vec w\|^2)$.  The argument of Lemma 2.44 (iii) $\implies$ (ii) then applies.

(ii) $\implies$ (iii): By the Chain Rule, $(\varphi\circ\alpha)'(t)=d\varphi_{\alpha(t)}(\alpha'(t))$.  Moreover,
$$\int_I\|(\varphi\circ\alpha)'(t)\|\,dt=\int_I\|d\varphi_{\alpha(t)}(\alpha'(t))\|\,dt=\int_I\|\alpha'(t)\|\,dt$$
(using (ii)), so that $\varphi\circ\alpha$ and $\alpha$ have the same length.

(iii) $\implies$ (ii): Let $\alpha:(-\varepsilon,\varepsilon)\to S_1$ be any smooth curve with $\alpha(0)=p,\alpha'(0)=\vec v$.  Then for any $0\leqslant\delta<\varepsilon$, (iii) [for the curve $\alpha|_{[0,\delta]}$] implies
$$\int_0^\delta\|\alpha'(t)\|\,dt=\int_0^\delta\|(\varphi\circ\alpha)'(t)\|\,dt=\int_0^\delta\|d\varphi_{\alpha(t)}(\alpha'(t))\|\,dt.$$
Differentiating with respect to $\delta$ gives $\|\alpha'(\delta)\|=\|d\varphi_{\alpha(\delta)}(\alpha'(\delta))\|$ by the first fundamental theorem of calculus, and taking $\delta=0$ we get $\|\vec v\|=\|d\varphi_p(\vec v)\|$ as desired.
\end{proof}

\noindent Thus we have a notion of isometries of regular surfaces; it uses smooth maps which preserve lengths of tangent vectors. % , rather than arbitrary maps which preserve a distance metric (in the sense $S\times S\to\mathbb R_{\geqslant 0}$). (moved before)

Several examples are in order.  For instance, if $S_1$ is the strip $z=0,-\pi<x<\pi$ (which can be parametrized via $\mathbf x(u,v)=(u,v,0),u\in(-\pi,\pi),v\in\mathbb R$), and $S_2$ is the portion of the cylinder $x^2+y^2=1$ which omits the meridian $x=-1$, then the map $\varphi:S_1\to S_2$ given by
$$\varphi(u,v,0)=(\cos u,\sin u,v)$$
is an isometry.  After all, it is readily seen to be a smooth bijection, and for $p=(u,v,0)\in S_1$ and $\vec v\in\mathbb R^2=T_pS_1$, say $\vec v=(a,b)$, we have
$$d\varphi_p(\vec v)=\begin{bmatrix}-\sin u&0\\\cos u&0\\0&1\end{bmatrix}(\vec v)=(-a\sin u,a\cos u,b),$$
so that $d\varphi_p(\vec v)$ and $\vec v$ both have magnitude $\sqrt{a^2+b^2}$.

On the other hand, suppose $S$ is an arbitrary surface of revolution; i.e.,
$$\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$$
where $\alpha(t)=(f(t),g(t))$ is an embedded regular curve with $f(t)>0$.  Then all rotations of $S$ around the $z$-axis are global isometries from $S$ to itself; we leave it to the reader to work out the details.\\

\noindent We recall (Section 2.6) that every isometry of $\mathbb R^n$ (in the metric preserving sense) is the composition of an orthogonal matrix and a translation.  The same is true for the differential definition of an isometry of $\mathbb R^n$.  We will prove the case $n=2$ by consideration of the $xy$-plane, henceforth to be denoted $P$. % Recall Section 2.6.  Prove that the *new* kind of isometries of the plane satisfies Proposition 2.46.  Exercise 1 will do this for the cylinder and sphere; the last exercise of the section will generalize to higher dimensions.
% You say "in the distance metric preserving sense" should be "in the metric preserving sense", though Sagun Chanillo also used the word "metric" for the differential kind

Let $\varphi:P\to P$ be an isometry.  Then $\varphi$ can be regarded as a smooth bijection from $\mathbb R^2$ to itself.  Every $d\varphi_p$ for $p\in\mathbb R^2$ is a $2\times 2$ matrix which preserves dot products (by definition of an isometry); hence Lemma 2.44 implies $d\varphi_p\in O(2)$.  Thus $\varphi$ has an orthogonal differential at every point.  However, we do not yet know if its differential is the \emph{same} at every point.

To diagnose this, we first note that
$$d\varphi_p=\begin{bmatrix}\uparrow&\uparrow\\\varphi_x&\varphi_y\\\downarrow&\downarrow\end{bmatrix}(p),$$
and since this is orthogonal, we have $\varphi_x\cdot\varphi_x=1,\varphi_x\cdot\varphi_y=0,\varphi_y\cdot\varphi_y=1$ throughout the domain plane $\mathbb R^2$.  Taking partial derivatives of these equations with respect to $x$ and $y$,
$$\varphi_x\cdot\varphi_{xx}=0,~~~~\varphi_x\cdot\varphi_{xy}=0$$
$$\varphi_{xx}\cdot\varphi_y+\varphi_x\cdot\varphi_{xy}=0,~~~~\varphi_{xy}\cdot\varphi_y+\varphi_x\cdot\varphi_{yy}=0$$
$$\varphi_y\cdot\varphi_{xy}=0,~~~~\varphi_y\cdot\varphi_{yy}=0.$$
[We have made use of Exercise 4(a) of Section 6.1, which implies, e.g., that $\frac{\partial}{\partial x}(\varphi_x\cdot\varphi_y)=\varphi_{xx}\cdot\varphi_y+\varphi_x\cdot\varphi_{xy}$.]  Note also that $\{\varphi_x,\varphi_y\}$ is an orthonormal \emph{basis}; hence $\varphi_x\cdot\vec v=\varphi_y\cdot\vec v=0$ implies $\vec v=\vec 0$ for any vector $\vec v$.  Using this:
\begin{itemize}
\item We have $\varphi_{xy}=0$, because $\varphi_x\cdot\varphi_{xy}$ and $\varphi_y\cdot\varphi_{xy}$ are both zero.

\item We may thus eliminate the terms involving $\varphi_{xy}$ from the middle two equations, to get $\varphi_{xx}\cdot\varphi_y=0$ and $\varphi_x\cdot\varphi_{yy}=0$.

\item Consequently, $\varphi_{xx}=\varphi_{yy}=0$ because each one is orthogonal to both $\varphi_x$ and $\varphi_y$.
\end{itemize}
Thus we've proven all of the second-order derivatives $\varphi_{xx},\varphi_{xy},\varphi_{yy}$ to be zero.  This means that $\varphi_x$ and $\varphi_y$ are \emph{constant}.  Hence, $d\varphi_p$ is constant and independent of the point $p$.

We now know that $d\varphi_p$ is a constant orthogonal matrix, say $A=\begin{bmatrix}a&b\\c&d\end{bmatrix}\in O(2)$.  If we write $\varphi(x,y)=(f(x,y),g(x,y))$ with $f,g:\mathbb R^2\to\mathbb R$ smooth, this means that $A=d\varphi_p=\begin{bmatrix}f_x&f_y\\g_x&g_y\end{bmatrix}(p)$; since $f_x$ and $f_y$ are the respective constants $a,b$, we have $f(x,y)=ax+by+j$ for some constant $j\in\mathbb R$.  Similarly, $g(x,y)=cx+dy+k$ for some $k\in\mathbb R$.

From this we conclude at once that $\varphi(\vec x)=A\vec x+\vec v$ with $\vec v=(j,k)$; in other words, $\varphi$ is the composition of an orthogonal matrix and a translation.  The same argument works in higher dimensions, but a little more casework will be called for; See Exercise 8(b).\\

\noindent Sometimes it helps to know that surfaces are isometric in small regions but not necessarily as their whole selves.  If $p\in S_1$ and $q\in S_2$, the surfaces are said to be \textbf{locally isometric} (at $p$ and $q$), if there exist open neighborhoods $p\in U_1\subset S_1,q\in U_2\subset S_2$ and an isometry $\varphi:U_1\to U_2$ such that $\varphi(p)=q$.

For example, the plane $z=0$ is locally isometric to the cylinder $x^2+y^2=1$; if we let $U=\{(u,v,0):-\pi<u<\pi\}$ and $V=\{(x,y,z):x^2+y^2=1,x\ne -1\}$ then the map
$$(u,v,0)\mapsto(\cos u,\sin u,v)$$
is a local isometry; yet it is clear that they are not \emph{globally} isometric.  The plane is also locally isometric to the cone [Exercise 2].

$S_1$ and $S_2$ are said to be \textbf{locally isometric} if for all $p\in S_1$ and $q\in S_2$, the surfaces are locally isometric at $p$ and $q$. % "\forall p,\forall q,locally isometric at p and q" and "\forall p,\exists q,locally isometric at p and q" are both worthy terms.  I think the former's the right definition.  Also, I think you have the definition of f being a local isometry, whereas I was trying to define two surfaces being locally isometric (without a prescribed function).
The fundamental thing about local isometries is that they can be easily read off of the first fundamental form coefficients:\\

\noindent\textbf{Proposition 6.6.} \emph{Let $S_1$ and $S_2$ be regular surfaces, and $p\in S_1,q\in S_2$ points.  Then $S_1$ is locally isometric to $S_2$ at $p$ and $q$, if and only if there exist an open set $0\in U\subset\mathbb R^2$ and local coordinate charts $\mathbf x:U\to S_1,\mathbf y:U\to S_2$ with $\mathbf x(0)=p,\mathbf y(0)=q$, which have the same first fundamental form as functions in $U$.}
\begin{proof}
Suppose $S_1$ is locally isometric to $S_2$ at $p$ and $q$, and that $\varphi:W\to V$ is an isometry of open neighborhoods sending $p\mapsto q$.  Let $\mathbf x:U\to S_1$ be a local coordinate chart sending $0\mapsto p$; by shrinking $U$ to $U\cap\mathbf x^{-1}(W)$, we may assume $\mathbf x(U)\subset W$.  Moreover, let $\mathbf y=\varphi\circ\mathbf x$.  Then $\mathbf y$ has a continuous inverse, since it is a composition of two functions that do.  We have
$$\mathbf y_u=(\varphi\circ\mathbf x)_u=d\varphi(\mathbf x_u),$$
where $d\varphi$ is taken at whatever point it needs to be.  Likewise $\mathbf y_v=d\varphi(\mathbf x_v)$.  From this we get $\mathbf y_u\cdot\mathbf y_u=d\varphi(\mathbf x_u)\cdot d\varphi(\mathbf x_u)=\mathbf x_u\cdot\mathbf x_u$ ($\varphi$ is an isometry), hence $\mathbf x$ and $\mathbf y$ have the same first fundamental coefficient $E$.  Similar arguments show that the other first fundamental form coefficients match, hence we have the desired charts.  [In addition, we have only now confirmed that $\mathbf y$'s differential has rank $2$, because that is equivalent to saying $EG-F^2>0$ and $\mathbf x,\mathbf y$ have that expression the same.]

Conversely, suppose the local coordinate charts $\mathbf x:U\to S_1,\mathbf y:U\to S_2$ exist with the same coefficients $E,F,G$.  Then if $W=\mathbf x(U)$, $\mathbf x$ has a continuous inverse $W\to U$ (condition (ii) in the definition of a regular surface).  Let $\varphi=\mathbf y\circ\mathbf x^{-1}:W\to S_2$; we claim that $\varphi$ is an isometry sending $p\mapsto q$.  It is clear that $\varphi$ sends $p$ to $q$ and is a bijection between open sets.  To show that $\varphi$ is an isometry, we just need to take $b=\mathbf x(a)\in W$ and show that $\|d\varphi_b(\vec v)\|=\|\vec v\|$ for tangent vectors $\vec v=c\mathbf x_u+d\mathbf x_v$ (Proposition 6.5).  Indeed,
$$\|\vec v\|=\|c\mathbf x_u+d\mathbf x_v\|=\sqrt{Ec^2+2Fcd+Gd^2};$$
and since $d\varphi_b(\vec v)=c\mathbf y_u+d\mathbf y_v$ by $\varphi\circ\mathbf x=\mathbf y$ and the Chain Rule, we have
$$\|d\varphi_b(\vec v)\|=\|c\mathbf y_u+d\mathbf y_v\|=\sqrt{Ec^2+2Fcd+Gd^2}$$
as well.
\end{proof}
\noindent Using this, we shall prove an isometry which is slightly less obvious: the helicoid and the catenoid from Section 6.2.  We recall that the helicoid is parametrized by taking the line $y=z=0$, and rotating around the $z$-axis while translating it along the $z$-axis:
$$\mathbf h(u,v)=(u\cos v,u\sin v,v)$$
But for our purposes, we will think of $u$ as a different parameter from the arc length along the generating line.  We will specifically reparametrize the generating line so that the arc length is the hyperbolic sine of $u$:
\begin{equation}\tag{1}\mathbf h(u,v)=(\sinh u\cos v,\sinh u\sin v,v)\end{equation}
The catenoid, on the other hand, is the surface of revolution given by the curve $t\mapsto(\cosh t,t)$:
\begin{equation}\tag{2}\mathbf c(u,v)=(\cosh u\cos v,\cosh u\sin v,u)\end{equation}
Direct computation shows that $E=G=\cosh^2u$ and $F=0$ for each of the equations (1), (2).  For instance, $\mathbf h_u=(\cosh u\cos v,\cosh u\sin v,0)$ and $\mathbf h_v=(-\sinh u\sin v,\sinh u\cos v,1)$, and one can directly take the dot products (recalling $\sinh^2a+1=\cosh^2a$).  Hence, the helicoid and catenoid are locally isometric by Proposition 6.6, and going backwards through one local coordinate chart and forwards through the other gives the isometry.\\

\noindent\textbf{CONFORMAL MAPPINGS}\\

\noindent We recall (Section 4.1) that a mapping is said to be \textbf{conformal} if it preserves angles.  In differential geometry, there is a rather brief way to characterize this property, just as there was for isometries.  Take, for example, stereographic projection:
$$\mathbf x:\mathbb R^2\to S^2,\mathbf x(u,v)=\left(\frac{2u}{u^2+v^2+1},\frac{2v}{u^2+v^2+1},\frac{u^2+v^2-1}{u^2+v^2+1}\right)$$
Exercise 4(b) of Section 4.1 shows that this map is conformal, by showing that for each $p\in\mathbb R^2$ there is a constant $\lambda>0$ which depends only on $p$, such that $d\mathbf x_p(\vec v_1)\cdot d\mathbf x_p(\vec v_2)=\lambda^2\vec v_1\cdot\vec v_2$ for all $\vec v_1,\vec v_2$ tangent vectors at $p$.

The concept carries over to general regular surfaces:\\

\noindent\textbf{Proposition 6.7 and Definition.} \emph{Let $S_1$ and $S_2$ be regular surfaces, and $\varphi:S_1\to S_2$ a smooth map.  Then the following are equivalent:}

(i) \emph{$\varphi$ preserves angles between curves.}

(ii) \emph{For each $p\in S_1$, $d\varphi_p$ preserves angles between tangent vectors.}

(iii) \emph{For each $p\in S_1$, there exists a scalar $\lambda>0$ (which depends only on $p$) such that $d\varphi_p(\vec v)\cdot d\varphi_p(\vec w)=\lambda^2\vec v\cdot\vec w$ for all $\vec v,\vec w\in T_pS_1$.}

\emph{In these conditions, $\varphi$ is said to be a \textbf{conformal mapping}.  If, moreover, $\varphi$ is bijective, it is called a \textbf{conformal equivalence}.}
\begin{proof}
(i) $\iff$ (ii) because by the Chain Rule, $\varphi$ and $d\varphi_p$ commute with taking tangent vectors to curves.

(ii) $\implies$ (iii). Let $\{\vec u_1,\vec u_2\}$ be an \emph{orthonormal} basis of $T_pS_1$.  Then $d\varphi_p(\vec u_1)\perp d\varphi_p(\vec u_2)$, because $d\varphi_p$ preserves the right angle by assumption.  Moreover, $\vec u_1,\vec u_1+\vec u_2$ have an angle of exactly $\pi/4$ (why?), hence so do $d\varphi_p(\vec u_1)$ and $d\varphi_p(\vec u_1+\vec u_2)=d\varphi_p(\vec u_1)+d\varphi_p(\vec u_2)$.  A basic geometric argument then shows that $\|d\varphi_p(\vec u_1)\|=\|d\varphi_p(\vec u_2)\|$ (and this is nonzero because the angle is defined); let $\lambda$ be this common value.  Then one can easily prove (iii) is satisfied by writing $\vec v,\vec w$ as linear combinations of $\vec u_1,\vec u_2$.

(iii) $\implies$ (ii). The angle between $\vec v$ and $\vec w$ is $\cos^{-1}\frac{\vec v\cdot\vec w}{\|\vec v\|\|\vec w\|}$.  For each $p\in S_1$, we may let $\lambda$ be the scalar from the hypothesis (iii).  Note that $\|d\varphi(\vec v)\|=\lambda\|\vec v\|$ follows from taking $\vec v=\vec w$.  Hence we have
$$\cos^{-1}\frac{d\varphi_p(\vec v)\cdot d\varphi_p(\vec w)}{\|d\varphi_p(\vec v)\|\|d\varphi_p(\vec w)\|}=\cos^{-1}\frac{\lambda^2\vec v\cdot\vec w}{\lambda\|\vec v\|~\lambda\|\vec w\|}=\cos^{-1}\frac{\vec v\cdot\vec w}{\|\vec v\|\|\vec w\|},$$
from which (ii) follows.
\end{proof}

\noindent If $p\in S_1$ and $q\in S_2$, then $S_1$ is said to be \textbf{locally conformal} to $S_2$ (at $p$ and $q$), if there exist open neighborhoods $p\in U_1\subset S_1,q\in U_2\subset S_2$ and a conformal equivalence $\varphi:U_1\to U_2$ such that $\varphi(p)=q$.  $S_1$ and $S_2$ are said to be \textbf{locally conformal} at $p,q$ whenever $p\in S_1$ and $q\in S_2$. % Same response
We recall that local isometries can be measured via charts with the same first fundamental form; local conformalities have a similar situation:\\

\noindent\textbf{Proposition 6.8.} \emph{Let $S_1$ and $S_2$ be regular surfaces, and $p\in S_1,q\in S_2$ points.  Then $S_1$ is locally conformal to $S_2$ at $p$ and $q$, if and only if there exist an open set $0\in U\subset\mathbb R^2$ and local coordinate charts $\mathbf x:U\to S_1,\mathbf y:U\to S_2$ with $\mathbf x(0)=p,\mathbf y(0)=q$, such that if $E,F,G$ are $\mathbf x$'s first fundamental form coefficients and $\overline E,\overline F,\overline G$ are $\mathbf y$'s, then there is a smooth positive function $\mu:U\to\mathbb R_{>0}$ such that $\overline E=\mu E,\overline F=\mu F,\overline G=\mu G$.}\\

\noindent In effect, two local coordinate charts relate conformally if and only if the first fundamental form coefficients of one are a single-scalar-function multiple of those of the other.  The special case where $S_1=\mathbb R^2$ and $\mathbf x:U\to\mathbb R^2$ is the inclusion map shows that $\mathbf y$ is a conformal parametrization if and only if $\overline E=\overline G(=\mu)$ and $\overline F=0$.
\begin{proof}
Suppose $S_1$ is locally conformal to $S_2$ at $p$ and $q$, and that $\varphi:W\to V$ is an conformal equivalence of open neighborhoods sending $p\mapsto q$.  Then let $\mathbf x:U\to W\subset S_1$ be a local coordinate chart sending $0\mapsto p$, and let $\mathbf y=\varphi\circ\mathbf x$.  Let $E,F,G$ be the first fundamental form coefficients for $\mathbf x$, and let $\overline E,\overline F,\overline G$ be those for $\mathbf y$.  Since $\varphi$ is conformal, there is (by Proposition 6.5) a smooth positive function $\lambda:W\to\mathbb R_{>0}$ such that $d\varphi_p(\vec v)\cdot d\varphi_p(\vec w)=\lambda(p)^2\vec v\cdot\vec w$ whenever $p\in W$ and $\vec v,\vec w$ are tangent at $p$.  Moreover,
$$\overline E=\mathbf y_u\cdot\mathbf y_u=d\varphi_p(\mathbf x_u)\cdot d\varphi_p(\mathbf x_u)=\lambda(p)^2\mathbf x_u\cdot\mathbf x_u=\lambda(p)^2E,$$
and likewise $\overline F=\lambda^2F$ and $\overline G=\lambda^2G$, so we may take $\mu=\lambda^2$.  [Again we only now know that $\mathbf y$ has injective differentials \---- why?]

Conversely, suppose the local coordinate charts $\mathbf x:U\to S_1,\mathbf y:U\to S_2$ have first fundamental form coefficients such that $\overline E=\mu E,\overline F=\mu F,\overline G=\mu G$.  As in Proposition 6.6, we use the fact that $\mathbf x$ has a continuous inverse to construct $\varphi=\mathbf y\circ\mathbf x^{-1}:W\to S_2$.  We claim that $\varphi$ is a conformal mapping sending $p\mapsto q$.  For $b=\mathbf x(a)\in W$, we claim that $d\varphi_b(\vec v)\cdot d\varphi_b(\vec w)=\mu\vec v\cdot\vec w$ whenever $\vec v,\vec w$ are tangent at $b$; then we can take $\lambda=\sqrt{\mu}$ and satisfy criterion (iii) in Proposition 6.5.

Well, this is a straightforward calculation.  Write $\vec v=c_1\mathbf x_u+d_1\mathbf x_v$ and $\vec w=c_2\mathbf x_u+d_2\mathbf x_v$.  Then
$$\vec v\cdot\vec w=c_1c_2E+(c_1d_2+d_1c_2)F+d_1d_2G$$
Also, $d\varphi_b(\vec v)=c_1\mathbf y_u+d_1\mathbf y_v$ and $d\varphi_b(\vec w)=c_2\mathbf y_u+d_2\mathbf y_v$ since $\varphi\circ\mathbf x=\mathbf y$; moreover,
$$d\varphi_b(\vec v)\circ d\varphi_b(\vec w)=(c_1\mathbf y_u+d_1\mathbf y_v)\cdot(c_2\mathbf y_u+d_2\mathbf y_v)$$
$$=c_1c_2\overline E+(c_1d_2+d_1c_2)\overline F+d_1d_2\overline G=\mu[c_1c_2E+(c_1d_2+d_1c_2)F+d_1d_2G].$$
Since the last expression is equal to $\mu[\vec v\cdot\vec w]$, this completes the proof.
\end{proof}

\noindent We conclude this section by giving examples of conformal mappings and projections.

First, suppose $S_1$ is any regular surface; fix $\lambda>0$ and let $S_2=\{\lambda\vec x:\vec x\in S_1\}$.  Then $S_2$ is also a regular surface, as one can readily see.  The map $\vec x\mapsto\lambda\vec x$ is a conformal equivalence from $S_1$ to $S_2$ (Exercise 4(b)), though it is not an isometry unless $\lambda=1$.

Stereographic projection is conformal, when regarded as a map from $S^2-\{(0,0,1)\}$ to the $xy$-plane.  For open sets $U\subset\mathbb C$, holomorphic functions $f:U\to\mathbb C$ with nonzero derivative are conformal.  M\"obius transformations are conformal when viewed on dense open subsets of the $xy$-plane.  In particular, the central inversion $(x,y)\mapsto\left(\frac x{x^2+y^2},\frac y{x^2+y^2}\right)$ is a conformal map on the $xy$-plane with the origin deleted.

An example of a conformal map which we have not previously covered is \textbf{Mercator's projection}, which takes all latitudes and longitudes of the sphere to Euclidean lines in the plane which are horizontal and vertical respectively.  Geographical maps of the world can be shown using this projection (since the Earth is spherical):
\begin{center}
\includegraphics[scale=.2]{MercatorsProjection.jpg}
\end{center}
To establish this, we will parametrize the sphere and the $xy$-plane in ways that satisfy Proposition 6.8's conditions:
$$\mathbf x(u,v)=(\sin u\cos v,\sin u\sin v,\cos u),0<u<\pi,0<v<2\pi$$
$$\mathbf y(u,v)=(v,\ln(\tan(u/2)),0),\text{ for all }u,v\in\mathbb R$$
We recall that $\mathbf x$'s first fundamental form coefficients are $E=1,F=0,G=\sin^2u$.  Direct computation shows that the first fundamental coefficients of $\mathbf y$ are $\overline E=\csc^2u,\overline F=0,\overline G=1$ (we recall that $\frac{d}{du}\ln(\tan(u/2))=\csc u$).  Proposition 6.8's criterion is thus satisfied with $\mu=\csc^2u$.  Hence the proposition implies that $\mathbf x$ and $\mathbf y$ relate by a local conformal mapping $\varphi$.  We may define $\varphi$ on the dense subset of the sphere in the image of $\mathbf x$, and it turns out to be
$$\varphi(x,y,z)=\left(\tan^{-1}_2(y,x),\ln\frac{\sqrt{x^2+y^2}}{1+z},0\right)\text{ for }(x,y,z)\in S^2,y\ne 0\text{ or }x<0$$
(this formula uses $\tan(u/2)=\frac{\sin u}{1+\cos u}$).  A formula for the inverse (from the plane to the sphere) would be
$$\varphi^{-1}(x,y,0)=(\operatorname{sech}y\cos x,\operatorname{sech}y\sin x,\tanh y).$$
[We leave it to the reader to show that $(u,v)\mapsto(\operatorname{sech}v\cos u,\operatorname{sech}v\sin u,\tanh v)$ is in fact a conformal map $\mathbb R^2\to\mathbb R^3$, which gives another way to view Mercator's projection.]

We recall loxodromes on the sphere from the previous section ($v\cot\beta+A=\ln(\tan(u/2))$).  Observe that \emph{Mercator's projection sends loxodromes to straight lines} \---- the projection is conformal, so as the loxodrome meets all the meridians at a constant angle, its projection meets all vertical lines at a constant angle.  This fact and the formula for the projection together provide another way to find the equations of loxodromes.

It is actually a deep theorem that any two regular surfaces are locally conformal.  In view of Proposition 6.6, this is equivalent to saying that any regular surface has isothermal coordinates (Exercise 2 of Section 6.3).  We will not delve into details here. % No, it's not the Riemann mapping theorem ---- the Riemann mapping theorem states that every open set of \mathbb R^2, which is simply connected, and not all of \mathbb R^2, is globally conformal to the open disk.  This statement is local conformality of regular surfaces in \mathbb R^3.  See page 227 of the green do Carmo book.

\subsection*{Exercises 6.4. (Isometries and Conformal Mappings)} % Recall isometries from Chapters 2, 4, 5 of Euclidean, hyperbolic and spherical geometry.
% Then explain the desire to generalize the idea.  A diff. map is an isometry iff it preserves lengths of curves, i.e., its differential everywhere
% is an orthogonal map of inner product spaces.  Also discuss conformal maps, using stereographic projection as motivation.
% POTENTIAL: Show that the plane, cone, cylinder are locally isometric; as are the helicoid and catenoid
\begin{enumerate}
\item (a) Show that every isometry of the cylinder $x^2+y^2=1$ in $\mathbb R^3$ is the composition of an orthogonal transformation in the $xy$-plane, with either a translation or reflection in the $z$-direction.  [Imitate the argument given for the plane.  It would help to use local coordinates.]  Note that there exist isometries of the cylinder with exactly two fixed points. %Do Carmo, Exercise 12 of p. 229

(b) Show that every isometry of the sphere $x^2+y^2+z^2=1$ is an orthogonal transformation (in $O(3)$).  [Use Proposition 1.9 to assume the isometry fixes a certain point and tangent vector at your convenience.  Then use a local coordinate argument.]

\item Show that the plane is locally isometric to the cone.  [Think of the plane in polar coordinates.  The lines on the cone from the tip correspond to the rays in the plane coming from the origin.]

\item If $S$ is a surface of revolution, every rotation of $S$ around the $z$-axis is a global isometry.

\item Let $S$ be a regular surface.

(a) Applying an element of $\operatorname{Isom}(\mathbb R^3)$ to $S$ results in a surface which is globally isometric to $S$.

(b) Applying a dilation $\vec v\mapsto\lambda\vec v,\lambda>0$ results in a surface which is conformally equivalent to $S$.  [Show that dilations are conformal.]

\item Suppose a triangle on $S^2$ is made up of three segments of loxodromes (see Section 6.3), and does not touch either of the poles.  Then the interior angles of the triangle add to $\pi=180^\circ$.  [Consider Mercator's projection.]

\item (a) Let $\varphi:S_1\to S_2$ be a smooth map of regular surfaces.  Then $\varphi$ preserves area if and only if for every point $p\in S_1$, there exist local coordinate charts $\mathbf x:U\to S_1$ (sending $0\mapsto p$) and $\mathbf y=\varphi\circ\mathbf x:U\to S_2$, such that if $E,F,G$ are $\mathbf x$'s first fundamental form coefficients and $\overline E,\overline F,\overline G$ are $\mathbf y$'s, then $EG-F^2=\overline E\,\overline G-\overline F^2$.  [Compare to Exercise 8 of Section 6.3.]

(b) Prove that every area-preserving conformal equivalence is an isometry.  [Use part (a) and Proposition 6.8.]

\item\emph{(Surfaces of revolution relate to the plane.)} \---- Let $S$ be a surface of revolution, parametrized via
$$\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$$
with $f>0$ and $(f',g')\ne 0$.

(a) Parametrize the $xy$-plane via
$$\mathbf y(u,v)=\left(v,\int_{u_c}^u\frac{\sqrt{f'(t)^2+g'(t)^2}}{f(t)}\,dt\right);$$
with $u_c$ fixed, and show that $\mathbf x$ and $\mathbf y$ have first fundamental coefficients satisfying Proposition 6.8's criterion.

(b) Conclude that there is a local conformal mapping from $S$ to the plane, sending parallels of latitude ($v\mapsto\mathbf x(u_0,v)$) to horizontal lines, and meridians to vertical lines.

(c) Now parametrize the $xy$-plane via
$$\mathbf z(u,v)=\left(v,\int_{u_c}^uf(t)\sqrt{f'(t)^2+g'(t)^2}\,dt\right)$$
and use Exercise 6(a) to get a (local) area-preserving map from $S$ to the plane, also sending parallels of latitude to horizontal lines and meridians to vertical lines.

\item All of the material of this section can readily be generalized to arbitrary dimensions.

Let $m_1,m_2\leqslant n$ and let $S_1,S_2\subset\mathbb R^n$ be regular manifolds of respective dimensions $m_1,m_2$.  If $\varphi:S_1\to S_2$ is a smooth map, then $\varphi$ is said to be \textbf{isometric} if $\varphi$ preserves lengths of curves, and an \textbf{isometry} if it is an isometric homeomorphism.

(a) Recall the first fundamental form coefficients $F_{ij}$ from Exercise 11 of the previous section.  Prove analogues of Propositions 6.5 and 6.6 for arbitrary dimensions.  [Note that an isometric mapping $\varphi$ is necessarily an immersion, and hence $m_1\leqslant m_2$.  If $m_1\ne m_2$, then Proposition 6.6's analogue is a little different.]

(b) Prove that any isometry from $\mathbb R^n$ to itself (in the sense of this chapter) is the composition of an orthogonal matrix and a translation.  Is an isometric mapping from $\mathbb R^m$ to $\mathbb R^n$ necessarily the composition of a matrix with orthonormal columns and a translation?

$\varphi$ is to be \textbf{conformal} if $\varphi$ is an immersion which preserves angles between curves.  [This implies $m_1\leqslant m_2$.]  By reasons we already know, this is equivalent to $\varphi$ preserving angles between tangent vectors.

(c) Prove analogues of Propositions 6.7 and 6.8 for arbitrary dimensions.  [A conformal map is necessarily an immersion.]

(d) If $\varphi$ is conformal and preserves volumes, then $\varphi$ is an isometry.
\end{enumerate}

\subsection*{6.5. The Gauss Map.  Gaussian Curvature}
\addcontentsline{toc}{section}{6.5. The Gauss Map.  Gaussian Curvature}
In Section 6.3, we have used parametrizations of surfaces to measure lengths and areas on them.  We would still like to be able to tell the ``level of curviness'' of a surface, which is not determined by the isometry class.  For this we introduce the concept of the Gauss map.

If $S$ is a regular surface, then at each $p\in S$, there are two unit normal vectors \---- specifically the unit vectors perpendicular to the tangent plane.  If $\mathbf x:U\to S$ is a local coordinate chart, we can actually form a continuous family of these unit normal vectors, by defining
$$\mathbf N:U\to\mathbb R^3\text{ via }\mathbf N(p)=\frac{\mathbf x_u\times\mathbf x_v}{\|\mathbf x_u\times\mathbf x_v\|}(p).$$
\begin{center}
\includegraphics[scale=.5]{GaussMap.png}
\end{center}
Then $\mathbf N(p)$ is a unit vector normal to $S$ at $\mathbf x(p)$ [note $\|\mathbf x_u\times\mathbf x_v\|\ne 0$ since $d\mathbf x_p$ has rank $2$].  It is worth asking whether one can define a continuous map on the \emph{entire surface} which outputs unit normal vectors like this.  On certain surfaces, this is clearly possible: if $U=S^2$, define $\mathbf N(p)=\vec p$, where $p$ is regarded as a unit vector (noting that the tangent plane to $S^2$ at $p$ is precisely the orthogonal complement of $p$).  If $U$ is the $xy$-plane, define $\mathbf N(p)=\vec e_3=(0,0,1)$ for all $p$.  If $U$ is the cylinder $x^2+y^2=1$, define $\mathbf N((x,y,z))=(x,y,0)$.  All of these are examples of \emph{outward} normals, since the normal vector points outward.  However, there are regular surfaces on which you cannot define $\mathbf N$ globally (see Exercise 1).

Note first that $\mathbf N:S\to\mathbb R^3$ and every $\mathbf N(p)$ is a unit vector, and therefore, $\mathbf N$ can be regarded as a function $S\to S^2$.  Any continuous function $S\to S^2$ (or $U\to S^2$ with $U$ an open set of $S$) sending each $p$ to a normal unit vector to the surface at $p$ is called a \textbf{Gauss map}.  If a regular surface has a global Gauss map, it is said to be \textbf{orientable}.  Observe in this case that it has exactly two distinct Gauss maps: why?

From this point on, we will stick to orientable surfaces, equipped with a particular Gauss map.  If $\mathbf x:U\to S$ is a local coordinate chart, $\mathbf N$ may be regarded as a function in either $U$ or $\mathbf x(U)\subset S$, and we shall use the two interchangeably when $\mathbf x$ is clear from the discussion.  For $p\in U$, $\mathbf N(p)$ is either equal to $\frac{\mathbf x_u\times\mathbf x_v}{\|\mathbf x_u\times\mathbf x_v\|}(p)$ or its negative; and by continuity, either the expressions are equal throughout $U$ (in which case we say $\mathbf x$ is \textbf{orientation-preserving}) or negatives of one another throughout $U$ (where we say $\mathbf x$ is \textbf{orientation-reversing}).
We shall assume every local coordinate chart used to cover the surface is orientation-preserving.\\

\noindent To study curviness, we differentiate the Gauss map.  First, for $p\in S$, since $\mathbf N(p)$ is normal to the surface at $p$, it is orthogonal to the tangent plane of $S$ at $p$.  Yet since $\mathbf N(p)\in S^2$, it is also orthogonal to the tangent plane to the sphere at $\mathbf N(p)$ itself (since the tangent plane is orthogonal to the radius) \---- this means that the two tangent planes coincide in practice.  Hence, $d\mathbf N_p$ is a linear \emph{operator} on a two-dimensional vector space, and we also have an extra property involving dot products:\\

\noindent\textbf{Proposition 6.9.} \emph{Let $S$ be an oriented regular surface (i.e., equipped with a particular Gauss map).  For each $p\in S$,}

(i) \emph{$d\mathbf N_p$ is a linear operator on the two-dimensional space $T_pS$;}

(ii) \emph{$d\mathbf N_p$ is self-adjoint; i.e., $d\mathbf N_p(\vec v)\cdot\vec w=\vec v\cdot d\mathbf N_p(\vec w)$ for all $\vec v,\vec w\in T_pS$.}\\

In the next section, we shall find the matrix for $d\mathbf N_p$ with respect to the basis $\mathbf x_u,\mathbf x_v$.

Note, however, that (ii) does \emph{not} imply said matrix will be symmetric.  In the ordinary inner product space $\mathbb R^n$, it is known that the adjoint of any matrix is its transpose, hence a matrix is self-adjoint if and only if it symmetric.  This means that by (ii), $d\mathbf N_p$ would be symmetric when written with respect to any \emph{orthonormal} basis (in this case the dot product would coincide with the coordinate one); but $\mathbf x_u,\mathbf x_v$ is seldom an orthonormal basis for these parametrizations.
\begin{proof}
(i) has been shown in the paragraph preceding the proposition statement.

(ii) Let $\mathbf x:U\to S$ be a local coordinate chart, and write $\vec v=a\mathbf x_u+b\mathbf x_v$ and $\vec w=c\mathbf x_u+d\mathbf x_v$.  Note that $d\mathbf N_p(\mathbf x_u)=\mathbf N_u$ by the Chain Rule, where the right-hand side views $\mathbf N$ as a function on $U$.  Likewise, $d\mathbf N_p(\mathbf x_v)=\mathbf N_v$.  Hence,
$$d\mathbf N_p(\vec v)\cdot\vec w=(a\mathbf N_u+b\mathbf N_v)\cdot(c\mathbf x_u+d\mathbf x_v)$$
$$=ac(\mathbf N_u\cdot\mathbf x_u)+ad(\mathbf N_u\cdot\mathbf x_v)+bc(\mathbf N_v\cdot\mathbf x_u)+bd(\mathbf N_v\cdot\mathbf x_v)$$
and
$$\vec v\cdot d\mathbf N_p(\vec w)=(a\mathbf x_u+b\mathbf x_v)\cdot(c\mathbf N_u+d\mathbf N_v)$$
$$=ac(\mathbf N_u\cdot\mathbf x_u)+ad(\mathbf N_v\cdot\mathbf x_u)+bc(\mathbf N_u\cdot\mathbf x_v)+bd(\mathbf N_v\cdot\mathbf x_v);$$
to complete the proof we need only show $\mathbf N_u\cdot\mathbf x_v=\mathbf N_v\cdot\mathbf x_u$.

To do this, we note that $\mathbf N\cdot\mathbf x_u=0$ throughout the open set $U$, because $\mathbf N$ is normal to the surface and $\mathbf x_u$ is tangent to it, hence they are perpendicular vectors.  Since this equation holds throughout $U$, its partial derivative with respect to $v$ holds:
$$(\mathbf N\cdot\mathbf x_u)_v=\mathbf N_v\cdot\mathbf x_u+\mathbf N\cdot\mathbf x_{uv}=0$$
(using Exercise 4(a) of Section 6.1).  Similarly, taking the partial derivative of $\mathbf N\cdot\mathbf x_v=0$ with respect to $u$ entails $\mathbf N_u\cdot\mathbf x_v+\mathbf N\cdot\mathbf x_{uv}=0$.  Consequently, $\mathbf N_u\cdot\mathbf x_v=\mathbf N_v\cdot\mathbf x_u$ because both are equal to $-\mathbf N\cdot\mathbf x_{uv}$ (note that we used the symmetry of mixed partial derivatives here).
\end{proof}

\noindent As a linear operator, $d\mathbf N_p$ has a determinant.  Changing the orientation of the Gauss map negates the matrix, but leaves the determinant unchanged (because the matrix has even dimensions).  The determinant of $d\mathbf N_p$ is called the \textbf{Gaussian curvature} of the surface at $p$, and is denoted $K$ (or $K(p)$).

Since $d\mathbf N_p$ is self-adjoint, examining it with respect to an orthonormal basis and using the Spectral Theorem (Exercise 1 of Section 3.5), shows that $d\mathbf N_p$ is diagonalizable over $\mathbb R$, and its eigenvectors are perpendicular (unless $d\mathbf N_p$ is a scalar multiple of the identity, in which case every nonzero vector is an eigenvector).  The eigenvectors, which are tangent vectors to the surface, are called \textbf{principal directions}, and their eigenvalues are called the \textbf{principal curvatures} in these directions.  Note that the principal curvatures negate when the orientation is switched (even though the Gaussian curvature does not change).\\

\noindent\textbf{Proposition 6.10 and Definition.} \emph{Let $S$ be an oriented regular surface and $p\in S$.  Then exactly one of the following holds:}

(i) \emph{$K>0$, and $d\mathbf N_p$'s eigenvalues have the same sign.  In this case the point $p$ is said to be an \textbf{elliptic} point.}

(ii) \emph{$K<0$, and $d\mathbf N_p$'s eigenvalues have opposite signs.  In this case the point $p$ is said to be a \textbf{hyperbolic} (or \textbf{saddle}) point.}

(iii) \emph{$K=0$ and $d\mathbf N_p\ne 0$.  In this case the point $p$ is said to be a \textbf{parabolic} point.}

(iv) \emph{$d\mathbf N_p=0$.  In this case the point $p$ is said to be a \textbf{planar} point.}

\emph{Moreover, if $d\mathbf N_p$ is a scalar multiple of the identity, $p$ is said to be an \textbf{umbilical} (or \textbf{spherical}) point.}
\begin{proof}
The statements follow readily from elementary linear algebra.  $K$, being the determinant of $d\mathbf N_p$, is the product of its eigenvalues.
\end{proof}

\noindent\textbf{Examples.}

To illustrate the principle we shall take some sample surfaces, determine their Gaussian curvature, and tell what kinds of points they have.

(1) The $xy$-plane $\mathbf x(u,v)=(u,v,0)$ has constant Gauss map $\mathbf N(p)=(0,0,1)$.  Hence $d\mathbf N_p=0$ for all $p$, so that $K=0$ and every point is planar [the terminology should make sense here!]\\

(2) If $S$ is the sphere $S^2$, then since the outward normal vector to each $p\in S^2$ is parallel to $p$ itself, $\mathbf N:S\to S^2$ is the identity map.  Hence every $d\mathbf N_p$ is the identity, so that $K=1$, and every point on $S^2$ is umbilical, and in particular elliptic.\\

(3) The cylinder $x^2+y^2=1$, when parametrized by $\mathbf x(u,v)=(\cos u,\sin u,v)$, is readily seen to have $K=0$ and consist purely of parabolic points.  For example, at $p=(1,0,0)$, $\mathbf N(p)=(1,0,0)$, so that $d\mathbf N_p$ is a linear operator on the $yz$-plane; with respect to the basis $\vec e_2,\vec e_3$ its matrix is $\begin{bmatrix}1&0\\0&0\end{bmatrix}$, satisfying case (iii) of Proposition 6.10.

Similarly, the cone $z^2=x^2+y^2$ consists of parabolic points.\\

(4) For our benefit, we will find explicit formulas for $\mathbf N$ for our two particularly important kinds of surfaces.

If $U\subset\mathbb R^2$, $f:U\to\mathbb R$ and $S$ is the graph of $z=f(x,y)$, it can be parametrized via $\mathbf x(u,v)=(u,v,f(u,v))$.  We recall that $\mathbf x_u=(1,0,f_u)$ and $\mathbf x_v=(0,1,f_v)$.  With that, we get $\mathbf N=\frac{\mathbf x_u\times\mathbf x_v}{\|\mathbf x_u\times\mathbf x_v\|}=\frac 1{\sqrt{1+f_u^2+f_v^2}}(-f_u,-f_v,1)=\frac 1{\sqrt{1+\|\vec\nabla f\|^2}}(-f_u,-f_v,1)$.

Now, let $\alpha(t)=(f(t),g(t))$ be a regular curve with $f>0$; and consider the surface of revolution $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$.  Then $\mathbf x_u=(f'(u)\cos v,f'(u)\sin v,g'(u))$ and $\mathbf x_v=(-f(u)\sin v,f(u)\cos v,0)$, from which we compute $\mathbf x_u\times\mathbf x_v=(-g'(u)f(u)\cos v,-g'(u)f(u)\sin v,f'(u)f(u))$, and therefore $\mathbf N=\frac 1{\sqrt{f'(u)^2+g'(u)^2}}(-g'(u)\cos v,-g'(u)\sin v,f'(u))$.  We recall the feasibility and convenience of assuming $(f')^2+(g')^2=1$; if that holds, then $\mathbf N=(-g'(u)\cos v,-g'(u)\sin v,f'(u))$.\\

(5) Here is an example of a planar point on a surface which is not locally a plane.  Consider the graph of the quartic surface $z=x^4+y^4$.  By the formulas in the previous example, $\mathbf N((x,y,z))=\frac 1{1+(4x^3)^2+(4y^3)^2}(-4x^3,-4y^3,1)$.  We leave it to the reader to show that the derivative of this is entirely zero at $x=y=0$; hence $(0,0,0)$ is a planar point, even though the graph is clearly not locally a plane.\\

(6) The paraboloid $z=x^2+y^2$ has an (elliptic) umbilical point at the vertex $(0,0,0)$; however, its other points are elliptic but not umbilical.  Contrast with the sphere in example (2).\\

(7) Recall the torus from Exercise 11 of Section 6.3:
$$\mathbf x(u,v)=((a+b\cos u)\cos v,(a+b\cos u)\sin v,b\sin u)$$
In the next section we will see that the outer surface of the torus consists of elliptic points, the inner surface consists of hyperbolic points, and there are two circles worth of parabolic points.\\

\noindent In the remainder of this section we shall show a particularly curious fact about umbilical points:\\

\noindent\textbf{Proposition 6.11.} \emph{Let $S$ be a connected regular surface, on which all points are umbilical.  Then $S$ is contained in either a plane or a sphere.}\\

\noindent It is obvious why one must assume $S$ is connected; otherwise, e.g., it could be the disjoint union of two planes and three spheres and the statement would not hold.
\begin{proof}
Let us first prove this for the image of a local coordinate chart $\mathbf x:U\to S$ ($U$ connected).  Since $\mathbf x(p)$ is umbilical for all $p\in U$, we have $d\mathbf N_p=\lambda(p)\cdot I$ for some scalar $\lambda(p)$ depending on $p$.  It is readily verified that $\lambda$ is smooth with respect to $p$.  Moreover, since $d\mathbf N_p(\mathbf x_u)=\mathbf N_u$ and $d\mathbf N_p(\mathbf x_v)=\mathbf N_v$, we get these equations:
\begin{equation}\tag{1}\mathbf N_u=\lambda\mathbf x_u\end{equation}
\begin{equation}\tag{2}\mathbf N_v=\lambda\mathbf x_v\end{equation}
Differentiating (1) with respect to $v$, (2) with respect to $u$, and subtracting, one ends up with
$$\lambda_v\mathbf x_u-\lambda_u\mathbf x_v=0.$$
[This uses the Product Rule, which entails that, e.g., $\frac{\partial}{\partial v}(\lambda\mathbf x_u)=\lambda_v\mathbf x_u+\lambda\mathbf x_{uv}$.]  Since $\mathbf x_u,\mathbf x_v$ are linearly independent, we conclude $\lambda_u=\lambda_v=0$ throughout $U$.  Hence $\lambda$ is actually constant.

Either $\lambda=0$ or $\lambda\ne 0$.  If $\lambda=0$, then $\mathbf N_u=\mathbf N_v=0$, so that $\mathbf N$ is constant, say $\mathbf N_0$.  For fixed $p_0\in U$, the function $p\mapsto\mathbf N_0\cdot(\mathbf x(p)-\mathbf x(p_0))$ is identically zero, because it sends $p_0\mapsto 0$, and it is constant (its partial derivatives are $\mathbf N\cdot\mathbf x_u$ and $\mathbf N\cdot\mathbf x_v$, which are both zero).  This means that $\mathbf x(U)$ is contained in the plane through $\mathbf x(p_0)$ perpendicular to $\mathbf N_0$.

Now suppose $\lambda\ne 0$.  Then set $\mathbf y=\mathbf x-\frac 1{\lambda}\mathbf N$; equations (1) and (2) above entail $\mathbf y_u=\mathbf y_v=0$, and hence $\mathbf y$ is constant.  Moreover, for all $p\in U$,
$$\left\|\mathbf x(p)-\mathbf y\right\|=\left\|\frac 1{\lambda}\mathbf N\right\|=\frac 1{|\lambda|}$$
(because $\mathbf N$ is a unit vector), so that $\mathbf x(U)$ is contained in the sphere centered at $\mathbf y$ with radius $\frac 1{|\lambda|}$.

This proves the proposition locally.  For the global case, the connectedness of $S$ will be essential.  We will use the topological fact that $S$ must be path-connected; i.e., for all $x,y\in S$, there is a continuous function $[0,1]\to S$ sending $0\mapsto x,1\mapsto y$.  Let $\mathbf x_\alpha:U_\alpha\to S$ be a family of local coordinate charts that cover the surface. % Regarding your footnote, connected => path-connected for locally path-connected spaces (1).  "Locally path-connected" means every point has a neighborhood base consisting entirely of path-connected sets.  Moreover, topological manifolds (like S) are locally path-connected (2), hence connected => path-connected for manifolds.  Nothing to do with second countability or separability.
% PROOF OF (1): For x,y\in S, define x\sim y if there is a path from x to y (i.e., a continuous map [0,1] -> S sending 0 to x and 1 to y).  This is an equivalence relation, and is all of S\times S \iff S is path connected.  Yet if S is locally path-connected, every equivalence class is open: indeed, for x\in S, x has a path-connected neighborhood (a neighborhood base consisting of them for that matter), and such a neighborhood is clearly contained in the equivalence class of x.  Thus every point of the equivalence class is an interior point.  Since the equivalence classes are open and partition S, they are clopen.  Thus if S is connected, they must be the entire space, so that S is path connected.
% PROOF OF (2): If S is a topological manifold of dimension n, then every x\in S has a neighborhood U and a homeomorphism \mathbb R^n\cong U sending 0 to x.  Moreover, the images of the B_r(0)\subset\mathbb R^n for all positive rational r, form the desired neighborhood base of x.

Fix $p\in S$ and pick a coordinate chart $\mathbf x_\rho$ which covers $p$.  This chart is contained in some sphere or plane; our claim is that the entire surface is contained in said sphere or plane as well.  First suppose the chart is contained in a plane.  Then for any $q\in S$, let $\alpha:[0,1]\to S$ be a path from $p$ to $q$.  Since $[0,1]$ is compact, the open covering consisting of the sets $\alpha^{-1}(\mathbf x_\alpha(U_\alpha))\subset[0,1]$ has a Lebesgue number $\delta$ (this means $\delta>0$ and every interval of the form $(x,x+\delta)$ is contained within some set of the covering); let $N=1+\lfloor 1/\delta\rfloor$.  By definition of $\delta$, for every interval $I_m=[m/N,(m+1)/N]$ for $m=0,1,\dots,N-1$ there exists $\alpha_m$ with $\alpha(I_m)\subset\mathbf x_{\alpha_m}(U_{\alpha_m})$. % Lebesgue number concept is used rarely in the book.  I'll add the definition here.

Since $\mathbf x_{\alpha_0}(U_{\alpha_0})$ is an open set of the surface containing $p$, we must have that $\mathbf x_{\alpha_0}(U_{\alpha_0})$ is contained in the plane that $\mathbf x_\rho$ is contained in.  Moreover, $\mathbf x_{\alpha_0}(U_{\alpha_0})\cap\mathbf x_{\alpha_1}(U_{\alpha_1})$ is an open neighborhood of $\alpha(1/N)$, also contained in this plane.  Such an open set is not contained in any other plane or in any sphere; hence $\mathbf x_\rho$'s plane must contain $\mathbf x_{\alpha_1}(U_{\alpha_1})$, too.  Like reasoning shows that since $\mathbf x_{\alpha_1}(U_{\alpha_1})\cap\mathbf x_{\alpha_2}(U_{\alpha_2})$ is an open neighborhood of $\alpha(2/N)$ in the surface, the same plane contains $\mathbf x_{\alpha_2}(U_{\alpha_2})$.  The argument continues across the path, and eventually shows that $\mathbf x_{\alpha_N}(U_{\alpha_N})$ is contained in the plane.  Yet $q=\alpha(1)\in\mathbf x_{\alpha_N}(U_{\alpha_N})$, and hence $q$ is in the plane.  Since $q$ is arbitrary, we conclude that the plane containing $\mathbf x_\rho$ contains the entire surface.

If $\mathbf x_\rho$ is contained in a sphere, pretty much the same argument is valid; for that use the fact that a nonempty open set of the sphere is not contained in any other sphere or in any plane.
\end{proof}

\subsection*{Exercises 6.5. (The Gauss Map.  Gaussian Curvature)}
% POTENTIAL EXERCISE: mean curvature, principal directions, lines of curvature, normal curvature, Joachimsthal's
\begin{enumerate}
\item The \textbf{M\"obius band} is obtained by taking a rectangular strip and gluing its ends together, but after applying a half-twist, so that opposite vertices of the original rectangle meet.
\begin{center}
\includegraphics[scale=.4]{Moebius_Band.png}
\end{center}
(a) Show that it can be parametrized as follows:
$$\mathbf x(u,v)=((2+u\sin(v/2))\cos v,(2+u\sin(v/2))\sin v,u\cos(v/2)),~~~~-1<u<1$$
The fact that $\mathbf x(u,v)=\mathbf x(-u,v+2\pi)$ illustrates the half-twist.

(b) Show that the M\"obius band is a regular surface, but there is no globally continuous Gauss map on it.

\item Let $S$ be a regular surface.  If $S$ is tangent to a plane along a curve, show that every point on the curve is a parabolic or planar point of $S$.

\item Show that every compact regular surface $S$ has an elliptic point.  [By compactness, $S$ is bounded, hence there is a sphere which engulfs $S$ and is tangent to $S$ at a particular point.  Show that this point is elliptic by considering fundamental results from calculus.]

\item If $S$ is a regular surface and $p\in S$, the \textbf{mean curvature} of $S$ at $p$ \-- denoted $H$ \-- is defined to be $\frac 12\operatorname{tr}(d\mathbf N_p)$.  Note that unlike Gaussian curvature, $H$ negates when the orientation is switched.  Find the mean curvature of the sphere, plane, cylinder and cone.

Note that the plane, cylinder and cone are locally isometric; they all have Gaussian curvature $0$, but they have different mean curvatures.  We shall later see that locally isometric surfaces \emph{always} have the same Gaussian curvature; this is what makes this kind of curvature particularly special.

\item Recall that the \textbf{principal directions} to a point $p\in S$ are the tangent vectors which are eigenvectors of $d\mathbf N_p$, and that the \textbf{principal curvatures} are their eigenvalues.

(a) A point is umbilical if and only if all its directions are principal directions.

(b) If a point is umbilical, then all of its principal curvatures are equal.  Explain why this is false for arbitrary points on regular surfaces.

(c) Directions $\vec v,\vec w\in T_pS$ are said to be \textbf{conjugate} if $d\mathbf N_p(\vec v)\cdot\vec w=0$.  Since $d\mathbf N_p$ is self-adjoint, this is a symmetric relation on directions.  For the sphere, plane and cylinder, give explicit descriptions of conjugate directions.  Use this to show that conjugate directions need \emph{not} be preserved by local isometries.

(d) A direction $\vec v\in T_pS$ is said to be \textbf{asymptotic} if it is conjugate to itself; i.e., $d\mathbf N_p(\vec v)\cdot\vec v=0$.  Show that such directions exist at $p$ if and only if $p$ is not elliptic.

\item A regular curve $\alpha:I\to S$ is called a \textbf{line of curvature} if all its derivatives are principal directions on the surface; i.e., $\alpha'(t)$ is a principal direction at $\alpha(t)$ for each $t\in I$.

(a) Show that $\alpha$ is a line of curvature if and only if there is a smooth function $\lambda:I\to\mathbb R$ such that $\mathbf N'(t)=\lambda(t)\alpha'(t)$.

(b) Assume $S$ has no umbilical points.  If the coordinate curves ($u\mapsto\mathbf x(u,v_0)$ and $v\mapsto\mathbf x(u_0,v)$) of a coordinate chart are lines of curvature, show that $F=0$ ($F$ being the first fundamental form coefficient).  Is the converse necessarily true? % If L,M,N are the SFF coefficients, then the coordinate curves are lines of curvature \iff F=M=0.

\item\emph{(Normal curvature.)} \---- Let $\alpha:I\to S$ be a regular curve on a regular surface $S$.  An important observation is that, though $\mathbf N$ is normal to the curve where it appears on the curve, it is not generally parallel to the normal vector given in the Frenet trihedron.  The latter normal vector thinks directly in the curve's sense of direction, and doesn't know about the surface structure.  Yet, the normal vectors do relate.

Let $\alpha(t)$ be a particular point of the curve.  Then let $\mathbf N$ be the Gauss map, $\boldsymbol\nu$ the unit normal vector to $\alpha$ (in the Frenet trihedron), and $\kappa$ the curvature of $\alpha$.  Then if $\theta$ is the angle between $\mathbf N$ and $\boldsymbol\nu$, the number $\kappa\cos\theta$ is denoted $\kappa_n$ and called the \textbf{normal curvature}.

(a) If $\alpha$ is parametrized by arc length, show that the absolute value of the normal curvature is equal to the magnitude of the orthogonal projection of $\alpha''(s)$ onto $\mathbf N$, and that the normal curvature is negative if and only if the projection points opposite to $\mathbf N$.

(b) Show that the normal curvature of $\alpha$ at $t$ is equal to $-d\mathbf N_p(\boldsymbol\tau(t))\cdot\boldsymbol\tau(t)$, where $\boldsymbol\tau(t)=\frac{\alpha'(t)}{\|\alpha'(t)\|}$ (the tangential unit vector).  [Explain why one may assume $\alpha$ is parametrized by arc length; then the goal is to show $\kappa_n=-d\mathbf N_p(\alpha'(s))\cdot\alpha'(s)$.  To do this, differentiate the equation $\mathbf N(s)\cdot\alpha'(s)=0$, and show that $\kappa_n=\kappa(\boldsymbol\nu\cdot\mathbf N)=\mathbf N\cdot\alpha''$.]  Hence, the normal curvature only depends on the point and the tangent line at the point, not the particular curve (Meusnier).

(c) Show that at $p\in S$, the mean curvature $H=-\frac 1{\pi}\int_0^\pi\kappa_n(\theta)\,d\theta$, where $\kappa_n(\theta)$ is the normal curvature at $p$ in a direction of angle $\theta$ from a fixed direction.  [Since $d\mathbf N_p$ is self-adjoint, it has an orthonormal eigenbasis.  If $\{\vec u_1,\vec u_2\}$ is such an eigenbasis, and the fixed direction is $\vec u_1$, use part (b) to show that $\kappa_n(\theta)=-\kappa_1\cos^2\theta-\kappa_2\sin^2\theta$ where $\kappa_1,\kappa_2$ are the principal curvatures (the eigenvalues corresponding to $\vec u_1$ and $\vec u_2$ respectively).  Then familiar trigonometric laws can be used to evaluate the integral.]

\item\emph{(Joachimsthal's Theorem.)} \---- Let $S$ and $\overline S$ be regular surfaces in $\mathbb R^3$ which intersect in a regular curve $\alpha:I\to\mathbb R^3$.  (Thus $\alpha$ is both a curve on $S$ and a curve on $\overline S$.)  Assume also that $\alpha$ is a line of curvature of $S$.  Show that $\alpha$ is a line of curvature of $\overline S$ if and only if the dihedral angle between the surfaces is constant along the curve.

[Let $\mathbf N,\overline{\mathbf N}$ be the respective Gauss maps of the surfaces.  Then the dihedral angle is constant along the curve if and only if $\mathbf N\cdot\overline{\mathbf N}$ is constant along the curve (why?).  Show that $\alpha$ is a line of curvature of $S$ if and only if $\frac{d\mathbf N}{dt}\cdot\overline{\mathbf N}$ (where $\mathbf N$ is regarded as a function in $t$) is zero; a similar statement holds with the surfaces switched.  Then use Exercise 4(a) of Section 6.1.]

\item Show that the parallels of latitude and meridians of any surface of revolution are lines of curvature.  [Use the previous exercise.]

\item In preparation for the next exercise, we will cover cross products of arbitrary-dimensional vectors.  In $\mathbb R^n$, the cross product takes exactly $n-1$ operands.

(a) If $\vec v_1,\dots\vec v_{n-1}\in\mathbb R^n$, then there is a unique vector $\vec a$ such that $\vec a\cdot\vec v=\det\begin{bmatrix}\uparrow&\dots&\uparrow&\uparrow\\\vec v_1&\dots&\vec v_{n-1}&\vec v\\\downarrow&\dots&\downarrow&\downarrow\end{bmatrix}$ for all $\vec v$.  [It suffices to show that the map from $\vec v$ to said determinant is a linear map $\mathbb R^n\to\mathbb R$.]  This vector $\vec a$ is called the \textbf{cross product} of $\vec v_1,\dots,\vec v_{n-1}$, and is occasionally denoted $\vec v_1\times\dots\times\vec v_{n-1}$.

(b) This cross product is multilinear, and alternating (i.e., if $\vec v_j=\vec v_k$ for some $j\ne k$ then the cross product is zero).  Moreover, $\vec e_1\times\dots\times\vec e_{n-1}=\vec e_n$.

(c) $\vec v_1\times\dots\times\vec v_{n-1}=\vec 0$ if and only if $\vec v_1,\dots,\vec v_{n-1}$ are linearly dependent.

(d) $\vec v_1\times\dots\times\vec v_{n-1}$ is perpendicular to the vectors $\vec v_1,\dots,\vec v_{n-1}$, and its magnitude is the $(n-1)$-dimensional volume of the parallelepiped spanned by the vectors.

(e) In $\mathbb R^2$, the operation is unary, and one denotes the cross product of $\vec v\in\mathbb R^2$ as $^\times\vec v$.  Show that $^\times(a,b)=(-b,a)$.  [Note the exact way that the unit normal vector for a plane curve was defined.]

(f) In $\mathbb R^3$, the operation is binary, and coincides with the cross product covered in Exercise 4 of Section 2.5.

\item As in every section, we will generalize the material to arbitrary dimensions.

Let $S\subset\mathbb R^n$ be a \textbf{hypersurface}, i.e., a regular manifold of dimension $n-1$.  Then a Gauss map on an open set $U\subset S$ is a continuous map $U\to S^{n-1}$ which takes each point to a unit normal vector to the hypersurface.  [I.e., a normal vector in the orthogonal complement of the tangent space to the point.]  Here, $S^{n-1}$ is the unit hypersphere in $\mathbb R^n$.

(a) Show that Gauss maps exist on local coordinate charts.  [Use the previous exercise to imitate the argument in the text for $n=3$.]

If $S$ has a global Gauss map, it is said to be \textbf{orientable}; otherwise, it is said to be \textbf{nonorientable}.  There exist nonorientable hypersurfaces whenever $n\geqslant 3$, such as $M\times\mathbb R^{n-3}$ with $M$ the M\"obius band (Exercise 1).

For the rest of the exercise, we assume $S$ is an orientable hypersurface equipped with a Gauss map $\mathbf N$.

(b) For each $p\in\mathbf N$, $d\mathbf N_p$ is a self-adjoint linear operator on $T_pS$.  [Pretty much everything is identical to the case $n=3$.  To show the operator is self-adjoint, show that if $\mathbf x$ is a local coordinate chart, $\mathbf N_{u_j}\cdot\mathbf x_{u_k}=\mathbf N_{u_k}\cdot\mathbf x_{u_j}=-\mathbf N\cdot\mathbf x_{u_ju_k}$.]

It follows that $d\mathbf N_p$ has an orthonormal eigenbasis.  Its eigenvectors are called \textbf{principal directions} and the eigenvectors are called the \textbf{principal curvatures}.  The product of the principal curvatures, which is $\det(d\mathbf N_p)$, is called the \textbf{main curvature} of the hypersurface.

(c) When does switching the orientation keep the main curvature the same?  When doesn't it?

(d) For a regular curve (the case where $n=2$), show that the main curvature at a point is the signed curvature.

(e) A point $p$ is said to be \textbf{umbilical} if $d\mathbf N_p$ is a scalar multiple of the identity operator.  Show that if $n>2$, a connected hypersurface on which every point is umbilical is contained in a hyperplane or a hypersphere.  [Adapt the proof of Proposition 6.11.]  Explain why this is false for $n=2$.

General types of curvature are not intrinsic to the isometry class of the manifold, hence will not be covered in Section 6.10 when metrics are customized.  However, there are types of curvature for general manifolds; the Riemann curvature tensor and Ricci curvature.  They are beyond the scope of this book. % Weird to say, now that the last exercise of Section 6.10 introduces the Levi-Civita connection.  The curvature tensor and Ricci curvature can easily be defined from that, am I missing an opportunity?
\end{enumerate}

\subsection*{6.6. The Second Fundamental Form}
\addcontentsline{toc}{section}{6.6. The Second Fundamental Form}
In the previous section we defined the Gauss map and went over its basic properties.  In this section, our aim is to express its differential in coordinates, for the benefit of the study.

Given a local coordinate chart $\mathbf x:U\to S$, the first fundamental form determines (in effect) how much distance is covered on $S$ by paths in $U$.  However, it does not indicate how much the surface curves in certain directions.  The curving nature of the surface is captured by the Gauss map, as seen in the last section.  Thus we would like for there to be another form, which has to do with the Gauss map.

To define this, we first recall how the first fundamental form works: if $\vec v=a\mathbf x_u+b\mathbf x_v\in T_pS$, then
$$I_p(\vec v)=\vec v\cdot\vec v=a^2\mathbf x_u\cdot\mathbf x_u+2ab\mathbf x_u\cdot\mathbf x_v+b^2\mathbf x_v\cdot\mathbf x_v=Ea^2+2Fab+Gb^2.$$
This is the quadratic form induced by the symmetric bilinear form $(\vec v,\vec w)\mapsto\vec v\cdot\vec w$.  If $\mathbf N$ is the Gauss map, then $(\vec v,\vec w)\mapsto -d\mathbf N_p(\vec v)\cdot\vec w$ is also a bilinear form, which is symmetric by Proposition 6.9(ii).  [The reason we adjoined a negative sign will be clear in a moment.  Some mathematicians omit it.]  This symmetric bilinear form corresponds with a quadratic form itself.\\

\noindent\textbf{Definition.} \emph{If $S$ is a regular surface and $\mathbf N$ its Gauss map, then the \textbf{second fundamental form} on $T_pS$ is defined to be the quadratic form $\vec v\mapsto I\!I_p(\vec v)=-d\mathbf N_p(\vec v)\cdot\vec v$.}\\ % I couldn't find such a symbol on DeTeXify.  I changed it to I\!I

\noindent Just as the first fundamental form has its coefficients ($E,F,G$) which make it easy to apply to any tangent vector given in the basis $\{\mathbf x_u,\mathbf x_v\}$, we have the same thing for the second fundamental form.  If $\vec v=a\mathbf x_u+b\mathbf x_v$, then regarding $\mathbf N$ as a function in $U$ and using the Chain Rule, $d\mathbf N_p(\vec v)=a\mathbf N_u+b\mathbf N_v$.  Moreover,
$$-d\mathbf N_p(\vec v)\cdot\vec v=-(a\mathbf N_u+b\mathbf N_v)\cdot(a\mathbf x_u+b\mathbf x_v)$$
$$=a^2(-\mathbf N_u\cdot\mathbf x_u)+ab(-\mathbf N_u\cdot\mathbf x_v-\mathbf N_v\cdot\mathbf x_u)+b^2(-\mathbf N_v\cdot\mathbf x_v).$$
We recall, from the proof of Proposition 6.9(ii), that $\mathbf N_u\cdot\mathbf x_v=\mathbf N_v\cdot\mathbf x_u$ since both are equal to $-\mathbf N\cdot\mathbf x_{uv}$.  We may do similar things to the other dot products; differentiating $\mathbf N\cdot\mathbf x_u=0$ and $\mathbf N\cdot\mathbf x_v=0$ with respect to $u$ and $v$ respectively leads to $\mathbf N_u\cdot\mathbf x_u=-\mathbf N\cdot\mathbf x_{uu}$ and $\mathbf N_v\cdot\mathbf x_v=-\mathbf N\cdot\mathbf x_{vv}$.  Hence we may write
$$-d\mathbf N_p(\vec v)\cdot\vec v=a^2(\mathbf N\cdot\mathbf x_{uu})+2ab(\mathbf N\cdot\mathbf x_{uv})+b^2(\mathbf N\cdot\mathbf x_{vv}),$$
and we have our coefficients, which do not involve differentiating $\mathbf N$ at all.  [Due to being consciously normalized, $\mathbf N$ usually has square roots in its expression, making it a tad bit unsatisfying to differentiate it, so it is convenient that we have eliminated the computation of $\mathbf N$'s derivatives.]\\ % Your revisions give "If we had not defined \mathbf N to be a unit vector, it would have square roots in its expression, making it a tad bit unsatisfying to differentiate it, so it is convenient that we have defined it to be a unit vector."
% That statement is not only not what I meant, but it's wrong.  The cross product \mathbf x_u\times\mathbf x_v doesn't involve any square roots unless \mathbf x_u,\mathbf x_v already have them.  It's the *normalization* which brings in square roots.  So if we didn't define \mathbf N to be a unit vector, it wouldn't have square roots more often than the chart itself.  (You're probably thinking about the fact that if \|\mathbf N\| is not assumed to be 1, it is an expression which often has square roots.)
% Also, the convenient thing is not that \mathbf N is normalized.  It's that we eliminated the need to differentiate \mathbf N.  The naturally-materializing square roots from the normalization make it harder to differentiate \mathbf N once than \mathbf x three times.  (I'll change it to make it clear.)

\noindent\textbf{Proposition 6.12 and Definition.} \emph{The second fundamental form $I\!I_p(a\mathbf x_u+b\mathbf x_v)=La^2+2Mab+Nb^2$, where $L=\mathbf N\cdot\mathbf x_{uu}$, $M=\mathbf N\cdot\mathbf x_{uv}$ and $N=\mathbf N\cdot\mathbf x_{vv}$.  The scalars $L,M,N$ are called the \textbf{coefficients of the second fundamental form}.}\\

\noindent Let us compute these coefficients in some particular cases.

For the plane or a surface contained in the plane, $\mathbf N$ is constant, hence the second fundamental form is clearly zero (because $-d\mathbf N_p(\vec v)\cdot\vec v=-\vec 0\cdot\vec v=0$).  In other words, for the plane, $L=M=N=0$.  [In fact, see Exercise 1.]

Now let us consider a surface of revolution $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$ ($f>0$).  Then we compute
$$\mathbf x_u=(f'(u)\cos v,f'(u)\sin v,g'(u))$$
$$\mathbf x_v=(-f(u)\sin v,f(u)\cos v,0)$$
$$\therefore\mathbf x_u\times\mathbf x_v=(-f(u)g'(u)\cos v,-f(u)g'(u)\sin v,f(u)f'(u))$$
Furthermore, $\|\mathbf x_u\times\mathbf x_v\|=\sqrt{f(u)^2g'(u)^2(\cos^2v+\sin^2v)+f(u)^2f'(u)^2}$\\
\noindent$=\sqrt{f(u)^2(f'(u)^2+g'(u)^2)}=f(u)\sqrt{f'(u)^2+g'(u)^2}$, so that normalizing the above entails
$$\mathbf N=\frac 1{\sqrt{f'(u)^2+g'(u)^2}}(-g'(u)\cos v,-g'(u)\sin v,f'(u)).$$
We may also compute the second-order derivatives of $\mathbf x$, which will lead us to the second fundamental form coefficients:
$$\mathbf x_{uu}=(f''(u)\cos v,f''(u)\sin v,g''(u))$$
$$\mathbf x_{uv}=(-f'(u)\sin v,f'(u)\cos v,0)$$
$$\mathbf x_{vv}=(-f(u)\cos v,-f(u)\sin v,0)$$
And therefore,
$$L=\mathbf N\cdot\mathbf x_{uu}=\frac{f'(u)g''(u)-f''(u)g'(u)}{\sqrt{f'(u)^2+g'(u)^2}}$$
$$M=\mathbf N\cdot\mathbf x_{uv}=0$$
$$N=\mathbf N\cdot\mathbf x_{vv}=\frac{f(u)g'(u)}{\sqrt{f'(u)^2+g'(u)^2}}$$
Earlier we mentioned that we may assume $(f')^2+(g')^2=1$.  In fact, such an assumption is convenient, as it gets rid of the radicals in the expressions: $L=f'(u)g''(u)-f''(u)g'(u)$, $M=0$ and $N=f(u)g'(u)$.

In particular, taking $f(u)=\sin u,g(u)=\cos u$ shows that for the standard parametrization $(\sin u\cos v,\sin u\sin v,\cos u)$ of the sphere, the second fundamental form coefficients are $L=-1$, $M=0$ and $N=-\sin^2u$.  Taking $f(u)=1$, $g(u)=u$ shows that for the cylinder, $L=0$, $M=0$ and $N=1$.

The final example concerns an explicit-function graph, $z=f(x,y)$, which can be parametrized via $\mathbf x(u,v)=(u,v,f(u,v))$.  We recall that $\mathbf x_u=(1,0,f_u)$, $\mathbf x_v=(0,1,f_v)$ and $\mathbf N=\frac 1{\sqrt{1+\|\vec\nabla f\|^2}}(-f_u,-f_v,1)$.  Since $\mathbf x_{uu}=(0,0,f_{uu})$, $\mathbf x_{uv}=(0,0,f_{uv})$ and $\mathbf x_{vv}=(0,0,f_{vv})$, we compute the second fundamental form coefficients to be
$$L=\frac{f_{uu}}{\sqrt{1+\|\vec\nabla f\|^2}},~~~~M=\frac{f_{uv}}{\sqrt{1+\|\vec\nabla f\|^2}},~~~~N=\frac{f_{vv}}{\sqrt{1+\|\vec\nabla f\|^2}}.$$
Now that we understand the coefficients for both the first and second fundamental forms, we can readily derive formulas for the matrix of $d\mathbf N_p$ with respect to the basis $\{\mathbf x_u,\mathbf x_v\}$.

Let us suppose that this matrix is $\begin{bmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix}$, where $a_{ij}$ are differentiable functions in $u,v$.  Then by construction,
$$\mathbf N_u=d\mathbf N_p(\mathbf x_u)=a_{11}\mathbf x_u+a_{21}\mathbf x_v;$$
$$\mathbf N_v=d\mathbf N_p(\mathbf x_v)=a_{12}\mathbf x_u+a_{22}\mathbf x_v.$$
Taking the dot products of each equation with $\mathbf x_u$ and $\mathbf x_v$, we get
$$\left\{\begin{array}{c l}-L=\mathbf N_u\cdot\mathbf x_u=a_{11}E+a_{21}F\\-M=\mathbf N_u\cdot\mathbf x_v=a_{11}F+a_{21}G\end{array}\right.$$
$$\left\{\begin{array}{c l}-M=\mathbf N_v\cdot\mathbf x_u=a_{12}E+a_{22}F\\-N=\mathbf N_v\cdot\mathbf x_v=a_{12}F+a_{22}G\end{array}\right.$$
This can be rewritten in matrix form:
\begin{equation}\tag{*}-\begin{bmatrix}L&M\\M&N\end{bmatrix}=\begin{bmatrix}E&F\\F&G\end{bmatrix}\begin{bmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix}.\end{equation}
Since $\det\begin{bmatrix}E&F\\F&G\end{bmatrix}=EG-F^2>0$, the matrix is nonsingular, and therefore this system in the variables $a_{ij}$ has a unique solution.  In fact, $\begin{bmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix}=-\frac 1{EG-F^2}\begin{bmatrix}G&-F\\-F&E\end{bmatrix}\begin{bmatrix}L&M\\M&N\end{bmatrix}$, from which we derive
$$a_{11}=\frac{FM-GL}{EG-F^2},~~~~a_{12}=\frac{FN-GM}{EG-F^2},~~~~a_{21}=\frac{FL-EM}{EG-F^2},~~~~a_{22}=\frac{FM-EN}{EG-F^2}$$
These equations that formulate the matrix entries are called the \textbf{Weingarten equations}.  Incidentally, the Gaussian curvature $K$ is the determinant of the matrix $\begin{bmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix}$ (being the linear operator's determinant), hence can be computed in terms of these formulas; however, there is a quicker way to get it.  The determinant respects matrix multiplication, so we apply the determinant to both sides of (*) to get
$$LN-M^2=(EG-F^2)K,\text{ and hence }K=\frac{LN-M^2}{EG-F^2}.$$
This provides an easy formula for the Gaussian curvature.  Now for some examples.\\

\noindent The stuation is obvious for the plane $z=0$.  There, $\mathbf N$ is constant, hence $L=M=N=0$ and the Gauss map differential is also zero.

For an explicit-function graph $z=f(x,y)$, parametrized by $\mathbf x(u,v)=(u,v,f(u,v))$, we recall that:
$$E=1+f_u^2,~~~~F=f_uf_v,~~~~~G=1+f_v^2,~~~~EG-F^2=1+f_u^2+f_v^2=1+\|\vec\nabla f\|^2$$
$$L=\frac{f_{uu}}{\sqrt{1+\|\vec\nabla f\|^2}},~~~~M=\frac{f_{uv}}{\sqrt{1+\|\vec\nabla f\|^2}},~~~~N=\frac{f_{vv}}{\sqrt{1+\|\vec\nabla f\|^2}}$$
With that, we immediately compute
$$K=\frac{LN-M^2}{EG-F^2}=\frac{f_{uu}f_{vv}-f_{uv}^2}{(1+\|\vec\nabla f\|)^2}.$$
From this we conclude that, for the graph of an implicit function $z=f(x,y)$, elliptic points occur where $f_{uu}f_{vv}-f_{uv}^2>0$, parabolic and planar points occur where $f_{uu}f_{vv}-f_{uv}^2=0$ and hyperbolic (saddle) points occur where $f_{uu}f_{vv}-f_{uv}^2<0$.  The number $f_{uu}f_{vv}-f_{uv}^2$, which you may observe to be the determinant of $d(\vec\nabla f)=\begin{bmatrix}f_{uu}&f_{uv}\\f_{uv}&f_{vv}\end{bmatrix}$, is called the \textbf{Hessian} of $f$.\footnote{Named after German mathematician, Ludwig Otto Hesse.}

For explicit function graphs, a particularly interesting set of questions revolves around the critical points; i.e., those where $f_u=f_v=0$.  In this case, $K=f_{uu}f_{vv}-f_{uv}^2$ (the denominator is $1$).  If the Hessian is positive, then the graph has a local minimum or maximum at the point, which is a minimum if $f_{uu}>0$ at the point and a maximum if $f_{uu}<0$.  If the Hessian is negative, then the point is a saddle point. % I occasionally get 'implicit' and 'explicit' mixed up, because of the name of the "Implicit Function Theorem": If f:\mathbb R^n\to\mathbb R^m is a smooth map, S = f^{-1}(q), and x\in S where df_x has the first m columns forming a basis of \mathbb R^m, then in some neighborhood of p, S is given by the first m variables being explicit smooth functions in the other n-m.  I keep thinking the theorem is named after what it claims exists, like the Inverse Function Theorem.

We shall now segue to the case of a surface of revolution,\\
\noindent $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$.  Assume further that $(f')^2+(g')^2=1$.  We recall that
$$E=1,~~~~F=0,~~~~G=f(u)^2$$
$$L=f'(u)g''(u)-f''(u)g'(u),~~~~M=0,~~~~N=f(u)g'(u)$$
from which we get:
$$d\mathbf N_p=\begin{bmatrix}-f'(u)g''(u)+f''(u)g'(u)&0\\0&-\frac{g'(u)}{f(u)}\end{bmatrix},$$
(from the Weingarten equations), and
$$K=\frac{LN-M^2}{EG-F^2}=\frac{g'(u)[f'(u)g''(u)-f''(u)g'(u)]}{f(u)}.$$
Note that none of these coefficients involve $v$, because the surface is symmetric about axial rotations, which are the only things that happen when $v$ changes.

If $f(u)=1$ and $g(u)=u$ (the cylinder), then $d\mathbf N_p=\begin{bmatrix}0&0\\0&-1\end{bmatrix}$, which is essentially like what has been found before.

In the case where $f(u)=\sin u$ and $g(u)=\cos u$ (the sphere), $d\mathbf N_p=\begin{bmatrix}1&0\\0&1\end{bmatrix}$ (which makes sense because $\mathbf N$ is the identity map on $S^2$!), and $K=1$.

The last example we will consider is the torus, which is given by $f(u)=a+b\cos(u/b)$ and $g(u)=b\sin(u/b)$, with $a>b>0$ fixed real numbers [we divide $u$ by $b$ in the inputs to ensure the condition $(f')^2+(g')^2=1$, which the above formulas assume].  Curiously enough, half of its points are elliptic and half are hyperbolic.  To see this, we compute the Gaussian curvature (using the above formula) and get
$$K=\frac{\cos(u/b)}{b(a+b\cos(u/b))}.$$
The denominator is always positive since $a>b>0$ and $|\cos(u/b)|\leqslant 1$.  Moreover, the elliptic points occur when $\cos(u/b)>0$, i.e., where $f(u)>a$.  These are the points on the ``outer'' part of the torus.  The hyperbolic points occur when $\cos(u/b)<0$ or $f(u)<a$, the ``inner'' part of the torus, which touches the hole.  Finally, the points where $\cos(u/b)=0$ (i.e., $f(u)=a$) are parabolic points, consisting of the two circles where the tangent planes $z=\pm b$ meet [we leave it to the reader to verify the points are parabolic and not planar].  When you put a sugar-coated donut on the table and pick it up again, the circle of sugar you see on the table came from the parabolic points.\\

\noindent We conclude this section by looking at conjugate directions.  We recall from Exercise 5 of the previous section that $\vec v,\vec w\in T_pS$ are said to be \textbf{conjugate directions} provided that $d\mathbf N_p(\vec v)\cdot\vec w=0$.

Since $d\mathbf N_p$ is self-adjoint, it has an orthonormal eigenbasis $\{\vec u_1,\vec u_2\}$.  We may further assume that this basis is positive in the sense that $\vec u_1\times\vec u_2=\mathbf N(p)$.  Moreover, let $\kappa_1$ and $\kappa_2$ be the respective eigenvalues of $\vec u_1,\vec u_2$.  Then if $\vec v,\vec w\in T_pS$ are unit vectors, one can write $\vec v=(\cos\theta)\vec u_1+(\sin\theta)\vec u_2$ and $\vec w=(\cos\varphi)\vec u_1+(\sin\varphi)\vec u_2$.  Moreover,
$$d\mathbf N_p(\vec v)=d\mathbf N_p((\cos\theta)\vec u_1+(\sin\theta)\vec u_2)=(\cos\theta)d\mathbf N_p(\vec u_1)+(\sin\theta)d\mathbf N_p(\vec u_2)$$
$$=(\cos\theta)(\kappa_1\vec u_1)+(\sin\theta)(\kappa_2\vec u_2)=(\kappa_1\cos\theta)\vec u_1+(\kappa_2\sin\theta)\vec u_2$$
$$\therefore d\mathbf N_p(\vec v)\cdot\vec w=[(\kappa_1\cos\theta)\vec u_1+(\kappa_2\sin\theta)\vec u_2]\cdot[(\cos\varphi)\vec u_1+(\sin\varphi)\vec u_2]$$
$$=\kappa_1\cos\theta\cos\varphi+\kappa_2\sin\theta\sin\varphi.$$
In particular, conjugate directions occur when $\kappa_1\cos\theta\cos\varphi+\kappa_2\sin\theta\sin\varphi=0$.  If the point is planar ($\kappa_1=\kappa_2=0$), clearly all directions are conjugate.  If the point is parabolic (say, $\kappa_1=0$ but $\kappa_2\ne 0$), then conjugate directions occur when $\sin\theta\sin\varphi=0$; i.e., either $\theta$ or $\varphi$ (or both) is an integer multiple of $\pi$.  Thus in the parabolic case, there is one pair of opposite directions conjugate to all the directions.

However, for elliptic or hyperbolic points, the situation is more subtle.  Every direction is then conjugate to exactly two (opposite) directions.  See Exercise 10 for a geometric construction of conjugate directions.

A direction $\vec v\in T_pS$ is said to be \textbf{asymptotic} if it is conjugate to itself.  If $\vec v=(\cos\theta)\vec u_1+(\sin\theta)\vec u_2$, then we get (from the above formula) that $\vec v$ is asymptotic if and only if $\kappa_1\cos^2\theta+\kappa_2\sin^2\theta=0$.  Observe that such directions do not exist at elliptic points, but they do exist at parabolic points (if $\kappa_1=0$ and $\kappa_2\ne 0$, then the only asymptotic directions are $\pm\vec u_1$).  At a planar point, every direction is asymptotic.  A hyperbolic point has exactly four asymptotic directions, since for such a point $\kappa_1$ and $\kappa_2$ have opposite signs and one can arrange for $\theta=\pm\tan^{-1}\sqrt{-\kappa_1/\kappa_2}$.

A curve $\alpha:I\to S$ is said to be an \textbf{asymptotic curve} if all its derivatives go in asymptotic directions; i.e., $\alpha'(t)$ is an asymptotic direction at $\alpha(t)$ for each $t\in I$.  Basic examples are any curve in the plane, and a generating line of the cylinder.  Exercise 11 covers a curious theorem about them.

\subsection*{Exercises 6.6. (The Second Fundamental Form)} % Introduce the SFF coefficients, use them to get the Gauss map's differential's matrix
% Also cover conjugate directions.
% POTENTIAL EXERCISE: the Dupin indicatrix, Beltrami-Enneper's (last page of https://faculty.math.illinois.edu/~kapovich/423-14/surfacesofrevolution.pdf )
\begin{enumerate}
\item Show that a connected regular surface is contained in a plane if and only if, for some parametrization, its second fundamental form coefficients $L,M,N$ are all zero.

\item Let $S$ be a regular surface with no umbilical points, and $\mathbf x$ a local coordinate chart.  Show that the coordinate curves are lines of curvature if and only if $F=M=0$.  [Compare with Exercise 6(b) of the previous section.]

\item Show that dilating a regular surface by a factor of $\lambda>0$ multiplies the Gaussian curvature by $\frac 1{\lambda^2}$.  In particular, the sphere of radius $r$ has Gaussian curvature $\frac 1{r^2}$.

\item Suppose $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$ (with $f>0$) is a surface of revolution, and that there is a smooth function $\theta(u)$ such that $f'(u)=\sin\theta(u)$ and $g'(u)=\cos\theta(u)$.  Hence, the generating curve is parametrized by arc length, and $\theta$ gives the direction in which it points.

Recall (from Section 6.3) that the first fundamental form is $ds^2=du^2+f(u)^2\,dv^2$; i.e., $E=1$, $F=0$ and $G=f(u)^2$.

(a) Find $\mathbf N$ in terms of $u$ and $v$.

(b) Show that $L=-\theta'(u)$, $M=0$ and $N=f(u)\cos\theta(u)$.  Conclude that the Gaussian curvature $K$ is equal to $\frac{-\theta'(u)\cos\theta(u)}{f(u)}$.  [See if you can get an informal understanding of why $K$ varies directly with the generating curve's curvature (which is $\theta'(u)$), directly with the slope toward or away from the revolution axis (which is $\cos\theta(u)$), and inversely with the parallel's radius (which is $f(u)$).]

\item If $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$ is a surface of revolution, but we are \emph{not} assuming that $(f')^2+(g')^2=1$, show that
$$K=\frac{g'(u)[f'(u)g''(u)-f''(u)g'(u)]}{f(u)[f'(u)^2+g'(u)^2]}.$$
[This essentially mirrors the argument when $(f')^2+(g')^2=1$.]

\item Let $\mathbf x$ be the pseudosphere (Exercise 6(d) of Section 6.3):
$$\mathbf x(u,v)=(\sin u\cos v,\sin u\sin v,\cos u+\ln(\tan(u/2)))$$
Show that the Gaussian curvature of $\mathbf x$ is $-1$ everywhere.  [This is the kind of surface on which hyperbolic geometry can be done, at least locally.  For further details, see Section 6.10.]

\item If $\vec v,\vec w\in T_pS$ are orthonormal, show that $K=I\!I_p(\vec v)I\!I_p(\vec w)-[d\mathbf N_p(\vec v)\cdot\vec w]^2$.  [This expression is the determinant of the matrix $\begin{bmatrix}-I\!I_p(\vec v)&d\mathbf N_p(\vec v)\cdot\vec w\\d\mathbf N_p(\vec v)\cdot\vec w&-I\!I_p(\vec w)\end{bmatrix}$, and this matrix is equal to
$$\begin{bmatrix}d\mathbf N_p(\vec v)\cdot\vec v&d\mathbf N_p(\vec v)\cdot\vec w\\d\mathbf N_p(\vec v)\cdot\vec w&d\mathbf N_p(\vec w)\cdot\vec w\end{bmatrix}=\begin{bmatrix}\leftarrow&\vec v&\rightarrow\\\leftarrow&\vec w&\rightarrow\end{bmatrix}d\mathbf N_p\begin{bmatrix}\uparrow&\uparrow\\\vec v&\vec w\\\downarrow&\downarrow\end{bmatrix}$$
Now take determinants throughout.]

\item Let $\alpha:I\to S$ be a regular curve, say $\alpha(t)=\mathbf x(u(t),v(t))$ where $\mathbf x$ is a local coordinate chart.  If $E,F,G$ and $L,M,N$ are the fundamental form coefficients for $\mathbf x$, show that $\alpha$ is a line of curvature if and only if $\det\begin{bmatrix}(v')^2&-u'v'&(u')^2\\E&F&G\\L&M&N\end{bmatrix}=0$.  [If $d\mathbf N_p(\alpha'(t))=\lambda(t)\alpha'(t)$, taking coordinates with respect to the basis $\mathbf x_u,\mathbf x_v$ shows that $a_{11}u'+a_{12}v'=\lambda u'$ and $a_{21}u'+a_{22}v'=\lambda v'$, where $a_{ij}$ are given by the Weingarten equations.  Now multiply these equations by $v'$ and $u'$ respectively and subtract them.]

\item\emph{(Surfaces of revolution with constant Gaussian curvature.)} \---- The aim of this exercise is to classify surfaces of revolution with constant Gaussian curvature.  We recall that we may parametrize a surface of revolution via $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$ with $f>0$ and $(f')^2+(g')^2=1$.  We also know that
$$K=\frac{g'(u)[f'(u)g''(u)-f''(u)g'(u)]}{f(u)}.$$
Now let us suppose $K$ is constant.

(a) Show that $f$ satisfies the differential equation $f''+Kf=0$.  [First show that $f'f''+g'g''=0$ by differentiating the equation $(f')^2+(g')^2=1$.  Then show that $K=\frac{(f'f''+g'g'')f'-f''}f$ by basic algebraic manipulation, at one point using the fact that $(g')^2=1-(f')^2$.  From this it follows that $K=-\frac{f''}f$, hence $f''+Kf=0$.]

(b) If $K=0$, then part (a) implies $f''=0$, hence $f'$ is constant, say $f'(u)=c$.  With that, $g'(u)=\sqrt{1-f'(u)^2}=\sqrt{1-c^2}$.  Use this to show that the surface is part of either a plane, cone or cylinder.

(c) If $K=1$, then we get $f(u)=c_1\cos u+c_2\sin u$ by solving part (a)'s equation.  We may assume $c_2=0$ by translating the input $u$ in the parametrization $\mathbf x$; hence $f(u)=c\cos(u)$.  Show that $g'(u)=\sqrt{1-c^2\sin^2u}$.  Determine that the surface may have three different appearances, depending on whether $c>1$, $c=1$ or $c<1$.  Sketch each appearance.

(d) If $K=-1$, then we have $f(u)=c_1e^u+c_2e^{-u}$ by solving part (a)'s equation.  Show that by translating the input $u$ and/or negating, $f(u)$ can be arranged to exactly one of the following forms:

~~~~(i) $f(u)=c\cosh u$;

~~~~(ii) $f(u)=c\sinh u$;

~~~~(iii) $f(u)=ce^u$.

In each case, determine and sketch the appearance of the surface.  Also show that in case (iii) the surface is the pseudosphere.

(e) Use Exercise 3 to complete the classification with $K$ any real constant whatsoever.

\item\emph{(The Dupin Indicatrix.)} \---- Let $p\in S$ be a point of a regular surface.  The \textbf{Dupin indicatrix} at $p$ is defined to be the set $\Delta(p)=\{\vec v\in T_pS:I\!I_p(\vec v)=\pm 1\}$.

(a) If $p$ is planar, $\Delta(p)=\varnothing$.  If $p$ is parabolic, $\Delta(p)$ is a union of two parallel lines.  If $p$ is elliptic, $\Delta(p)$ is an ellipse.  If $p$ is hyperbolic, $\Delta(p)$ is a union of two hyperbolas sharing the same asymptotes.

(b) A vector $\vec v\in T_pS$ is asymptotic if and only if either $p$ is planar, $p$ is parabolic and $\vec v$ is parallel to the lines, or $p$ is hyperbolic and $\vec v$ is contained in one of the asymptotes of the hyperbolas.

(c) If a vector $\vec v\in T_pS$ extends to meet the Dupin indicatrix $\Delta(p)$ at $q$, and $\ell$ is the tangent line to the Dupin indicatrix at $q$, then $\vec v$ is conjugate to any direction parallel to $\ell$.  This is a geometric derivation of conjugate directions from the Dupin indicatrix.  [Fix an orthonormal eigenbasis of $d\mathbf N_p$, find an algebraic formula for the Dupin indicatrix, and use multivariable calculus.]

\item\emph{(Beltrami-Enneper's Theorem.)} \---- Let $\alpha:I\to S$ be an asymptotic curve on a regular surface, and let $\tau$ be its torsion.

(a) Show that the normal curvature of $\alpha$ is zero; i.e., $\boldsymbol\nu\perp\mathbf N$ along $\alpha$ where $\boldsymbol\nu$ is the normal unit vector in the Frenet trihedron.  [Use Exercise 7(b) of the previous section, which states that the normal curvature in a direction is the second fundamental form applied to the unit vector in that direction.]

(b) Show that $\tau^2=-K$ throughout $\alpha$.  [Reparametrize to assume $\alpha$ is an arc-length parametrization $J\to S$.  In the Frenet trihedron of $\alpha$, $\boldsymbol\tau$ is tangent to $S$, and so is $\boldsymbol\nu$ by part (a).  Therefore $\boldsymbol\beta$ is normal to $S$; i.e., $\boldsymbol\beta=\pm\mathbf N$.  Moreover, $\tau(s)=-\boldsymbol\beta'(s)\cdot\boldsymbol\nu(s)=\mp\mathbf N'(s)\cdot\boldsymbol\nu(s)=\mp d\mathbf N_p(\boldsymbol\tau(s))\cdot\boldsymbol\nu(s)$.  Now apply Exercise 7 with $\vec v=\boldsymbol\tau(s),\vec w=\boldsymbol\nu(s)$.]

\item Let $S\subset\mathbb R^n$ be a hypersurface.  Recall that on a local coordinate chart $\mathbf x:U\to S$, one can define a Gauss map $\mathbf N$.  With enough mathematical experience, it should be easy to generalize this chapter's results.

For $1\leqslant i,j\leqslant n-1$, define $M_{ij}=-\mathbf N_{u_i}\cdot\mathbf x_{u_j}=\mathbf N\cdot\mathbf x_{u_iu_j}$.  Note that $M_{ij}=M_{ji}$.  The $M_{ij}$'s are called the \textbf{coefficients of the second fundamental form}.  We let $M$ be the (symmetric) matrix $\begin{bmatrix}M_{ij}\end{bmatrix}_{1\leqslant i,j\leqslant n-1}$.

(a) Let $A$ be the matrix of $d\mathbf N_p$ with respect to the basis $\{\mathbf x_{u_1},\dots,\mathbf x_{u_{n-1}}\}$.  Show that $-M=FA$.  [By definition, $\mathbf N_{u_i}=d\mathbf N_p(\mathbf x_{u_i})=\sum_{j=1}^{n-1}A_{ji}\mathbf x_{u_j}$.  Hence taking the dot product with $\mathbf x_{u_k}$ entails $-M_{ik}=\sum_{j=1}^{n-1}A_{ji}F_{jk}$.]  Conclude that $A=-F^{-1}M$, and hence each entry of $A$ can be expressed as the quotient of a polynomial in the $F_{ij}$ and $M_{ij}$, by $\det F$ (and that this polynomial doesn't depend on what the $F_{ij}$ and $M_{ij}$ are). % What I mean is that, there is a single element of \mathbb R[x_{ij},y_{ij}] such that, for any hypersurface whatsoever, substituting F_{ij}=x_{ij},M_{ij}=x_{ij} gives the matrix entries of (\det F)\cdot A.  The element of the polynomial ring doesn't depend on the particular hypersurface.  Is there a better way to say this?

(b) Show that the main curvature at the point is $(-1)^{n-1}\frac{\det M}{\det F}$.

(c) The main curvature of a hyperplane is $0$, and the main curvature of a hypersphere of radius $R$ (when suitably oriented) is $\frac 1{R^{n-1}}$.

(d) Suppose $\mathbf x(u_1,\dots,u_{n-1})=(u_1,\dots,u_{n-1},f(u_1,\dots,u_{n-1}))$ is a graph of an explicit function.  We recall (Exercise 12(f) of Section 6.3) that $F_{ij}=\delta_{ij}+f_{u_i}f_{u_j}$ (so that $F_{ij}=f_{u_i}f_{u_j}$ for $i\ne j$ and $F_{ii}=1+f_{u_i}^2$).  We also have $\det F=1+\|\vec\nabla f\|^2$.  Show that $\mathbf N=\frac 1{\sqrt{1+\|\vec\nabla f\|^2}}(-f_{u_1},-f_{u_2},\dots,-f_{u_{n-1}},1)$. [It would help to use Exercise 10 of the previous section, and think of Laplacian expansion when determining the entries of the cross product.]  Conclude that $M_{ij}=\frac{f_{u_iu_j}}{\sqrt{1+\|\vec\nabla f\|^2}}$, and that the main curvature of the graph is $\frac{\det\mathbf H}{(1+\|\vec\nabla f\|^2)^{(n+1)/2}}$, where $\mathbf H$ is the Hessian matrix $\begin{bmatrix}f_{u_1u_1}&\dots&f_{u_1u_{n-1}}\\\vdots&\ddots&\vdots\\f_{u_{n-1}u_1}&\dots&f_{u_{n-1}u_{n-1}}\end{bmatrix}$.

(e) Let $\alpha(t)=(f(t),g(t))$ be an embedded curve with $f>0$ and $(f')^2+(g')^2=1$, and let $\mathbf x$ be the \textbf{3-manifold of ball-and-socket revolution}:
$$\mathbf x(u,v,w)=(f(u)\sin v\cos w,f(u)\sin v\sin w,f(u)\cos v,g(u)).$$
[Notice that the coordinate $2$-surfaces $(v,w)\mapsto\mathbf x(u_0,v,w)$ are spheres.]  Find the partial derivatives of $\mathbf x$, the first fundamental form coefficients, the Gauss map, the second fundamental form coefficients, and the main curvature.
\end{enumerate}

\subsection*{6.7. The Gauss and Mainardi-Codazzi Equations}
\addcontentsline{toc}{section}{6.7. The Gauss and Mainardi-Codazzi Equations}
Now that we thoroughly understand the Gauss map and second fundamental form, we shall introduce yet \emph{another} collection of coefficients, which (unlike the second fundamental form), can be derived algebraically from the first fundamental form coefficients and some of their derivatives.  We will then use them to derive equations giving necessary and sufficient conditions on the coefficients $E,F,G,L,M,N$ (in addition to the inequalities $E>0,EG-F^2>0$) for regular surfaces to exist.

Let $\mathbf x:U\to S$ be a local coordinate chart, and assume $\mathbf N=\frac{\mathbf x_u\times\mathbf x_v}{\|\mathbf x_u\times\mathbf x_v\|}$.  Then at any point $p\in U$, $\mathbf x_u,\mathbf x_v,\mathbf N$ form a basis of $\mathbb R^3$ (because $\mathbf x_u,\mathbf x_v$ are linearly independent and $\mathbf N$ is normal to the plane they span).  Thus we can express any vector \---- in particular $\mathbf x_{uu},\mathbf x_{uv},\mathbf x_{vv},\mathbf N_u,\mathbf N_v$ \---- as a unique linear combination of them with scalar coefficients.  Of course, the scalars depend on the point $p$, but it is not hard to see that they are differentiable with respect to $p$.

In other words, we may fill in the question marks below with unique scalar functions:
\begin{align*}
\mathbf x_{uu}&=[?]~\mathbf x_u+[?]~\mathbf x_v+[?]~\mathbf N\\
\mathbf x_{uv}&=[?]~\mathbf x_u+[?]~\mathbf x_v+[?]~\mathbf N\\
\mathbf x_{vv}&=[?]~\mathbf x_u+[?]~\mathbf x_v+[?]~\mathbf N\\
\mathbf N_u&=[?]~\mathbf x_u+[?]~\mathbf x_v+[?]~\mathbf N\\
\mathbf N_v&=[?]~\mathbf x_u+[?]~\mathbf x_v+[?]~\mathbf N
\end{align*}
The reason we did not write fifteen separate variables is that most of the coefficients can already be found from the previous results of the chapter.  For example, the coefficients of $\mathbf N$ on each line are obtained by merely taking dot products of the linear combinations with $\mathbf N$.  (Though $\mathbf x_u,\mathbf x_v,\mathbf N$ is not generally an orthonormal basis, $\mathbf N$ is still a unit vector orthogonal to the other two.)

Thus, the coefficient of $\mathbf N$ on the top line is $\mathbf N\cdot\mathbf x_{uu}$, which is the second fundamental form coefficient $L$.  Similarly, the next two lines have $M$ and $N$ for the coefficient of $\mathbf N$, and the last two lines (with $\mathbf N_u$ and $\mathbf N_v$) have zero for the coefficient of $\mathbf N$: since $\mathbf N\cdot\mathbf N=1$, a constant, taking partial derivatives entails $\mathbf N\cdot\mathbf N_u=0$ and $\mathbf N\cdot\mathbf N_v=0$.  This fills in the third column of question marks.

We also know the remaining coefficients of $\mathbf N_u$'s and $\mathbf N_v$'s linear combinations; they are given by the Weingarten equations covered in the previous section.  If
$$a_{11}=\frac{FM-GL}{EG-F^2},~~~~a_{12}=\frac{FN-GM}{EG-F^2},~~~~a_{21}=\frac{FL-EM}{EG-F^2},~~~~a_{22}=\frac{FM-EN}{EG-F^2},$$
then $\mathbf N_u=a_{11}\mathbf x_u+a_{21}\mathbf x_v$ and $\mathbf N_v=a_{12}\mathbf x_u+a_{22}\mathbf x_v$, filling in the question marks on the bottom two rows.

However, the remaining six question marks \---- the coefficients of $\mathbf x_u,\mathbf x_v$ in the second partial derivatives of $\mathbf x$ \---- are actually new to us; we have not explicitly dealt with them before.  They are denoted $\Gamma_{ij}^k$ and are called the \textbf{Christoffel symbols} for the local parametrization.  We will shortly show how to derive them from the other coefficients; first, let us rewrite our handy linear combinations:
\begin{align*}
&~~~~\text{(A)}\\
\mathbf x_{uu}&=\Gamma_{11}^1\mathbf x_u+\Gamma_{11}^2\mathbf x_v+L\mathbf N\\
\mathbf x_{uv}&=\Gamma_{12}^1\mathbf x_u+\Gamma_{12}^2\mathbf x_v+M\mathbf N\\
\mathbf x_{vv}&=\Gamma_{22}^1\mathbf x_u+\Gamma_{22}^2\mathbf x_v+N\mathbf N\\
\mathbf N_u&=a_{11}\mathbf x_u+a_{21}\mathbf x_v\\
\mathbf N_v&=a_{12}\mathbf x_u+a_{22}\mathbf x_v
\end{align*}
[The superscript $2$'s have nothing to do with squaring; every Christoffel symbol has a superscript which indexes the term it touches.]  Observe that if we rewrite $u=u_1,v=u_2$, then in each case $\Gamma_{ij}^k$ is the coefficient of $\mathbf x_{u_k}$ in $\mathbf x_{u_iu_j}$.  Under said convention, we would also have $\mathbf x_{vu}=\Gamma_{21}^1\mathbf x_u+\Gamma_{21}^2\mathbf x_v+M\mathbf N$; however, the symmetry $\mathbf x_{vu}=\mathbf x_{uv}$ entails that $\Gamma_{12}^k=\Gamma_{21}^k$ for each $k$.  Thus Christoffel symbols are symmetric in their lower indices.

To derive the Christoffel symbols, we start by taking the dot products of each of the first three equations (A) with $\mathbf x_u$ and $\mathbf x_v$.  In each line below, the second expression is obtained by plugging in the linear combination and using the bilinearity of the dot product; and the third expression can be worked out using Exercise 4(a) of Section 6.1.  For example, $(\Gamma_{11}^1\mathbf x_u+\Gamma_{11}^2\mathbf x_v+L\mathbf N)\cdot\mathbf x_u=\Gamma_{11}^1(\mathbf x_u\cdot\mathbf x_u)+\Gamma_{11}^2(\mathbf x_v\cdot\mathbf x_u)+L(\mathbf N\cdot\mathbf x_u)=\Gamma_{11}^1E+\Gamma_{11}^2F$; and we have $E_u=\frac{\partial E}{\partial u}=\frac{\partial}{\partial u}(\mathbf x_u\cdot\mathbf x_u)=2\mathbf x_u\cdot\mathbf x_{uu}$.
$$\left\{\begin{array}{c l}\mathbf x_{uu}\cdot\mathbf x_u=\Gamma_{11}^1E+\Gamma_{11}^2F=\frac 12E_u~~~~~~~\\\mathbf x_{uu}\cdot\mathbf x_v=\Gamma_{11}^1F+\Gamma_{11}^2G=F_u-\frac 12E_v\end{array}\right.$$
$$\left\{\begin{array}{c l}\mathbf x_{uv}\cdot\mathbf x_u=\Gamma_{12}^1E+\Gamma_{12}^2F=\frac 12E_v~~~~~~~\\\mathbf x_{uv}\cdot\mathbf x_v=\Gamma_{12}^1F+\Gamma_{12}^2G=\frac 12G_u~~~~~~~\end{array}\right.$$
$$\left\{\begin{array}{c l}\mathbf x_{vv}\cdot\mathbf x_u=\Gamma_{22}^1E+\Gamma_{22}^2F=F_v-\frac 12G_u\\\mathbf x_{vv}\cdot\mathbf x_v=\Gamma_{22}^1F+\Gamma_{22}^2G=\frac 12G_v~~~~~~~\end{array}\right.$$
Each braced pair of equations is a system of linear equations in the variables $\Gamma_{ij}^k$, whose determinant is $EG-F^2>0$; hence the equations have unique solutions.  In fact, the Christoffel symbols are polynomials in $E,F,G$ and their first order derivatives divided by $EG-F^2$ (as linear algebra readily shows), and said polynomials apply universally to any local coordinate chart of any regular surface, without depending on the functions $E,F,G$ themselves.

Perhaps an example would be in order.  Let $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$ be a surface of revolution with $f>0$ and $(f')^2+(g')^2=1$.  We recall that:
$$E=1,~~~~F=0,~~~~G=f(u)^2$$
and we may solve the above linear equations.  For instance, $\Gamma_{11}^1E+\Gamma_{11}^2F=\frac 12E_u$ becomes $\Gamma_{11}^1=0$ when substituting $E,F,G$ from above.  Similarly, from $\Gamma_{12}^1F+\Gamma_{12}^2G=\frac 12G_u$, we get $\Gamma_{12}^2f(u)^2=f'(u)f(u)$, and hence $\Gamma_{12}^2=\frac{f'(u)}{f(u)}$.  The rest of the coefficients can be likewise computed:
$$\Gamma_{11}^1=0,~~~~\Gamma_{11}^2=0$$
$$\Gamma_{12}^1=0,~~~~\Gamma_{12}^2=\frac{f'(u)}{f(u)}$$
$$\Gamma_{22}^1=-f'(u)f(u),~~~~\Gamma_{22}^2=0$$
and the reader is encouraged to actually check the equations (A).\\

\noindent\textbf{THE EQUATIONS OF COMPABILITY}\\

\noindent We recall (Proposition 6.3) that regular curves exist with any prescribed curvature and torsion as differentiable functions in the arc length parameter (with the curvature positive).  As we are about to see, this is not quite true for the six coefficients $E,F,G,L,M,N$ for regular surfaces, as there are necessary conditions involving them (and their derivatives) which obviously don't hold for arbitrary functions in general.

To start, we use the symmetry of mixed partial derivatives to state:
$$(\mathbf x_{uu})_v=(\mathbf x_{uv})_u,~~~~~~~~(\mathbf x_{uv})_v=(\mathbf x_{vv})_u,~~~~~~~~(\mathbf N_u)_v=(\mathbf N_v)_u$$
We shall first substitute the equations (A) into $(\mathbf x_{uu})_v=(\mathbf x_{uv})_u$ to get:
$$\left(\Gamma_{11}^1\mathbf x_u+\Gamma_{11}^2\mathbf x_v+L\mathbf N\right)_v=\left(\Gamma_{12}^1\mathbf x_u+\Gamma_{12}^2\mathbf x_v+M\mathbf N\right)_u$$
Then we expand both sides, using the Product Rule several times:
$$(\Gamma_{11}^1)_v\mathbf x_u+\Gamma_{11}^1\mathbf x_{uv}+(\Gamma_{11}^2)_v\mathbf x_v+\Gamma_{11}^2\mathbf x_{vv}+L_v\mathbf N+L\mathbf N_v$$
$$=(\Gamma_{12}^1)_u\mathbf x_u+\Gamma_{12}^1\mathbf x_{uu}+(\Gamma_{12}^2)_u\mathbf x_v+\Gamma_{12}^2\mathbf x_{uv}+M_u\mathbf N+M\mathbf N_u$$
And now we have $\mathbf x_{uu},\mathbf x_{uv},\mathbf x_{vv},\mathbf N_u,\mathbf N_v$ once again, into which we can substitute the equations (A); e.g., we substitute $\Gamma_{11}^1\mathbf x_u+\Gamma_{11}^2\mathbf x_v+L\mathbf N$ into $\mathbf x_{uu}$. % The equations (A) already indicate which to substitute into which, but I guess I'll state an example.
However, doing so directly will give us long convoluted expressions, so we will focus on just \emph{one coordinate at a time}.  Let us write out the $\mathbf x_v$ coordinates when both sides of the above equation are written as linear combinations of $\mathbf x_u,\mathbf x_v,\mathbf N$.  In other words, we will pass the equation through the linear map $a\mathbf x_u+b\mathbf x_v+c\mathbf N\mapsto b$.  Then, for example, $\mathbf x_{uv}$ gets passed to $\Gamma_{12}^2$, and $\mathbf x_u$ and $\mathbf N$ get passed to zero. % Reworded.  I don't think the reader will think of it as \mathbf x_u becoming zero now.
$$\Gamma_{11}^1\Gamma_{12}^2+(\Gamma_{11}^2)_v+\Gamma_{11}^2\Gamma_{22}^2+La_{22}=\Gamma_{12}^1\Gamma_{11}^2+(\Gamma_{12}^2)_u+\Gamma_{12}^2\Gamma_{12}^2+Ma_{21}$$
We substitute the Weingarten equations for $a_{22}$ and $a_{21}$, then rearrange terms algebraically to get:
$$\Gamma_{11}^1\Gamma_{12}^2+(\Gamma_{11}^2)_v+\Gamma_{11}^2\Gamma_{22}^2-\Gamma_{12}^1\Gamma_{11}^2-(\Gamma_{12}^2)_u-\Gamma_{12}^2\Gamma_{12}^2=E\frac{LN-M^2}{EG-F^2}=EK.$$
This is known as the \textbf{Gauss equation}.  Since $E>0$, we can even divide both sides by $E$, to make the right-hand side equal to the Gaussian curvature $K$.

It is difficult to realize just what we have come to at this point.  Through an extreme amount of algebra and calculus, we have managed to derive $K$ directly from $E,F,G$ and their derivatives up to the second order.  [After all, the Christoffel symbols $\Gamma_{ij}^k$ are derivable from $E,F,G$ and their first-order derivatives, and the expression above uses the Christoffel symbols and \emph{their} first order derivatives.]  By Proposition 6.6, if surfaces are locally isometric, then (for suitable coordinate charts) they have identical first fundamental form coefficients, hence agree on anything directly derivable from them.  This proves\\

\noindent\textbf{Theorem 6.13.} \textsc{(Theorema Egregium)} \emph{The Gaussian curvature of regular surfaces is invariant under local isometries.}\\

\noindent Of course, this was not obvious before, as the Gaussian curvature seemed to depend on the Gauss map and the second fundamental form.  The above formula for $K$ in terms of $E,F,G$ and their derivatives up to the second order is called the \textbf{Gauss formula}.

For an example of how to use this, consider the surface of revolution $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$ with $f>0$ and $(f')^2+(g')^2=1$. % I guess overfull hboxes are fatal since you said "at the end, check all chapters for overfull hboxes" in red.  I could check overfull hboxes, but I don't know a better way to get rid of them besides making a new paragraph without an indent.
Then we have previously found $\Gamma_{11}^1=\Gamma_{11}^2=\Gamma_{12}^1=\Gamma_{22}^2=0$, $\Gamma_{12}^2=\frac{f'(u)}{f(u)}$ and $\Gamma_{22}^1=-f'(u)f(u)$.  Thus four terms vanish in the Gauss formula, leaving us with:
$$-(\Gamma_{12}^2)_u-\Gamma_{12}^2\Gamma_{12}^2=EK=K$$
Elementary calculus shows that $(\Gamma_{12}^2)_u=\frac{f''(u)f(u)-f'(u)^2}{f(u)^2}$, and hence
$$K=-\frac{f''(u)f(u)-f'(u)^2}{f(u)^2}-\left(\frac{f'(u)}{f(u)}\right)^2=-\frac{f''(u)}{f(u)}.$$
We have previously found the Gaussian curvature to be $\frac{g'(u)[f'(u)g''(u)-f''(u)g'(u)]}{f(u)}$, but from the assumption $(f')^2+(g')^2=1$ it follows that that expression is equal to $-\frac{f''(u)}{f(u)}$; see the hint for Exercise 9(a) of the previous section.  Thus when the generating curve is parametrized by arc-length, (well, this fact doesn't have to do with how it's parametrized anyway), the curvature is positive when the curve is concave towards the rotation axis (so that $f''<0$), negative when the curve is concave away from the axis, and zero at the curve's inflection points; this is easy to see just by mentally picturing the surface.

So far, we have obtained a formula by working out the $\mathbf x_v$ coordinate of the equation $(\mathbf x_{uu})_v=(\mathbf x_{uv})_u$.  Recall that there are three of these starting points:
$$(\mathbf x_{uu})_v=(\mathbf x_{uv})_u,~~~~~~~~(\mathbf x_{uv})_v=(\mathbf x_{vv})_u,~~~~~~~~(\mathbf N_u)_v=(\mathbf N_v)_u$$
and for each one, there are three ways to get equations: by taking the coordinates of $\mathbf x_u$, $\mathbf x_v$ or $\mathbf N$.  Thus there are nine equations in total we can get out of the symmetry of mixed partial derivatives.  However, most of them are actually redundant; for instance, taking the $\mathbf x_u$ coefficients in $(\mathbf x_{uu})_v=(\mathbf x_{uv})_u$ entails
$$(\Gamma_{11}^1)_v+\Gamma_{11}^2\Gamma_{22}^1-(\Gamma_{12}^1)_u-\Gamma_{12}^2\Gamma_{12}^1=Ma_{11}-La_{12}=-FK;$$
this can be proven to be equivalent to the Gauss formula (at least when $F=0$), but we will not delve into details here.  

We leave it to the reader skilled in manipulation of expressions to try all the other possibilities, and observe that there are only two more which are irredundant from each other and from the Gauss formula: these two are
$$L_v-M_u=\Gamma_{12}^1L+(\Gamma_{12}^2-\Gamma_{11}^1)M-\Gamma_{11}^2N$$
(obtained by taking $\mathbf N$'s coordinates in $(\mathbf x_{uu})_v=(\mathbf x_{uv})_u$), and
$$M_v-N_u=\Gamma_{22}^1L+(\Gamma_{22}^2-\Gamma_{12}^1)M-\Gamma_{12}^2N$$
(obtained by doing the same for $(\mathbf x_{uv})_v=(\mathbf x_{vv})_u$).  These two equations are called the \textbf{Mainardi-Codazzi equations}.

The Gauss and Mainardi-Codazzi equations are thus tantamount to $\mathbf x_u,\mathbf x_v,\mathbf N$ having symmetry of mixed partial derivatives when expanded.  Using this fact, one can show that they are exactly what's needed for regular surfaces to exist with prescribed coefficients $E,F,G,L,M,N$.  These equations are known as the \textbf{equations of compatibility}.

Before we can get going with the proof, we need another result on differential equations, similar to that of Theorem 6.2.  Its proof is due to the same authors as Theorem 6.2.\\

\noindent\textbf{Theorem 6.14.} \textsc{(Fundamental Theorem of Partial Differential Equations)} \emph{If $f_i(u,v,x_1,\dots,x_n),g_i(u,v,x_1,\dots,x_n),1\leqslant i\leqslant n$ are differentiable functions which are defined for $(u,v)\in U$, and $(u_0,v_0)$ is a fixed element of $U$, then consider the system of partial differential equations}
$$\frac{\partial x_1}{\partial u}=f_1(u,v,x_1,x_2,\dots,x_n),~~~~\frac{\partial x_1}{\partial v}=g_1(u,v,x_1,x_2,\dots,x_n)$$
$$\frac{\partial x_2}{\partial u}=f_2(u,v,x_1,x_2,\dots,x_n),~~~~\frac{\partial x_2}{\partial v}=g_2(u,v,x_1,x_2,\dots,x_n)$$
$$\vdots$$
$$\frac{\partial x_n}{\partial u}=f_n(u,v,x_1,x_2,\dots,x_n),~~~~\frac{\partial x_n}{\partial v}=g_n(u,v,x_1,x_2,\dots,x_n)$$
\begin{center}
\emph{with given initial conditions $x_i(u_0,v_0)=a_i$ ($a_i\in\mathbb R$).}
\end{center}
\emph{If, for every $1\leqslant i\leqslant n$, $\frac{\partial f_i}{\partial v}=\frac{\partial g_i}{\partial u}$ (i.e., the expressions coincide when the Chain Rule is used and then the above equations are substituted for the derivatives of the $x$'s), the system has a unique solution in some open neighborhood of $(u_0,v_0)$ in $U$.}\\

\noindent In other words, partial differential equations have a unique solution provided that the partial derivatives of the same functions satisfy mixed partial derivative symmetry.  With that, we have our desired theorem about surfaces with prescribed coefficients:\\

\noindent\textbf{Theorem 6.15.} \textsc{(Bonnet's Theorem)} \emph{Let $U\subset\mathbb R^2$ be an open set, and $E,F,G,L,M,N:U\to\mathbb R$ differentiable functions, such that $E>0$, $EG-F^2>0$, and the Gauss and Mainardi-Codazzi equations hold.  Then for any point $u_0\in U$ there is a neighborhood $u_0\in V\subset U$, a regular surface $S$ and a local coordinate chart $\mathbf x:V\to S$, for which $E,F,G$ are the first fundamental coefficients and $L,M,N$ are the second fundamental form coefficients.  Moreover, any two such charts on the same domain $V$ are related by an element of $\operatorname{Isom}^+(\mathbb R^3)$.}\\

\noindent Remember, the Christoffel symbols $\Gamma_{ij}^k$ are directly derivable from $E,F,G$ and their first order derivatives.  Hence it makes sense to say the equations of compatibility hold up above, even though these equations involve the Christoffel symbols.
\begin{proof}
We start by writing the functions $\mathbf x_u,\mathbf x_v,\mathbf N$ to be constructed in the proof, using scalar functions on $U$:
$$\mathbf x_u=\boldsymbol\zeta=(\zeta_1,\zeta_2,\zeta_3)$$
$$\mathbf x_v=\boldsymbol\eta=(\eta_1,\eta_2,\eta_3)$$
$$\mathbf N=\boldsymbol\theta=(\theta_1,\theta_2,\theta_3)$$
In accordance with the equations (A), we should have, for $i=1,2,3$,
$$(\zeta_i)_u=\Gamma_{11}^1\zeta_i+\Gamma_{11}^2\eta_i+L\theta_i$$
$$(\zeta_i)_v=\Gamma_{12}^1\zeta_i+\Gamma_{12}^2\eta_i+M\theta_i$$
$$(\eta_i)_u=\Gamma_{12}^1\zeta_i+\Gamma_{12}^2\eta_i+M\theta_i$$
$$(\eta_i)_v=\Gamma_{22}^1\zeta_i+\Gamma_{22}^2\eta_i+N\theta_i$$
$$(\theta_i)_u=a_{11}\zeta_i+a_{21}\eta_i$$
$$(\theta_i)_v=a_{12}\zeta_i+a_{22}\eta_i$$
(for instance, the first equation is obtained by taking coordinates of $(\mathbf x_u)_u=\mathbf x_{uu}=\Gamma_{11}^1\mathbf x_u+\Gamma_{11}^2\mathbf x_v+L\mathbf N$).  This is a system of partial differential equations in the form of Theorem 6.14, where the $x_i$ are the nine functions $\zeta_i,\eta_i,\theta_i,i=1,2,3$.  We have the condition of mixed partial derivative symmetry (e.g., $\frac{\partial}{\partial v}(\zeta_i)_u=\frac{\partial}{\partial u}(\zeta_i)_v$) because that condition is equivalent to the Gauss and Mainardi-Codazzi equations, which hold by assumption.

Now we fix a $u_0\in U$, and we also impose the following initial conditions, which are plausible because $E>0$ and $EG-F^2>0$:
$$\boldsymbol\zeta(u_0)=(\sqrt E,0,0)\big|_{u_0}$$
$$\boldsymbol\eta(u_0)=\left(\frac F{\sqrt E},\frac{\sqrt{EG-F^2}}{\sqrt E},0\right)\big|_{u_0}$$
$$\boldsymbol\theta(u_0)=(0,0,1).$$
We can henceforth use Theorem 6.14 to get a solution to the PDE system, in some open set $u_0\in V\subset U$.  Thus $\boldsymbol\zeta,\boldsymbol\eta,\boldsymbol\theta:V\to\mathbb R^3$ are differentiable, and their derivatives are given by substituting themselves in the equation (A).

Our next step of the proof is to show that $\boldsymbol\zeta\cdot\boldsymbol\zeta=E$, $\boldsymbol\zeta\cdot\boldsymbol\eta=F$, $\boldsymbol\eta\cdot\boldsymbol\eta=G$, $\boldsymbol\zeta\cdot\boldsymbol\theta=\boldsymbol\eta\cdot\boldsymbol\theta=0$ and $\boldsymbol\theta\cdot\boldsymbol\theta=1$.  Clearly all these hold at $u_0$, but we need for them to hold throughout $V$.  This follows from the uniqueness in Theorem 6.14, as this system of partial differential equations, with initial conditions
$$(\delta_1)_u=2\Gamma_{11}^1\delta_1+2\Gamma_{11}^2\delta_2+2L\delta_4$$
$$(\delta_1)_v=2\Gamma_{12}^1\delta_1+2\Gamma_{12}^2\delta_2+2M\delta_4$$
$$(\delta_2)_u=\Gamma_{12}^1\delta_1+(\Gamma_{11}^1+\Gamma_{12}^2)\delta_2+\Gamma_{11}^2\delta_3+M\delta_4+L\delta_5$$
$$(\delta_2)_v=\Gamma_{22}^1\delta_1+(\Gamma_{12}^1+\Gamma_{22}^2)\delta_2+\Gamma_{12}^2\delta_3+N\delta_4+M\delta_5$$
$$(\delta_3)_u=2\Gamma_{12}^1\delta_2+2\Gamma_{12}^2\delta_3+2M\delta_5$$
$$(\delta_3)_v=2\Gamma_{22}^1\delta_2+2\Gamma_{22}^2\delta_3+2N\delta_5$$
$$(\delta_4)_u=a_{11}\delta_1+a_{21}\delta_2+\Gamma_{11}^1\delta_4+\Gamma_{11}^2\delta_5+L\delta_6$$
$$(\delta_4)_v=a_{12}\delta_1+a_{22}\delta_2+\Gamma_{12}^1\delta_4+\Gamma_{12}^2\delta_5+M\delta_6$$
$$(\delta_5)_u=a_{11}\delta_2+a_{21}\delta_3+\Gamma_{12}^1\delta_4+\Gamma_{12}^2\delta_5+M\delta_6$$
$$(\delta_5)_v=a_{12}\delta_2+a_{22}\delta_3+\Gamma_{22}^1\delta_4+\Gamma_{22}^2\delta_5+N\delta_6$$
$$(\delta_6)_u=2a_{11}\delta_4+2a_{21}\delta_5$$
$$(\delta_6)_v=2a_{12}\delta_4+2a_{22}\delta_5$$
$$\delta_1(u_0)=E(u_0),~~\delta_2(u_0)=F(u_0),~~\delta_3(u_0)=G(u_0),~~\delta_4(u_0)=0,~~\delta_5(u_0)=0,~~\delta_6(u_0)=1,$$
has both of these as solutions:
\begin{center}
\begin{tabular}{c|cccccc}
&$\delta_1$&$\delta_2$&$\delta_3$&$\delta_4$&$\delta_5$&$\delta_6$\\\hline
Solution 1&$\boldsymbol\zeta\cdot\boldsymbol\zeta$&$\boldsymbol\zeta\cdot\boldsymbol\eta$&$\boldsymbol\eta\cdot\boldsymbol\eta$&$\boldsymbol\zeta\cdot\boldsymbol\theta$&$\boldsymbol\eta\cdot\boldsymbol\theta$&$\boldsymbol\theta\cdot\boldsymbol\theta$\\
Solution 2&$E$&$F$&$G$&$0$&$0$&$1$
\end{tabular}
\end{center}
so the solutions must coincide in $V$.  We observe, furthermore, that $\boldsymbol\zeta,\boldsymbol\eta$ are always linearly independent (since their cross product has norm $\sqrt{EG-F^2}$), and $\boldsymbol\theta$ is a unit normal vector to the plane they span.  In particular, $(\boldsymbol\zeta\times\boldsymbol\eta)\cdot\boldsymbol\theta$ is nonzero throughout $V$, hence is positive (because it is positive at $u_0$), so that $\boldsymbol\zeta,\boldsymbol\eta,\boldsymbol\theta$ forms a \emph{positive} basis of $\mathbb R^3$ and $\boldsymbol\theta=\frac{\boldsymbol\zeta\times\boldsymbol\eta}{\|\boldsymbol\zeta\times\boldsymbol\eta\|}$.

Our next step is to observe that $\boldsymbol\zeta_v=\boldsymbol\eta_u$, because both are equal to $\Gamma_{12}^1\boldsymbol\zeta+\Gamma_{12}^2\boldsymbol\eta+M\boldsymbol\theta$.  Hence by Theorem 6.14 or otherwise we may construct (at least locally) a function $\mathbf x$ with $\mathbf x_u=\boldsymbol\zeta,\mathbf x_v=\boldsymbol\eta,\mathbf x(u_0)=\vec 0$.  By construction we have $\mathbf x_u\cdot\mathbf x_u=E,\mathbf x_u\cdot\mathbf x_v=F,\mathbf x_v\cdot\mathbf x_v=G$.  Therefore $d\mathbf x$ is injective (since $\det((d\mathbf x)^T(d\mathbf x))=\det\begin{bmatrix}E&F\\F&G\end{bmatrix}=EG-F^2>0$), hence by shrinking $V$ one may assume $\mathbf x$ embeds $V$ into $\mathbb R^3$.

Then $\mathbf x$ is a local coordinate chart for a regular surface, and as we have effectively shown, $E,F,G$ are its first fundamental form coefficients.  Since $\mathbf x_u=\boldsymbol\zeta$, $\mathbf x_v=\boldsymbol\eta$, and $\mathbf N=\frac{\mathbf x_u\times\mathbf x_v}{\|\mathbf x_u\times\mathbf x_v\|}=\boldsymbol\theta$, the partial differential equations satisfied by $\boldsymbol\zeta,\boldsymbol\eta,\boldsymbol\theta$ (at the beginning of this proof) entail the equations (A) for $\mathbf x$.  Finally, reading off the first three such equations:
\begin{align*}
\mathbf x_{uu}&=\Gamma_{11}^1\mathbf x_u+\Gamma_{11}^2\mathbf x_v+L\mathbf N\\
\mathbf x_{uv}&=\Gamma_{12}^1\mathbf x_u+\Gamma_{12}^2\mathbf x_v+M\mathbf N\\
\mathbf x_{vv}&=\Gamma_{22}^1\mathbf x_u+\Gamma_{22}^2\mathbf x_v+N\mathbf N
\end{align*}
and dotting each one with $\mathbf N$, gives $\mathbf N\cdot\mathbf x_{uu}=L$, $\mathbf N\cdot\mathbf x_{uv}=M$ and $\mathbf N\cdot\mathbf x_{vv}=N$.  Hence $L,M,N$ are the second fundamental form coefficients.  This proves the existence of the local parametrization satisfying all the desired properties.

What's left is to show that any two such charts relate by an element of $\operatorname{Isom}^+(\mathbb R^3)$.  To do this, we let $\overline{\mathbf x}:V\to\mathbb R^3$ be another chart of another surface, with the same first fundamental form and second fundamental form coefficients.  The reader can readily verify that the vectors
$$\vec u_1=\frac{\overline{\mathbf x}_u}{\sqrt E}\big|_{u_0},~~~~\vec u_2=\frac{E\overline{\mathbf x}_v-F\overline{\mathbf x}_u}{\sqrt E\sqrt{EG-F^2}}\big|_{u_0},~~~~\vec u_3=\vec u_1\times\vec u_2$$
form a positive orthonormal basis of $\mathbb R^3$.  Hence we may let $A=\begin{bmatrix}\uparrow&\uparrow&\uparrow\\\vec u_1&\vec u_2&\vec u_3\\\downarrow&\downarrow&\downarrow\end{bmatrix}\in SO(3)$.  Then $A$ sends $(\sqrt E,0,0)\mapsto\overline{\mathbf x}_u$, $\left(\frac F{\sqrt E},\frac{\sqrt{EG-F^2}}{\sqrt E},0\right)\mapsto\overline{\mathbf x}_v$ and $(0,0,1)\mapsto\overline{\mathbf N}$ with all expressions being applied to $u_0$.

Now define $\mathbf x_1:V\to\mathbb R^3$ by $\mathbf x_1(u)=A^{-1}(\overline{\mathbf x}(u)-\overline{\mathbf x}(u_0))$.  This is the result of applying the isometry $[A^{-1},-A^{-1}\overline{\mathbf x}(u_0)]\in\operatorname{Isom}^+(\mathbb R^3)$ to $\overline{\mathbf x}$.  Hence it is clear that $\mathbf x_1$ has the same coefficients as $\overline{\mathbf x}$ (and $\mathbf x$), and we need only show that $\mathbf x_1=\mathbf x$.  Well, all this basically uses uniqueness in initial value problems; first $\mathbf x_u,\mathbf x_v,\mathbf N$ satisfy the same differential equations (A) that those things for $\mathbf x_1$ do, and they have the same values at $u_0$ (verify!).  Hence $\mathbf x_u,\mathbf x_v,\mathbf N$ coincide with $(\mathbf x_1)_u,(\mathbf x_1)_v,\mathbf N_1$.  Since $\mathbf x-\mathbf x_1$ has partial derivatives of zero, it is constant, and construction also shows it sends $u_0\mapsto\vec 0$.  Thus $\mathbf x-\mathbf x_1$ is identically zero and $\mathbf x_1=\mathbf x$ as desired.
\end{proof}

\noindent Note that in Section 6.10, we will handle things differently.  We will be dealing with a customized metric on an open set in $\mathbb R^2$, and there is no such thing as Gauss maps; hence there is no second fundamental form, and no equations of compatibility (but there will still be Gaussian curvature, given by the Gauss formula established in this section). % Okay, but I said "important to us", not "important".  I believe Bonnet's Theorem is important, it's just not relevant to customized surfaces

\subsection*{Exercises 6.7. (The Gauss and Mainardi-Codazzi Equations)}
% POTENTIAL EXERCISES: the first three on Page 237 of Do Carmo
\begin{enumerate}
\item Given a local coordinate parametrization $\mathbf x:U\to S$ with first fundamental form coefficients $E=1,F=\sin u,G=2$, find the Gaussian curvature in terms of $u$ and $v$.

\item Are any of the following surfaces locally isometric? % Theorema Egregium easily answers this with no.  See Do Carmo, p. 237, Ex. 9.

(a) Sphere $x^2+y^2+z^2=1$;

(b) Cylinder $x^2+y^2=1$;

(c) Hyperbolic paraboloid / saddle $z=x^2-y^2$.

\item If $\mathbf x$ is a parametrization in which $F\equiv 0$, show that:
$$\Gamma_{11}^1=\frac 12\frac{E_u}E,~~~~\Gamma_{11}^2=-\frac 12\frac{E_v}G$$
$$\Gamma_{12}^1=\frac 12\frac{E_v}E,~~~~\Gamma_{12}^2=\frac 12\frac{G_u}G$$
$$\Gamma_{22}^1=-\frac 12\frac{G_u}E,~~~~\Gamma_{22}^2=\frac 12\frac{G_v}G$$
and use this to show that
$$K=-\frac 1{2\sqrt{EG}}\left[\left(\frac{E_v}{\sqrt{EG}}\right)_v+\left(\frac{G_u}{\sqrt{EG}}\right)_u\right].$$

\item If $\mathbf x$ is a parametrization with isothermal coordinates $E\equiv G\equiv\lambda,F\equiv 0$, show that $K=-\frac 1{2\lambda}\Delta(\ln\lambda)$, where $\Delta(f)=f_{uu}+f_{vv}$ (the Laplacian operator).

\item Show that the converse of Theorema Egregium is false by showing the following parametrizations have the same Gaussian curvature but do not relate by an isometry:
$$\mathbf x(u,v)=(u\cos v,u\sin v,\ln u)$$
$$\overline{\mathbf x}(u,v)=(u\cos v,u\sin v,v)$$

\item Suppose $\mathbf x$'s coordinate curves form a Chebyshev net (Exercise 4 of Section 6.3).

(a) Show that $\mathbf x$ can be reparametrized so that $E=G=1$ and $F=\cos\theta(u,v)$.

(b) Now show that $K=-\frac{\theta_{uv}}{\sin\theta}$.

\item For which coefficients do there exist a regular surface?

(a) $E=G=1$, $F=0$, $L=N=-1$, $M=0$

(b) $E=1$, $F=0$, $G=\cos^2u$, $L=\cos^2u$, $M=0$, $N=1$

\item What happens to the coefficients of a surface when you apply an orientation-reversing isometry, i.e., an element of $\operatorname{Isom}(\mathbb R^3)-\operatorname{Isom}^+(\mathbb R^3)$? % E,F,G stay the same and L,M,N negate, because the Gauss map is the only thing which utilizes the orientation

\item Effectively everything in this section has generalizations to arbitrary dimensions.  Let $S$ be a hypersurface in $\mathbb R^n$, and let $\mathbf x:U\to S$ be a local coordinate chart with $U\subset\mathbb R^{n-1}$. % arbitrary dimensions, but don't cover vector fields yet (they're for 6.8); introduce the (A) equations for manifolds that need not be hypersurfaces.
Recall the Gauss map $\mathbf N=\frac{\mathbf x_{u_1}\times\dots\times\mathbf x_{u_{n-1}}}{\|\mathbf x_{u_1}\times\dots\times\mathbf x_{u_{n-1}}\|}$, and the coefficients $F_{ij}=\mathbf x_{u_i}\cdot\mathbf x_{u_j}$ and $M_{ij}=\mathbf N\cdot\mathbf x_{u_iu_j}$.

(a) Show that there are equations ($1\leqslant i,j\leqslant -1$)
$$\mathbf x_{u_iu_j}=\sum_{k=1}^{n-1}\Gamma_{ij}^k\mathbf x_{u_k}+M_{ij}\mathbf N$$
$$\mathbf N_{u_i}=\sum_{k=1}^{n-1}a_{ki}\mathbf x_{u_k}$$
where the $\Gamma_{ij}^k$ can be derived directly from the $F_{ij}$ and their first order derivatives, and the $a_{ij}$ are the entries of the matrix $A$ of Exercise 12(a) of the previous section.  [First show that $\mathbf x_{u_iu_j}\cdot\mathbf x_{u_k}=\frac 12\left[(F_{ik})_{u_j}+(F_{jk})_{u_i}-(F_{ij})_{u_k}\right]$.  The rest essentially mirrors the case $n=3$ covered in the section.]

(b) Explain how the facts $(\mathbf x_{u_iu_j})_{u_k}=(\mathbf x_{u_iu_k})_{u_j}$ and $(\mathbf N_{u_i})_{u_j}=(\mathbf N_{u_j})_{u_i}$ can be rewritten in the form of equations in $F_{ij},M_{ij}$ and their derivatives up to the second order.  These equations are called the \textbf{equations of compatibility}.

(c) Assume the generalization of Theorem 6.14 to more than two variables.  Generalize Bonnet's Theorem: suppose $U\subset\mathbb R^{n-1}$ is open, and $F,M:U\to\operatorname{Sym}_{n-1}(\mathbb R)$ are differentiable functions from $U$ to the symmetric matrices, such that $F$ is positive definite, and the equations of compatibility hold.  Then there is locally a hypersurface with $F$ and $M$ as the fundamental forms, and any two such hypersurfaces differ by a transform in $\operatorname{Isom}^+(\mathbb R^n)$.  [Exercise 8 of Section 3.5 may help.]

(d) The special case of (a)-(c) with $n=3$ are the exact same things covered in this section.

%Let $\boldsymbol\tau(t)=\frac{\alpha'(t)}{\|\alpha'(t)\|}$ and $\boldsymbol\nu(t)={^\times\boldsymbol\tau(t)}$ be the tangential and normal unit vectors.\footnote{We recall that $^\times(a,b)=(-b,a)$ for $(a,b)\in\mathbb R^2$, i.e., $^\times\vec v=\begin{bmatrix}0&-1\\1&0\end{bmatrix}\vec v$.}
(e) In the case $n=2$, $\mathbf x$ is a regular curve $\alpha:I\to\mathbb R^2$.  Then show that $F_{11}=\|\alpha'(t)\|^2$, $\Gamma_{11}^1=\frac 1{\|\alpha'(t)\|}\frac d{dt}\|\alpha'(t)\|$, $M_{11}=\kappa(t)\|\alpha'(t)\|^2$ and $a_{11}=-\kappa(t)$.  [Rewrite the equations in part (a) and use Exercise 6 of Section 6.1.]

(f) Now suppose $S$ is an $m$-dimensional manifold in $\mathbb R^n$, where $m$ need not equal $n-1$.  Explain why one may write
$$\mathbf x_{u_iu_j}=\sum_{k=1}^m\Gamma_{ij}^k\mathbf x_{u_k}+\mathbf P$$
with $\mathbf P\perp T_pS$ everywhere, and why the $\Gamma_{ij}^k$ and $\mathbf P$ vary differentiably with $S$.  Show (as in part (a)) that the $\Gamma_{ij}^k$ can be derived directly from the $F_{ij}$ and their first order derivatives.  Then work out the case $n=3,m=1$ for curves in space.
\end{enumerate}

\subsection*{6.8. Vector Fields and the Covariant Derivative}
\addcontentsline{toc}{section}{6.8. Vector Fields and the Covariant Derivative}
Vector fields are an important concept in the study of regular surfaces.  In a more sophisticated differential geometry course, they would be covered early, but here, we have decided to put them off until they were truly important.  In fact, this section will have more than one exercise generalizing to arbitrary dimensions, because that is the best way to learn the interesting general things about vector fields.

If $S$ is a regular surface, a \textbf{vector field} on $S$ is defined to be a differentiable map $X:S\to\mathbb R^3$ such that $X(p)\in T_pS$ for all $p\in S$.  In other words, it is a smooth assignment of each point of $S$ to a tangent vector.
\begin{center}
\includegraphics[scale=.5]{VectorField.png}
\end{center}
For example, if $S$ is the $xy$-plane, then a vector field is merely a differentiable map $S\to\mathbb R^2$, where $\mathbb R^2$ is regarded as the linear-algebra plane $z=0$.  However, for other surfaces, situations become a bit more complicated because the tangent plane is changing.  For example, a vector field on the sphere $S^2$ is a map $X:S^2\to\mathbb R^3$ such that every $X(p)$ is tangent to the sphere at $p$, or what is the same thing, every $p\perp X(p)$.

If $\alpha:I\to S$ is a regular curve, it is possible to have a vector field along $\alpha$ as well.  It is a map $\vec v:I\to\mathbb R^3$ such that for each $t\in I$, $\vec v(t)\in T_{\alpha(t)}S$.  Clearly every vector field on $S$ restricts to one on $\alpha$.  It turns out that vector fields on $\alpha$ can conversely be extended to $S$, but that will not be of importance to us.  Another important observation is that a vector field along $\alpha$, though it is tangent to $S$, need not be tangent to the curve $\alpha$.

This is a fairly basic concept (except for the introductory Lie theory in the exercises), but it will be extremely important in the next section.\\

\noindent\textbf{THE COVARIANT DERIVATIVE}\\

\noindent One thing we can use a vector field to do is differentiate a function.  If $X$ is a vector field on $S$ and $f:S\to\mathbb R$ is differentiable, we have a differentiable function $X(f)$ sending $p\mapsto df_p(X(p))$; in other words, at each point it differentiates $f$ in the direction of the vector field's value at that point.  [See Exercise 4(g).]
$X$ does not have to be defined throughout $S$; if it is just defined on a curve $\alpha$, then $X(f)$ is still defined on $\alpha$.  It is worth noting that if $X=\alpha'$ along $\alpha$, then $X(f)=(f\circ\alpha)'$ by the Chain Rule.

However, what if we try to differentiate a vector field $Y$ with respect to $X$?  This is where trouble ensues, as the codomain of the vector field is the tangent plane to the surface, which varies depending on the point.  So even if $Y$'s vectors are tangent to the surface $S$, differentiating them in the direction of $X$ may yield vectors that are \emph{not} tangent to $S$.  For example, suppose $\alpha(t)=(\cos t,\sin t,0)$ is the equator on the sphere and $Y=\alpha'(t)=(-\sin t,\cos t,0)$; then the derivative of $Y$ in the direction of $t$ is $(-\cos t,-\sin t,0)$, which is not tangent to the sphere.

It really shouldn't be a surprise that regular surfaces aren't closed under differentiating vector fields.  After all, it would be hard to tell when the derivative is zero, because that suggests the vectors must always point in the same direction, preventing them from being tangent at the different points.  There is not really such a thing as a vector field being ``absolutely stationary.''  Nevertheless, there does exist a concept of differentiating one vector field with respect to another.  This is accomplished by merely taking the orthogonal projection of the derivative to the tangent plane to the surface.

Let us take the time to formalize what was meant in the previous paragraph.  Suppose $\alpha:I\to S$ is a curve, and $\vec v:I\to\mathbb R^3$ is a vector field along $I$.  Then $\frac{d\vec v}{dt}\in\mathbb R^3$; however, $\frac{d\vec v}{dt}(t)$ may not be in $T_{\alpha(t)}S$ even though $\vec v(t)$ is.  We define the \textbf{covariant derivative} of $\vec v$ with respect to $t$, denoted $\frac{D\vec v}{dt}$, to be the orthogonal projection of $\frac{d\vec v}{dt}$ onto $T_{\alpha(t)}S$.  It captures how much the vector field changes, but only in accordance with the surface.\\

\noindent To illustrate this principle we recall the equator $\alpha(t)=(\cos t,\sin t,0)$ on the unit sphere.  Then $\vec v(t)=(-\sin t,\cos t,0)$ is a tangent vector field to the sphere along the equator.  To find $\frac{D\vec v}{dt}$, we first note that $\frac{d\vec v}{dt}=(-\cos t,-\sin t,0)$, and we want to orthogonally project that vector onto $T_pS^2$ (which is the orthogonal complement of $(\cos t,\sin t,0)$).  In this case $\frac{d\vec v}{dt}$ is actually perpendicular to the tangent space, and hence $\frac{D\vec v}{dt}=\vec 0$.  Similarly, if $\vec w(t)=(0,0,1)$ throughout the curve $\alpha$, then $\frac{D\vec w}{dt}=\vec 0$ because $\frac{d\vec w}{dt}=\vec 0$.

Now let $\eta(t)=(a\cos t,a\sin t,b)$ be a latitude with $a,b>0,a^2+b^2=1$ fixed, and let $\vec v(t)=\eta'(t)=(-a\sin t,a\cos t,0)$.  What is the covariant derivative $\frac{D\vec v}{dt}$ in this case?  Well, $\frac{d\vec v}{dt}=(-a\cos t,-a\sin t,0)$, and we wish to orthogonally project this to the tangent plane to $\eta(t)$.  Note that if $\vec a,\vec u\in\mathbb R^n$ and $\|\vec u\|=1$, then the orthogonal projection of $\vec a$ onto the hyperplane to which $\vec u$ is normal is $\vec a-(\vec a\cdot\vec u)\vec u$.  Taking $\vec u=\eta(t)$ and $\vec a=\frac{d\vec v}{dt}$, we compute that $\frac{D\vec v}{dt}=(-ab^2\cos t,-ab^2\sin t,a^2b)$.  This vector goes slightly up the sphere toward the north pole.\\

\noindent To conclude this section, we shall show that on a local coordinate chart, the covariant derivative is uniquely determined by the first fundamental form.  After all, suppose $\mathbf x:U\to S$ is a local coordinate chart, $\alpha:I\to U$ is a curve and $\vec v$ is a tangent vector field on $\alpha$.  With each tangent vector written in terms of the basis $\{\mathbf x_u,\mathbf x_v\}$, one may think of $\vec v$ as a function $I\to\mathbb R^2$, where $\vec v(t)=(a(t),b(t))=a(t)\mathbf x_u+b(t)\mathbf x_v$.

The first trick is to note that for any function $f$ in $u,v$, whether vector-valued or scalar valued, we have $\frac{df}{dt}=u'(t)f_u+v'(t)f_v$ by the Chain Rule.  In particular,
$$\frac d{dt}\mathbf x_u=u'(t)\mathbf x_{uu}+v'(t)\mathbf x_{uv}$$
and by the equations (A) from the previous section,
$$u'(t)\mathbf x_{uu}+v'(t)\mathbf x_{uv}=u'\left(\Gamma_{11}^1\mathbf x_u+\Gamma_{11}^2\mathbf x_v+L\mathbf N\right)+v'\left(\Gamma_{12}^1\mathbf x_u+\Gamma_{12}^2\mathbf x_v+M\mathbf N\right)$$
$$=(u'\Gamma_{11}^1+v'\Gamma_{12}^1)\mathbf x_u+(u'\Gamma_{11}^2+v'\Gamma_{12}^2)\mathbf x_v+(u'L+v'M)\mathbf N.$$
Similarly, $\frac d{dt}\mathbf x_v=(u'\Gamma_{12}^1+v'\Gamma_{22}^1)\mathbf x_u+(u'\Gamma_{12}^2+v'\Gamma_{22}^2)\mathbf x_v+(u'M+v'N)\mathbf N$.

Finally, we may compute $\frac{d\vec v}{dt}$ as $a'(t)\mathbf x_u+a(t)\frac{d\mathbf x_u}{dt}+b'(t)\mathbf x_v+b(t)\frac{d\mathbf x_v}{dt}$.  We leave it to the reader to combine like terms to get:
\begin{align*}
\frac{d\vec v}{dt}
&=\left(a'+au'\Gamma_{11}^1+(av'+bu')\Gamma_{12}^1+bv'\Gamma_{22}^1\right)\mathbf x_u\\
&+\left(b'+au'\Gamma_{11}^2+(av'+bu')\Gamma_{12}^2+bv'\Gamma_{22}^2\right)\mathbf x_v\\
&+\left(au'L+(av'+bu')M+bv'N\right)\mathbf N
\end{align*}
Finally, the covariant derivative is obtained by orthogonally projecting to the tangent plane.  Since $\mathbf x_u,\mathbf x_v$ are in the tangent plane and $\mathbf N$ is orthogonal to it, we simply slay the $\mathbf N$ term in order to do this:
\begin{align*}
\frac{D\vec v}{dt}
&=\left(a'+au'\Gamma_{11}^1+(av'+bu')\Gamma_{12}^1+bv'\Gamma_{22}^1\right)\mathbf x_u\\
&+\left(b'+au'\Gamma_{11}^2+(av'+bu')\Gamma_{12}^2+bv'\Gamma_{22}^2\right)\mathbf x_v
\end{align*}
This formula uses only $\alpha$ and $\vec v$ (as defined on the coordinate chart) and the first fundamental form, in the form of the Christoffel symbols.  Observe that the second fundamental form coefficients were in the coefficient of $\mathbf N$, hence the derivative \emph{would} have depended on them if we hadn't gotten rid of the term.  This is indeed the main idea of the second fundamental form: it measures how much differentiating one tangent vector field on $S$ with respect to another pushes vectors aside into the ambient space, and varies from the covariant derivative.

This may seem like a basic concept, but since the next section will be more intricate, it is important to grasp the concepts for a good understanding.

\subsection*{Exercises 6.8. (Vector Fields and the Covariant Derivative)}
% POTENTIAL EXERCISE: Lie groups (I said in 6.2 I would have one)
\begin{enumerate}
\item Let $\mathbf L$ be the set of all one-dimensional subspaces of the vector space $\mathbb R^3$.  A \textbf{direction field} on $S$ is defined to be a smooth map $D:S\to\mathbf L$ such that for every $p\in S$, $D(p)$ is tangent to $p$ at $S$.  For example, every \emph{nonvanishing} vector field $X$ entails a direction field, obtained by assigning each $D(p)$ to be the span of $X(p)$.

(a) Show by example that a direction field need not come from a nonvanishing vector field.  [In $\mathbb R^2-\{0\}$, draw three rays coming out of the (deleted) origin, which come at $120$-degree angles.  Then in each of the three resulting regions, draw a family of curves asymptotically approaching both bounding rays.  This should lead you to a direction field which cannot have a nonvanishing vector field without entailing a contradiction.]

(b) If two nonvanishing vector fields have the same direction field, show that they have the same flow curves, though parametrized differently in general.

\item If $\vec v$ and $\vec w$ are vector fields along a curve $\alpha:I\to S$, show that $\frac d{dt}(\vec v\cdot\vec w)=\frac{D\vec v}{dt}\cdot\vec w+\vec v\cdot\frac{D\vec w}{dt}$.  [First explain why $\frac{D\vec v}{dt}\cdot\vec w=\frac{d\vec v}{dt}\cdot\vec w$.]

\item (a) Let $X$ be a vector field on $S$ and $p\in S$.  Show that there exists, for some $\varepsilon$, a differentiable map $\alpha:(-\varepsilon,\varepsilon)\to S$ such that $\alpha(0)=p$ and $\alpha'(t)=X(\alpha(t))$ for all $t$, and that this map is unique when given the choice of $\varepsilon$.  [Take a local coordinate chart.  Then this works out to a system of differential equations.] $\alpha$ is called the \textbf{flow} of $X$ through $p$.

(b) If $p\in S$ and $X(p)\ne\vec 0$, show that there exists a local coordinate chart $U\to S$ at $p$, on which $X$ is the image of $\vec e_1\in U$.  [You may assume the flow of $X$ through $p$ varies smoothly with $p$.]

\item\emph{(Derivations and the Lie bracket.)} \---- Let $S\subset\mathbb R^n$ be an $m$-dimensional manifold. % Should be in the second half of exercises, since it is already generalizing dimensions

For fixed $p\in S$, let $\mathcal E(p)$ be the set of all pairs $(U,f)$ such that $p\in U\subset S$ is an open neighborhood and $f:U\to\mathbb R$ is a differentiable function.  [These are local differentiable functions at $p$.]  A \textbf{derivation at $p$} is defined to be a function $D:\mathcal E(p)\to\mathbb R$ such that for $a,b\in\mathbb R,(U,f),(V,g)\in\mathcal E(p)$, we have

~~~~(i) \emph{Linearity}: $D(af+bg)=aD(f)+bD(g)$

~~~~(ii) \emph{Leibniz's Law}: $D(fg)=f(p)D(g)+g(p)D(f)$

when all functions are restricted to $U\cap V$.

% (owing to the fact that $D(f)=D(f|_W)$, as can be seen by taking $a=1,b=0,g=0|_W$ in the linearity condition)
Observe that $\mathcal E(p)$ is not a vector space because of the varying domains of the functions.  However, one can define $(U,f)\sim(V,g)$ to mean that there exists an open set $p\in W\subset U\cap V$ such that $f|_W=g|_W$; this is an equivalence relation on $\mathcal E(p)$, and a derivation $D$ at $p$ necessarily maps equivalent elements to the same number, because taking $a=1,b=0,g=0|_W$ in the linearity condition gives $D(f)=D(f|_W)$. % No, equivalent elements of \mathcal E(p) literally map to the same number -- not like there's some kind of equivalence relation on \mathbb R !
Moreover, if $\mathcal G(p)=\mathcal E(p)/\sim$, the derivation $D$ induces a map $\mathcal G(p)\to\mathbb R$, which \emph{is} a linear map between vector spaces.  We will not delve into details as they are not important to us.  [Elements of $\mathcal G(p)$ are called \textbf{germs} of smooth functions at $p$.]

(a) If $\vec v\in T_pS$, show that $f\mapsto df_p(\vec v)$ is a derivation at $p$.

(b) Derivations at $p$ send constant functions to zero.  [Take $f,g$ to be the constant function $1$ in Leibniz's Law.]

By using a local coordinate chart, one may assume $n=m$, $S$ is an open set of $\mathbb R^m$, and $p=0$.  This shall be done in parts (c) and (d).

(c) If $f\in\mathcal E(p)$ and $df_p=0$, every derivation at $p$ sends $f\mapsto 0$.  [By part (b) and linearity, $D(f)=D(f-f(p))$, so one may assume $f(p)=0$.  With that, use L'H\^opital's Rule to write each function $f(u_1,\dots,u_{k-1},u_k,0,\dots,0)-f(u_1,\dots,u_{k-1},0,0,\dots,0)$ as a product of two functions which send $p$ to zero.  We thus get a summation $f=\sum_{i=1}^mg_ih_i$ where $g_i,h_i\in\mathcal E(p)$ and $g_i(p)=h_i(p)=0$.  Now use Leibniz's Law.]

(d) Show that the map from $\vec v$ to $f\mapsto df_p(\vec v)$ in part (a) is a linear isomorphism from the tangent vectors at $p$ to the derivations at $p$.  [Linearity is clear, as is injectivity, because for $\vec v\ne\vec 0$, the map $f:\vec x\mapsto\vec x\cdot\vec v$ satisfies $df_p(\vec v)\ne 0$.  As for surjectivity, use part (c) to show that if $D$ is a derivation at $p$, and $a_i=D(u_i)$ for each $1\leqslant i\leqslant m$, $D$ must coincide with the image of the vector $(a_1,\dots,a_m)$.]

Let $\mathcal C^\infty(S)$ be the linear space of all differentiable functions $S\to\mathbb R$.  A \textbf{global derivation} is defined to be a linear map $D:\mathcal C^\infty(S)\to\mathcal C^\infty(S)$ such that for all $f,g\in C^\infty(S)$, Leibniz's Law holds: $D(fg)=fD(g)+gD(f)$.

(e) Assume the following nontrivial fact: if $p\in U$ with $U$ open, there exists a differentiable function $f:S\to\mathbb R$ such that $f(p)=1$ and $f|_{S-U}=0$.  If $D$ is a global derivation, $f\in C^\infty(S)$, and $U$ is an open set such that $f|_U=0$, show that $D(f)|_U=0$.  [To show that $D(f)(p)=0$ for $p\in U$, use Liebniz's Law with $f$ and a function given by the first statement.]

(f) If $X$ is a vector field on $S$, define $X(f)(p)=df_p(X(p))$ for $p\in S$.  Show that $X(f)$ is a differentiable function, and that $f\mapsto X(f)$ is a global derivation.

(g) Show that a map $D$ is a global derivation if and only if $D$ is differentiable and, at every $p\in S$, $D$ restricts to a derivation at $p$.  [From part (e) it follows that $D$ can be well-restricted to open sets of $S$.]  Conclude, using part (d), that $X\mapsto[f\mapsto X(f)]$ is a linear isomorphism between vector fields and global derivations.

(h) If $V$ and $W$ are global derivations, show that $V\circ W-W\circ V$ is a global derivation.  This is denoted $[V,W]$ and called the \textbf{Lie bracket} of $V$ and $W$.

Give an example to show that $V\circ W$ may not be a global derivation.

Since global derivations correspond bijectively to vector fields by part (g), we may pass the operation over, to get what is known as the \textbf{Lie bracket of vector fields}: if $X$ and $Y$ are vector fields, then $[X,Y]$ is the vector field such that $[X,Y](f)=X(Y(f))-Y(X(f))$.

(i) Show that $[X,Y]$ is bilinear (over $\mathbb R$); $[X,X]=0$; $[X,Y]=-[Y,X]$; and the \textbf{Jacobi identity} holds: $[X,[Y,Z]]+[Y,[Z,X]]+[Z,[X,Y]]=0$.

(j) If $f\in\mathcal C^\infty(S)$, show that $[X,fY]=f[X,Y]+X(f)Y$ and $[fX,Y]=f[X,Y]-Y(f)X$.

(k) Now suppose we are in local coordinates ($S$ is an open subset of $\mathbb R^m)$, and $X=(g_1,\dots,g_m),Y=(h_1,\dots,h_m)$.  Find a formula for $[X,Y]$.  [By symmetry of mixed partial derivatives, $[u_i,u_j]=0$ for $1\leqslant i,j\leqslant m$.  Now use bilinearity and part (j) to expand $[X,Y]$ into already-known terms.]

\item\emph{(Lie groups.)} \---- A Lie group $G$ is a regular manifold which is equipped with the structure of a group, such that the group operations are smooth, by which we mean that the maps $\mu(g,h)=gh$ and $\iota(g)=g^{-1}$ are differentiable. % The group structure doesn't mean the smoothness of the maps!  I added a few extra words.
Examples include $\mathbb R^n$ under addition; the nonzero complex numbers under multiplication; $GL_n(\mathbb R)$ [as an open set of $\mathbb R^{n^2}$]; $SL_n(\mathbb R)$; $GL_n(\mathbb C)$; $SL_n(\mathbb C)$; $O(n)$; $SO(n)$; $U(n)$; and $SU(n)$.

For each $g\in G$, we let $L_g$ be the translation $h\mapsto gh$, which is smooth by definition.

(a) A vector field $X$ on $G$ is said to be \textbf{left invariant} provided that for all $g,h\in G$, $d(L_g)_h(X(h))=X(gh)$.  Show that this can be reduced to saying that $d(L_g)_1(X(1))=X(g)$, and that the map $X\mapsto X(1)$ is a linear isomorphism from left invariant vector fields to tangent vectors at $1$.

(b) Show that a vector field is left invariant if and only if the corresponding global derivation $X$ satisfies $X(f\circ L_g)=X(f)\circ L_g$ for all $f\in\mathcal C^\infty(G),g\in G$.  Conclude that left invariant vector fields are closed under the Lie bracket.

A \textbf{Lie algebra} (over $\mathbb R$ or $\mathbb C$) is defined to be a vector space $V$ equipped with a bracket operator $[-,-]:V\times V\to V$ such that: (i) the operator is bilinear; (ii) the operator is alternating: $[v,v]=0$ for all $v\in V$; (iii) the Jacobi identity holds: $[u,[v,w]]+[v,[w,u]]+[w,[u,v]]=0$.  They are usually denoted via lowercase fraktur symbols like $\mathfrak g$; we will eventually see why this is so.

A basic example is $\mathbb R^3$ under the cross product (the Jacobi identity for such is Exercise 4(g) of Section 2.5).  Also, $M_n(\mathbb R)$ \---- the set of \emph{all} $n\times n$ matrices with real entries \---- is a Lie algebra under the operation $[A,B]=AB-BA$.  By the previous exercise, the space of vector fields on a manifold is a Lie algebra.

(c) If $\mathfrak g$ is a Lie algebra, show that $[v,w]=-[w,v]$ for all $v,w\in\mathfrak g$.  [Expand $[v+w,v+w]$ via bilinearity.]

(d) If $G$ is a Lie group, then the left invariant vector fields form a Lie algebra (because by part (b), they are closed under the bracket).  Yet since this space is canonically isomorphic to $T_1G$ by part (a), we conclude that $T_1G$ is a Lie algebra whose vector space dimension equals the (manifold) dimension of $G$.  $T_1G$ is denoted $\operatorname{Lie}(G)$ or $\mathfrak g$ and is called the \textbf{associated Lie algebra of the Lie group $G$}.

If $G$ and $H$ are Lie groups, a \textbf{Lie group homomorphism} is defined to be a differentiable map $\varphi:G\to H$ which is also a group homomorphism.  Since $\varphi(1)=1$ by Proposition 1.10(i), $d\varphi_1$ (henceforth to be denoted $\varphi_*$) is a linear map of the associated Lie algebras, $\operatorname{Lie}(G)\to\operatorname{Lie}(H)$.

If $X$ is a vector field on $G$ and $Y$ is a vector field on $H$, $X$ and $Y$ are said to \textbf{relate via $\varphi$} provided that for all $p\in G$, $d\varphi_p(X(p))=Y(\varphi(p))$.  Since $\varphi$ need not be injective or surjective, it does not make sense to pass vector fields either way in general, but this is a useful notion.

(e) Show that $X$ and $Y$ relate via $\varphi$ if and only if $Y(f)\circ\varphi=X(f\circ\varphi)$ for $f\in\mathcal C^\infty(H)$.  Use this to show that if $X,X'$ are vector fields on $G$, $Y,Y'$ are vector fields on $H$, $X$ and $Y$ relate via $\varphi$, and $X'$ and $Y'$ relate via $\varphi$, then $[X,Y]$ and $[X',Y']$ relate via $\varphi$.

(f) If $\varphi$ is a Lie group homomorphism and $X$ and $Y$ are left invariant, then $X$ and $Y$ relate via $\varphi$ if and only if $\varphi_*(X)=Y$ [with $X,Y$ viewed as tangent vectors to the identities].  Conclude, using part (e), that $\varphi_*$ is a Lie algebra homomorphism; i.e., $\varphi_*([X,X'])=[\varphi_*(X),\varphi_*(X')]$.

(g) Show that $d\iota_1=-I_{\mathfrak g}$, where $\iota:G\to G$ is the inversion map.  [First find $d\mu_1$ by using curves at the identity, then differentiate the statement $\mu(g,\iota(g))=1$.]  Use this to show that if $G$ is an abelian Lie group, $\mathfrak g$'s Lie bracket is identically zero.  [$\iota$ is a Lie group homomorphism in this case.]

(h) For each $X\in\mathfrak g$, there is a unique Lie group homomorphism $\alpha:\mathbb R\to G$ such that $\alpha'(0)=X$.  [The differential equation $\alpha'(t)=d(L_{\alpha(t)})_1(X)$ has a solution for some interval in $\mathbb R$; now use the Lie group homomorphism property to extend it to all of $\mathbb R$.]  We set $\exp(X)=\alpha(1)$, thus defining a map $\exp:\mathfrak g\to G$ called the \textbf{exponential}.

(i) If $\varphi:G\to H$ is a Lie group homomorphism, this diagram commutes:
\begin{diagram}
\mathfrak g & \rTo{\varphi_*} & \mathfrak h \\
\dTo{\exp} & & \dTo{\exp} \\
G & \rTo{\varphi} & H
\end{diagram}
(j) If $G=GL_n(\mathbb R)$, show that $\mathfrak g=M_n(\mathbb R)$ and for $A\in\mathfrak g$, $\exp(A)=I+A+\frac{A^2}{2!}+\frac{A^3}{3!}+\dots$.  [The trick is to note that $\exp(tA)=\alpha(t)$ where $\alpha$ is the homomorphism from part (h) with $\alpha'(0)=A$.  Conclude that $\frac{d}{dt}\exp(tA)=A\exp(tA)$.]  This operation is known as the \textbf{matrix exponential}.  Moreover show that if $\exp(A^T)=\exp(A)^T$; if $AB=BA$ then $\exp(A+B)=\exp(A)\exp(B)$; $\exp(0)=I$ and $\exp(-A)=\exp(A)^{-1}$.

(k) Use part (i) to show that in the Lie algebra $\mathfrak g$ of $GL_n(\mathbb R)$, the bracket is given by $[A,B]=AB-BA$.

(l) If $G=SL_n(\mathbb R)$, then $\mathfrak g$ consists of matrices in $M_n(\mathbb R)$ with trace zero, and the bracket is again given by $[A,B]=AB-BA$.  If $G=O(n)$, then $\mathfrak g$ consists of skew-symmetric matrices ($A^T=-A$) and the bracket is given by $[A,B]=AB-BA$.  [In each case, for $X\in\mathfrak g$, find an equation for the curve $\alpha(t)=\exp(tX)$ to be contained in $G$, and differentiate it at zero; this will give the condition on $X$.  As for the bracket, use part (f).]  Figure out the Lie algebras for the other matrix groups as well.

\item\emph{(The covariant derivative in arbitrary dimensions.)} \---- This generalizes the final result of this section to arbitrary dimensions.  Let $S$ be a manifold in $\mathbb R^n$, let $\mathbf x:U\to S$ be a local coordinate chart, and let $\alpha:I\to U$ be a curve and $\vec v$ a vector field along $\alpha$.  The \textbf{covariant derivative} of $\vec v$ is obtained by differentiating $\vec v$ with respect to $t$, then orthogonally projecting to the tangent space of the manifold.

(a) If $S$ is a hypersurface, use Exercise 9 of the previous section to derive the Christoffel symbols from the First Fundamental Form.  Then show that the covariant derivative is uniquely determined by them, and work out the expression.

(b) Now show that this may be done without the assumption that $S$ is a hypersurface, and (using Exercise 9(f) of the previous section to get $\Gamma_{ij}^k$), the covariant derivative is given by the same expression as in part (a).  [It will help to take the dot product of the covariant derivative with the $\mathbf x_{u_i}$'s, noting that the dot product is the same if the derivative is used instead of the covariant derivative.]
\end{enumerate}

\subsection*{6.9. Parallel Transport and Geodesics}
\addcontentsline{toc}{section}{6.9. Parallel Transport and Geodesics}
Now that we have covered vector fields, we are ready to do several final things before going to custom surfaces: defining what it means for a curve to be a ``straight line,'' a theorem regarding the sum of the angles of a triangle, and the exponential map (used to get circles).

Let $S$ be a regular surface, and $\alpha:I\to S$ be a regular curve, and let $\vec v$ be a vector field on $S$ along $\alpha$ (it need not be tangent to $\alpha$).  We recall that, from the previous section, one cannot talk about the derivative of $\vec v$ as a vector field on $S$, but one can talk about the \emph{covariant} derivative of $\vec v$; i.e., the orthogonal projection of $\frac{d\vec v}{dt}$ to the tangent planes to the surface.  The first natural thing to ask is when the covariant derivative is zero, i.e., $\frac{d\vec v}{dt}$ is normal to the surface.  Vector fields with that property do their best to stay stationary while also staying tangent to the surface, so their only motions relieve ``normal stick-out.''  Such vector fields are thus given a special name.\\

\noindent\textbf{Definition.} \emph{If $\vec v$ is a vector field along a curve $\alpha$, $\vec v$ is said to be \textbf{parallel} provided that $\frac{D\vec v}{dt}=0$ throughout $\alpha$.}\\

\noindent Several examples are in order.  First, if $S=\mathbb R^2$ (viewed as the $xy$-plane), then covariant derivatives coincide with the ordinary derivatives, and hence a parallel vector field along $\alpha$ is just a vector field along $\alpha$ which is a constant vector in $\mathbb R^2$.  It is hard to construct other genuine examples of parallel vector fields, thus we will start by proving a few results.

First, we formulate parallel vector fields in local coordinates.  Let $\mathbf x:U\to S$ be a local coordinate chart, and assume the curve can be given by $\alpha(t)=\mathbf x(u(t),v(t))$.  Then a vector field $\vec v$ can be given by $\vec v(t)=(a(t),b(t))=a(t)\mathbf x_u+b(t)\mathbf x_v$.  As shown in the previous section,
\begin{align*}
\frac{D\vec v}{dt}
&=\left(a'+au'\Gamma_{11}^1+(av'+bu')\Gamma_{12}^1+bv'\Gamma_{22}^1\right)\mathbf x_u\\
&+\left(b'+au'\Gamma_{11}^2+(av'+bu')\Gamma_{12}^2+bv'\Gamma_{22}^2\right)\mathbf x_v,
\end{align*}
and hence $\vec v$ is parallel if and only if both coefficients above are zero.  Thus, a parallel vector field is characterized by the equations:
\begin{equation}\tag{PT}\begin{array}{c l}a'+au'\Gamma_{11}^1+(av'+bu')\Gamma_{12}^1+bv'\Gamma_{22}^1=0\\b'+au'\Gamma_{11}^2+(av'+bu')\Gamma_{12}^2+bv'\Gamma_{22}^2=0\end{array}\end{equation}
Now note that if the curve $\alpha$ is already prescribed, then the $\Gamma_{ij}^k$ are already known functions in $t$, as are $u$ and $v$.  Moreover, easy examination shows that we have a \emph{linear} system of differential equations in the unknowns $a,b$.  Thus by Theorem 6.2 we conclude\\

\noindent\textbf{Proposition 6.16 and Definition.} \emph{If $\alpha:I\to S$ is a regular curve, $t_0\in I$ and $\vec v_0\in T_{\alpha(t_0)}S$, there is a unique parallel $\vec v$ throughout $\alpha$ such that $\vec v(t_0)=\vec v_0$.  $\vec v$ is called \textbf{parallel transport} from $\vec v_0$.}\\

\noindent Also note that since the $\Gamma_{ij}^k$ are derivable from the coefficients $E,F,G$, any local coordinate chart of another regular surface with the same first fundamental form coefficients has the same differential equations (PT) for parallel transport.  In accordance with Proposition 6.6, this tells us\\ % What do you mean, "will call these"?  The (PT) label was already used.

\noindent\textbf{Proposition 6.17.} \emph{Parallel transport along vector fields is preserved under (local) isometries.}\\

\noindent In other words, parallel transport is part of intrinsic geometry.

From the homogeneous linear system, it is clear that parallel vector fields form a vector subspace of vector fields (Exercise 2).  One important thing about parallel vector fields is that they move ``rigidly'' along $\alpha$ in the following sense:\\

\noindent\textbf{Proposition 6.18.} \emph{If $\vec v$ and $\vec w$ are parallel vector fields along $\alpha$, then $\vec v\cdot\vec w$ is constant.  In particular, $\|\vec v\|$ and the angle between $\vec v$ and $\vec w$ are constant.}
\begin{proof}
By Exercise 2 of Section 6.8,
$$\frac{d}{dt}(\vec v\cdot\vec w)=\overset 0{\overbrace{\frac{D\vec v}{dt}}}\cdot\vec w+\vec v\cdot\overset 0{\overbrace{\frac{D\vec w}{dt}}}=0,$$
hence $\vec v\cdot\vec w$ is constant.  The second statement follows from $\|\vec v\|=\sqrt{\vec v\cdot\vec v}$ and the fact that the angle between $\vec v$ and $\vec w$ is $\cos^{-1}\frac{\vec v\cdot\vec w}{\|\vec v\|\|\vec w\|}$.
\end{proof}

\noindent Now we shall present a specific example of parallel transport other than the trivial one on the plane.

Suppose $\alpha$ is a latitude of the sphere $S^2$.  Then if $\alpha$ is the equator, the unit tangent vectors along $\alpha$ are a parallel vector field.  However, if $\alpha$ is a latitude other than the equator, the unit tangent vectors have derivative pointing to the center of the latitude circle; thus the derivative is not normal to the sphere, and the vector field is not parallel.

Nevertheless, if we take a unit tangent vector at one point, we can extend it to a parallel vector field $\vec v$ by Proposition 6.17.  To work out what $\vec v$ is, we first construct the cone tangent to the sphere along the latitude; it is obtained by taking the tangent line to the generating semicircle arc and revolving it around the $z$-axis:
\begin{center}
\includegraphics[scale=.3]{ConeOverSphere.png}
\end{center}
By Exercise 1, $\vec v$ is also a parallel vector field along the cone.  We recall that the cone is locally isometric to the plane (Exercise 2 of Section 6.4); specifically, lines on the cone from the tip correspond to rays in the plane from the origin.  Such an isometry takes the latitude to an origin-centered circle in the plane.  By Proposition 6.17, the isometry takes $\vec v$ to a parallel vector field in the plane; but such a vector field is a constant vector in $\mathbb R^2$, and hence it looks like:
\begin{center}
\includegraphics[scale=.3]{LatitudeParallelTransport.png}
\end{center}
Note that this vector immediately stops being tangent to the latitude as it turns down toward the equator.  It does so at a steady rate, and lands back at its starting point, but pointing in a different direction from its original (why?).\\ % The PT vector rotates down (away from the cone tip), I'll change the word "curves"

\noindent\textbf{GEODESICS}\\

\noindent Perhaps the natural thing to ask is when the velocity of a curve $\alpha$ is itself a parallel vector field along $\alpha$.  Intuitively, this occurs when forward travel along $\alpha$ does its best to keep a stationary direction; i.e., when $\alpha$ travels closest to a straight line.  Thus, we have a notion of ``straight lines'' which will be important to us:\\

\noindent\textbf{Definition.} \emph{A curve $\alpha:I\to S$ is called a \textbf{geodesic} provided that its velocity $\vec v=\frac{d\alpha}{dt}$ is a parallel vector field along $\alpha$.}\\

\noindent For instance, we recall that in $S=\mathbb R^2$, a parallel vector field along $\alpha$ is precisely a vector field which is a constant vector in $\mathbb R^2$.  Thus $\alpha$ is a geodesic if and only if $\alpha'$ is a constant vector, say $\vec v$; with that, $\alpha(t)=\vec u+t\vec v$ for some $\vec u$, and $\alpha$ is a straight line in the ordinary sense of Euclidean geometry.  Thus the geodesics in $\mathbb R^2$ are precisely the straight lines parametrized at a steady rate.

We have also shown before that the equator of a sphere, when parametrized by arc length, is a geodesic (because the unit tangent vectors are parallel).  Any great circle, parametrized by arc length, is a geodesic.  Yet, other latitudes are not geodesics.

It is worth noting that $\alpha$ being a geodesic does depend on the parametrization and not just the curve as a subset of $S$.  In $\mathbb R^2$, for example, $t\mapsto(t,t,0)$ is a geodesic, but $t\mapsto(\sin t+2t,\sin t+2t,0)$ is not, even though the image of the parametrizations is the same line.

Just like parallel vector fields, we may formulate geodesics in local coordinates.  But this time we have a property of just the curve, not a property of a tangent vector field along it.

Let $\mathbf x:U\to S$ be a local coordinate chart, and suppose $\alpha(t)=\mathbf x(u(t),v(t))$.  We recall that a vector field $\vec v$ along $\alpha$ can be given by $\vec v(t)=a(t)\mathbf x_u+b(t)\mathbf x_v$.  Note that $\frac{d\alpha}{dt}=u'(t)\mathbf x_u+v'(t)\mathbf x_v$ by the Chain Rule; hence in the special case $\vec v=\frac{d\alpha}{dt}$ we have $a=u'$ and $b=v'$.  $\alpha$ is a geodesic if and only if this $\vec v$ is a parallel vector field, which is expressed by taking $a=u',b=v'$ in the parallel equations (PT) to get:
\begin{equation}\tag{G}\begin{array}{c l}u''+(u')^2\Gamma_{11}^1+2u'v'\Gamma_{12}^1+(v')^2\Gamma_{22}^1=0\\v''+(u')^2\Gamma_{11}^2+2u'v'\Gamma_{12}^2+(v')^2\Gamma_{22}^2=0\end{array}\end{equation}
Observe that this is a \emph{second} order system of differential equations in the unknowns $u$ and $v$.  One can readily apply Theorem 6.2 by defining two new variables $\mathfrak u,\mathfrak v$ to be $u',v'$ respectively, obtaining a (first-order) system in the \emph{four} unknowns.\footnote{Specifically, $u'=\mathfrak u$, $\mathfrak u'+\mathfrak u^2\Gamma_{11}^1+2\mathfrak u\mathfrak v\Gamma_{12}^1+\mathfrak v^2\Gamma_{22}^1=0$, etc.}  However, it is not linear, because the derivatives of the unknowns are being multiplied together, and because the $\Gamma_{ij}^k$ are functions in the position on the chart (not of $t$), even they depend on $u$ and $v$.  Thus solutions may only exist locally. % "Is this the first time something in Ch. 6 has depended on a 2nd-order system ODE?"  Maybe the proof of Bonnet's Theorem involved them, but the existence and uniqueness of solutions to the equations still followed from (6.2) and (6.14).

We have the next two results exactly as we did for parallel vector fields:\\

\noindent\textbf{Proposition 6.19.} \emph{If $p\in S$ and $\vec v\in T_pS$, there exists $\varepsilon>0$ and a geodesic $\alpha:(-\varepsilon,\varepsilon)\to S$ such that $\alpha(0)=p$ and $\alpha'(0)=\vec v$.  Any two such geodesics coincide on the intersection of their domains.}\\

\noindent\textbf{Proposition 6.20.} \emph{Geodesics are preserved under (local) isometries.}\\

\noindent Geodesics also have constant speed; in fact, we leave it to the reader to verify that a reparametrization of a geodesic is itself a geodesic \emph{if and only if} it has constant speed.  Thus geodesics can be assumed to be parametrized by arc length, whenever convenient.\\

\noindent\textbf{Proposition 6.21.} \emph{If $\alpha$ is a geodesic, then the speed $\|\frac{d\alpha}{dt}\|$ is constant.}
\begin{proof}
Take $\vec v=\frac{d\alpha}{dt}$ in Proposition 6.18.
\end{proof}

\noindent A plane curve has a curvature, which is zero if and only if the curve is a line.  The same is true for curves on a surface:\\

\noindent\textbf{Proposition 6.22 and Definition.} \emph{Let $\alpha:J\to S$ be a regular curve, $\vec v$ a unit vector field along $\alpha$.  Then $\frac{D\vec v}{dt}=\lambda(t)(\mathbf N\times\vec v(t))$ for some $\lambda:J\to\mathbb R$.  Furthermore, $\lambda=\frac{d\vec v}{dt}\cdot(\mathbf N\times\vec v)$.  This $\lambda$ is called the \textbf{algebraic value of the covariant derivative} of $\vec v$, and is denoted $\left[\frac{D\vec v}{dt}\right]$.}

\emph{If $\alpha$ is parametrized by arc length, the algebraic value of the covariant derivative of $\frac{d\alpha}{ds}$ is called the \textbf{geodesic curvature} of $\alpha$ and is denoted $\kappa_g(s)$.}
\begin{proof}
Clearly $\vec v(t)$ and $\mathbf N$ are linearly independent.  Hence to show that $\frac{D\vec v}{dt}$ is a scalar multiple of $\mathbf N\times\vec v(t)$, it suffices to show that it is perpendicular to each of the vectors $\mathbf N,\vec v(t)$.  It is manifestly perpendicular to $\mathbf N$ because it is tangent to the surface.  As for $\vec v(t)$, note that $\vec v(t)$ is a \emph{unit} vector field.  Differentiating $\vec v\cdot\vec v=1$ and applying Exercise 2 of the previous section entails $2\frac{D\vec v}{dt}\cdot\vec v=0$.

This proves that $\frac{D\vec v}{dt}$ is orthogonal to the plane spanned by $\vec v,\mathbf N$, hence is of the form $\lambda(\mathbf N\times\vec v)$.  As for the second statement, note that $\frac{d\vec v}{dt}\cdot(\mathbf N\times\vec v)=\frac{D\vec v}{dt}\cdot(\mathbf N\times\vec v)$ because the right-hand operand is tangent to the surface, hence adding or subtracting a normal vector to the left-hand operand does not change the dot product.  Finally,
$$\frac{D\vec v}{dt}\cdot(\mathbf N\times\vec v)=\lambda(\mathbf N\times\vec v)\cdot(\mathbf N\times\vec v)=\lambda,$$
because $\mathbf N,\vec v$ are unit vectors which are orthogonal to one another, and hence their cross product is a unit vector.
\end{proof}

\noindent Several observations follow.  First, a unit vector field is parallel if and only if the algebraic value of its covariant derivative is zero.  In particular, $\alpha$ is a geodesic if and only if its geodesic curvature is zero.  Secondly, the geodesic curvature of $\alpha$ changes sign when you change the orientation of either $\alpha$ or the surface $S$.

To give an example of geodesic curvature, consider the latitude $z=\cos\beta$ of the sphere $S^2$, with $0<\beta<\pi$ fixed.  A suitable arc-length parametrization of such would be
$$\alpha(s)=\left(\sin\beta\cos\left(\frac s{\sin\beta}\right),\sin\beta\sin\left(\frac s{\sin\beta}\right),\cos\beta\right).$$
By Proposition 6.22, one can compute $\kappa_g(s)=\frac{d^2\alpha}{ds^2}\cdot\left(\mathbf N\times\frac{d\alpha}{ds}\right)$.  On the sphere, $\mathbf N$ is equal to $\alpha(s)$ itself, and:
$$\alpha'(s)=\left(-\sin\left(\frac s{\sin\beta}\right),\cos\left(\frac s{\sin\beta}\right),0\right)$$
$$\alpha''(s)=\left(-\frac 1{\sin\beta}\cos\left(\frac s{\sin\beta}\right),-\frac 1{\sin\beta}\sin\left(\frac s{\sin\beta}\right),0\right)$$
so that $\mathbf N\times\alpha'(s)=\left(-\cos\beta\cos\left(\frac s{\sin\beta}\right),-\cos\beta\sin\left(\frac s{\sin\beta}\right),\sin\beta \right )$, and $\kappa_g(s)=\alpha''(s)\cdot(\mathbf N\times\alpha'(s))=\frac{\cos\beta}{\sin\beta}=\cot\beta$.  Hence, $\alpha$ is a geodesic $\iff$ this is zero $\iff\beta=\pi/2\iff\alpha$ is the equator.

We recall that geodesic curvature has already been mentioned in Exercise 4(d) of Section 6.2, where it was derived in a different way.  We can easily show that these notions of geodesic curvature are identical.  For, suppose $\alpha:J\to S$ is a curve paramterized by arc length, and let $\boldsymbol\tau,\boldsymbol\nu,\boldsymbol\beta$ be its Frenet trihedron.  Then we recall that $\alpha'(s)=\boldsymbol\tau(s)$, and $\alpha''(s)=\boldsymbol\tau'(s)=\kappa(s)\boldsymbol\nu(s)$.  We may then let $\mathbf n=\mathbf N\times\alpha'(s)$, so that $\mathbf n$ is tangent to $S$, normal to the curve, and $\{\mathbf N,\alpha'(s),\mathbf n\}$ is a positive orthonormal basis of $\mathbb R^3$.  The idea is to write $\alpha''(s)$ with respect to this basis; since the basis is orthonormal we have
$$\alpha''(s)=\left[\alpha''(s)\cdot\mathbf N\right]\,\mathbf N+\left[\alpha''(s)\cdot\alpha'(s)\right]\,\alpha'(s)+\left[\alpha''(s)\cdot\mathbf n\right]\,\mathbf n.$$ % I didn't really like periods on math expressions
The coordinate of $\mathbf N$ is $\alpha''(s)\cdot\mathbf N=\kappa\boldsymbol\nu\cdot\mathbf N=\kappa_n$, the normal curvature (see either Exercise 4 of Section 6.2, or Exercise 7 of Section 6.5).  The coordinate of $\alpha'(s)$ is $\alpha''(s)\cdot\alpha'(s)$, which is zero, because $\alpha'(s)$ is a unit vector (Exercise 4(b) of Section 6.1).  Finally, the coordinate of $\mathbf n$ is $\alpha''(s)\cdot\mathbf n=\alpha''\cdot(\mathbf N\times\alpha')=\kappa_g$, the geodesic curvature (in the sense we have here).

This entails the exact formula from the exercise:
$$\alpha''(s)=\kappa_g\mathbf n+\kappa_n\mathbf N.$$
Intuitively, the way $\alpha$ curves in the ambient space $\mathbb R^3$ has a certain behavior from the surface's viewpoint, given by the geodesic curvature; and another behavior going normally away from the surface, given by the normal curvature.  Note that since $\mathbf n,\mathbf N$ are orthonormal vectors, taking the magnitude of both sides yields the \emph{Pythagorean identity for curvatures}:
$$\kappa(s)=\sqrt{\kappa_g^2+\kappa_n^2}.$$
This is a useful basic relation between the three types of curvatures of the curve on the surface.\\

\noindent We now turn our attention a surface of revolution:
$$\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u)),~~~~f>0,~~~~(f')^2+(g')^2=1$$
as we find a clever characterization of its geodesics.  We recall that:
$$E=1,~~~~F=0,~~~~G=f(u)^2$$
$$L=f'(u)g''(u)-f''(u)g'(u),~~~~M=0,~~~~N=f(u)g'(u)$$
$$\Gamma_{11}^1=0,~~~~\Gamma_{11}^2=0,~~~~\Gamma_{12}^1=0,~~~~\Gamma_{12}^2=\frac{f'(u)}{f(u)},~~~~\Gamma_{22}^1=-f'(u)f(u),~~~~\Gamma_{22}^2=0$$
If $\alpha:I\to S$ is a geodesic on the surface of revolution, we may write $\alpha(t)=\mathbf x(u(t),v(t))$ and substitute the above in the equations (G) to get:
$$u''-(v')^2f'(u)f(u)=0$$
$$v''+2u'v'\frac{f'(u)}{f(u)}=0$$
The first natural question is, which latitudes and meridians are necessarily geodesics?  First, \emph{all} meridians are geodesics: if $\alpha(t)=\mathbf x(t,v_0)$, so that $u=t$ and $v=v_0$, then the equations are easily seen to hold, since $u''=v'=v''=0$.  As for parallels of latitude $\alpha(t)=\mathbf x(u_0,t)$, we have $u''=u'=v''=0$ and $v'=1$; hence the equations boil down to $f'(u_0)f(u_0)=0$.  Since $f>0$, this is in turn equivalent to $f'(u_0)=0$, which occurs when the distance of the generating curve to the $z$-axis is at a critical point, and the surface is tangent to the $z$-axis centered cylinder along this latitude.  Hence such latitudes are the ony ones which are geodesics:
\begin{center}
\includegraphics[scale=.4]{ParallelGeodesics.png}
\end{center}
Now, what if $\alpha$ is neither a latitude nor a meridian?  Certainly many such geodesics exist, and this question is harder.  Given a geodesic which is not a parallel latitude, we can at least guarantee that it is closest to the $z$-axis when it is parallel to the latitude; there is this result from Alexis Claude de Clairaut:\\

\noindent\textbf{Theorem 6.23.} \textsc{(Clairaut's relation)} \emph{Let $\alpha$ be a geodesic on a surface of revolution $S$, which is not a parallel of latitude.  If, at any point on $\alpha$, $R$ is the radius of the latitude (the distance from the $z$-axis), and $\theta$ is the angle the geodesic makes with the latitude, then $R\cos\theta$ is constant along $\alpha$.  In particular, this constant is the closest distance $\alpha$ can have to the $z$-axis, and it attains this distance when and only when the geodesic is tangent to the latitude.}\\

\noindent Note that for meridians, the first statement is trivial, since $\theta=\pi/2$ and $\cos\theta=0$ throughout.  However, $\alpha$ would never be tangent to the latitude, unless the radius degenerates to zero.
\begin{proof}
We assume that $\mathbf x(u,v)=(f(u)\cos v,f(u)\sin v,g(u))$ with the convenient properties $f>0$ and $(f')^2+(g')^2=1$, and that $\alpha(t)=\mathbf x(u(t),v(t))$.  We compute $R$ and $\theta$ as follows:

1. $R(t)=f(u)$, because $f$ is the distance from the $z$-axis at any point on the surface.

2. As for $\theta(t)$, we note that the latitudes are just the coordinate curves where $u$ is held constant; hence their tangent vectors are the $\mathbf x_v$.  We compute the angle between that vector and $\alpha'(t)=u'(t)\mathbf x_u+v'(t)\mathbf x_v$ as follows:
$$\cos\theta(t)=\frac{\alpha'(t)\cdot\mathbf x_v}{\|\alpha'(t)\|\|\mathbf x_v\|}=\frac{(u'\mathbf x_u+v'\mathbf x_v)\cdot\mathbf x_v}{\|u'\mathbf x_u+v'\mathbf x_v\|\|\mathbf x_v\|}=\frac{u'F+v'G}{\sqrt{(u')^2E+2u'v'F+(v')^2G}\sqrt G}$$
$$=\frac{v'f(u)^2}{\sqrt{(u')^2+(v')^2f(u)^2}\sqrt{f(u)^2}}=\frac{v'f(u)}{\sqrt{(u')^2+(v')^2f(u)^2}}.$$

Multiplying the previous two results gives us $R\cos\theta=\frac{v'f(u)^2}{\sqrt{(u')^2+(v')^2f(u)^2}}$, which we wish to show is constant.  Our ambition is to show that its derivative with respect to $t$ is zero.  This will take a sophisticated amount of tabular differentiation, along with the equations that make $\alpha$ a geodesic, which we restate here for convenience:
$$u''-(v')^2f'(u)f(u)=0$$
$$v''+2u'v'\frac{f'(u)}{f(u)}=0$$
We arrange our work as follows:

1. Let $A=v'f(u)^2$ and $B=\sqrt{(u')^2+(v')^2f(u)^2}$.  Then $R\cos\theta=\frac AB$.  We claim that, in fact, $A'=B'=0$; from this it will follow that $A$ and $B$ are both constant, hence certainly $R\cos\theta$ is.

2. By the Product Rule, $\frac{d}{dt}(v'f(u)^2)=\frac{dv'}{dt}f(u)^2+v'\left(\frac{d}{dt}f(u)^2\right)=f(u)^2v''+v'\cdot 2f(u)\left(\frac{d}{dt}f(u)\right)$.

Here we need to be careful, however: $\frac{d}{dt}f(u)$ is \emph{not} equal to $f'(u)$: that would be the derivative of $f(u)$ with respect to $u$ instead of $t$.  To find the derivative with respect to $t$, we apply the Chain Rule to get $\frac{d}{dt}f(u)=\frac{d}{du}f(u)\cdot\frac{du}{dt}=f'(u)u'$.

Therefore, $A'=\frac{d}{dt}(v'f(u)^2)=f(u)^2v''+2f(u)f'(u)u'v'$.

3. The geodesic equations then entail $A'=0$, because
$$A'=f(u)^2v''+2f(u)f'(u)u'v'=f(u)^2\left(v''+2u'v'\frac{f'(u)}{f(u)}\right)=0.$$

4. To differentiate $B$, we first compute $\frac d{dt}(u')^2=2u'\left(\frac d{dt}u'\right)=2u'u''$.  Likewise, $\frac d{dt}(v')^2=2v'v''$, and $\frac d{dt}f(u)^2=2f(u)\left(\frac d{dt}f(u)\right)=2f(u)f'(u)u'$.  Therefore we have
$$\frac{d}{dt}\left((v')^2f(u)^2\right)=\left(\frac d{dt}(v')^2\right)f(u)^2+(v')^2\left(\frac d{dt}f(u)^2\right)$$
$$=2f(u)^2v'v''+2f(u)f'(u)u'(v')^2$$
$$\therefore\frac d{dt}(B^2)=\frac d{dt}\left((u')^2+(v')^2f(u)^2\right)=2u'u''+2f(u)^2v'v''+2f(u)f'(u)u'(v')^2.$$

5. Since clearly $\frac d{dt}(B^2)=2BB'$, we have $B'=\frac{\frac d{dt}(B^2)}{2B}$, which by the previous step is
$$B'=\frac{u'u''+f(u)^2v'v''+f(u)f'(u)u'(v')^2}{B}=\frac{u'u''+f(u)^2v'v''+f(u)f'(u)u'(v')^2}{\sqrt{(u')^2+(v')^2f(u)^2}}.$$

6. We claim that the numerator of the above expression is actually zero, making $B'=0$.  To see why, first observe that $u''=(v')^2f'(u)f(u)$ by the geodesic equations, so that
$$u'u''+f(u)^2v'v''+f(u)f'(u)u'(v')^2=u'(v')^2f'(u)f(u)+f(u)^2v'v''+f(u)f'(u)u'(v')^2$$
$$=f(u)^2v'v''+2f(u)f'(u)u'(v')^2.$$
Furthermore, we have $f(u)^2v'v''+2f(u)f'(u)u'(v')^2=f(u)^2v'\left(v''+2u'v'\frac{f'(u)}{f(u)}\right)=0$, proving our claim as desired.
\end{proof}

\noindent\textbf{THE GAUSS-BONNET THEOREM}\\

\noindent The remainder of this section aims to focus on how Gaussian curvature relates to the total turning angle of a simple loop.  This angle is $2\pi$ in the Euclidean plane, but on various surfaces, it is usually off by an amount caused by the consistent curving of the surface region inside the loop.  This curving is accounted for by the Gaussian curvature.  We wish to make this into a formal statement.

We start by letting $\alpha:[a,b]\to S$ be a closed, piecewise smooth curve.  This means that:
\begin{itemize}
\item $\alpha(a)=\alpha(b)$;

\item There is a finite sequence $a=a_0<a_1<\dots<a_m=b$ such that each $\alpha|_{[a_j,a_{j+1}]}$ is a smooth map $[a_j,a_{j+1}]\to S$.  However, there may be ``corners'' at the $a_j$'s.
\end{itemize}
This is the most convenient kind of loop one can deal with.  We will further want it to be \emph{simple}, i.e., it does not cross itself and it has a well-defined interior.  Examples of such loops are triangles and smooth curves.
\begin{center}
\includegraphics[scale=.4]{ArbTriangle.png}
\end{center}
$\alpha$ must also be positively oriented: this means that if you curl the fingers on your right hand in the direction that $\alpha$ goes in, and stick out your right thumb, your thumb points in the direction of $\mathbf N$ (the \emph{right-hand rule}).  The reader can readily see how the quantities we compute will be negative for negative orientation.

Finally, one may assume $\alpha$ is arc-length parametrized (Proposition 6.1 can be adapted).  Given such a curve, one can diagnose how it turns with just some of the notions we have previously gone over:\\

\noindent\textbf{Theorem 6.24.} \textsc{(Gauss-Bonnet theorem)} \emph{Suppose $\alpha:[a,b]\to S$ is a simple, closed, positively oriented, piecewise smooth curve parametrized by arc length, and $R$ is its interior.  Suppose further that $K$ is the Gaussian curvature, $\kappa_g$ is the geodesic curvature of $\alpha$, and the corners of $\alpha$ make exterior angles of $\theta_1,\theta_2,\dots,\theta_m$.  Then}
$$\int_R K\,d\sigma+\int_a^b\kappa_g(s)\,ds+\sum_{i=1}^m\theta_i=2\pi.$$
In effect, the theorem states that a full turn can always be achieved if you account for all the important attributes of the curve on the surface: (1) the geodesic curvature, which tells how much turning happens on the smooth parts (similar to Exercise 9(c) of Section 6.1); (2) the exterior angles of the corners, which can be thought of as ``discontinuous'' turns;\footnote{Those who have studied applied math may think of it as scalar multiples of Dirac delta functions added to the geodesic curvature.} and (3) the integral of the Gaussian curvature, which indicates how far the twisting of the surface prevents (1) and (2) from working alone.\\

\noindent Before proving the Gauss-Bonnet Theorem, we need a few lemmas, which we will not prove.  For proofs of Lemmas 6.25 and 6.26, see Manfredo Perdigao do~Carmo [12].  Lemma 6.27 can be found in many multivariable calculus texts.\\

\noindent\textbf{Lemma 6.25.} \emph{Every regular surface has an orthogonal parametrization, i.e., one where $F\equiv 0$ throughout.}\\ % Do Carmo page 183

\noindent\textbf{Lemma 6.26.} \textsc{(Theorem of Turning Tangents.)} \emph{If $\alpha:[a,b]\to\mathbb R^2-\{0\}$ is a simple closed positively-oriented curve around the origin, which is injective on $[a,b)$, its winding number is $1$.  In other words, if $\alpha(t)=(r(t)\cos\theta(t),r(t)\sin\theta(t))$ with $r,\theta$ smooth functions and $r>0$, then $\theta(b)-\theta(a)=2\pi$.}\\

\noindent\textbf{Lemma 6.27.} \textsc{(Green's Theorem)} \emph{If $\alpha(t)=(u(t),v(t)):[a,b]\to\mathbb R^2$ is a simple closed positively-oriented curve with interior $R$, and $f$ and $g$ are smooth real-valued functions on an open set containing both $R$ and the curve, then}
$$\int_a^b (fu'+gv')\,dt=\int\!\!\int_R\left(\frac{\partial g}{\partial u}-\frac{\partial f}{\partial v}\right)\,dxdy.$$

\noindent\textbf{Lemma 6.28.} \emph{If a compact region on a surface has an open covering, it can be split into finitely many pieces, each of which is contained in one of the sets of the covering.}
\begin{proof}
This follows from the general fact that any open covering of a compact metric space has a Lebesgue number.  For instance, given the closed cell $[0,1]^n\subset\mathbb R^n$, if $(U_\alpha)_{\alpha\in A}$ is an open covering, there exists $\delta>0$ such that every subset of $[0,1]^n$ with diameter $<\delta$ is contained in one of the $U_\alpha$'s.  Now let $N$ be an integer large enough so that $\frac 1N<\frac{\delta}{\sqrt n}$; then the cell can be gridded into $N^n$ smaller cells (of side length $\frac 1N$), and each will be contained in one of the $U_\alpha$.

The argument can readily be adapted to work for any compact regions.
\end{proof}

\noindent We are finally in a position where we can prove the Gauss-Bonnet Theorem.

\begin{proof}
\emph{(of Theorem 6.24)} \---- First, $S$'s local coordinate charts form an open covering of the curve $\alpha$ and the interior $R$.  In view of Lemma 6.28, the region can be split into finitely many pieces so that each one is contained in a single chart.  It is easy to show that if the Gauss-Bonnet theorem holds for each piece, then it holds for the original $\alpha$.

Hence, we may assume $\alpha$ is contained in a single local coordinate chart $\mathbf x:U\to S$, and we may think of $\alpha(t)=\mathbf x(u(t),v(t))$.  In view of Lemma 6.25, we may also assume $F\equiv 0$ throughout the chart.

By Exercise 6, we may let $\varphi:[a,b]\to\mathbb R$ be a piecewise smooth function such that $\cos\varphi(s)=\frac{\alpha'(s)\cdot\mathbf x_u}{\|\alpha'(s)\|\|\mathbf x_u\|}$ and
$$\kappa_g(s)=\frac 1{2\sqrt{EG}}\left(G_u\frac{dv}{ds}-E_v\frac{du}{ds}\right)+\frac{d\varphi}{ds}.$$
Note that by the Theorem of Turning Tangents, $\varphi(b)-\varphi(a)=2\pi$.  Integrating this with respect to the arc length parameter $s$ gives
\begin{equation}\tag{*}\int_a^b\kappa_g(s)\,ds=\int_a^b\frac 1{2\sqrt{EG}}\left(G_u\frac{dv}{ds}-E_v\frac{du}{ds}\right)\,ds+\int_a^b\frac{d\varphi}{ds}\,ds.\end{equation}
Green's Theorem (6.27) may be used to rewrite the left-hand summand as an integral over $R$:
$$\int_a^b\frac 1{2\sqrt{EG}}\left(G_u\frac{dv}{ds}-E_v\frac{du}{ds}\right)\,ds=\int_R\frac{\partial}{\partial u}\left(\frac 1{2\sqrt{EG}}G_u\right)+\frac{\partial}{\partial v}\left(\frac 1{2\sqrt{EG}}E_v\right)\,dudv$$
$$=\int_R\frac 12\left[\left(\frac{G_u}{\sqrt{EG}}\right)_u+\left(\frac{E_v}{\sqrt{EG}}\right)_v\right]\,dudv.$$
Since $d\sigma$ (the surface metric) is $\sqrt{EG-F^2}\,dudv=\sqrt{EG}\,dudv$, this becomes
$$=\int_R\frac 1{2\sqrt{EG}}\left[\left(\frac{G_u}{\sqrt{EG}}\right)_u+\left(\frac{E_v}{\sqrt{EG}}\right)_v\right]\,d\sigma=-\int_RK\,d\sigma;$$
the integrand is precisely $-K$ by Exercise 3 of Section 6.7.

As for $\int_a^b\frac{d\varphi}{ds}\,ds$, it is tempting to think it is $\varphi(b)-\varphi(a)=2\pi$ by the Second Fundamental Theorem of Calculus.  However, this is not the case, because $\varphi$ is not necessarily continuous.  The Fundamental Theorem of Calculus may only be applied to regions where the function is wholly continuous, so we suppose $a=a_0<a_1<\dots<a_m=b$ is a sequence with each $\alpha|_{[a_j,a_{j+1}]}$ smooth.  Then $\varphi|_{[a_j,a_{j+1}]}$ is smooth as well, and one may compute
$$\int_a^b\frac{d\varphi}{ds}\,ds=\sum_{j=0}^{m-1}\int_{a_j}^{a_{j+1}}\frac{d\varphi}{ds}\,ds=\sum_{j=0}^{m-1}\left(\lim_{t\to{a_{j+1}}^-}\varphi(t)-\lim_{t\to{a_j}^+}\varphi(t)\right).$$
The summand has been written like this because the $\varphi(a_j)$ are not necessarily defined, but you can take the limit as elements of the interval $[a_j,a_{j+1}]$ approach the endpoints.  Note that for $1\leqslant i<m$, $\theta_i=\lim_{t\to a_i^+}\varphi(t)-\lim_{t\to a_i^-}\varphi(t)$; being the exterior angle, it indicates how much the angle changed instantaneously at the corner.  Similarly, $\theta_m=\lim_{t\to a^+}\varphi(t)-\lim_{t\to b^-}\varphi(t)+2\pi$ (the $2\pi$ term arises from the fact that $\varphi(b)-\varphi(a)=2\pi$).  Thus rearranging the summands above gets us
$$\sum_{j=0}^{m-1}\left(\lim_{t\to a_{j+1}^-}\varphi(t)-\lim_{t\to a_j^+}\varphi(t)\right)=2\pi-\sum_{i=1}^m\theta_i.$$
We substitute our results into (*) to get
$$\int_a^b\kappa_g(s)\,ds=-\int_RK\,d\sigma+2\pi-\sum_{i=1}^m\theta_i,$$
which is manifestly equivalent to the theorem statement.
\end{proof}

\noindent\textbf{Corollary 6.29.} \emph{Let $T$ be a triangle made up of three segments of geodesics.  If $T$ has angles $\alpha,\beta,\gamma$, then $\alpha+\beta+\gamma-\pi=\int_RK\,d\sigma$ where $R$ is the interior of $T$.}
\begin{proof}
Apply Theorem 6.24, noting that $\kappa_g=0$ and the $\theta_i$ are equal to $\pi-\alpha,\pi-\beta,\pi-\gamma$.
\end{proof}

\noindent\textbf{Corollary 6.30.} \emph{Let $\alpha$ be a closed geodesic (a loop with no corners at all), and $R$ its interior.  Then $\int_R K\,d\sigma=2\pi$.  In particular, $K$ is strictly positive somewhere on $S$, and if $K$ is strictly positive throughout $S$ then any two closed geodesics intersect.}\\

\noindent This corollary applies to the sphere, where geodesics are great circles.
\begin{proof}
Apply Theorem 6.24, noting that $\kappa_g=0$ and there are no corners.  The last statement holds because, if two closed geodesics do not intersect, the Gaussian curvature must integrate to zero on the strip between them.
\end{proof}

\subsection*{Exercises 6.9. (Parallel Transport and Geodesics)} % Motivate and cover parallel transport and geodesics.
% Show that parallel transports have constant dot product, length and angle, and find one on sphere latitude.
% Cover the geodesic curvature concept as well; also Clairaut's relation for surfaces of revolution.
% AMBITION: Gauss-Bonnet Theorem here, since geodesics and geodesic curvature are finally covered ( http://www.math.uchicago.edu/~may/VIGRE/VIGRE2010/REUPapers/Rotskoff.pdf )
% POTENTIAL EXERCISES: Geodesic is line of curvature <=> plane curve; exponential map; torus geodesics exercise using Clairaut's relation; angle of holonomy
\begin{enumerate}
\item Let $S_1$ and $S_2$ be two regular surfaces, tangent to one another along a curve $\alpha$.  Show that a vector field along $\alpha$ is parallel on $S_1$ if and only if it is transport on $S_2$.

\item Show that parallel vector fields along $\alpha$ form a ($2$-dimensional) vector subspace of all vector fields along $\alpha$.

\item Show that all the geodesics on the sphere are the great circles.  [Use Proposition 6.19.]

\item Let $S$ be a regular surface, $\alpha:I\to S$ a geodesic.

(a) If $\alpha$ is a line of curvature, then it is a plane curve.  [Use Proposition 6.18 to assume $\alpha$ is parametrized by arc length.  Then assume (Exercise 6(a) of Section 6.5) that $\mathbf N'(s)=\lambda(s)\alpha'(s)$.  Since $\alpha$ is a geodesic, however, $\alpha'(s)$ is parallel, and so $\alpha''(s)$ (in $\mathbb R^3$) is normal to the surface.  Since $\alpha''(s)=\boldsymbol\tau'(s)=\kappa(s)\boldsymbol\nu(s)$, conclude that $\boldsymbol\nu=\pm\mathbf N$.  Hence $\boldsymbol\nu'(s)=\pm\lambda(s)\boldsymbol\tau(s)$, which means $\kappa(s)=\pm\lambda(s)$ and $\tau(s)=0$; now use Exercise 8 of Section 6.1.]

(b) Conversely, if $\alpha$ is a plane curve and its curvature in the plane is never zero, then $\alpha$ is a line of curvature.

(c) Let $S$ be a connected regular surface on which all geodesics are plane curves.  Show that $S$ is contained in either a plane or a sphere.  [By part (b), all geodesics are lines of curvature.  Use Proposition 6.19 to show that every direction from every point is a principal direction; i.e., every point is umbilical.  Then conclude using Proposition 6.11.]

\item Verify the Pythagorean identity for curvatures for a latitude on a sphere.

\item (a) If $\alpha:I\to S$ is a piecewise smooth, regular path on a surface, show that there exists a piecewise smooth map $\varphi:I\to\mathbb R$ such that for each $t\in I$, $\cos\varphi(t)=\frac{\alpha'(t)\cdot\mathbf x_u}{\|\alpha'(t)\|\|\mathbf x_u\|}$; in other words, a piecewise smooth function defined on $I$ which equals the angle between the path and $u$'s coordinate curves.

(b) Now suppose $\mathbf x:U\to S$ is a local coordinate chart for which $F\equiv 0$ throughout, and $\alpha:J\to U\to S$ is an arc length parametrization of a piecewise smooth regular path, say $\alpha(s)=(u(s),v(s))$.  Show that the geodesic curvature along $\alpha$ is given by
$$\kappa_g(s)=\frac 1{2\sqrt{EG}}\left(G_u\frac{dv}{ds}-E_v\frac{du}{ds}\right)+\frac{d\varphi}{ds}.$$
[The geodesic curvature can be computed as $\alpha''(s)\cdot(\mathbf N\times\alpha'(s))$.  Use the results from Sections 6.6 and 6.7 to write this as a linear combination of $\mathbf x_u\cdot(\mathbf N\times\mathbf x_v)=-\sqrt{EG}$ and $\mathbf x_v\cdot(\mathbf N\times\mathbf x_u)=\sqrt{EG}$.]

\item What is the generalization of Corollary 6.29 to arbitrary polygons?

\item\emph{(Exponential map for geodesics.)} \---- If $p\in S$, and $U$ is an open neighborhood of $\vec 0(=p)$ in $T_pS$, we say that $\exp_p:U\to S$ is an \textbf{exponential map} at $p$, provided that $\exp_p(\vec 0)=p$, and for each $\vec v\in U$, if $\alpha:I\to S$ is the geodesic with $\alpha(0)=p,\alpha'(0)=\vec v$, then $\exp_p(\vec v)=\alpha(1)$.

(a) Show that an exponential map exists for a sufficiently small neighborhood $U$.  Moreover, for each $\vec v\in T_pS$, the map $\beta:t\mapsto\exp_p(t\vec v)$ (for $t$ in some neighborhood $(-\varepsilon,\varepsilon)$ of the origin) is the geodesic with $\beta(0)=p,\beta'(0)=\vec v$.

(b) Assume that in Theorem 6.2, the solutions vary smoothly with respect to the equations and initial conditions.  Show that $\exp_p$ is a smooth map, and that $d(\exp_p)_{\vec 0}$ is the identity on $T_pS$.

From part (b) it follows that for sufficiently small $U$, $\exp_p$ is also a local coordinate chart.  Its coordinates, called \textbf{normal coordinates}, satisfy the property that any Euclidean line through the origin is a geodesic and its speed coincides with the Euclidean speed.

(c) For sufficiently small $r>0$, one can take the curve $t\mapsto\exp_p(r\cos t,r\sin t)$, which is a circle in the Euclidean coordinates.  This is called the \textbf{geodesic circle} of radius $r$ centered at $p$.  Show that its radii (i.e., the images of the Euclidean radii of $(r\cos t,r\sin t)$, which are geodesics) are orthogonal to the circle on $S$.  This was first stated by Gauss.  [Define $\varphi(r,\theta)=\exp_p(r\cos\theta,r\sin\theta)$.  By normality of the coordinates, $\frac{\partial^2\varphi}{\partial r^2}=0$.  Furthermore, $\|\frac{\partial\varphi}{\partial r}\|^2=1$ throughout; differentiating with respect to $\theta$ entails $\frac{\partial\varphi}{\partial r}\cdot\frac{\partial^2\varphi}{\partial r\partial\theta}=0$.  Use this to show that $\frac{\partial\varphi}{\partial r}\cdot\frac{\partial\varphi}{\partial\theta}$ is independent of $r$.  To see why that dot product is zero, take the limit as $r\to 0$ and note the conformality of the chart at the origin.]

If $S$ is a Lie group $G$ such that the translations $x\mapsto gx$ and $x\mapsto xg$ ($g\in G$) are isometries, it can be shown that Lie group homomorphisms $\mathbb R\to G$ are precisely geodesics sending $0\mapsto 1\in G$, and that this exponential map at $1$ coincides with the map of Exercise 5(h) of the previous section.  In this case, $\exp_1$ is defined on the \emph{whole} tangent plane, unlike the general case of regular surfaces (or, Riemannian manifolds).  However, this is harder to prove.

\item Fix $a>b>0$ in $\mathbb R$, and let $S$ be the torus:
$$\mathbf x(u,v)=((a+b\cos u)\cos v,(a+b\cos u)\sin v,b\sin u)$$
(a) Let $\alpha_1(t)=\mathbf x(\pi/2,t)$.  This is the circular arc along which the plane $z=b$ is tangent to the torus.  Find the geodesic curvature of $\alpha_1$ and the parallel transport along $\alpha_1$ of the vector $\alpha_1'(0)=(0,a,0)$ at $\alpha_1(0)=(a,0,b)$.

(b) Now let $\alpha$ be the geodesic tangent to $\alpha_1$ at the point $\alpha(0)=(a,0,b)$.  Show that $\alpha$ is only present on the outer half of the torus with $|u|\leqslant\pi/2$.  [Use Theorem 6.23.]

(c) Let $\alpha_2(t)=\mathbf x(0,t)$ be the outer ring; this is a geodesic.  If $\beta$ is the geodesic which passes through $\alpha_2$ at the point $(a+b,0,0)$, at an angle of $\theta$, show that $\beta$ passes through the inner ring $t\mapsto\mathbf x(\pi,t)$ if and only if $\cos\theta<\frac{a-b}{a+b}$.

\item Suppose $\alpha:[a,b]\to S$ is a closed, piecewise smooth curve, so that $\alpha(a)=\alpha(b)=p$.  If $\vec v\in T_pS$, the parallel transport along $\alpha$ which starts at $\vec v$ may end at a different vector from $\vec v$ at the same point $p$.

(a) Give an example of where this happens.  [On the sphere, consider either a latitude which is not the equator, or a triangle with three right angles (which has side length $\pi/2$ by Proposition 5.9).]

(b) Show that the \emph{angle} that the ending vector makes with the starting vector $\vec v$ is independent of the point on the geodesic and the starting vector at it.  [Proposition 6.18.]  This angle is called the \textbf{angle of holonomy} of the closed curve.

\item\emph{(Euler characteristic.)} \---- Using the Gauss-Bonnet theorem, it is easy to study the Euler characteristic of compact surfaces.

(a) Recall that the surface area of $S^2$ is $4\pi$.  Suppose $S^2$ is tiled with regions whose boundaries are curve segments.  The segments may not be geodesics, but graph-theoretically, they make the surface into $F$ faces, $E$ edges and $V$ vertices.  Show that $F-E+V=2$.  [Parametrize the rim of each face by arc length, positively oriented.  Explain why at each edge, the two faces meeting it have their parametrizations traverse the edge in opposite directions.  Apply the Gauss-Bonnet Theorem to each face, and sum over all the faces.  This gets you $2F\pi$ on the right-hand side, since there are $F$ faces.  The $\int_RK\,d\sigma$ terms sum to $\int_{S^2}K\,d\sigma=\int_{S^2}1\,d\sigma=4\pi$, the surface area of $S^2$.  Since reversing the direction of a path negates the geodesic curvature, the terms which integrate geodesic curvature all cancel each other out (why?).  Finally, each exterior angle can be written as $\pi$ minus the interior angle, and the interior angles of all the faces add to $2V\pi$.  The rest should be easy algebra.]

(b) Show that for \emph{any} regular surface $S$ globally diffeomorphic to $S^2$, the integral over $S$ of the Gaussian curvature $K$ is $4\pi$.  [If you tile $S$ into $F$ faces, $E$ edges and $V$ vertices, how do you know that $F-E+V=2$ like in part (a)?]  Note the special case where $S$ is a sphere of radius $r$: in such a case, $K=\frac 1{r^2}$ (Exercise 3 of Section 6.6) and the surface area is $4\pi r^2$.

(c) Let $S$ be a compact orientable surface, and tile the surface into $F$ faces, $E$ edges and $V$ vertices.  Show that $F-E+V=\frac 1{2\pi}\int_SK\,d\sigma$, and hence does not depend on the particular tiling.  This integer is called the \textbf{Euler characteristic} of $S$ and is denoted $\chi(S)$.

(d) The Euler characteristic of $S^2$ is $2$, and the Euler characteristic of the torus is $0$.

Any compact orientable surface other than the sphere is, in fact, obtained by taking a certain number of tori and pasting them in chain by cutting circular holes out of them and gluing them together around the holes.  If there are $n$ tori involved, the surface is said to have \textbf{genus $n$}, and its Euler characteristic is $2-2n$.

\item Generalize the concepts of parallel transport and geodesics to arbitrary dimensional manifolds, and prove analogues of Propositions 6.16-6.21.
\end{enumerate}

\subsection*{6.10. Customized Surfaces}
\addcontentsline{toc}{section}{6.10. Customized Surfaces}
Now that we have studied regular surfaces, fundamental form coefficients, curvatures, parallel vector fields, geodesics, and how to find length and area, we are ready to bring back the geometries from Chapters 2, 4 and 5, with the ability to study more about them!  They follow the general notion of a \textbf{customized surface}, which is an open set of $\mathbb R^2$ equipped with a formal collection of first fundamental form coefficients, as we will eventually see.

The mainof idea is to recall all the \emph{intrinsic geometry}: the things that are determined by the first fundamental form coefficients, or what is the same thing, preserved by local isometries.  For example, Gaussian curvature (Theorem 6.13) and geodesics (Proposition 6.20) are part of intrinsic geometry, but the second fundamental form and principal directions are not intrinsic geometry.

In order to be able to deal with intrinsic geometry, we only need a metric given by a first fundamental form.  We no longer imagine that the surface is embedded in $\mathbb R^3$ or in any $\mathbb R^n$.  Thus, we define\\

\noindent\textbf{Definition.} \emph{A \textbf{customized surface} is a pair $(U,g)$ where $U\subset\mathbb R^2$ is an open set, and $g=(E,F,G)$ where $E,F,G:U\to\mathbb R$ are differentiable functions such that $E>0$ and $EG-F^2>0$ throughout $U$.}

\emph{If $p\in U$ and $\vec v=(a,b),\vec w=(c,d)\in\mathbb R^2$ (viewed as vectors at $p$), we define $\left<\vec v,\vec w\right>=E(p)ac+F(p)(ad+bc)+G(p)bd$.  We set $\|\vec v\|_g=\sqrt{\left<\vec v,\vec v\right>}$ and declare the angle between $\vec v$ and $\vec w$ to be $\cos^{-1}\frac{\left<\vec v,\vec w\right>}{\|\vec v\|_g\|\vec w\|_g}$.}\\

\noindent Of course, the conditions $E>0$ and $EG-F^2>0$ must be explicitly imposed to ensure positive definiteness of the inner product.  Also, we are using the notation $\left<-,-\right>$ for inner products to distinguish them from the ordinary Euclidean dot product.  The idea is that the choice of $(E,F,G)$ gives the surface its own notion of length and angle.

Before digging into the examples, we note that (as in Section 6.3), it is customary to write the metric of a customized surface like this:
$$ds^2=E\,du^2+2F\,du\,dv+G\,dv^2.$$

\noindent\textbf{Examples.}

(1) If $U=\mathbb R^2$, and $E=G=1$ and $F=0$ throughout $U$, we have the Euclidean plane with the usual metric $\left<\vec v,\vec w\right>=\vec v\cdot\vec w$.\\

(2) If $U$ is an open set and $\lambda$ is a differentiable function from $U$ to the positive reals $\mathbb R_{>0}$, then $U$ becomes a customized surface with $E=G=\lambda$ and $F=0$.  These are called \textbf{isothermal} or \textbf{conformal coordinates}; they are precisely the kinds of coordinates for which angles have the same measures as the Euclidean angles.  Most of the customized surfaces we will deal with fall under this category.\\

(3) Let $U=\mathbb R^2$, equipped with:
$$E=\frac{v^2+1}{(u^2+v^2+1)^2},~~~~F=-\frac{uv}{(u^2+v^2+1)^2},~~~~G=\frac{u^2+1}{(u^2+v^2+1)^2}$$
Then $EG-F^2=\frac{1}{(u^2+v^2+1)^3}$; hence $E>0$ and $EG-F^2>0$, so we have a customized surface.  We will later see that this is isometric to half the sphere, under the gnomonic projection of Section 5.6.\\

\noindent We now find equations that characterize isometries and conformal mappings etween custized sface.

If $(U,g)$ and $(\overline U,\overline g)$ are customized surfaces with $g=(E,F,G)$ and $\overline g=(\overline E,\overline F,\overline G)$, then a differentiable map $\varphi:U\to\overline U$ is said to be an \textbf{isometric mapping} if for $p\in U$ and $\vec v,\vec w\in T_pU$, we have $\left<d\varphi_p(\vec v),d\varphi_p(\vec w)\right>_{\overline g}=\left<\vec v,\vec w\right>_g$; in other words, $\varphi$ preserves the inner products given by the surface structures.  Since both sides of the equation % "Does \varphi have to be injective?" For an isometric mapping, I don't think so.  However, isometry <=> isometric mapping & diffeomorphism.
$$\left<d\varphi_p(\vec v),d\varphi_p(\vec w)\right>_{\overline g}=\left<\vec v,\vec w\right>_g$$
are symmetric and bilinear, it suffices for the equation to hold for $\vec v,\vec w\in\{\vec e_1,\vec e_2\}$; then it will automatically hold in the general case:
$$\left<d\varphi_p(\vec e_1),d\varphi_p(\vec e_1)\right>_{\overline g}=\left<\vec e_1,\vec e_1\right>_g=E$$
$$\left<d\varphi_p(\vec e_1),d\varphi_p(\vec e_2)\right>_{\overline g}=\left<\vec e_1,\vec e_2\right>_g=F$$
$$\left<d\varphi_p(\vec e_2),d\varphi_p(\vec e_2)\right>_{\overline g}=\left<\vec e_2,\vec e_2\right>_g=G.$$
Let $\begin{bmatrix}a&b\\c&d\end{bmatrix}$ be the matrix of $d\varphi_p$ with respect to the basis $\vec e_1,\vec e_2$; then $d\varphi_p(\vec e_1)=a\vec e_1+c\vec e_2$ and $d\varphi_p(\vec e_2)=b\vec e_1+d\vec e_2$, from which we get
$$\left<d\varphi_p(\vec e_1),d\varphi_p(\vec e_1)\right>_{\overline g}=\overline Ea^2+2\overline Fac+\overline Gc^2$$
$$\left<d\varphi_p(\vec e_1),d\varphi_p(\vec e_2)\right>_{\overline g}=\overline Eab+\overline F(ad+bc)+\overline Gcd$$
$$\left<d\varphi_p(\vec e_2),d\varphi_p(\vec e_2)\right>_{\overline g}=\overline Eb^2+2\overline Fbd+\overline Gd^2.$$
For $\varphi$ to be an isometry, these expressions need to equal $E$, $F$ and $G$ respectively.  Note that this is tantamount to saying
$$\begin{bmatrix}a&c\\b&d\end{bmatrix}\begin{bmatrix}\overline E&\overline F\\\overline F&\overline G\end{bmatrix}\begin{bmatrix}a&b\\c&d\end{bmatrix}=\begin{bmatrix}E&F\\F&G\end{bmatrix},$$
because the left-hand side works out to be a symmetric matrix with the above three expressions in $\overline E,\overline F,\overline G,a,b,c,d$.  If $g$ and $\overline g$ are regarded as the symmetric matrices involving the given coefficients, this says $(d\varphi_p)^T\overline g(d\varphi_p)=g$.  The symmetric matrices involving the first fundamental form coefficients are important anyway, hence are called the \textbf{first fundamental form matrices}.  We have proved:\\

\noindent\textbf{Proposition 6.31.} \emph{Let $(U,g)$ and $(\overline U,\overline g)$ be customized surfaces.  A differentiable map $\varphi:U\to\overline U$ is isometric if and only if, for each $p\in U$, $(d\varphi_p)^T\overline g(d\varphi_p)=g$, where $g=\begin{bmatrix}E&F\\F&G\end{bmatrix}$ and $\overline g=\begin{bmatrix}\overline E&\overline F\\\overline F&\overline G\end{bmatrix}$, the first fundamental form matrices.}\\

\noindent As usual, an isometry is defined to be an isometric diffeomorphism.

Several remarks are in order.  First, suppose $(\overline U,\overline g)$ is a customized surface, and $U$ is an open set of $\mathbb R^2$ (not given a metric).  Assume further that $\varphi:U\to\overline U$ is a differentiable map such that each $d\varphi_p,p\in U$ is a linear isomorphism.  Then $U$ admits a unique metric $g$ such that $\varphi$ is an isometry.  After all, any such metric $g$ must be given by $(d\varphi_p)^T\overline g(d\varphi_p)$ by Proposition 6.31; to show that this is indeed a metric it suffices to show that $g$ is a positive definite symmetric matrix.  Symmetry is clear, and since $d\varphi_p$ is an isomorphism we have
$$\vec v\ne\vec 0\implies d\varphi_p(\vec v)\ne\vec 0\implies\vec v\cdot g\vec v=\vec v\cdot(d\varphi_p)^T\overline g(d\varphi_p)(\vec v)=d\varphi_p(\vec v)\cdot\overline g(d\varphi_p(\vec v))>0$$
(by positive definiteness of $\overline g$), hence $g$ is positive definite.

$g$ is called the \textbf{pullback} of the metric $\overline g$ under the map $\varphi$.  It can be thought of as answering the question ``If we think of $\overline U$ with the different coordinate system that $U$ has, what is our metric?''  If $\varphi$ is a conformal mapping (in the Euclidean sense), then Exercise 4 of Section 4.1 shows that $(d\varphi_p)^T(d\varphi_p)$ is a scalar multiple of the identity, hence the pullback of a conformal metric is a conformal metric.

Conformal mappings are similar to isometries; such maps are characterized by there being a fixed function $\lambda:U\to\mathbb R$ such that $\left<d\varphi_p(\vec v),d\varphi_p(\vec w)\right>_{\overline g}=\lambda(p)^2\left<\vec v,\vec w\right>_g$, which entails:\\

\noindent\textbf{Proposition 6.32.} \emph{Let $(U,g)$ and $(\overline U,\overline g)$ be customized surfaces.  A differentiable map $\varphi:U\to\overline U$ is a conformal mapping if and only if there is a differentiable function $\lambda:U\to\mathbb R$ such that, for each $p\in U$, $(d\varphi_p)^T\overline g(d\varphi_p)=\lambda^2g$, using the notation of Proposition 6.31.}\\

\noindent From this definition, it is clear that $U$ has conformal coordinates if and only if the identity map $(U,\mathfrak e)\to(U,g)$ (where $\mathfrak e$ is the usual Euclidean metric) is conformal.

We now introduce the intrinsic-geometry concepts we have covered in earlier sections:\\

\noindent\textbf{Definition.} \emph{Let $(U,g)$ be a customized surface.  Then:}

\emph{If $\alpha(t)=(u(t),v(t))$ is a curve $I\to U$, the \textbf{length} of $\alpha$ is defined to be $\int_I\|\alpha'(t)\|_g\,dt=\int_I\sqrt{E(u')^2+2Fu'v'+G(v')^2}\,dt$.}

\emph{The area of a region $R\subset U$, in the metric of $U$, is defined as $\int_R\sqrt{EG-F^2}\,du\,dv$.  More generally, $\sqrt{EG-F^2}\,du\,dv$ is the \textbf{surface metric} $d\sigma$ and for $f:R\to\mathbb R$, we set $\int_Rf\,d\sigma=\int_Rf\sqrt{EG-F^2}\,du\,dv$.}

\emph{The \textbf{Christoffel symbols} $\Gamma_{ij}^k,i,j,k\in\{1,2\}$ are the functions $U\to\mathbb R$ which solve the following linear systems:}
$$\left\{\begin{array}{c l}\Gamma_{11}^1E+\Gamma_{11}^2F=\frac 12E_u~~~~~~~\\\Gamma_{11}^1F+\Gamma_{11}^2G=F_u-\frac 12E_v\end{array}\right.$$
$$\left\{\begin{array}{c l}\Gamma_{12}^1E+\Gamma_{12}^2F=\frac 12E_v~~~~~~~\\\Gamma_{12}^1F+\Gamma_{12}^2G=\frac 12G_u~~~~~~~\end{array}\right.$$
$$\left\{\begin{array}{c l}\Gamma_{22}^1E+\Gamma_{22}^2F=F_v-\frac 12G_u\\\Gamma_{22}^1F+\Gamma_{22}^2G=\frac 12G_v~~~~~~~\end{array}\right.$$
\emph{The \textbf{Gaussian curvature} of $U$ at $p$ is defined to be}
$$K=\frac 1E\left(\Gamma_{11}^1\Gamma_{12}^2+(\Gamma_{11}^2)_v+\Gamma_{11}^2\Gamma_{22}^2-\Gamma_{12}^1\Gamma_{11}^2-(\Gamma_{12}^2)_u-\Gamma_{12}^2\Gamma_{12}^2\right).$$
\emph{If $\alpha(t)=(u(t),v(t))$ is a smooth curve $I\to U$, then a vector field $t\mapsto(a(t),b(t))$ along $\alpha$ is called \textbf{parallel} provided}
$$\begin{array}{c l}a'+au'\Gamma_{11}^1+(av'+bu')\Gamma_{12}^1+bv'\Gamma_{22}^1=0\\b'+au'\Gamma_{11}^2+(av'+bu')\Gamma_{12}^2+bv'\Gamma_{22}^2=0\end{array}$$
\emph{and $\alpha$ is called a \textbf{geodesic} if $\frac{d\alpha}{dt}$ is parallel along $\alpha$, or what is the same thing,}
$$\begin{array}{c l}u''+(u')^2\Gamma_{11}^1+2u'v'\Gamma_{12}^1+(v')^2\Gamma_{22}^1=0\\v''+(u')^2\Gamma_{11}^2+2u'v'\Gamma_{12}^2+(v')^2\Gamma_{22}^2=0\end{array}$$
\emph{holds.}\\

\noindent The formula for the area is inherited from Propsition 6.4, and it makes sense since $EG-F^2>0$ by law.  The equations for the Christoffel symbols are from Section 6.7; we may still use them to uniquely determine the Christoffel symbols even though $\mathbf x_u$ and $\mathbf x_v$ do not exist.  The formula for the Gaussian curvature is given by the Gauss equation, again from Section 6.7 (note that $L,M,N$ do not appear or are not needed in this context).  Parallel transport and geodesics are given by the differential equations (PT) and (G) of the previous section.

As before, it can be shown that these things are preserved by local isometries, the length of a curve is independent of how it is parametrized, and parallel vector fields and geodesics also satisfy Propositions 6.16-6.21.  Also, the Gauss-Bonnet Theorem (6.24) holds; this is a bit trickier to prove, but it imitates the argument of the previous section.

Furthermore, it is worth noting that Exercises 3 and 4 of Section 6.7 hold for customized surfaces.  In particular, in conformal coordinates $E=G=\lambda,F=0$, we have $K=-\frac 1{2\lambda}\Delta(\ln\lambda)$.

With this material on generic customized surfaces \-- which are really known as \emph{Riemannian 2-manifolds} \-- we may return to our study on the spherical, Euclidean and hyperbolic planes.\\

\noindent We first let $(U,g)$ be the customized surface with $U=\mathbb R^2$, and $E=G=\frac 4{(u^2+v^2+1)^2}$ and $F=0$.  In accordance with Exercise 2 below, stereographic projection is an isometry from $U$ to $S^2$, which covers $S^2-\{(0,0,1)\}$.  This customized surface is thus the stereographic projection model of the spherical plane: if $\alpha$ is a geodesic of $U$, $\alpha$ maps to a geodesic on $S^2$, which is a great circle by Exercise 3 of Section 6.9.  Hence Exercise 7 of Section 4.2 shows that $\alpha$ is either a Euclidean line through the origin or a Euclidean circle with radius $r$ and center $a$ such that $r^2-\|a\|^2=1$.

In fact, one can show that, when parametrized correctly, those lines and circles satisfy the differential equations for a geodesic (given in the definition).  We will not explicitly do this because it will be more important for us to do the analogous things for the hyperbolic plane.  See Exercise 5, however.

Taking $\lambda=\frac 4{(u^2+v^2+1)^2}$, we compute $-\frac 1{2\lambda}\Delta(\ln\lambda)=1$.  After all, $\ln\lambda=\ln 4-2\ln(u^2+v^2+1)$; differentiating with respect to $u$ entails $-\frac{4u}{u^2+v^2+1}$; differentiating again with respect to $u$ yields $-\frac{4(-u^2+v^2+1)}{(u^2+v^2+1)^2}$; and the rest can similarly be computed.  Thus the Gaussian curvature $K$ is equal to $1$ for this customized surface.  This makes sense, because it is isometric to the unit sphere.

We now set $U'=\{(u,v)\in\mathbb R^2:u^2+v^2<1\}$ and equip it with the same metric as $U$.  We let $(\overline U,\overline g)$ be the customized surface of Example 3 above: $\overline U=\mathbb R^2$ and
$$\overline E=\frac{v^2+1}{(u^2+v^2+1)^2},~~~~\overline F=-\frac{uv}{(u^2+v^2+1)^2},~~~~\overline G=\frac{u^2+1}{(u^2+v^2+1)^2}.$$
Define $\varphi:U'\to\overline U$ via $\varphi(u,v)=\left(\frac{2u}{1-u^2-v^2},\frac{2v}{1-u^2-v^2}\right)$.  We claim that $\varphi$ is an isometry of customized surfaces.  To see this, observe that if $p=(u,v)$ then
$$d\varphi_p=\begin{bmatrix}\frac{2(1+u^2-v^2)}{(1-u^2-v^2)^2}&\frac{4uv}{(1-u^2-v^2)^2}\\\frac{4uv}{(1-u^2-v^2)^2}&\frac{2(1-u^2+v^2)}{(1-u^2-v^2)^2}\end{bmatrix}.$$
And if $\overline u=\frac{2u}{1-u^2-v^2}$ and $\overline v=\frac{2v}{1-u^2-v^2}$ are the \emph{output points} of the input $p$, then the matrix $\overline g$ is given by
$$\begin{bmatrix}\frac{\overline v^2+1}{(\overline u^2+\overline v^2+1)^2}&-\frac{\overline u\,\overline v}{(\overline u^2+\overline v^2+1)^2}\\-\frac{\overline u\,\overline v}{(\overline u^2+\overline v^2+1)^2}&\frac{\overline u^2+1}{(\overline u^2+\overline v^2+1)^2}\end{bmatrix}=$$
$$\frac{(1-u^2-v^2)^2}{(1+u^2+v^2)^4}\begin{bmatrix}(1-u^2+v^2)^2+(2uv)^2&-4uv\\-4uv&(1+u^2-v^2)^2+(2uv)^2\end{bmatrix}.$$

To show that $\varphi$ is an isometry is merely a matter of  computation: by Proposition 6.31, it suffices to show that $(d\varphi_p)^T\overline g(d\varphi_p)=g=\frac 4{(u^2+v^2+1)^2}I_2$, or what is the same thing, $(d\varphi_p)\overline g(d\varphi_p)=\frac 4{(u^2+v^2+1)^2}I_2$ ($d\varphi_p$ is symmetric).  Since $AB=I_2\iff BA=I_2$ for square matrices $A,B$, one can alternatively show $(d\varphi_p)^2\overline g=\frac 4{(u^2+v^2+1)^2}I_2$.

Observe that $\varphi$ passes the stereographic projection of a point on the sphere to the gnomonic projection of the same point.  It is the composition $\mathbf y^{-1}\circ\mathbf x$ where $\mathbf x:U'\to S^2,\mathbf y:\mathbb R^2\to S^2$ are given by
$$\mathbf x(u,v)=\left(\frac{2u}{u^2+v^2+1},\frac{2v}{u^2+v^2+1},\frac{u^2+v^2-1}{u^2+v^2+1}\right)$$
$$\mathbf y(u,v)=\left(\frac u{\sqrt{u^2+v^2+1}},\frac v{\sqrt{u^2+v^2+1}},-\frac 1{\sqrt{u^2+v^2+1}}\right);$$
and $\mathbf x$ and $\mathbf y$ parametrize the southern hemisphere via stereographic projection and gnomonic projection respectively.  Since $\varphi$ is an isometry of customized surfaces, it follows that $\overline U$ has exactly the metric of the gnomonic projection.\\

\noindent\emph{Warning}: The equation $(d\varphi_p)^T\overline g(d\varphi_p)=g$ requires $\overline g$ to be the first fundamental form matrix at the target point $\varphi(p)$, not the domain point $p$.  Notice how $\overline g$ has been carefully computed above.  Inexperienced readers commonly make this mistake, especially when the domain and range of $\varphi$ are the same subset of $\mathbb R^2$.\\

\noindent We have just studied a customized surface which embodies spherical geometry.  Euclidean and hyperbolic geometries have them too.  For Euclidean geometry, you merely take $E=G=1$ and $F=0$ on $\mathbb R^2$.  Then the first fundamental form matrix is the identity matrix.  If $\varphi:\mathbb R^2\to\mathbb R^2$ is an isometry, Proposition 6.31 entails $(d\varphi_p)^T(d\varphi_p)=I_2$ for all $p$, i.e., $d\varphi_p\in O(2)$.  As shown in Section 6.4, it follows that $d\varphi_p$ is constant, and hence $\varphi$ is of the form $[A,\vec v]$ with $A\in O(2),\vec v\in\mathbb R^2$.

But what about the hyperbolic plane?  Suppose $U=\{(u,v)\in\mathbb R^2:u^2+v^2<1\}$ is the open disk, and we take
$$E=G=\frac 4{(1-u^2-v^2)^2},~~~~F=0$$
First, one can compute from the formula $K=-\frac 1{2\lambda}\Delta(\ln\lambda)$ that the Gaussian curvature of this customized surface is $-1$.  In fact, this surface is precisely the Poincar\'e disk model of the hyperbolic plane (without the ideal points).  Perhaps the easiest way to verify this is to relate it to the Poincar\'e half-plane model by an isometry.

For the half-plane model, we take $\overline U=\{(u,v)\in\mathbb R^2:v>0\}$, and $\overline E=\overline G=\frac 1{v^2},\overline F=0$.  Then to begin with, using Exercise 3 of Section 6.7 to get the Christoffel symbols, the differential equations for a geodesic $\alpha(t)=(u(t),v(t))$ are
$$\begin{array}{c l}u''-\frac{2u'v'}v=0\\v''+\frac{(u')^2}v-\frac{(v')^2}v=0\end{array}$$
For fixed $a,r\in\mathbb R,r>0$, one can verify that each of the following curves is a geodesic by plugging into the equations:
$$\alpha(t)=(a,e^t)$$
$$\alpha(t)=(a+r\tanh t,r\operatorname{sech}t)$$
The former is the vertical line $x=a$ and the latter is the semicircle arc of radius $r$ centered at $(a,0)$.  Hence, all lines of the half-plane model \---- vertical lines and semicircle arcs centered on the $x$-axis \---- are geodesics on the customized surface, when parametrized in a particular way.  Observe that every vector at every point in $\overline U$ is tangent to one of these curves; hence, since geodesics are determined by their starting speed and direction, it follows that conversely, every geodesic is a line of the half-plane model.
\begin{center}\includegraphics[scale=.3]{PoincareHalfPlane.png}\end{center}
We recall (Section 4.3, Exercise 8) that the isometry group of the half-plane model is generated by the maps $(u,v)\mapsto(u+r,v)$ for $r\in\mathbb R$, $(u,v)\mapsto(su,sv)$ for $s\in\mathbb R_{>0}$, and the maps $(u,v)\mapsto(-u,v)$ and $(u,v)\mapsto\left(\frac{u}{u^2+v^2},\frac{v}{u^2+v^2}\right)$.  We claim that these maps, and hence all the isometries of the half-plane model, are also isometries in the sense of this chapter.

Using the criterion of Proposition 6.31, this is clear for the maps $(u,v)\mapsto(u+r,v)$, $(u,v)\mapsto(su,sv)$ and $(u,v)\mapsto(-u,v)$.  [Not much work is even needed for $(u,v)\mapsto(u+r,v)$ if you use Exercise 4.]  For the map $\psi(u,v)=\left(\frac u{u^2+v^2},\frac v{u^2+v^2}\right)$, things are tougher to compute, but we will still reach an affirmative conclusion.  Note that its differential is
$$d\psi_{(u,v)}=\begin{bmatrix}\frac{-u^2+v^2}{(u^2+v^2)^2}&-\frac{2uv}{(u^2+v^2)^2}\\-\frac{2uv}{(u^2+v^2)^2}&\frac{u^2-v^2}{(u^2+v^2)^2}\end{bmatrix};$$
and the remaining calculation is straightforward (possibly using Exercise 1).

Thus, the customized surface $(\overline U,\overline g)$ shares all the same geodesics and isometries as the half-plane model we studied in Chapter 4.  According to Exercise 6 of Section 4.3, the conversion map from the disk $U$ to the half-plane $\overline U$ is given by $w\mapsto i\frac{1+w}{1-w}$ for $w\in\mathbb C,|w|<1$; in real coordinates this is
$$\varphi:U\to\overline U,~~~~\varphi(u,v)=\left(-\frac{2v}{u^2-2u+1+v^2},\frac{1-u^2-v^2}{u^2-2u+1+v^2}\right).$$
We leave it to the reader to compute the differential $d\varphi_p$ and use it to verify that $\varphi$ is indeed an isometry of customized surfaces.  Hence $(U,g)$, as a customized surface, shares the credentials of the Poincar\'e disk model, the same way $(\overline U,\overline g)$ shares those of the half-plane model.\\

\noindent We now have our previously studied geometries as customized surfaces:
\begin{itemize}
\item Sphere (stereographic projection): $U=\mathbb R^2$, $E=G=\frac 4{(u^2+v^2+1)^2},F=0$

\item Half-sphere / elliptic plane (gnomonic projection): $U=\mathbb R^2$, $E=\frac{v^2+1}{(u^2+v^2+1)^2},F=-\frac{uv}{(u^2+v^2+1)^2},G=\frac{u^2+1}{(u^2+v^2+1)^2}$

\item Euclidean plane: $U=\mathbb R^2$, $E=G=1,F=0$

\item Hyperbolic plane (Poincar\'e disk model): $U=\{(u,v)\in\mathbb R^2:u^2+v^2<1\}$, $E=G=\frac 4{(1-u^2-v^2)^2},F=0$

\item Hyperbolic plane (Poincar\'e half-plane model): $U=\{(u,v)\in\mathbb R^2:v>0\}$, $E=G=\frac 1{v^2},F=0$
\end{itemize}
Of course, the Beltrami-Klein model of the hyperbolic plane, and the Lambert azimuthal equal-area projection model of the sphere, also have (nonconformal) customized-surface credentials, but we will not compute them. % Honest question, is the Beltrami-Klein model's FFF worth computing?  My previous attempts were beyond my comfort zone

There is also a simultaneous generalization of both regular surfaces and customized surfaces that we will not delve into: one can take a regular surface, but give it each tangent plane a custom symmetric positive definite bilinear form, which varies smoothly with the point.\footnote{More generally, one can take a regular manifold, and equip each tangent space with a symmetric positive definite bilinear form, which varies smoothly from point to point.  Doing so yields a \textbf{Riemannian manifold}.  See Exercise 11 for generalizations to arbitrary manifolds.}  As before, the bilinear forms are a suitable metric, allowing computation of lengths, angles and areas.  In this case, \emph{both} the shape of the surface \emph{and} the metric on it are custom, even though for either regular surfaces or customized surfaces, only one of those is.  Examples of this are the hyperboloid and hemisphere models of the hyperbolic plane (Section 4.9); it is clear why they are not just regular surfaces.

In the remainder of this chapter, we will cover a few results that we were not able to cover before.  First, the spherical, Euclidean and hyperbolic geometries have respective Gaussian curvatures $1,0,-1$.  (By Theorema Egregium, it makes no difference which model we use in each case.)  To each case we apply Corollary 6.29 of the Gauss-Bonnet Theorem; we get the following results:\\

\noindent\textbf{Proposition 6.33.}

(i) \emph{In spherical geometry, the area of a triangle is equal to the sum of the angles minus $\pi$.}

(ii) \emph{In Euclidean geometry, the sum of the angles of a triangle is equal to $\pi$.}

(iii) \emph{In hyperbolic geometry, the area of a triangle is equal to $\pi$ minus the sum of the angles.}\\

\noindent This supports the definitions of area in Exercise 3 of Section 4.5 and Exercise 8 of Section 5.2.  It incidentally aids Exercise 1 of Section 5.3, as it yields an easier proof of part (c) than the one given in the hint.

Since we now know the areas of triangles (and hence polygons), it is only natural to study the circumference and area of a circle.  You may recall that a Euclidean circle of radius $r$ has circumference $2\pi r$ and area $\pi r^2$.  To see why this is really so, parametrize such a circle via
$$\alpha(t)=(r\cos t,r\sin t),~~~~0\leqslant t\leqslant 2\pi$$
in the Euclidean plane.  We have made the choice of range of $t$ so that the circle is traversed exactly once.  With that, the circumference of the circle \---- the arc length of $\alpha$ \---- is $\int_0^{2\pi}\|\alpha'(t)\|\,dt=\int_0^{2\pi}\|(-r\sin t,r\cos t)\|\,dt=\int_0^{2\pi}r\,dt=2\pi r$.
There are also many ways to find the area.  One way is to consider the map
$$\rho:[0,r]\times[0,2\pi]\to\mathbb R^2\text{ given by }\rho(a,\theta)=(a\cos\theta,a\sin\theta).$$
Then the range of $\rho$ is the disk $D$ of radius $r$, and $\rho$ does not have overlaps.  Hence, one can compute the area of $D$ by pulling back an integral to $[0,r]\times[0,2\pi]$ using the Jacobian determinant; in other words, $\operatorname{Area}(D)=\int_{[0,r]\times[0,2\pi]}\det(d\rho_p)\,da\,d\theta$.  Now, basic computation shows that for $p=(\theta,a)$,
$$d\rho_p=\begin{bmatrix}\cos\theta&-a\sin\theta\\\sin\theta&a\cos\theta\end{bmatrix},\text{ and hence }\det(d\rho_p)=a.$$
Therefore said integral is equal to
$$\int_{[0,r]\times[0,2\pi]}a\,da\,d\theta=\int_0^{2\pi}\int_0^r a\,da\,d\theta=\int_0^{2\pi}\left(\int_0^ra\,da\right)\,d\theta=\int_0^{2\pi}\frac{r^2}2\,d\theta=\pi r^2.$$

Our ambition is to do the same thing for circles in the spherical and hyperbolic planes.  It is not hard, given the material we have already covered.  We will do the hyperbolic case here; the spherical case will be left to the reader.

Let $C$ be circle of radius $r$ in the hyperbolic plane; we may assume we are in the Poincar\'e disk model and $C$ is centered at the origin.  Here we must be careful, because $r$ is the \emph{hyperbolic} radius of $C$, not the Euclidean radius.  We recall (Exercise 1(c) of Section 4.5) that if $z\in\mathbb C,|z|<1$ is regarded as a point of the Poincar\'e disk model, then its hyperbolic distance from the origin is $2\tanh^{-1}|z|$.  Hence $C$ is given by the equation $2\tanh^{-1}|z|=r$, or what is the same thing, $|z|=\tanh(r/2)$.  Hence % I added "then."  "And" would make the sentence have the same grammar as "I know that if I drink alcohol today, and I will feel unhappy tomorrow." -- clearly not right.
\begin{center}
\textbf{If $C$ is a circle of radius $r$, centered at the origin of the Poincar\'e disk, its Euclidean radius is $\tanh(r/2)$.}
\end{center}
This makes sense, because $r$ can be an arbitrary positive real number, but the Euclidean radius must be $<1$ so the circle is contained in the disk.  We henceforth parametrize $C$ via $\alpha(t)=(\tanh(r/2)\cos t,\tanh(r/2)\sin t)$; and we parametrize its interior $D$ by the coordinates
$$\rho(a,\theta)=(a\cos\theta,a\sin\theta),~~~~0\leqslant a\leqslant\tanh(r/2),0\leqslant\theta\leqslant 2\pi.$$
In a customized surface, the arc length of a curve $\alpha(t)=(u(t),v(t))$ is equal to $\int_I\sqrt{E(u')^2+2Fu'v'+G(v')^2}\,dt$.  For the Poincar\'e disk model of the hyperbolic plane, $E=G=\frac 4{(1-u^2-v^2)^2}$ and $F=0$; in this case the arc length is $\int_I\frac 2{1-u^2-v^2}\sqrt{(u')^2+(v')^2}\,dt=\int_I\frac 2{1-u^2-v^2}\|\alpha'(t)\|\,dt$.  Hence we compute the circumference of the above circle as
$$\int_0^{2\pi}\frac 2{1-u^2-v^2}\|\alpha'(t)\|\,dt,\text{ with }\alpha(t)=(\tanh(r/2)\cos t,\tanh(r/2)\sin t).$$
Direct computation shows $\alpha'(t)=(-\tanh(r/2)\sin t,\tanh(r/2)\cos t)$, so that $\|\alpha'(t)\|=\tanh(r/2)$; and $\frac 2{1-u^2-v^2}=\frac 2{1-\tanh^2(r/2)}=\frac 2{\operatorname{sech}^2(r/2)}=2\cosh^2(r/2)$.  Hence
$$\int_0^{2\pi}\frac 2{1-u^2-v^2}\|\alpha'(t)\|\,dt=\int_0^{2\pi}2\cosh^2(r/2)\tanh(r/2)\,dt.$$
Yet $2\cosh^2(r/2)\tanh(r/2)=2\cosh^2(r/2)\frac{\sinh(r/2)}{\cosh(r/2)}=2\cosh(r/2)\sinh(r/2)=\sinh r$, and hence $\int_0^{2\pi}2\cosh^2(r/2)\tanh(r/2)\,dt=\int_0^{2\pi}\sinh r\,dt$
$$=2\pi\sinh r.$$ % Okie, even though this result is prominent in (6.34)

It is really striking how, in hyperbolic geometry, the circumference of a circle of radius $r$ is proportional to $\sinh r$, hence grows (asymptotically) exponentially with $r$.

We may similarly find the area; since $\sqrt{EG-F^2}=\frac 4{(1-u^2-v^2)^2}$ in this case, this may be computed as $\int_D\frac 4{(1-u^2-v^2)^2}\,du\,dv$.  Taking the pullback to the domain of $\rho$, we get
$$\int_D\frac 4{(1-u^2-v^2)^2}\,du\,dv=\int_{[0,\tanh(r/2)]\times[0,2\pi]}\frac 4{(1-u^2-v^2)^2}\det(d\rho_p)\,da\,d\theta.$$
As before $\det(d\rho_p)=a$, and since $(u,v)=(a\cos\theta,a\sin\theta)$ we have $\frac 4{(1-u^2-v^2)^2}=\frac 4{(1-a^2)^2}$.  Hence this evaluates to
$$\int_{[0,\tanh(r/2)]\times[0,2\pi]}\frac{4a}{(1-a^2)^2}\,da\,d\theta=\int_0^{2\pi}\int_0^{\tanh(r/2)}\frac{4a}{(1-a^2)^2}\,da\,d\theta$$
$$=\int_0^{2\pi}\left(\int_0^{\tanh(r/2)}\frac{4a}{(1-a^2)^2}\,da\right)\,d\theta=\int_0^{2\pi}\left(\frac 2{1-a^2}\big|_0^{\tanh(r/2)}\right)\,d\theta$$
$$=\int_0^{2\pi}\left(\frac 2{1-\tanh^2(r/2)}-2\right)\,d\theta=\int_0^{2\pi}2\left(\frac 1{\operatorname{sech}^2(r/2)}-1\right)\,d\theta$$
$$=\int_0^{2\pi}2\left(\cosh^2(r/2)-1\right)\,d\theta=\int_0^{2\pi}2\sinh^2(r/2)\,d\theta$$
$$=4\pi\sinh^2(r/2).$$
For spherical geometry, just imitate the computations, with the stereographic projection model in place of the Poincar\'e disk model, $\frac 4{(u^2+v^2+1)^2}$ in place of $\frac 4{(1-u^2-v^2)^2}$, and trigonometric functions in place of hyperbolic functions.  Note in this case that we must have $0<r<\pi$.  The circumference works out to be $2\pi\sin r$, and the area is $4\pi\sin^2(r/2)$.\\

\noindent\textbf{Proposition 6.34.}

(i) \emph{In spherical geometry, a circle of radius $r$ has circumference $2\pi\sin r$ and area $4\pi\sin^2(r/2)$.}

(ii) \emph{In Euclidean geometry, a circle of radius $r$ has circumference $2\pi r$ and area $\pi r^2$.}

(iii) \emph{In hyperbolic geometry, a circle of radius $r$ has circumference $2\pi\sinh r$ and area $4\pi\sinh^2(r/2)$.}\\

\noindent The areas of various figures, such as lunes on the sphere, can likewise be computed.  There are vast geometrical areas to explore.  From the Ancient Greeks to the Middle Ages, from Euclid and Descartes to Euler and Gauss and Riemann, geometry has been around for centuries, and will continue to have interesting properties today and in the future.

\subsection*{Exercises 6.10. (Customized Surfaces)} % Mention that all the intrinsic geometry works if we declare any positive definite matrix we please to be the FFF.
% Then use it to define customized surfaces.  Define dot product, length, angle, area, isometries, conformal mappings, Gaussian curvature, parallel transport, geodesics
% through what can be purely derived from the FFF.  Then bring back the spherical, Euclidean and hyperbolic planes from previous chapters, but in the calculus setting!
% Take the opportunity to do several things that couldn't be done before; e.g., circumference and area of a circle.
% POTENTIAL EXERCISES: In Poincar\'e half-plane model of hyperbolic plane, area of triangle = \pi - sum of angles (there is an easier proof that avoids the general Gauss-Bonnet Theorem); find FFF for the other models of the hyperbolic plane; growth in a uniform tiling of number of faces a circle passes through w.r.t. to radius of circle; and, of course, the last exercise will generalize to higher dimensions.
\begin{enumerate}
\item Suppose $(U,g)$ and $(\overline U,\overline g)$ are customized surfaces, both with conformal coordinates, say $E=G=\lambda,F=0$ for $g$, and $\overline E=\overline G=\mu,\overline F=0$ for $\overline g$.  Then a differentiable map $\varphi:U\to\overline U$ is an isometry if and only if $(d\varphi_p)^T(d\varphi_p)=\frac{\lambda}{\mu}I_2$ for all $p\in U$.  [Proposition 6.31.]

Note that $\varphi$ is conformal by the criterion (vi) of Exercise 4 of Section 4.1.  It should be clear (from the get-go) that a differentiable map $U\to\overline U$ is conformal in the Euclidean sense if and only if it is a conformal mapping of the customized surfaces.

\item Suppose the sphere is parametrized by stereographic projection:
$$\mathbf x(u,v)=\left(\frac{2u}{u^2+v^2+1},\frac{2v}{u^2+v^2+1},\frac{u^2+v^2-1}{u^2+v^2+1}\right).$$
Show that if the sphere has the ordinary metric inherited from $\mathbb R^3$, then the first fundamental form coefficients of the parametrization are $E=G=\frac 4{(u^2+v^2+1)^2}$ and $F=0$.

\item Let $\alpha:I\to U$ be a smooth curve in a customized surface.  Come up with a rigorous definition of the algebraic value of the covariant derivative of a unit vector field, as well as of $\alpha$'s geodesic curvature (see the definition in Proposition 6.22).

\item Let $(U,g)$ be a customized surface with conformal coordinates $E=G=\lambda$, $F=0$, and let $T$ be an element of $\operatorname{Isom}(\mathbb R^2)$ which fixes $U$.  Show that $T$ is an isometry of $U$ if and only if $\lambda(p)=\lambda(T(p))$ for all $p\in U$.

\item Let $(\mathbb R^2,g)$ be the stereographic projection model of the sphere, $E=G=\frac 4{(u^2+v^2+1)^2}$ and $F=0$.  Fix $0\leqslant\theta<2\pi$.

(a) Show that $\alpha(t)=\left(\frac{\cos t}{1-\sin\theta\sin t},\frac{\cos\theta\sin t}{1-\sin\theta\sin t}\right)$ is a geodesic.  [If $\lambda=\frac 4{(u^2+v^2+1)^2}$, then $\frac{\lambda_u}{\lambda}=(\ln\lambda)_u=\frac{-4u}{u^2+v^2+1}$ and similarly $\frac{\lambda_v}{\lambda}=\frac{-4v}{u^2+v^2+1}$.  Use Exercise 3 of Section 6.7 to get the Christoffel symbols, then compute $\alpha$ and $\alpha'$ and verify the differential equations.]

(b) If $\theta\ne\pi/2,3\pi/2$, then $\alpha$ is the Euclidean circle centered at $(0,\tan\theta)$ with radius $\sec\theta$.  Conclude that it has radius $r$ and center $a$ such that $r^2-\|a\|^2=1$.

(c) Show that for $|t|<\pi$, $\beta(t)=(\tan(t/2),0)$ is a geodesic, which is a Euclidean line through the origin.

\item Verify that, if isometric maps are defined by the equation in Proposition 6.31, then the composition $(U_0,g_0)\overset{\varphi}{\to}(U_1,g_1)\overset{\psi}{\to}(U_2,g_2)$ of isometric maps is isometric, and the identity map $(U,g)\to(U,g)$ is an isometry.  Moreover, the inverse of a (bijective) isometry is also an isometry.  Do the same for conformal mappings.

\item Without using Proposition 6.33, prove that:

(a) If a triangle in the hyperbolic plane has exactly one ideal vertex, and angles $\alpha,\beta$ at the regular vertices, then the area of the triangle is $\pi-\alpha-\beta$.  [Assume we are in the Poincar\'e half-plane model, the ideal vertex is $\infty$, and the other two vertices are $(-\cos\alpha,\sin\alpha)$ and $(\cos\beta,\sin\beta)$ (on the origin-centered unit semicircle).  Since $\sqrt{EG-F^2}=\frac 1{v^2}$, the area of the triangle is $\int_R\frac 1{v^2}\,du\,dv$ where $R$ is the interior; now compute this as $\int_{-\cos\alpha}^{\cos\beta}\int_{\sqrt{1-u^2}}^\infty\frac 1{v^2}\,dv\,du$.]

(b) If a regular triangle in the hyperbolic plane has angles $\alpha,\beta,\gamma$, then its area is $\pi-\alpha-\beta-\gamma$.  [Extend one side to meet the rim at infinity, and connect this ideal point to the vertex opposite the side.  This yields two triangles with exactly one ideal vertex; apply part (a) to each one.]

\item Let $(U,g)$ be the customized surface with $U=\{(u,v)\in\mathbb R^2:|v|<\pi/2\}$, and the conformal coordinates $E=G=\sec^2v,F=0$.

(a) Show that the Gaussian curvature is $-1$ everywhere.

(b) Show that $(U,g)$ is the band model of the hyperbolic plane (Section 4.9, Exercise 4).  [Recall that $(u,v)\mapsto(-e^u\sin v,e^u\cos v)$ is a conversion map from the band model to the Poincar\'e half-plane model; verify that this is an isometry in this chapter's sense.]

\item If $(U,g)$ is the customized surface with $U=\mathbb R^2$, $E=G=\operatorname{sech}^2v,F=0$, show that $(U,g)$ is locally isometric to the sphere, under Mercator's projection (covered in Section 6.4).

\item Consider a face-transitive tiling in either the spherical, Euclidean or hyperbolic plane; i.e., the isometry group of the tiling acts transitively on the faces.  Then fix a point $p$ on the plane.  For each positive real $r$, one can consider the number of faces whose distance to $p$ is roughly $r$.  Though this is not exactly rigorous, one can still study its (asymptotic) growth with respect to $r$.  Explain why:

(a) In the Euclidean plane, the number of faces at a distance $r$ from $p$ grows roughly proportionally with respect to $r$.

(b) In the hyperbolic plane, the number of faces at a distance $r$ from $p$ grows roughly exponentially with respect to $r$.

[Use Proposition 6.34.]

\item This exercise, as before, generalizes the material to arbitrary dimensions.  If $m$ is a positive integer, we define a \textbf{customized $m$-manifold} (or a piece of a \textbf{Riemannian $m$-manifold}) to be a pair $(U,g)$ where $U\subset\mathbb R^m$ is an open set, and $g=\begin{bmatrix}g_{ij}\end{bmatrix}_{1\leqslant i,j\leqslant m}$ is a matrix consisting of differentiable functions $g_{ij}:U\to\mathbb R$ such that $g$ is symmetric and positive definite throughout $U$.

For $p\in U$ and $\vec v,\vec w$ vectors at $p$, define $\left<\vec v,\vec w\right>=\vec v\cdot g\vec w$.  Then this is a symmetric positive definite bilinear form.  As in the $2$-dimensional case, it is used to define the length of curves, and angles between vectors.

(a) If $V\subset\mathbb R^{m'}$ is open and $\alpha:V\to U$ is smooth, then the $m'$-dimensional volume of $\alpha(V)$ is defined as
$$\int_V\sqrt{\det((d\alpha)^Tg(d\alpha))}\,dv_1\dots dv_{m'}.$$
In Exercise 12(d) of Section 6.3, we have shown this for regular manifolds embedded in Euclidean space.  Here, it is really a bona fide \emph{definition} of the volume of the $m'$-dimensional region, because this volume would otherwise have no meaning.  One still needs to verify that this volume is well-defined, and independent of how the region is parametrized.

To show this, suppose $\overline V\subset\mathbb R^{m'}$ is an open set and $\sigma:\overline V\to V$ is a diffeomorphism.  Show that replacing $\alpha$ with $\overline\alpha=\alpha\circ\sigma:\overline V\to U$ leaves the above integral invariant, and explain why that suffices.  [Verify that $\det((d\overline\alpha)^Tg(d\overline\alpha))=\det(d\sigma)^2\det((d\alpha)^Tg(d\alpha))$ by using the Chain Rule.  Then recall how an integral can be pulled back using the Jacobian determinant.]

(b) Taking $m'=m$ in part (a), the volume of an $m$-dimensional region $V\subset U$ is $\int_V\sqrt{\det g}\,dv_1\dots dv_m$.  Taking $m'=1$ in part (a), the length of a curve $\alpha:I\to U$ is equal to $\int_I\sqrt{\alpha'(t)\cdot g\alpha'(t)}\,dt=\int_I\sqrt{\left<\alpha'(t),\alpha'(t)\right>}\,dt$.  Of course, we rewrite $\sqrt{\left<\alpha'(t),\alpha'(t)\right>}$ as $\|\alpha'(t)\|_g$, the magnitude of the vector in the customized metric.

(c) If $\varphi:U\to\overline U$ is a smooth map of customized manifolds $(U,g)$ and $(\overline U,\overline g)$ (with possibly different dimensions), then $\varphi$ is isometric (i.e., its differential preserves the inner products) if and only if at each $p\in U$, $(d\varphi_p)^T\overline g(d\varphi_p)=g$.  Also, $\varphi$ is conformal if and only if there is a smooth map $\lambda:U\to\mathbb R$ such that $(d\varphi_p)^T\overline g(d\varphi_p)=\lambda^2g$.

(d) Use Exercise 9(a) of Section 6.7 to define the Christoffel symbols $\Gamma_{ij}^k$ with $1\leqslant i,j,k\leqslant m$.  In accordance with Exercise 12 of the previous section, if $\alpha(t)=(u_1(t),\dots,u_m(t))$ is a curve in $U$, and $\vec v(t)=(a_1(t),\dots,a_m(t))$ is a vector field along $\alpha$, $\vec v$ is defined to be \textbf{parallel} along $\alpha$ if
$$a_k'+\sum_{i=1}^m\sum_{j=1}^ma_iu_j'\Gamma_{ij}^k=0\text{ for all }k,$$
and $\alpha$ is defined to be a \textbf{geodesic} if its velocity is parallel, i.e.,
$$u_k''+\sum_{i=1}^m\sum_{j=1}^mu_i'u_j'\Gamma_{ij}^k=0\text{ for all }k.$$
Verify that these are invariant under isometries.

(e) Show that: (i) $U=\mathbb R^m,g=I_m$ gives Euclidean $m$-space; (ii) $U=\mathbb R^m,g=\frac 4{(u_1^2+\dots+u_m^2+1)^2}I_m$ gives the stereographic projection model of the $m$-sphere; (iii) $U=B_1(0)=\{(u_1,\dots,u_m)\in\mathbb R^m:u_1^2+\dots+u_m^2<1\}$ and $g=\frac 4{(1-u_1^2-\dots-u_m^2)^2}I_m$ gives the Poincar\'e ball model of hyperbolic space; and (iv) $U=\{(u_1,\dots,u_m)\in\mathbb R^m:u_m>0\}$ and $g=\frac 1{u_m^2}I_m$ gives the Poincar\'e half-space model of hyperbolic space.

[In each case, think of isometries and geodesics.]

The remaining parts of this exercise use the concepts of vector fields and the Lie bracket, from Exercise 4 of Section 6.8.  Let $\mathcal V(U)$ be the space of all vector fields on $U$; they are smooth functions $U\to\mathbb R^m$.  An \textbf{affine connection} is a map $\nabla:\mathcal V(U)\times\mathcal V(U)\to\mathcal V(U)$ sending $(X,Y)\mapsto\nabla_XY$, such that for $X,X_1,X_2,Y,Y_1,Y_2\in\mathcal V(U),f\in\mathcal C^\infty(U)$, we have:
$$\nabla_{X_1+X_2}Y=\nabla_{X_1}Y+\nabla_{X_2}Y$$
$$\nabla_X(Y_1+Y_2)=\nabla_XY_1+\nabla_XY_2$$
$$\nabla_{fX}Y=f\nabla_XY,~~~~~~\nabla_X(fY)=f\nabla_XY+X(f)Y.$$
Observe that $\nabla_XY$ is bilinear over $\mathbb R$, because for $a\in\mathbb R$, $X(a)=0$ with $a$ regarded as a constant function, and hence $\nabla_X(aY)=a\nabla_XY$, and the rest is clear.

If $(U,g)$ is a customized surface, $\nabla_XY$ is said to be a \textbf{Levi-Civita connection} provided that for $X,Y,Z\in\mathcal V(U)$,
\begin{center}
$\nabla_XY-\nabla_YX=[X,Y]$ (the Lie bracket is the commutator of the connection);

$X(\left<Y,Z\right>)=\left<\nabla_XY,Z\right>+\left<Y,\nabla_XZ\right>$ (similar to the dot product rule for differentiation).
\end{center}
(f) Show that there is a unique Levi-Civita connection on any customized surface.  [Verify that a Levi-Civita connection necessarily satisfies the equation
$$\left<\nabla_XY,Z\right>=\frac 12\left(X\left<Y,Z\right>+Y\left<Z,X\right>-Z\left<X,Y\right>+\left<[X,Y],Z\right>+\left<[Z,X],Y\right>-\left<[Y,Z],X\right>\right),$$
and use this to show uniqueness.  As for existence, show that for every $X,Y\in\mathcal V(U)$, there is a vector field whose inner product with any $Z$ is the above expression, and the function which returns this vector field is indeed a Levi-Civita connection.]

(g) Let $E_i$ be the constant vector field $\vec e_i$; note that for $f\in\mathcal C^\infty(U)$, $E_i(f)=\frac{\partial f}{\partial u_i}$.  Also, every vector field is uniquely expressible as $\sum_{i=1}^m a_iE_i$ with $a_i\in\mathcal C^\infty(U)$; the $a_i$ are then the coordinates.

Show that $\nabla_{E_i}E_j=\sum_{\ell=1}^m\Gamma_{ij}^\ell E_\ell$.  [Since the $E_i$'s form a basis, it suffices to show that $\left<\nabla_{E_i}E_j,E_k\right>=\left<\sum_{\ell=1}^m\Gamma_{ij}^\ell E_\ell,E_k\right>=\sum_{\ell=1}^m\Gamma_{ij}^\ell g_{k\ell}$.  For that, use part (f) and Exercise 9(a) of Section 6.7.]

(h) If $X=\sum_{i=1}^m a_iE_i$ and $Y=\sum_{j=1}^m b_jE_j$, then
$$\nabla_XY=\sum_{k=1}^m\left(\sum_{i,j=1}^ma_ib_j\Gamma_{ij}^k+\sum_{i=1}^ma_i\frac{\partial b_k}{\partial u_i}\right)E_k.$$
Note that each index that is summed from $1$ to $m$ occurs in multiple factors of the summand.  This is natural in differential geometry, as summations come from dot products, the Chain Rule and matrix multiplications, and in any of these cases, the index simultaneously counts for two factors.  Thus some mathematicians use \textbf{Einstein's summation convention}, where the symbol ``$\sum$'' is omitted and indices which occur in multiple factors are intended to be summed over; e.g., $a_ib_{ij}$ means $\sum_{i=1}^ma_ib_{ij}$ ($j$ is not summed over because it only appears once).

(i) Show that $\nabla_XY$ takes the covariant derivative of $Y$ with respect to $X$.

(j) Let $N$ be a (regular) submanifold of $U$.  Show that if $X=0$ on $N$ then $\nabla_XY=0$ on $N$; and if $X$ is tangent to $N$ and $Y=0$ on $N$ then $\nabla_XY=0$ on $N$.  [This holds for any affine connection, not just the Levi-Civita one.]

(k) Assume vector fields $N$ can be extended to $U$ (this fact can be confirmed using bump functions, but we do not do that here).  Use part (i) to show that, if $X$ is only defined on $N$ but $Y$ is defined throughout $U$, then $\nabla_XY$ can at least be defined on $N$.  [Extend $X$ to a vector field on $U$; part (i) shows that on $N$, $\nabla_XY$ doesn't depend on the particular way it was extended.]  Also, if $X$ is tangent to $N$ and $Y$ is only defined on $N$, $\nabla_XY$ can be defined on $N$.

(l) Conclude that if $\alpha:I\to U$ is a curve and $X$ is a vector field along $\alpha$, $\nabla_{\alpha'}X$ is defined along $\alpha$.  Show that $X$ is parallel along $\alpha$ if and only if $\nabla_{\alpha'}X=0$, and $\alpha$ is a geodesic if and only if $\nabla_{\alpha'}\alpha'=0$.  [This is the actual definition of these terms in advanced differential geometry.]
\end{enumerate}








































\cleardoublepage
\addtocontents{toc}{\vspace{\normalbaselineskip}}
\addcontentsline{toc}{section}{\textbf{Bibliography}}
\begin{center}
\Large{Bibliography}
\end{center}
\quad
\small{
\begin{enumerate}[label={[\arabic*]}]
\item ``Groups'', ``Topics in Group Theory.'' \emph{Abstract Algebra: an Introduction}, by Thomas W. Hungerford, 2nd ed., Brooks/Cole, Thomson Learning, 2001.

\item ``Burnside's Lemma.'' \emph{Wikipedia}, Wikimedia Foundation, 24 June 2003, 01:27, en.wikipedia.org/wiki/Burnside's\_lemma.

\item ``Euclidean Geometry.'' \emph{Wikipedia}, Wikimedia Foundation, 27 Aug. 2001, 22:56, en.wikipedia.org/wiki/Euclidean\_geometry.

\item Euclid, \emph{Elements}, 300 B.C.

\item ``Constructible number.'' \emph{Wikipedia}, Wikimedia Foundation, 28 May 2005, 17:09, en.wikipedia.org/wiki/Constructible\_number.

\item Weisstein, Eric W.~``Cube Duplication.''~From \emph{MathWorld}\----A Wolfram Web Resource.

\item Weisstein, Eric W.~``Glome.''~From \emph{MathWorld}\----A Wolfram Web Resource.

\item ``Pascal's theorem.'' \emph{Wikipedia}, Wikimedia Foundation, 3 June 2004, 00:47, en.wikipedia.org/wiki/Pascal\%27's\_theorem.

\item ``Non-Euclidean geometry.'' \emph{Wikipedia}, Wikimedia Foundation, 28 October 2003, 10:55, en.wikipedia.org/wiki/Non-Euclidean\_geometry.

\item ``HyperRogue.'' Retrieved from: https://zenorogue.itch.io/hyperrogue

\item A.~Redhunt. ``Tessellations of the Hyperbolic Plane and M.C. Escher.''  \emph{The Geometric Viewpoint}, Colby.

\item Chs. 1-4. \emph{Differential Geometry of Curves and Surfaces}, by Manfredo Perdigao do. Carmo, 2nd ed., Pearson Education Taiwan Ltd., 2009.

\item Rotskoff, G. ``The Gauss-Bonnet Theorem.'' Retrieved from:\\http://www.math.uchicago.edu/~may/VIGRE/VIGRE2010/REUPapers/Rotskoff.pdf
\end{enumerate}
}

\end{document}